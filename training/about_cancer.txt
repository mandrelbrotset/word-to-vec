Cancer - Wikipedia

Cancer is a group of diseases involving abnormal cell growth with the potential to invade or spread to other parts of the body.[2][8] These contrast with benign tumors, which do not spread to other parts of the body.[8] Possible signs and symptoms include a lump, abnormal bleeding, prolonged cough, unexplained weight loss and a change in bowel movements.[1] While these symptoms may indicate cancer, they may have other causes.[1] Over 100 types of cancers affect humans.[8]
Tobacco use is the cause of about 22% of cancer deaths.[2] Another 10% are due to obesity, poor diet, lack of physical activity or excessive drinking of alcohol.[2][9][10] Other factors include certain infections, exposure to ionizing radiation and environmental pollutants.[3] In the developing world, 15% of cancers are due to infections such as Helicobacter pylori, hepatitis B, hepatitis C, human papillomavirus infection, Epstein–Barr virus and human immunodeficiency virus (HIV).[2] These factors act, at least partly, by changing the genes of a cell.[11] Typically, many genetic changes are required before cancer develops.[11] Approximately 5–10% of cancers are due to inherited genetic defects from a person's parents.[12] Cancer can be detected by certain signs and symptoms or screening tests.[2] It is then typically further investigated by medical imaging and confirmed by biopsy.[13]
Many cancers can be prevented by not smoking, maintaining a healthy weight, not drinking too much alcohol, eating plenty of vegetables, fruits and whole grains, vaccination against certain infectious diseases, not eating too much processed and red meat and avoiding too much sunlight exposure.[14][15] Early detection through screening is useful for cervical and colorectal cancer.[16] The benefits of screening in breast cancer are controversial.[16][17] Cancer is often treated with some combination of radiation therapy, surgery, chemotherapy and targeted therapy.[2][4] Pain and symptom management are an important part of care.[2] Palliative care is particularly important in people with advanced disease.[2] The chance of survival depends on the type of cancer and extent of disease at the start of treatment.[11] In children under 15 at diagnosis, the five-year survival rate in the developed world is on average 80%.[18] For cancer in the United States, the average five-year survival rate is 66%.[5]
In 2015, about 90.5 million people had cancer.[6] About 14.1 million new cases occur a year (not including skin cancer other than melanoma).[11] It caused about 8.8 million deaths (15.7% of deaths).[7] The most common types of cancer in males are lung cancer, prostate cancer, colorectal cancer and stomach cancer.[19] In females, the most common types are breast cancer, colorectal cancer, lung cancer and cervical cancer.[11] If skin cancer other than melanoma were included in total new cancer cases each year, it would account for around 40% of cases.[20][21] In children, acute lymphoblastic leukemia and brain tumors are most common, except in Africa where non-Hodgkin lymphoma occurs more often.[18] In 2012, about 165,000 children under 15 years of age were diagnosed with cancer.[19] The risk of cancer increases significantly with age, and many cancers occur more commonly in developed countries.[11] Rates are increasing as more people live to an old age and as lifestyle changes occur in the developing world.[22] The financial costs of cancer were estimated at $1.16 trillion USD per year as of  2010[update].[23]
Cancers are a large family of diseases that involve abnormal cell growth with the potential to invade or spread to other parts of the body.[2][8] They form a subset of neoplasms. A neoplasm or tumor is a group of cells that have undergone unregulated growth and will often form a mass or lump, but may be distributed diffusely.[24][25]
All tumor cells show the six hallmarks of cancer. These characteristics are required to produce a malignant tumor. They include:[26]
The progression from normal cells to cells that can form a detectable mass to outright cancer involves multiple steps known as malignant progression.[27][28]
When cancer begins, it produces no symptoms. Signs and symptoms appear as the mass grows or ulcerates. The findings that result depend on the cancer's type and location. Few symptoms are specific. Many frequently occur in individuals who have other conditions. Cancer is a "great imitator". Thus, it is common for people diagnosed with cancer to have been treated for other diseases, which were hypothesized to be causing their symptoms.[29]
People may become anxious or depressed post-diagnosis. The risk of suicide in people with cancer is approximately double.[30]
Local symptoms may occur due to the mass of the tumor or its ulceration. For example, mass effects from lung cancer can block the bronchus resulting in cough or pneumonia; esophageal cancer can cause narrowing of the esophagus, making it difficult or painful to swallow; and colorectal cancer may lead to narrowing or blockages in the bowel, affecting bowel habits. Masses in breasts or testicles may produce observable lumps. Ulceration can cause bleeding that, if it occurs in the lung, will lead to coughing up blood, in the bowels to anemia or rectal bleeding, in the bladder to blood in the urine and in the uterus to vaginal bleeding. Although localized pain may occur in advanced cancer, the initial swelling is usually painless. Some cancers can cause a buildup of fluid within the chest or abdomen.[29]
General symptoms occur due to effects that are not related to direct or metastatic spread. These may include: unintentional weight loss, fever, excessive fatigue and changes to the skin.[31] Hodgkin disease, leukemias and cancers of the liver or kidney can cause a persistent fever.[29]
Some cancers may cause specific groups of systemic symptoms, termed paraneoplastic syndrome. Examples include the appearance of myasthenia gravis in thymoma and clubbing in lung cancer.[29]
Cancer can spread from its original site by local spread, lymphatic spread to regional lymph nodes or by hematogenous spread via the blood to distant sites, known as metastasis. When cancer spreads by a hematogenous route, it usually spreads all over the body. However, cancer 'seeds' grow in certain selected site only ('soil') as hypothesized in the soil and seed hypothesis of cancer metastasis. The symptoms of metastatic cancers depend on the tumor location and can include enlarged lymph nodes (which can be felt or sometimes seen under the skin and are typically hard), enlarged liver or enlarged spleen, which can be felt in the abdomen, pain or fracture of affected bones and neurological symptoms.[29]
The majority of cancers, some 90–95% of cases, are due to genetic mutations from environmental factors.[3] The remaining 5–10% are due to inherited genetics.[3] Environmental, as used by cancer researchers, means any cause that is not inherited genetically, such as lifestyle, economic and behavioral factors and not merely pollution.[32] Common environmental factors that contribute to cancer death include tobacco (25–30%), diet and obesity (30–35%), infections (15–20%), radiation (both ionizing and non-ionizing, up to 10%), stress, lack of physical activity and pollution.[3][33]
It is not generally possible to prove what caused a particular cancer because the various causes do not have specific fingerprints. For example, if a person who uses tobacco heavily develops lung cancer, then it was probably caused by the tobacco use, but since everyone has a small chance of developing lung cancer as a result of air pollution or radiation, the cancer may have developed for one of those reasons. Excepting the rare transmissions that occur with pregnancies and occasional organ donors, cancer is generally not a transmissible disease.[34]
Exposure to particular substances have been linked to specific types of cancer. These substances are called carcinogens.
Tobacco smoke, for example, causes 90% of lung cancer.[35] It also causes cancer in the larynx, head, neck, stomach, bladder, kidney, esophagus and pancreas.[36] Tobacco smoke contains over fifty known carcinogens, including nitrosamines and polycyclic aromatic hydrocarbons.[37]
Tobacco is responsible for about one in five cancer deaths worldwide[37] and about one in three in the developed world.[38] Lung cancer death rates in the United States have mirrored smoking patterns, with increases in smoking followed by dramatic increases in lung cancer death rates and, more recently, decreases in smoking rates since the 1950s followed by decreases in lung cancer death rates in men since 1990.[39][40]
In Western Europe, 10% of cancers in males and 3% of cancers in females are attributed to alcohol exposure, especially liver and digestive tract cancers.[41] Cancer from work-related substance exposures may cause between 2 and 20% of cases,[42] causing at least 200,000 deaths.[43] Cancers such as lung cancer and mesothelioma can come from inhaling tobacco smoke or asbestos fibers, or leukemia from exposure to benzene.[43]
Diet, physical inactivity and obesity are related to up to 30–35% of cancer deaths.[3][44] In the United States, excess body weight is associated with the development of many types of cancer and is a factor in 14–20% of cancer deaths.[44] A UK study including data on over 5 million people showed higher body mass index to be related to at least 10 types of cancer and responsible for around 12,000 cases each year in that country.[45] Physical inactivity is believed to contribute to cancer risk, not only through its effect on body weight but also through negative effects on the immune system and endocrine system.[44] More than half of the effect from diet is due to overnutrition (eating too much), rather than from eating too few vegetables or other healthful foods.
Some specific foods are linked to specific cancers. A high-salt diet is linked to gastric cancer.[46] Aflatoxin B1, a frequent food contaminant, causes liver cancer.[46] Betel nut chewing can cause oral cancer.[46] National differences in dietary practices may partly explain differences in cancer incidence. For example, gastric cancer is more common in Japan due to its high-salt diet[47] while colon cancer is more common in the United States. Immigrant cancer profiles develop mirror that of their new country, often within one generation.[48]
Worldwide approximately 18% of cancer deaths are related to infectious diseases.[3] This proportion ranges from a high of 25% in Africa to less than 10% in the developed world.[3] Viruses are the usual infectious agents that cause cancer but cancer bacteria and parasites may also play a role.
Oncoviruses (viruses that can cause cancer) include human papillomavirus (cervical cancer), Epstein–Barr virus (B-cell lymphoproliferative disease and nasopharyngeal carcinoma), Kaposi's sarcoma herpesvirus (Kaposi's sarcoma and primary effusion lymphomas), hepatitis B and hepatitis C viruses (hepatocellular carcinoma) and human T-cell leukemia virus-1 (T-cell leukemias). Bacterial infection may also increase the risk of cancer, as seen in Helicobacter pylori-induced gastric carcinoma.[49][50] Parasitic infections associated with cancer include Schistosoma haematobium (squamous cell carcinoma of the bladder) and the liver flukes, Opisthorchis viverrini and Clonorchis sinensis (cholangiocarcinoma).[51]
Up to 10% of invasive cancers are related to radiation exposure, including both ionizing radiation and non-ionizing ultraviolet radiation.[3] Additionally, the majority of non-invasive cancers are non-melanoma skin cancers caused by non-ionizing ultraviolet radiation, mostly from sunlight. Sources of ionizing radiation include medical imaging and radon gas.
Ionizing radiation is not a particularly strong mutagen.[52] Residential exposure to radon gas, for example, has similar cancer risks as passive smoking.[52] Radiation is a more potent source of cancer when combined with other cancer-causing agents, such as radon plus tobacco smoke.[52] Radiation can cause cancer in most parts of the body, in all animals and at any age. Children and adolescents are twice as likely to develop radiation-induced leukemia as adults; radiation exposure before birth has ten times the effect.[52]
Medical use of ionizing radiation is a small but growing source of radiation-induced cancers. Ionizing radiation may be used to treat other cancers, but this may, in some cases, induce a second form of cancer.[52] It is also used in some kinds of medical imaging.[53]
Prolonged exposure to ultraviolet radiation from the sun can lead to melanoma and other skin malignancies.[54] Clear evidence establishes ultraviolet radiation, especially the non-ionizing medium wave UVB, as the cause of most non-melanoma skin cancers, which are the most common forms of cancer in the world.[54]
Non-ionizing radio frequency radiation from mobile phones, electric power transmission and other similar sources have been described as a possible carcinogen by the World Health Organization's International Agency for Research on Cancer.[55] However, studies have not found a consistent link between mobile phone radiation and cancer risk.[56]
The vast majority of cancers are non-hereditary (sporadic). Hereditary cancers are primarily caused by an inherited genetic defect. Less than 0.3% of the population are carriers of a genetic mutation that has a large effect on cancer risk and these cause less than 3–10% of cancer.[57] Some of these syndromes include: certain inherited mutations in the genes BRCA1 and BRCA2 with a more than 75% risk of breast cancer and ovarian cancer,[57] and hereditary nonpolyposis colorectal cancer (HNPCC or Lynch syndrome), which is present in about 3% of people with colorectal cancer,[58] among others.
Statistically for cancers causing most mortality, the relative risk of developing colorectal cancer when a first-degree relative (parent, sibling or child) has been diagnosed with it is about 2.[59] The corresponding relative risk is 1.5 for lung cancer,[60] and 1.9 for prostate cancer.[61] For breast cancer, the relative risk is 1.8 with a first-degree relative having developed it at 50 years of age or older, and 3.3 when the relative developed it when being younger than 50 years of age.[62]
Taller people have an increased risk of cancer because they have more cells than shorter people. Since height is genetically determined to a large extent, taller people have a heritable increase of cancer risk.[63]
Some substances cause cancer primarily through their physical, rather than chemical, effects.[64] A prominent example of this is prolonged exposure to asbestos, naturally occurring mineral fibers that are a major cause of mesothelioma (cancer of the serous membrane) usually the serous membrane surrounding the lungs.[64] Other substances in this category, including both naturally occurring and synthetic asbestos-like fibers, such as wollastonite, attapulgite, glass wool and rock wool, are believed to have similar effects.[64] Non-fibrous particulate materials that cause cancer include powdered metallic cobalt and nickel and crystalline silica (quartz, cristobalite and tridymite).[64] Usually, physical carcinogens must get inside the body (such as through inhalation) and require years of exposure to produce cancer.[64]
Physical trauma resulting in cancer is relatively rare.[65] Claims that breaking bones resulted in bone cancer, for example, have not been proven.[65] Similarly, physical trauma is not accepted as a cause for cervical cancer, breast cancer or brain cancer.[65] One accepted source is frequent, long-term application of hot objects to the body. It is possible that repeated burns on the same part of the body, such as those produced by kanger and kairo heaters (charcoal hand warmers), may produce skin cancer, especially if carcinogenic chemicals are also present.[65] Frequent consumption of scalding hot tea may produce esophageal cancer.[65] Generally, it is believed that cancer arises, or a pre-existing cancer is encouraged, during the process of healing, rather than directly by the trauma.[65] However, repeated injuries to the same tissues might promote excessive cell proliferation, which could then increase the odds of a cancerous mutation.
Chronic inflammation has been hypothesized to directly cause mutation.[65][66] Inflammation can contribute to proliferation, survival, angiogenesis and migration of cancer cells by influencing the tumor microenvironment.[67][68] Oncogenes build up an inflammatory pro-tumorigenic microenvironment.[69]
Some hormones play a role in the development of cancer by promoting cell proliferation.[70] Insulin-like growth factors and their binding proteins play a key role in cancer cell proliferation, differentiation and apoptosis, suggesting possible involvement in carcinogenesis.[71]
Hormones are important agents in sex-related cancers, such as cancer of the breast, endometrium, prostate, ovary and testis and also of thyroid cancer and bone cancer.[70] For example, the daughters of women who have breast cancer have significantly higher levels of estrogen and progesterone than the daughters of women without breast cancer. These higher hormone levels may explain their higher risk of breast cancer, even in the absence of a breast-cancer gene.[70] Similarly, men of African ancestry have significantly higher levels of testosterone than men of European ancestry and have a correspondingly higher level of prostate cancer.[70] Men of Asian ancestry, with the lowest levels of testosterone-activating androstanediol glucuronide, have the lowest levels of prostate cancer.[70]
Other factors are relevant: obese people have higher levels of some hormones associated with cancer and a higher rate of those cancers.[70] Women who take hormone replacement therapy have a higher risk of developing cancers associated with those hormones.[70] On the other hand, people who exercise far more than average have lower levels of these hormones and lower risk of cancer.[70] Osteosarcoma may be promoted by growth hormones.[70] Some treatments and prevention approaches leverage this cause by artificially reducing hormone levels and thus discouraging hormone-sensitive cancers.[70]
There is an association between celiac disease and an increased risk of all cancers. People with untreated celiac disease have a higher risk, but this risk decreases with time after diagnosis and strict treatment, probably due to the adoption of a gluten-free diet, which seems to have a protective role against development of malignancy in people with celiac disease. However, the delay in diagnosis and initiation of a gluten-free diet seems to increase the risk of malignancies.[72] Rates of gastrointestinal cancers are increased in people with Crohn's disease and ulcerative colitis, due to chronic inflammation. Also, immunomodulators and biologic agents used to treat these diseases may promote developing extra-intestinal malignancies.[73]
Cancer is fundamentally a disease of tissue growth regulation. In order for a normal cell to transform into a cancer cell, the genes that regulate cell growth and differentiation must be altered.[74]
The affected genes are divided into two broad categories. Oncogenes are genes that promote cell growth and reproduction. Tumor suppressor genes are genes that inhibit cell division and survival. Malignant transformation can occur through the formation of novel oncogenes, the inappropriate over-expression of normal oncogenes, or by the under-expression or disabling of tumor suppressor genes. Typically, changes in multiple genes are required to transform a normal cell into a cancer cell.[75]
Genetic changes can occur at different levels and by different mechanisms. The gain or loss of an entire chromosome can occur through errors in mitosis. More common are mutations, which are changes in the nucleotide sequence of genomic DNA.
Large-scale mutations involve the deletion or gain of a portion of a chromosome. Genomic amplification occurs when a cell gains copies (often 20 or more) of a small chromosomal locus, usually containing one or more oncogenes and adjacent genetic material. Translocation occurs when two separate chromosomal regions become abnormally fused, often at a characteristic location. A well-known example of this is the Philadelphia chromosome, or translocation of chromosomes 9 and 22, which occurs in chronic myelogenous leukemia and results in production of the BCR-abl fusion protein, an oncogenic tyrosine kinase.
Small-scale mutations include point mutations, deletions, and insertions, which may occur in the promoter region of a gene and affect its expression, or may occur in the gene's coding sequence and alter the function or stability of its protein product. Disruption of a single gene may also result from integration of genomic material from a DNA virus or retrovirus, leading to the expression of viral oncogenes in the affected cell and its descendants.
Replication of the data contained within the DNA of living cells will probabilistically result in some errors (mutations). Complex error correction and prevention is built into the process and safeguards the cell against cancer. If a significant error occurs, the damaged cell can self-destruct through programmed cell death, termed apoptosis. If the error control processes fail, then the mutations will survive and be passed along to daughter cells.
Some environments make errors more likely to arise and propagate. Such environments can include the presence of disruptive substances called carcinogens, repeated physical injury, heat, ionising radiation or hypoxia.[76]
The errors that cause cancer are self-amplifying and compounding, for example:
The transformation of a normal cell into cancer is akin to a chain reaction caused by initial errors, which compound into more severe errors, each progressively allowing the cell to escape more controls that limit normal tissue growth. This rebellion-like scenario is an undesirable survival of the fittest, where the driving forces of evolution work against the body's design and enforcement of order. Once cancer has begun to develop, this ongoing process, termed clonal evolution, drives progression towards more invasive stages.[77] Clonal evolution leads to intra-tumour heterogeneity (cancer cells with heterogeneous mutations) that complicates designing effective treatment strategies.
Characteristic abilities developed by cancers are divided into categories, specifically evasion of apoptosis, self-sufficiency in growth signals, insensitivity to anti-growth signals, sustained angiogenesis, limitless replicative potential, metastasis, reprogramming of energy metabolism and evasion of immune destruction.[27][28]
The classical view of cancer is a set of diseases that are driven by progressive genetic abnormalities that include mutations in tumor-suppressor genes and oncogenes and chromosomal abnormalities. Later epigenetic alterations' role was identified.[78]
Epigenetic alterations refer to functionally relevant modifications to the genome that do not change the nucleotide sequence. Examples of such modifications are changes in DNA methylation (hypermethylation and hypomethylation), histone modification[79] and changes in chromosomal architecture (caused by inappropriate expression of proteins such as HMGA2 or HMGA1).[80] Each of these alterations regulates gene expression without altering the underlying DNA sequence. These changes may remain through cell divisions, last for multiple generations and can be considered to be epimutations (equivalent to mutations).
Epigenetic alterations occur frequently in cancers. As an example, one study listed protein coding genes that were frequently altered in their methylation in association with colon cancer. These included 147 hypermethylated and 27 hypomethylated genes. Of the hypermethylated genes, 10 were hypermethylated in 100% of colon cancers and many others were hypermethylated in more than 50% of colon cancers.[81]
While epigenetic alterations are found in cancers, the epigenetic alterations in DNA repair genes, causing reduced expression of DNA repair proteins, may be of particular importance. Such alterations are thought to occur early in progression to cancer and to be a likely cause of the genetic instability characteristic of cancers.[82][83][84][85]
Reduced expression of DNA repair genes disrupts DNA repair. This is shown in the figure at the 4th level from the top. (In the figure, red wording indicates the central role of DNA damage and defects in DNA repair in progression to cancer.) When DNA repair is deficient DNA damage remains in cells at a higher than usual level (5th level) and cause increased frequencies of mutation and/or epimutation (6th level). Mutation rates increase substantially in cells defective in DNA mismatch repair[86][87] or in homologous recombinational repair (HRR).[88] Chromosomal rearrangements and aneuploidy also increase in HRR defective cells.[89]
Higher levels of DNA damage cause increased mutation (right side of figure) and increased epimutation. During repair of DNA double strand breaks, or repair of other DNA damage, incompletely cleared repair sites can cause epigenetic gene silencing.[90][91]
Deficient expression of DNA repair proteins due to an inherited mutation can increase cancer risks. Individuals with an inherited impairment in any of 34 DNA repair genes (see article DNA repair-deficiency disorder) have increased cancer risk, with some defects ensuring a 100% lifetime chance of cancer (e.g. p53 mutations).[92] Germ line DNA repair mutations are noted on the figure's left side. However, such germline mutations (which cause highly penetrant cancer syndromes) are the cause of only about 1 percent of cancers.[93]
In sporadic cancers, deficiencies in DNA repair are occasionally caused by a mutation in a DNA repair gene but are much more frequently caused by epigenetic alterations that reduce or silence expression of DNA repair genes. This is indicated in the figure at the 3rd level. Many studies of heavy metal-induced carcinogenesis show that such heavy metals cause a reduction in expression of DNA repair enzymes, some through epigenetic mechanisms. DNA repair inhibition is proposed to be a predominant mechanism in heavy metal-induced carcinogenicity. In addition, frequent epigenetic alterations of the DNA sequences code for small RNAs called microRNAs (or miRNAs). miRNAs do not code for proteins, but can "target" protein-coding genes and reduce their expression.
Cancers usually arise from an assemblage of mutations and epimutations that confer a selective advantage leading to clonal expansion (see Field defects in progression to cancer). Mutations, however, may not be as frequent in cancers as epigenetic alterations. An average cancer of the breast or colon can have about 60 to 70 protein-altering mutations, of which about three or four may be "driver" mutations and the remaining ones may be "passenger" mutations.[94]
Metastasis is the spread of cancer to other locations in the body. The dispersed tumors are called metastatic tumors, while the original is called the primary tumor. Almost all cancers can metastasize.[95] Most cancer deaths are due to cancer that has metastasized.[96]
Metastasis is common in the late stages of cancer and it can occur via the blood or the lymphatic system or both. The typical steps in metastasis are local invasion, intravasation into the blood or lymph, circulation through the body, extravasation into the new tissue, proliferation and angiogenesis. Different types of cancers tend to metastasize to particular organs, but overall the most common places for metastases to occur are the lungs, liver, brain and the bones.[95]
Most cancers are initially recognized either because of the appearance of signs or symptoms or through screening. Neither of these leads to a definitive diagnosis, which requires the examination of a tissue sample by a pathologist. People with suspected cancer are investigated with medical tests. These commonly include blood tests, X-rays, (contrast) CT scans and endoscopy.
The tissue diagnosis from the biopsy indicates the type of cell that is proliferating, its histological grade, genetic abnormalities and other features. Together, this information is useful to evaluate the prognosis and to choose the best treatment.
Cytogenetics and immunohistochemistry are other types of tissue tests. These tests provide information about molecular changes (such as mutations, fusion genes and numerical chromosome changes) and may thus also indicate the prognosis and best treatment.
Cancers are classified by the type of cell that the tumor cells resemble and is therefore presumed to be the origin of the tumor. These types include:
Cancers are usually named using -carcinoma, -sarcoma or -blastoma as a suffix, with the Latin or Greek word for the organ or tissue of origin as the root. For example, cancers of the liver parenchyma arising from malignant epithelial cells is called hepatocarcinoma, while a malignancy arising from primitive liver precursor cells is called a hepatoblastoma and a cancer arising from fat cells is called a liposarcoma. For some common cancers, the English organ name is used. For example, the most common type of breast cancer is called ductal carcinoma of the breast. Here, the adjective ductal refers to the appearance of cancer under the microscope, which suggests that it has originated in the milk ducts.
Benign tumors (which are not cancers) are named using -oma as a suffix with the organ name as the root. For example, a benign tumor of smooth muscle cells is called a leiomyoma (the common name of this frequently occurring benign tumor in the uterus is fibroid). Confusingly, some types of cancer use the -noma suffix, examples including melanoma and seminoma.
Some types of cancer are named for the size and shape of the cells under a microscope, such as giant cell carcinoma, spindle cell carcinoma and small-cell carcinoma.
An invasive ductal carcinoma of the breast (pale area at the center) surrounded by spikes of whitish scar tissue and yellow fatty tissue
An invasive colorectal carcinoma (top center) in a colectomy specimen
A squamous-cell carcinoma (the whitish tumor) near the bronchi in a lung specimen
A large invasive ductal carcinoma in a mastectomy specimen
Cancer prevention is defined as active measures to decrease cancer risk.[98] The vast majority of cancer cases are due to environmental risk factors. Many of these environmental factors are controllable lifestyle choices. Thus, cancer is generally preventable.[99] Between 70% and 90% of common cancers are due to environmental factors and therefore potentially preventable.[100]
Greater than 30% of cancer deaths could be prevented by avoiding risk factors including: tobacco, excess weight/obesity, poor diet, physical inactivity, alcohol, sexually transmitted infections and air pollution.[101] Not all environmental causes are controllable, such as naturally occurring background radiation and cancers caused through hereditary genetic disorders and thus are not preventable via personal behavior.
While many dietary recommendations have been proposed to reduce cancer risks, the evidence to support them is not definitive.[14][102] The primary dietary factors that increase risk are obesity and alcohol consumption. Diets low in fruits and vegetables and high in red meat have been implicated but reviews and meta-analyses do not come to a consistent conclusion.[103][104] A 2014 meta-analysis find no relationship between fruits and vegetables and cancer.[105] Coffee is associated with a reduced risk of liver cancer.[106] Studies have linked excess consumption of red or processed meat to an increased risk of breast cancer, colon cancer and pancreatic cancer, a phenomenon that could be due to the presence of carcinogens in meats cooked at high temperatures.[107][108] In 2015 the IARC reported that eating processed meat (e.g., bacon, ham, hot dogs, sausages) and, to a lesser degree, red meat was linked to some cancers.[109][110]
Dietary recommendations for cancer prevention typically include an emphasis on vegetables, fruit, whole grains and fish and an avoidance of processed and red meat (beef, pork, lamb), animal fats, pickled foods and refined carbohydrates.[14][102]
Medications can be used to prevent cancer in a few circumstances.[111] In the general population, NSAIDs reduce the risk of colorectal cancer; however, due to cardiovascular and gastrointestinal side effects, they cause overall harm when used for prevention.[112] Aspirin has been found to reduce the risk of death from cancer by about 7%.[113] COX-2 inhibitors may decrease the rate of polyp formation in people with familial adenomatous polyposis; however, it is associated with the same adverse effects as NSAIDs.[114] Daily use of tamoxifen or raloxifene reduce the risk of breast cancer in high-risk women.[115] The benefit versus harm for 5-alpha-reductase inhibitor such as finasteride is not clear.[116]
Vitamin supplementation does not appear to be effective at preventing cancer.[117] While low blood levels of vitamin D are correlated with increased cancer risk,[118][119][120] whether this relationship is causal and vitamin D supplementation is protective is not determined.[121][122] One 2014 review found that supplements had no significant effect on cancer risk.[122] Another 2014 review concluded that vitamin D3 may decrease the risk of death from cancer (one fewer death in 150 people treated over 5 years), but concerns with the quality of the data were noted.[123]
Beta-carotene supplementation increases lung cancer rates in those who are high risk.[124] Folic acid supplementation is not effective in preventing colon cancer and may increase colon polyps.[125] It is unclear if selenium supplementation has an effect.[126][needs update]
Vaccines have been developed that prevent infection by some carcinogenic viruses.[127] Human papillomavirus vaccine (Gardasil and Cervarix) decrease the risk of developing cervical cancer.[127] The hepatitis B vaccine prevents infection with hepatitis B virus and thus decreases the risk of liver cancer.[127] The administration of human papillomavirus and hepatitis B vaccinations is recommended when resources allow.[128]
Unlike diagnostic efforts prompted by symptoms and medical signs, cancer screening involves efforts to detect cancer after it has formed, but before any noticeable symptoms appear.[129] This may involve physical examination, blood or urine tests or medical imaging.[129]
Cancer screening is not available for many types of cancers. Even when tests are available, they may not be recommended for everyone. Universal screening or mass screening involves screening everyone.[130] Selective screening identifies people who are at higher risk, such as people with a family history.[130] Several factors are considered to determine whether the benefits of screening outweigh the risks and the costs of screening.[129] These factors include:
The U.S. Preventive Services Task Force (USPSTF) issues recommendations for various cancers:
Screens for gastric cancer using photofluorography due to the high incidence there.[22]
Genetic testing for individuals at high-risk of certain cancers is recommended by unofficial groups.[128][144] Carriers of these mutations may then undergo enhanced surveillance, chemoprevention, or preventative surgery to reduce their subsequent risk.[144]
Many treatment options for cancer exist. The primary ones include surgery, chemotherapy, radiation therapy, hormonal therapy, targeted therapy and palliative care. Which treatments are used depends on the type, location and grade of the cancer as well as the patient's health and preferences. The treatment intent may or may not be curative.
Chemotherapy is the treatment of cancer with one or more cytotoxic anti-neoplastic drugs (chemotherapeutic agents) as part of a standardized regimen. The term encompasses a variety of drugs, which are divided into broad categories such as alkylating agents and antimetabolites.[145] Traditional chemotherapeutic agents act by killing cells that divide rapidly, a critical property of most cancer cells.
Targeted therapy is a form of chemotherapy that targets specific molecular differences between cancer and normal cells. The first targeted therapies blocked the estrogen receptor molecule, inhibiting the growth of breast cancer. Another common example is the class of Bcr-Abl inhibitors, which are used to treat chronic myelogenous leukemia (CML).[4] Currently, targeted therapies exist for many of the most common cancer types, including bladder cancer, breast cancer, colorectal cancer, kidney cancer, leukemia, liver cancer, lung cancer, lymphoma, pancreatic cancer, prostate cancer, skin cancer, and thyroid cancer as well as other cancer types.[146]
The efficacy of chemotherapy depends on the type of cancer and the stage. In combination with surgery, chemotherapy has proven useful in cancer types including breast cancer, colorectal cancer, pancreatic cancer, osteogenic sarcoma, testicular cancer, ovarian cancer and certain lung cancers.[147] Chemotherapy is curative for some cancers, such as some leukemias,[148][149] ineffective in some brain tumors,[150] and needless in others, such as most non-melanoma skin cancers.[151] The effectiveness of chemotherapy is often limited by its toxicity to other tissues in the body. Even when chemotherapy does not provide a permanent cure, it may be useful to reduce symptoms such as pain or to reduce the size of an inoperable tumor in the hope that surgery will become possible in the future.
Radiation therapy involves the use of ionizing radiation in an attempt to either cure or improve symptoms. It works by damaging the DNA of cancerous tissue, killing it. To spare normal tissues (such as skin or organs, which radiation must pass through to treat the tumor), shaped radiation beams are aimed from multiple exposure angles to intersect at the tumor, providing a much larger dose there than in the surrounding, healthy tissue. As with chemotherapy, cancers vary in their response to radiation therapy.[152][153][154]
Radiation therapy is used in about half of cases. The radiation can be either from internal sources (brachytherapy) or external sources. The radiation is most commonly low energy x-rays for treating skin cancers, while higher energy x-rays are used for cancers within the body.[155] Radiation is typically used in addition to surgery and or chemotherapy. For certain types of cancer, such as early head and neck cancer, it may be used alone.[156] For painful bone metastasis, it has been found to be effective in about 70% of patients.[156]
Surgery is the primary method of treatment for most isolated, solid cancers and may play a role in palliation and prolongation of survival. It is typically an important part of definitive diagnosis and staging of tumors, as biopsies are usually required. In localized cancer, surgery typically attempts to remove the entire mass along with, in certain cases, the lymph nodes in the area. For some types of cancer this is sufficient to eliminate the cancer.[147]
Palliative care refers to treatment that attempts to help the patient feel better and may be combined with an attempt to treat the cancer. Palliative care includes action to reduce physical, emotional, spiritual and psycho-social distress. Unlike treatment that is aimed at directly killing cancer cells, the primary goal of palliative care is to improve quality of life.
People at all stages of cancer treatment typically receive some kind of palliative care. In some cases, medical specialty professional organizations recommend that patients and physicians respond to cancer only with palliative care.[157] This applies to patients who:[158]
Palliative care may be confused with hospice and therefore only indicated when people approach end of life. Like hospice care, palliative care attempts to help the patient cope with their immediate needs and to increase comfort. Unlike hospice care, palliative care does not require people to stop treatment aimed at the cancer.
Multiple national medical guidelines recommend early palliative care for patients whose cancer has produced distressing symptoms or who need help coping with their illness. In patients first diagnosed with metastatic disease, palliative care may be immediately indicated. Palliative care is indicated for patients with a prognosis of less than 12 months of life even given aggressive treatment.[159][160][161]
A variety of therapies using immunotherapy, stimulating or helping the immune system to fight cancer, have come into use since 1997. Approaches include antibodies, checkpoint therapy and adoptive cell transfer.[162]
Laser therapy uses high-intensity light to treat cancer by shrinking or destroying tumors or precancerous growths. Lasers are most commonly used to treat superficial cancers that are on the surface of the body or the lining of internal organs. It is used to treat basal cell skin cancer and the very early stages of others like cervical, penile, vaginal, vulvar, and non-small cell lung cancer. It is often combined with other treatments, such as surgery, chemotherapy, or radiation therapy. Laser-induced interstitial thermotherapy (LITT), or interstitial laser photocoagulation, uses lasers to treat some cancers using hyperthermia, which uses heat to shrink tumors by damaging or killing cancer cells. Laser are more precise than surgery and cause less damage, pain, bleeding, swelling, and scarring. A disadvantage is surgeons must have specialized training. It may be more expensive than other treatments.[163]
Complementary and alternative cancer treatments are a diverse group of therapies, practices and products that are not part of conventional medicine.[164] "Complementary medicine" refers to methods and substances used along with conventional medicine, while "alternative medicine" refers to compounds used instead of conventional medicine.[165] Most complementary and alternative medicines for cancer have not been studied or tested using conventional techniques such as clinical trials. Some alternative treatments have been investigated and shown to be ineffective but still continue to be marketed and promoted. Cancer researcher Andrew J. Vickers stated, "The label 'unproven' is inappropriate for such therapies; it is time to assert that many alternative cancer therapies have been 'disproven'."[166]
Survival rates vary by cancer type and by the stage at which it is diagnosed, ranging from majority survival to complete mortality five years after diagnosis. Once a cancer has metastasized, prognosis normally becomes much worse. About half of patients receiving treatment for invasive cancer (excluding carcinoma in situ and non-melanoma skin cancers) die from that cancer or its treatment.[22]
Survival is worse in the developing world,[22] partly because the types of cancer that are most common there are harder to treat than those associated with developed countries.[167]
Those who survive cancer develop a second primary cancer at about twice the rate of those never diagnosed.[168] The increased risk is believed to be due to the random chance of developing any cancer, the likelihood of surviving the first cancer, the same risk factors that produced the first cancer, unwanted side effects of treating the first cancer (particularly radiation therapy), and to better compliance with screening.[168]
Predicting short- or long-term survival depends on many factors. The most important are the cancer type and the patient's age and overall health. Those who are frail with other health problems have lower survival rates than otherwise healthy people. Centenarians are unlikely to survive for five years even if treatment is successful. People who report a higher quality of life tend to survive longer.[169] People with lower quality of life may be affected by depression and other complications and/or disease progression that both impairs quality and quantity of life. Additionally, patients with worse prognoses may be depressed or report poorer quality of life because they perceive that their condition is likely to be fatal.
Cancer patients have an increased risk of blood clots in veins. The use of heparin appears to improve survival and decrease the risk of blood clots.[170][needs update]
In 2008, approximately 12.7 million cancers were diagnosed (excluding non-melanoma skin cancers and other non-invasive cancers)[22] and in 2010 nearly 7.98 million people died.[171] Cancers account for approximately 13% of deaths. The most common are lung cancer (1.4 million deaths), stomach cancer (740,000), liver cancer (700,000), colorectal cancer (610,000) and breast cancer (460,000).[172] This makes invasive cancer the leading cause of death in the developed world and the second leading in the developing world.[22] Over half of cases occur in the developing world.[22]
Deaths from cancer were 5.8 million in 1990.[171] Deaths have been increasing primarily due to longer lifespans and lifestyle changes in the developing world.[22] The most significant risk factor for developing cancer is age.[173] Although it is possible for cancer to strike at any age, most patients with invasive cancer are over 65.[173] According to cancer researcher Robert A. Weinberg, "If we lived long enough, sooner or later we all would get cancer."[174] Some of the association between aging and cancer is attributed to immunosenescence,[175] errors accumulated in DNA over a lifetime[176] and age-related changes in the endocrine system.[177] Aging's effect on cancer is complicated by factors such as DNA damage and inflammation promoting it and factors such as vascular aging and endocrine changes inhibiting it.[178]
Some slow-growing cancers are particularly common, but often are not fatal. Autopsy studies in Europe and Asia showed that up to 36% of people have undiagnosed and apparently harmless thyroid cancer at the time of their deaths and that 80% of men develop prostate cancer by age 80.[179][180] As these cancers do not cause the patient's death, identifying them would have represented overdiagnosis rather than useful medical care.
The three most common childhood cancers are leukemia (34%), brain tumors (23%) and lymphomas (12%).[181] In the United States cancer affects about 1 in 285 children.[182] Rates of childhood cancer increased by 0.6% per year between 1975 and 2002 in the United States[183] and by 1.1% per year between 1978 and 1997 in Europe.[181] Death from childhood cancer decreased by half since 1975 in the United States.[182]
Cancer has existed for all of human history.[184] The earliest written record regarding cancer is from circa 1600 BC in the Egyptian Edwin Smith Papyrus and describes breast cancer.[184] Hippocrates (ca. 460 BC – ca. 370 BC) described several kinds of cancer, referring to them with the Greek word καρκίνος karkinos (crab or crayfish).[184] This name comes from the appearance of the cut surface of a solid malignant tumor, with "the veins stretched on all sides as the animal the crab has its feet, whence it derives its name".[185] Galen stated that "cancer of the breast is so called because of the fancied resemblance to a crab given by the lateral prolongations of the tumor and the adjacent distended veins".[186]:738 Celsus (ca. 25 BC – 50 AD) translated karkinos into the Latin cancer, also meaning crab and recommended surgery as treatment.[184] Galen (2nd century AD) disagreed with the use of surgery and recommended purgatives instead.[184] These recommendations largely stood for 1000 years.[184]
In the 15th, 16th and 17th centuries, it became acceptable for doctors to dissect bodies to discover the cause of death.[187] The German professor Wilhelm Fabry believed that breast cancer was caused by a milk clot in a mammary duct. The Dutch professor Francois de la Boe Sylvius, a follower of Descartes, believed that all disease was the outcome of chemical processes and that acidic lymph fluid was the cause of cancer. His contemporary Nicolaes Tulp believed that cancer was a poison that slowly spreads and concluded that it was contagious.[188]
The physician John Hill described tobacco snuff as the cause of nose cancer in 1761.[187] This was followed by the report in 1775 by British surgeon Percivall Pott that chimney sweeps' carcinoma, a cancer of the scrotum, was a common disease among chimney sweeps.[189] With the widespread use of the microscope in the 18th century, it was discovered that the 'cancer poison' spread from the primary tumor through the lymph nodes to other sites ("metastasis"). This view of the disease was first formulated by the English surgeon Campbell De Morgan between 1871 and 1874.[190]
Although many diseases (such as heart failure) may have a worse prognosis than most cases of cancer, cancer is the subject of widespread fear and taboos. The euphemism of "a long illness" to describe cancers leading to death is still commonly used in obituaries, rather than naming the disease explicitly, reflecting an apparent stigma.[191] In Nigeria, one local name for cancer translates into English as "the disease that cannot be cured".[192] This deep belief that cancer is necessarily a difficult and usually deadly disease is reflected in the systems chosen by society to compile cancer statistics: the most common form of cancer—non-melanoma skin cancers, accounting for about one-third of cancer cases worldwide, but very few deaths[193][194]—are excluded from cancer statistics specifically because they are easily treated and almost always cured, often in a single, short, outpatient procedure.[195]
Western conceptions of patients' rights for people with cancer include a duty to fully disclose the medical situation to the person, and the right to engage in shared decision-making in a way that respects the person's own values. In other cultures, other rights and values are preferred. For example, most African cultures value whole families rather than individualism. In parts of Africa, a diagnosis is commonly made so late that cure is not possible, and treatment, if available at all, would quickly bankrupt the family. As a result of these factors, African healthcare providers tend to let family members decide whether, when and how to disclose the diagnosis, and they tend to do so slowly and circuitously, as the person shows interest and an ability to cope with the grim news.[192] People from Asian and South American countries also tend to prefer a slower, less candid approach to disclosure than is idealized in the United States and Western Europe, and they believe that sometimes it would be preferable not to be told about a cancer diagnosis.[192] In general, disclosure of the diagnosis is more common than it was in the 20th century, but full disclosure of the prognosis is not offered to many patients around the world.[192]
In the United States and some other cultures, cancer is regarded as a disease that must be "fought" to end the "civil insurrection"; a War on Cancer was declared in the US. Military metaphors are particularly common in descriptions of cancer's human effects, and they emphasize both the state of the patient's health and the need to take immediate, decisive actions himself rather than to delay, to ignore or to rely entirely on others. The military metaphors also help rationalize radical, destructive treatments.[196][197]
In the 1970s, a relatively popular alternative cancer treatment in the US was a specialized form of talk therapy, based on the idea that cancer was caused by a bad attitude.[198] People with a "cancer personality"—depressed, repressed, self-loathing and afraid to express their emotions—were believed to have manifested cancer through subconscious desire. Some psychotherapists said that treatment to change the patient's outlook on life would cure the cancer.[198] Among other effects, this belief allowed society to blame the victim for having caused the cancer (by "wanting" it) or having prevented its cure (by not becoming a sufficiently happy, fearless and loving person).[199] It also increased patients' anxiety, as they incorrectly believed that natural emotions of sadness, anger or fear shorten their lives.[199] The idea was ridiculed by Susan Sontag, who published Illness as Metaphor while recovering from treatment for breast cancer in 1978.[198] Although the original idea is now generally regarded as nonsense, the idea partly persists in a reduced form with a widespread, but incorrect, belief that deliberately cultivating a habit of positive thinking will increase survival.[199] This notion is particularly strong in breast cancer culture.[199]
One idea about why people with cancer are blamed or stigmatized, called the just-world hypothesis, is that blaming cancer on the patient's actions or attitudes allows the blamers to regain a sense of control. This is based upon the blamers' belief that the world is fundamentally just and so any dangerous illness, like cancer, must be a type of punishment for bad choices, because in a just world, bad things would not happen to good people.[200]
The total health care expenditure on cancer in the US was estimated to be $80.2 billion in 2015.[201] Even though cancer-related health care expenditure have increased in absolute terms during recent decades, the share of health expenditure devoted to cancer treatment has remained close to 5% between the 1960s and 2004.[202][203] A similar pattern has been observed in Europe where about 6% of all health care expenditure are spent on cancer treatment.[204] In addition to health care expenditure, cancer causes indirect costs in terms of productivity losses due to lost work days and premature death, as well as informal care. Indirect costs are typically estimated to exceed the health care costs of cancer.[205]
In the United States, cancer is included as a protected condition by the Equal Employment Opportunity Commission (EEOC), mainly due to the potential for cancer having discriminating effects on workers.[206] Discrimination in the workplace could occur if an employer holds a false belief that a person with cancer is not capable of doing a job properly, and may ask for more sick leave than other employees. Employers may also make hiring or firing decisions based on  misconceptions about cancer disabilities, if present. The EEOC provides interview guidelines for employers, as well as lists of possible solutions for assessing and accommodating employees with cancer.[206]
Because cancer is a class of diseases,[207][208] it is unlikely that there will ever be a single "cure for cancer" any more than there will be a single treatment for all infectious diseases.[209] Angiogenesis inhibitors were once incorrectly thought to have potential as a "silver bullet" treatment applicable to many types of cancer.[210] Angiogenesis inhibitors and other cancer therapeutics are used in combination to reduce cancer morbidity and mortality.[211]
Experimental cancer treatments are studied in clinical trials to compare the proposed treatment to the best existing treatment. Treatments that succeeded in one cancer type can be tested against other types.[212] Diagnostic tests are under development to better target the right therapies to the right patients, based on their individual biology.[213]
Cancer research focuses on the following issues:
The improved understanding of molecular biology and cellular biology due to cancer research has led to new treatments for cancer since US President Richard Nixon declared the "War on Cancer" in 1971. Since then, the country has spent over $200 billion on cancer research, including resources from public and private sectors.[214] The cancer death rate (adjusting for size and age of the population) declined by five percent between 1950 and 2005.[215]
Competition for financial resources appears to have suppressed the creativity, cooperation, risk-taking and original thinking required to make fundamental discoveries, unduly favoring low-risk research into small incremental advancements over riskier, more innovative research. Other consequences of competition appear to be many studies with dramatic claims whose results cannot be replicated and perverse incentives that encourage grantee institutions to grow without making sufficient investments in their own faculty and facilities.[216][217][218][219]
Virotherapy, which uses convert viruses, is being studied.
Cancer affects approximately 1 in 1,000 pregnant women. The most common cancers found during pregnancy are the same as the most common cancers found in non-pregnant women during childbearing ages: breast cancer, cervical cancer, leukemia, lymphoma, melanoma, ovarian cancer and colorectal cancer.[220]
Diagnosing a new cancer in a pregnant woman is difficult, in part because any symptoms are commonly assumed to be a normal discomfort associated with pregnancy. As a result, cancer is typically discovered at a somewhat later stage than average. Some imaging procedures, such as MRIs (magnetic resonance imaging), CT scans, ultrasounds and mammograms with fetal shielding are considered safe during pregnancy; some others, such as PET scans, are not.[220]
Treatment is generally the same as for non-pregnant women. However, radiation and radioactive drugs are normally avoided during pregnancy, especially if the fetal dose might exceed 100 cGy. In some cases, some or all treatments are postponed until after birth if the cancer is diagnosed late in the pregnancy. Early deliveries are often used to advance the start of treatment. Surgery is generally safe, but pelvic surgeries during the first trimester may cause miscarriage. Some treatments, especially certain chemotherapy drugs given during the first trimester, increase the risk of birth defects and pregnancy loss (spontaneous abortions and stillbirths).[220]
Elective abortions are not required and, for the most common forms and stages of cancer, do not improve the mother's survival. In a few instances, such as advanced uterine cancer, the pregnancy cannot be continued and in others, the patient may end the pregnancy so that she can begin aggressive chemotherapy.[220]
Some treatments can interfere with the mother's ability to give birth vaginally or to breastfeed.[220] Cervical cancer may require birth by Caesarean section. Radiation to the breast reduces the ability of that breast to produce milk and increases the risk of mastitis. Also, when chemotherapy is given after birth, many of the drugs appear in breast milk, which could harm the baby.[220]
Veterinary oncology, concentrating mainly on cats and dogs, is a growing specialty in wealthy countries and the major forms of human treatment such as surgery and radiotherapy may be offered. The most common types of cancer differ, but the cancer burden seems at least as high in pets as in humans. Animals, typically rodents, are often used in cancer research and studies of natural cancers in larger animals may benefit research into human cancer.[221]
In non-humans, a few types of transmissible cancer have been described, wherein the cancer spreads between animals by transmission of the tumor cells themselves. This phenomenon is seen in dogs with Sticker's sarcoma (also known as canine transmissible venereal tumor), and in Tasmanian devils with devil facial tumour disease (DFTD).[222]



Cell growth - Wikipedia
The term cell growth is used in the contexts of biological cell development and cell division (reproduction). When used in the context of cell development, the term refers to increase in cytoplasmic and organelle volume (G1 phase), as well as increase in genetic material (G2 phase) following the replication during S phase.[1]
This is not to be confused with growth in the context of cell division, referred to as proliferation, where a cell, known as the "mother cell", grows and divides to produce two "daughter cells" (M phase). 
Cell populations go through a particular type of exponential growth called doubling. Thus, each generation of cells should be twice as numerous as the previous generation. However, the number of generations only gives a maximum figure as not all cells survive in each generation.
Cell size is highly variable among organisms, with some algae such as Caulerpa taxifolia being a single cell several meters in length.[2] Plant cells are much larger than animal cells, and protists such as Paramecium can be 330 μm long, while a typical human cell might be 10 μm. How these cells "decide" how big they should be before dividing is an open question. Chemical gradients are known to be partly responsible, and it is hypothesized that mechanical stress detection by cytoskeletal structures is involved. Work on the topic generally requires an organism whose cell cycle is well-characterized.
The relationship between cell size and cell division has been extensively studied in yeast. For some cells, there is a mechanism by which cell division is not initiated until a cell has reached a certain size. If the nutrient supply is restricted (after time t = 2 in the diagram, below), and the rate of increase in cell size is slowed, the time period between cell divisions is increased.[3] Yeast cell-size mutants were isolated that begin cell division before reaching a normal/regular size (wee mutants).[4]
Wee1 protein is a tyrosine kinase that normally phosphorylates the Cdc2 cell cycle regulatory protein (the homolog of CDK1 in humans), a cyclin-dependent kinase, on a tyrosine residue.  Cdc2 drives entry into mitosis by phosphorylating a wide range of targets.  This covalent  modification of the molecular structure of Cdc2 inhibits the enzymatic activity of Cdc2 and prevents cell division. Wee1 acts to keep Cdc2 inactive during early G2 when cells are still small.  When cells have reached sufficient size during G2, the phosphatase Cdc25 removes the inhibitory phosphorylation, and thus activates Cdc2 to allow mitotic entry.  A balance of Wee1 and Cdc25 activity with changes in cell size is coordinated by the mitotic entry control system.  It has been shown in Wee1 mutants, cells with weakened Wee1 activity, that Cdc2 becomes active when the cell is smaller.  Thus, mitosis occurs before the yeast reach their normal size.  This suggests that cell division may be regulated in part by dilution of Wee1 protein in cells as they grow larger.
The protein kinase Cdr2 (which negatively regulates Wee1) and the Cdr2-related kinase Cdr1 (which directly phosphorylates and inhibits Wee1 in vitro)[5] are localized to a band of cortical nodes in the middle of interphase cells.  After entry into mitosis, cytokinesis factors such as myosin II are recruited to similar nodes; these nodes eventually condense to form the cytokinetic ring.[6]  A previously uncharacterized protein, Blt1, was found to colocalize with Cdr2 in the medial interphase nodes.  Blt1 knockout cells had increased length at division, which is consistent with a delay in mitotic entry.  This finding connects a physical location, a band of cortical nodes, with factors that have been shown to directly regulate mitotic entry, namely Cdr1, Cdr2, and Blt1.
Further experimentation with GFP-tagged proteins and mutant proteins indicates that the medial cortical nodes are formed by the ordered, Cdr2-dependent assembly of multiple interacting proteins during interphase.  Cdr2 is at the top of this hierarchy and works upstream of Cdr1 and Blt1.[7]  Mitosis is promoted by the negative regulation of Wee1 by Cdr2.  It has also been shown that Cdr2 recruits Wee1 to the medial cortical node.  The mechanism of this recruitment has yet to be discovered.  A Cdr2 kinase mutant, which is able to localize properly despite a loss of function in phosphorylation, disrupts the recruitment of Wee1 to the medial cortex and delays entry into mitosis.  Thus, Wee1 localizes with its inhibitory network, which demonstrates that mitosis is controlled through Cdr2-dependent negative regulation of Wee1 at the medial cortical nodes.[7]
Cell polarity factors positioned at the cell tips provide spatial cues to limit Cdr2 distribution to the cell middle.  In fission yeast Schizosaccharomyces pombe (S. Pombe), cells divide at a defined, reproducible size during mitosis because of the regulated activity of Cdk1.[8]  The cell polarity protein kinase Pom1, a member of the dual-specificity tyrosine-phosphorylation regulated kinase (DYRK) family of kinases, localizes to cell ends.  In Pom1 knockout cells, Cdr2 was no longer restricted to the cell middle, but was seen diffusely through half of the cell.  From this data it becomes apparent that Pom1 provides inhibitory signals that confine Cdr2 to the middle of the cell.  It has been further shown that Pom1-dependent signals lead to the phosphorylation of Cdr2.  Pom1 knockout cells were also shown to divide at a smaller size than wild-type, which indicates a premature entry into mitosis.[7]
Pom1 forms polar gradients that peak at cell ends, which shows a direct link between size control factors and a specific physical location in the cell.[9]  As a cell grows in size, a gradient in Pom1 grows.  When cells are small, Pom1 is spread diffusely throughout the cell body.  As the cell increases in size, Pom1 concentration decreases in the middle and becomes concentrated at cell ends.  Small cells in early G2 which contain sufficient levels of Pom1 in the entirety of the cell have inactive Cdr2 and cannot enter mitosis.  It is not until the cells grow into late G2, when Pom1 is confined to the cell ends that Cdr2 in the medial cortical nodes is activated and able to start the inhibition of Wee1.  This finding shows how cell size plays a direct role in regulating the start of mitosis.  In this model, Pom1 acts as a molecular link between cell growth and mitotic entry through a Cdr2-Cdr1-Wee1-Cdk1 pathway.[7]  The Pom1 polar gradient successfully relays information about cell size and geometry to the Cdk1 regulatory system.  Through this gradient, the cell ensures it has reached a defined, sufficient size to enter mitosis.
Many different types of eukaryotic cells undergo size-dependent transitions during the cell cycle. These transitions are controlled by the cyclin-dependent kinase Cdk1.[10]  Though the proteins that control Cdk1 are well understood, their connection to mechanisms monitoring cell size remains elusive.
A postulated model for mammalian size control situates mass as the driving force of the cell cycle.  A cell is unable to grow to an abnormally large size because at a certain cell size or cell mass, the S phase is initiated.  The S phase starts the sequence of events leading to mitosis and cytokinesis.  A cell is unable to get too small because the later cell cycle events, such as S, G2, and M, are delayed until mass increases sufficiently to begin S phase.[11]
Many of the signal molecules that convey information to cells during the control of cellular differentiation or growth are called growth factors. The protein mTOR is a serine/threonine kinase that regulates
translation and cell division.[12] Nutrient availability influences mTOR
so that when cells are not able to grow to normal size they will not
undergo cell division.
The details of the molecular mechanisms of mammalian cell size control
are currently being investigated. The size of post-mitotic neurons
depends on the size of the cell body, axon and dendrites. In
vertebrates, neuron size is often a reflection of the number of
synaptic contacts onto the neuron or from a neuron onto other cells.
For example, the size of motoneurons usually reflects the size of
the motor unit that is controlled by the motoneuron.
Invertebrates often have giant neurons and axons that provide
special functions such as rapid action potential propagation.
Mammals also use this trick for increasing the speed of signals in the
nervous system, but they can also use myelin to accomplish this, so
most human neurons are relatively small cells.
One common means to produce very large cells is by cell fusion to form syncytia. For example, very long (several inches) skeletal muscle cells are formed by fusion of thousands of myocytes. Genetic studies of the fruit fly Drosophila have revealed several genes that are required for the formation of multinucleated muscle cells by fusion of myoblasts.[13] Some of the key proteins are important for cell adhesion between myocytes and some are involved in adhesion-dependent cell-to-cell signal transduction that allows for a cascade of cell fusion events.
Oocytes can be unusually large cells in species for which embryonic development takes place away from the mother's body. Their large size can be achieved either by pumping in cytosolic components from adjacent cells through cytoplasmic bridges (Drosophila) or by internalization of nutrient storage granules (yolk granules) by endocytosis (frogs).
Increases in the size of plant cells are complicated by the fact that almost all plant cells are inside of a solid cell wall. Under the influence of certain plant hormones the cell wall can be remodeled, allowing for increases in cell size that are important for the growth of some plant tissues.
Most unicellular organisms are microscopic in size, but there are some giant bacteria and protozoa that are visible to the naked eye. See: Table of cell sizes —Dense populations of a giant sulfur bacterium in Namibian shelf sediments[14]— Large protists of the genus Chaos, closely related to the genus Amoeba
In the rod-shaped bacteria E. coli, Caulobacter crescentus and B. subtilis cell size is controlled by a simple mechanisms in which cell division occurs after a constant volume has been added since the previous division.[15][16] By always growing by the same amount, cells born smaller or larger than average naturally converge to an average size equivalent to the amount added during each generation.
Cell reproduction is asexual. For most of the constituents of the cell, growth is a steady, continuous process, interrupted only briefly at M phase when the nucleus and then the cell divide in two.
The process of cell division, called cell cycle, has four major parts called phases. The first part, called G1 phase is marked by synthesis of various enzymes that are required for DNA replication.
The second part of the cell cycle is the S phase, where DNA replication produces two identical sets of  chromosomes. The third part is the G2 phase in which a significant protein synthesis occurs, mainly involving the production of microtubules that are required during the process of division, called mitosis.
The fourth phase, M phase, consists of nuclear division (karyokinesis) and cytoplasmic division (cytokinesis), accompanied by the formation of a new cell membrane. This is the physical division of "mother" and "daughter" cells. The M phase has been broken down into several distinct phases, sequentially known as prophase, prometaphase, metaphase, anaphase and telophase leading to cytokinesis.
Cell division is more complex in eukaryotes than in other organisms. Prokaryotic cells such as bacterial cells reproduce by binary fission, a process that includes DNA replication, chromosome segregation, and cytokinesis. Eukaryotic cell division either involves mitosis or a more complex process called meiosis. Mitosis and meiosis are sometimes called the two "nuclear division" processes. Binary fission is similar to eukaryote cell reproduction that involves mitosis. Both lead to the production of two daughter cells with the same number of chromosomes as the parental cell. Meiosis is used for a special cell reproduction process of diploid organisms. It produces four special daughter cells (gametes) which have half the normal cellular amount of DNA. A male and a female gamete can then combine to produce a zygote, a cell which again has the normal amount of chromosomes.
The rest of this article is a comparison of the main features of the three types of cell reproduction that either involve binary fission, mitosis, or meiosis. The diagram below depicts the similarities and differences of these three types of cell reproduction.
The DNA content of a cell is duplicated at the start of the cell reproduction process. Prior to DNA replication, the DNA content of a cell can be represented as the amount Z (the cell has Z chromosomes). After the DNA replication process, the amount of DNA in the cell is 2Z (multiplication: 2 x Z = 2Z). During Binary fission and mitosis the duplicated DNA content of the reproducing parental cell is separated into two equal halves that are destined to end up in the two daughter cells. The final part of the cell reproduction process is cell division, when  daughter cells physically split apart from a parental cell. During meiosis, there are two cell division steps that together produce the four daughter cells.
After the completion of binary fission or cell reproduction involving mitosis, each daughter cell has the same amount of DNA (Z) as what the parental cell had before it replicated its DNA. These two types of cell reproduction produced two daughter cells that have the same number of chromosomes as the parental cell. Chromosomes duplicate prior to cell division when forming new skin cells for reproduction. After meiotic cell reproduction the four daughter cells have half the number of chromosomes that the parental cell originally had. This is the haploid amount of DNA, often symbolized as N. Meiosis is used by diploid organisms to produce haploid gametes. In a diploid organism such as the human organism, most cells of  the body have the diploid amount of DNA, 2N. Using this notation for counting chromosomes we say that human somatic cells have 46 chromosomes (2N = 46) while human sperm and eggs have 23 chromosomes (N = 23). Humans have 23 distinct types of chromosomes, the 22 autosomes and the special category of sex chromosomes. There are two distinct sex chromosomes, the X chromosome and the Y chromosome. A diploid human cell has 23 chromosomes from that person's father and 23 from the mother. That is, your body has two copies of human chromosome number 2, one from each of your parents.
Immediately after DNA replication a human cell will have 46 "double chromosomes". In each double chromosome there are two copies of that chromosome's DNA molecule. During mitosis the double chromosomes are split to produce 92 "single chromosomes", half of which go into each daughter cell. During meiosis, there are two chromosome separation steps which assure that each of the four daughter cells gets one copy of each of the 23 types of chromosome.
Though cell reproduction that uses mitosis can reproduce eukaryotic cells, eukaryotes bother with the more complicated process of meiosis because sexual reproduction such as meiosis confers a selective advantage. Notice that when meiosis starts, the two copies of sister chromatids number 2 are adjacent to each other. During this time, there can be genetic recombination events. Parts of the chromosome 2 DNA gained from one parent (red) will swap over to the chromosome 2 DNA molecule that received from the other parent (green). Notice that in mitosis the two copies of chromosome number 2 do not interact. It is these new combinations of parts of chromosomes that provide the major advantage for sexually reproducing organisms by allowing for new combinations of genes and more efficient evolution.
However, in organisms with more than one set of chromosomes at the main life cycle stage, sex may also provide an advantage because, under random mating, it produces homozygotes and heterozygotes according to the Hardy-Weinberg ratio.
A series of growth disorders can occur at the cellular level and these consequently underpin much of the subsequent course in cancer, in which a group of cells display uncontrolled growth and division beyond the normal limits, invasion (intrusion on and destruction of adjacent tissues), and sometimes metastasis (spread to other locations in the body via lymph or blood).
The cell growth can be detected by a variety of methods.
The cell size growth can be visualized by microscopy, using suitable stains. But the increase of cells number is usually more significant. It can be measured by manual counting of cells under microscopy observation, using the dye exclusion method (i.e. trypan blue) to count only viable cells. Less fastidious, scalable, methods include the use of cytometers, while flow cytometry allows combining cell counts ('events') with other specific parameters: fluorescent probes for membranes, cytoplasm or nuclei allow distinguishing dead/viable cells, cell types, cell differentiation, expression of a biomarker such as Ki67.
Beside the increasing number of cells, one can be assessed regarding the metabolic activity growth, that is, the CFDA and calcein-AM measure (fluorimetrically) not only the membrane functionality (dye retention), but also the functionality of cytoplasmic enzymes (esterases). The MTT assays (colorimetric) and the resazurin assay (fluorimetric) dose the mitochondrial redox potential.
All these assays may correlate well, or not, depending on cell growth conditions and desired aspects (activity, proliferation). The task is even more complicated with populations of different cells, furthermore when combining cell growth interferences or toxicity.
Image result for cell growth
Cell growth (or interphase) is shorthand for the idea of "growth in cell populations" by means of cell reproduction. It is the stage which cells are preparing for the next division, biochemical activities and reactions are taking place, however no obvious changes can be seen at this stage.



Tobacco smoking - Wikipedia

Tobacco smoking is the practice of smoking tobacco and inhaling tobacco smoke (consisting of particle and gaseous phases). (A more broad definition may include simply taking tobacco smoke into the mouth, and then releasing it, as is done by some with tobacco pipes and cigars.) The practice is believed to have begun as early as 5000–3000 BC in Mesoamerica and South America.[1] Tobacco was introduced to Eurasia in the late 17th century by European colonists, where it followed common trade routes. The practice encountered criticism from its first import into the Western world onwards but embedded itself in certain strata of a number of societies before becoming widespread upon the introduction of automated cigarette-rolling apparatus.[2][3]
German scientists identified a link between smoking and lung cancer in the late 1920s, leading to the first anti-smoking campaign in modern history, albeit one truncated by the collapse of Nazi Germany at the end of World War II.[4] In 1950, British researchers demonstrated a clear relationship between smoking and cancer.[5] Evidence continued to mount in the 1980s, which prompted political action against the practice. Rates of consumption since 1965 in the developed world have either peaked or declined.[6] However, they continue to climb in the developing world.[7]
Smoking is the most common method of consuming tobacco, and tobacco is the most common substance smoked. The agricultural product is often mixed with additives[8] and then combusted. The resulting smoke is then inhaled and the active substances absorbed through the alveoli in the lungs or the oral mucosa.[9] Combustion was traditionally enhanced by addition of potassium or nitrates.[citation needed] Many substances in cigarette smoke trigger chemical reactions in nerve endings, which heighten heart rate, alertness[10] and reaction time, among other things.[11] Dopamine and endorphins are released, which are often associated with pleasure.[12] As of 2008 to 2010, tobacco is used by about 49% of men and 11% of women aged 15 or older in fourteen low-income and middle-income countries (Bangladesh, Brazil, China, Egypt, India, Mexico, Philippines, Poland, Russia, Thailand, Turkey, Ukraine, Uruguay and Vietnam), with about 80% of this usage in the form of smoking.[13] The gender gap tends to be less pronounced in lower age groups.[14][15]
Many smokers begin during adolescence or early adulthood.[16] During the early stages, a combination of perceived pleasure acting as positive reinforcement and desire to respond to social peer pressure may offset the unpleasant symptoms of initial use, which typically include nausea and coughing. After an individual has smoked for some years, the avoidance of withdrawal symptoms and negative reinforcement become the key motivations to continue.
A study of first smoking experiences of seventh-grade students found out that the most common factor leading students to smoke is cigarette advertisements. Smoking by parents, siblings and friends also encourages students to smoke.[17]
Smoking's history dates back to as early as 5000–3000 BC, when the agricultural product began to be cultivated in Mesoamerica and South America; consumption later evolved into burning the plant substance either by accident or with intent of exploring other means of consumption.[1] The practice worked its way into shamanistic rituals.[18] Many ancient civilizations — such as the Babylonians, the Indians, and the Chinese — burnt incense during religious rituals. Smoking in the Americas probably had its origins in the incense-burning ceremonies of shamans but was later adopted for pleasure or as a social tool.[19] The smoking of tobacco and various hallucinogenic drugs was used to achieve trances and to come into contact with the spirit world.
Eastern North American tribes would carry large amounts of tobacco in pouches as a readily accepted trade item and would often smoke it in ceremonial pipes, either in sacred ceremonies or to seal bargains.[20] Adults as well as children enjoyed the practice.[21] It was believed that tobacco was a gift from the Creator and that the exhaled tobacco smoke was capable of carrying one's thoughts and prayers to heaven.[22]
Apart from smoking, tobacco had a number of uses as medicine. As a pain killer it was used for earache and toothache and occasionally as a poultice. Smoking was said by the desert Indians to be a cure for colds, especially if the tobacco was mixed with the leaves of the small desert Sage, Salvia dorrii, or the root of Indian balsam or cough root, Leptotaenia multifida, the addition of which was thought to be particularly good for asthma and tuberculosis.[23]
In 1612, six years after the settlement of Jamestown, Virginia, John Rolfe was credited as the first settler to successfully raise tobacco as a cash crop. The demand quickly grew as tobacco, referred to as "brown gold", revived the Virginia joint stock company from its failed gold expeditions.[24] In order to meet demands from the Old World, tobacco was grown in succession, quickly depleting the soil. This became a motivator to settle west into the unknown continent, and likewise an expansion of tobacco production.[25] Indentured servitude became the primary labor force up until Bacon's Rebellion, from which the focus turned to slavery.[26] This trend abated following the American Revolution as slavery became regarded as unprofitable. However, the practice was revived in 1794 with the invention of the cotton gin.[27]
Frenchman Jean Nicot (from whose name the word nicotine is derived) introduced tobacco to France in 1560, and tobacco then spread to England. The first report of a smoking Englishman is of a sailor in Bristol in 1556, seen "emitting smoke from his nostrils".[2] Like tea, coffee and opium, tobacco was just one of many intoxicants that was originally used as a form of medicine.[28] Tobacco was introduced around 1600 by French merchants in what today is modern-day Gambia and Senegal. At the same time, caravans from Morocco brought tobacco to the areas around Timbuktu, and the Portuguese brought the commodity (and the plant) to southern Africa, establishing the popularity of tobacco throughout all of Africa by the 1650s.
Soon after its introduction to the Old World, tobacco came under frequent criticism from state and religious leaders. James VI and I, King of Scotland and England, produced the treatise A Counterblaste to Tobacco in 1604, and also introduced excise duty on the product. Murad IV, sultan of the Ottoman Empire 1623–40 was among the first to attempt a smoking ban by claiming it was a threat to public morals and health. The Chongzhen Emperor of China issued an edict banning smoking two years before his death and the overthrow of the Ming dynasty. Later, the Manchu rulers of the Qing dynasty, would proclaim smoking "a more heinous crime than that even of neglecting archery". In Edo period Japan, some of the earliest tobacco plantations were scorned by the shogunate as being a threat to the military economy by letting valuable farmland go to waste for the use of a recreational drug instead of being used to plant food crops.[29]
Religious leaders have often been prominent among those who considered smoking immoral or outright blasphemous. In 1634, the Patriarch of Moscow forbade the sale of tobacco, and sentenced men and women who flouted the ban to have their nostrils slit and their backs flayed. Pope Urban VIII likewise condemned smoking on holy places in a papal bull of 1624. Despite some concerted efforts, restrictions and bans were largely ignored. When James I of England, a staunch anti-smoker and the author of A Counterblaste to Tobacco, tried to curb the new trend by enforcing a 4000% tax increase on tobacco in 1604 it was unsuccessful, as suggested by the presence of around 7,000 tobacco outlets in London by the early 17th century. From this point on for some centuries, several administrations withdrew from efforts at discouragement and instead turned tobacco trade and cultivation into sometimes lucrative government monopolies.[30][31]
By the mid-17th century most major civilizations had been introduced to tobacco smoking and in many cases had already assimilated it into the native culture, despite some continued attempts upon the parts of rulers to eliminate the practice with penalties or fines. Tobacco, both product and plant, followed the major trade routes to major ports and markets, and then on into the hinterlands. The English language term smoking appears to have entered currency in the late 18th century, before which less abbreviated descriptions of the practice such as drinking smoke were also in use.[2]
Growth in the US remained stable until the American Civil War in 1860s, when the primary agricultural workforce shifted from slavery to sharecropping. This, along with a change in demand, accompanied the industrialization of cigarette production as craftsman James Bonsack created a machine in 1881 to partially automate their manufacture.[32]
In Germany, anti-smoking groups, often associated with anti-liquor groups,[33] first published advocacy against the consumption of tobacco in the journal Der Tabakgegner (The Tobacco Opponent) in 1912 and 1932. In 1929, Fritz Lickint of Dresden, Germany, published a paper containing formal statistical evidence of a lung cancer–tobacco link. During the Great Depression Adolf Hitler condemned his earlier smoking habit as a waste of money,[34] and later with stronger assertions. This movement was further strengthened with Nazi reproductive policy as women who smoked were viewed as unsuitable to be wives and mothers in a German family.[35]
The anti-tobacco movement in Nazi Germany did not reach across enemy lines during the Second World War, as anti-smoking groups quickly lost popular support. By the end of the Second World War, American cigarette manufacturers quickly reentered the German black market. Illegal smuggling of tobacco became prevalent,[36] and leaders of the Nazi anti-smoking campaign were silenced.[37] As part of the Marshall Plan, the United States shipped free tobacco to Germany; with 24,000 tons in 1948 and 69,000 tons in 1949.[36] Per capita yearly cigarette consumption in post-war Germany steadily rose from 460 in 1950 to 1,523 in 1963.[4] By the end of the 20th century, anti-smoking campaigns in Germany were unable to exceed the effectiveness of the Nazi-era climax in the years 1939–41 and German tobacco health research was described by Robert N. Proctor as "muted".[4]
In 1950, Richard Doll published research in the British Medical Journal showing a close link between smoking and lung cancer.[38]  Beginning in December 1952, the magazine Reader's Digest published "Cancer by the Carton", a series of articles that linked smoking with lung cancer.[39]
In 1954, the British Doctors Study, a prospective study of some 40 thousand doctors for about 2.5 years, confirmed the suggestion, based on which the government issued advice that smoking and lung cancer rates were related.[5] In January 1964, the United States Surgeon General's Report on Smoking and Health likewise began suggesting the relationship between smoking and cancer.[40]
As scientific evidence mounted in the 1980s, tobacco companies claimed contributory negligence as the adverse health effects were previously unknown or lacked substantial credibility. Health authorities sided with these claims up until 1998, from which they reversed their position. The Tobacco Master Settlement Agreement, originally between the four largest US tobacco companies and the Attorneys General of 46 states, restricted certain types of tobacco advertisement and required payments for health compensation; which later amounted to the largest civil settlement in United States history.[41]
Social campaigns have been instituted in many places to discourage smoking, such as Canada's National Non-Smoking Week.
From 1965 to 2006, rates of smoking in the United States declined from 42% to 20.8%.[6] The majority of those who quit were professional, affluent men. Although the per-capita number of smokers decreased, the average number of cigarettes consumed per person per day increased from 22 in 1954 to 30 in 1978. This paradoxical event suggests that those who quit smoked less, while those who continued to smoke moved to smoke more light cigarettes.[42] The trend has been paralleled by many industrialized nations as rates have either leveled-off or declined. In the developing world, however, tobacco consumption continues to rise at 3.4% in 2002.[7] In Africa, smoking is in most areas considered to be modern, and many of the strong adverse opinions that prevail in the West receive much less attention.[43] Today Russia leads as the top consumer of tobacco followed by Indonesia, Laos, Ukraine, Belarus, Greece, Jordan, and China.[44]
Tobacco is an agricultural product processed from the fresh leaves of plants in the genus Nicotiana. The genus contains a number of species, however, Nicotiana tabacum is the most commonly grown. Nicotiana rustica follows as second containing higher concentrations of nicotine. These leaves are harvested and cured to allow for the slow oxidation and degradation of carotenoids in tobacco leaf. This produces certain compounds in the tobacco leaves which can be attributed to sweet hay, tea, rose oil, or fruity aromatic flavors. Before packaging, the tobacco is often combined with other additives in order to enhance the addictive potency, shift the products pH, or improve the effects of smoke by making it more palatable. In the United States these additives are regulated to 599 substances.[8] The product is then processed, packaged, and shipped to consumer markets.

The active substances in tobacco, especially cigarettes, are administered by burning the leaves and inhaling the vaporized gas that results. This quickly and effectively delivers substances into the bloodstream by absorption through the alveoli in the lungs. The lungs contain some 300 million alveoli, which amounts to a surface area of over 70 m2 (about the size of a tennis court). This method is not completely efficient as not all of the smoke will be inhaled, and some amount of the active substances will be lost in the process of combustion, pyrolysis.[9] Pipe and Cigar smoke are not inhaled because of its high alkalinity, which are irritating to the trachea and lungs. However, because of its higher alkalinity (pH 8.5) compared to cigarette smoke (pH 5.3), non-ionized nicotine is more readily absorbed through the mucous membranes in the mouth.[50] Nicotine absorption from cigar and pipe, however, is much less than that from cigarette smoke.[51] Nicotine and cocaine activate similar patterns of neurons, which supports the existence of common substrates among these drugs.[52]
The inhaled nicotine mimics nicotinic acetylcholine which when bound to nicotinic acetylcholine receptors prevents the reuptake of acetylcholine thereby increasing that neurotransmitter in those areas of the body.[53] These nicotinic acetylcholine receptors are located in the central nervous system and at the nerve-muscle junction of skeletal muscles; whose activity increases heart rate, alertness,[10] and faster reaction times.[11] Nicotine acetylcholine stimulation is not directly addictive. However, since dopamine-releasing neurons are abundant on nicotine receptors, dopamine is released; and, in the nucleus accumbens, dopamine is associated with motivation causing reinforcing behavior.[54] Dopamine increase, in the prefrontal cortex, may also increase working memory.[55]
When tobacco is smoked, most of the nicotine is pyrolyzed. However, a dose sufficient to cause mild somatic dependency and mild to strong psychological dependency remains. There is also a formation of harmane (a MAO inhibitor) from the acetaldehyde in tobacco smoke. This may play a role in nicotine addiction, by facilitating a dopamine release in the nucleus accumbens as a response to nicotine stimuli.[56] Using rat studies, withdrawal after repeated exposure to nicotine results in less responsive nucleus accumbens cells, which produce dopamine responsible for reinforcement.[57]
As of 2000, smoking was practiced by around 1.22 billion people. At current rates of 'smoker replacement' and market growth, this may reach around 1.9 billion in 2025.[58]
Smoking may be up to five times more prevalent among men than women in some communities,[58] although the gender gap usually declines with younger age.[14][15] In some developed countries smoking rates for men have peaked and begun to decline, while for women they continue to climb.[59]
As of 2002, about twenty percent of young teenagers (13–15) smoked worldwide. From which 80,000 to 100,000 children begin smoking every day, roughly half of whom live in Asia. Half of those who begin smoking in adolescent years are projected to go on to smoke for 15 to 20 years.[7]
The World Health Organization (WHO) states that "Much of the disease burden and premature mortality attributable to tobacco use disproportionately affect the poor". Of the 1.22 billion smokers, 1 billion of them live in developing or transitional economies. Rates of smoking have leveled off or declined in the developed world.[60] In the developing world, however, tobacco consumption is rising by 3.4% per year as of 2002.[7]
The WHO in 2004 projected 58.8 million deaths to occur globally,[61] from which 5.4 million are tobacco-attributed,[62] and 4.9 million as of 2007.[63] As of 2002, 70% of the deaths are in developing countries.[63] As of 2017, smoking causes one in ten deaths worldwide, with half of those deaths in the US, China, India and Russia.[64]
Most smokers begin smoking during adolescence or early adulthood. Some studies also show that smoking can also be linked to various mental health complications.[66] Smoking has elements of risk-taking and rebellion, which often appeal to young people. The presence of peers that smoke and media featuring high-status models smoking may also encourage smoking. Because teenagers are influenced more by their peers than by adults, attempts by parents, schools, and health professionals at preventing people from trying cigarettes are often unsuccessful.[67][68]
Children of smoking parents are more likely to smoke than children with non-smoking parents. Children of parents who smoke are less likely to quit smoking.[16] One study found that parental smoking cessation was associated with less adolescent smoking, except when the other parent currently smoked.[69] A current study tested the relation of adolescent smoking to rules regulating where adults are allowed to smoke in the home. Results showed that restrictive home smoking policies were associated with lower likelihood of trying smoking for both middle and high school students.[70]
Behavioural research generally indicates that teenagers begin their smoking habits due to peer pressure, and cultural influence portrayed by friends. However, one study found that direct pressure to smoke cigarettes played a less significant part in adolescent smoking, with adolescents also reporting low levels of both normative and direct pressure to smoke cigarettes.[71] Mere exposure to tobacco retailers may motivate smoking behaviour in adults.[72] A similar study suggested that individuals may play a more active role in starting to smoke than has previously been thought and that social processes other than peer pressure also need to be taken into account.[73] Another study's results indicated that peer pressure was significantly associated with smoking behavior across all age and gender cohorts, but that intrapersonal factors were significantly more important to the smoking behavior of 12- to 13-year-old girls than same-age boys. Within the 14- to 15-year-old age group, one peer pressure variable emerged as a significantly more important predictor of girls' than boys' smoking.[74] It is debated whether peer pressure or self-selection is a greater cause of adolescent smoking.
Psychologists such as Hans Eysenck have developed a personality profile for the typical smoker. Extraversion is the trait that is most associated with smoking, and smokers tend to be sociable, impulsive, risk taking, and excitement seeking individuals.[75] Although personality and social factors may make people likely to smoke, the actual habit is a function of operant conditioning. During the early stages, smoking provides pleasurable sensations (because of its action on the dopamine system) and thus serves as a source of positive reinforcement.
The reasons given by some smokers for this activity have been categorized as addictive smoking, pleasure from smoking, tension reduction/relaxation, social smoking, stimulation, habit/automatism, and handling. There are gender differences in how much each of these reasons contribute, with females more likely than males to cite tension reduction/relaxation, stimulation and social smoking.[76]
Some smokers argue that the depressant effect of smoking allows them to calm their nerves, often allowing for increased concentration. However, according to the Imperial College London, "Nicotine seems to provide both a stimulant and a depressant effect, and it is likely that the effect it has at any time is determined by the mood of the user, the environment and the circumstances of use. Studies have suggested that low doses have a depressant effect, while higher doses have stimulant effect."[77]
A number of studies have established that cigarette sales and smoking follow distinct time-related patterns. For example, cigarette sales in the United States of America have been shown to follow a strongly seasonal pattern, with the high months being the months of summer, and the low months being the winter months.[78]
Similarly, smoking has been shown to follow distinct circadian patterns during the waking day—with the high point usually occurring shortly after waking in the morning, and shortly before going to sleep at night.[79]
In countries where there is a universally funded healthcare system, the government covers the cost of medical care for smokers who become ill through smoking in the form of increased taxes. Two broad debating positions exist on this front, the "pro-smoking" argument suggesting that heavy smokers generally don't live long enough to develop the costly and chronic illnesses which affect the elderly, reducing society's healthcare burden, and the "anti-smoking" argument suggests that the healthcare burden is increased because smokers get chronic illnesses younger and at a higher rate than the general population.  Data on both positions has been contested. The Centers for Disease Control and Prevention published research in 2002 claiming that the cost of each pack of cigarettes sold in the United States was more than $7 in medical care and lost productivity.[80] The cost may be higher, with another study putting it as high as $41 per pack, most of which however is on the individual and his/her family.[81] This is how one author of that study puts it when he explains the very low cost for others: "The reason the number is low is that for private pensions, Social Security, and Medicare — the biggest factors in calculating costs to society — smoking actually saves money. Smokers die at a younger age and don't draw on the funds they've paid into those systems."[81] Other research demonstrates that premature death caused by smoking may redistribute Social Security income in unexpected ways that affect behavior and reduce the economic well-being of smokers and their dependents.[82] To further support this, whatever the rate of smoking  consumption  is per day, smokers have a greater lifetime medical cost on average compared to a non smoker by an estimated $6000 [83] Between the cost for lost productivity and health care expenditures combined, cigarette smoking costs at least 193 billion dollars (Research also shows that smokers earn less money than nonsmokers[84]). As for secondhand smoke, the cost is over 10 billion dollars.[85]
By contrast, some non-scientific studies, including one conducted by Philip Morris in the Czech Republic called Public Finance Balance of Smoking in the Czech Republic[86] and another by the Cato Institute,[87] support the opposite position. Philip Morris has explicitly apologised for the former study, saying: "The funding and public release of this study which, among other things, detailed purported cost savings to the Czech Republic due to premature deaths of smokers, exhibited terrible judgment as well as a complete and unacceptable disregard of basic human values. For one of our tobacco companies to commission this study was not just a terrible mistake, it was wrong. All of us at Philip Morris, no matter where we work, are extremely sorry for this. No one benefits from the very real, serious and significant diseases caused by smoking."[86]
Between 1970 and 1995, per-capita cigarette consumption in poorer developing countries increased by 67 percent, while it dropped by 10 percent in the richer developed world. Eighty percent of smokers now live in less developed countries. By 2030, the World Health Organization (WHO) forecasts that 10 million people a year will die of smoking-related illness, making it the single biggest cause of death worldwide, with the largest increase to be among women. WHO forecasts the 21st century's death rate from smoking to be ten times the 20th century's rate ("Washingtonian" magazine, December 2007).
Cigarette smoking is the leading cause of preventable death and a major public health concern.[88]
There are 1.1 billion tobacco users in the world. One person dies every six seconds from a tobacco related disease.[89]
Tobacco use leads most commonly to diseases affecting the heart and lungs, with smoking being a major risk factor for heart attacks, strokes, chronic obstructive pulmonary disease (COPD), Idiopathic Pulmonary Fibrosis (IPF), emphysema, and cancer (particularly lung cancer, cancers of the larynx and mouth, esophageal cancer and pancreatic cancer).[16] Cigarette smoking increases the risk of Crohn's disease as well as the severity of the course of the disease.[91] It is also the number one cause of bladder cancer. The smoke from tobacco elicits carcinogenic effects on the tissues of the body that are exposed to the smoke.[92]
Tobacco smoke is a complex mixture of over 5,000 identified chemicals, of which 98 are known to have specific toxicological properties.[16][93] The most important chemicals causing cancer are those that produce DNA damage since such damage appears to be the primary underlying cause of cancer.[94][95]  Cunningham et al.[96] combined the microgram weight of the compound in the smoke of one cigarette with the known genotoxic effect per microgram to identify the most carcinogenic compounds in cigarette smoke.  The seven most important carcinogens in tobacco smoke are shown in the table, along with DNA alterations they cause. 
Tobacco smoke can combine with other carcinogens present within the environment in order to produce elevated degrees of lung cancer.
Cigarette smoking has also been associated with sarcopenia, the age-related loss of muscle mass and strength.[104]
The World Health Organization estimates that tobacco caused 5.4 million deaths in 2004[105] and 100 million deaths over the course of the 20th century.[106] Similarly, the United States Centers for Disease Control and Prevention describes tobacco use as "the single most important preventable risk to human health in developed countries and an important cause of premature death worldwide."[107] Although 70% of smokers state their intention to quit only 3-5% are actually successful in doing so.[83]
The probabilities of death from lung cancer before age 75 in the United Kingdom are 0.2% for men who never smoked (0.4% for women), 5.5% for male former smokers (2.6% in women), 15.9% for current male smokers (9.5% for women) and 24.4% for male “heavy smokers” defined as smoking more than 5 cigarettes per day (18.5% for women).[108] Tobacco smoke can combine with other carcinogens present within the environment in order to produce elevated degrees of lung cancer.
Rates of smoking have generally leveled-off or declined in the developed world. Smoking rates in the United States have dropped by half from 1965 to 2006, falling from 42% to 20.8% in adults.[109] In the developing world, tobacco consumption is rising by 3.4% per year.[110]
Second-hand smoke presents a known health risk, to which six hundred thousand deaths were attributed in 2004. It also has been known to produce skin conditions such as freckles and dryness.[111]
In 2015, a meta-analysis found that smokers were at greater risk of developing psychotic illness.[112] Tobacco has also been described an anaphrodisiac due to its propensity for causing erectile dysfunction.[113]
Famous smokers of the past used cigarettes or pipes as part of their image, such as Jean-Paul Sartre's Gauloises-brand cigarettes; Albert Einstein's, Douglas MacArthur's, Bertrand Russell's, and Bing Crosby's pipes; or the news broadcaster Edward R. Murrow's cigarette. Writers in particular seem to be known for smoking, for example, Cornell Professor Richard Klein's book Cigarettes are Sublime for the analysis, by this professor of French literature, of the role smoking plays in 19th and 20th century letters. The popular author Kurt Vonnegut addressed his addiction to cigarettes within his novels. British Prime Minister Harold Wilson was well known for smoking a pipe in public as was Winston Churchill for his cigars. Sherlock Holmes, the fictional detective created by Sir Arthur Conan Doyle smoked a pipe, cigarettes, and cigars. The DC Vertigo comic book character, John Constantine, created by Alan Moore, is synonymous with smoking, so much so that the first storyline by Preacher creator, Garth Ennis, centered around John Constantine contracting lung cancer. Professional wrestler James Fullington, while in character as "The Sandman", is a chronic smoker in order to appear "tough".
The problem of smoking at home is particularly difficult for women in many cultures (especially Arab cultures), where it may not be acceptable for a woman to ask her husband not to smoke at home or in the presence of her children. Studies have shown that pollution levels for smoking areas indoors are higher than levels found on busy roadways, in closed motor garages, and during fire storms.[clarification needed] Furthermore, smoke can spread from one room to another, even if doors to the smoking area are closed.[114]
The ceremonial smoking of tobacco, and praying with a sacred pipe, is a prominent part of the religious ceremonies of a number of Native American Nations. Sema, the Anishinaabe word for tobacco, is grown for ceremonial use and considered the ultimate sacred plant since its smoke is believed to carry prayers to the spirits. In most major religions, however, tobacco smoking is not specifically prohibited, although it may be discouraged as an immoral habit. Before the health risks of smoking were identified through controlled study, smoking was considered an immoral habit by certain Christian preachers and social reformers. The founder of the Latter Day Saint movement, Joseph Smith, recorded that on 27 February 1833, he received a revelation which discouraged tobacco use. This "Word of Wisdom" was later accepted as a commandment, and faithful Latter-day Saints abstain completely from tobacco.[115] Jehovah's Witnesses base their stand against smoking on the Bible's command to "clean ourselves of every defilement of flesh" (2 Corinthians 7:1). The Jewish Rabbi Yisrael Meir Kagan (1838–1933) was one of the first Jewish authorities to speak out on smoking. In Ahmadiyya Islam, smoking is highly discouraged, although not forbidden. During the month of fasting however, it is forbidden to smoke tobacco.[116] In the Bahá'í Faith, smoking tobacco is discouraged though not forbidden.[117]
One of the largest global enterprises in the world is known to be the tobacco industry. The six biggest tobacco companies made a combined profit of $35.1 billion (Jha et al., 2014) in 2010.[118] Tobacco smoking causes millions of deaths globally each year. According to The Tobacco Atlas, the use of tobacco has led 6 million deaths in 2011, 80% of these deaths occurred in low and middle-income countries.[119] Research has shown that there are many negative effects of smoking; some of these factors are health, social and psychological factors which can harm the life of a person.
On 27 February 2005 the WHO Framework Convention on Tobacco Control, took effect. The FCTC is the world's first public health treaty. Countries that sign on as parties agree to a set of common goals, minimum standards for tobacco control policy, and to cooperate in dealing with cross-border challenges such as cigarette smuggling. Currently the WHO declares that 4 billion people will be covered by the treaty, which includes 168 signatories.[120] Among other steps, signatories are to put together legislation that will eliminate secondhand smoke in indoor workplaces, public transport, indoor public places and, as appropriate, other public places.
Many governments have introduced excise taxes on cigarettes in order to reduce the consumption of cigarettes.
In 2002, the Centers for Disease Control and Prevention said that each pack of cigarettes[quantify] sold in the United States costs the nation more than $7 in medical care and lost productivity,[80] around $3400 per year per smoker. Another study by a team of health economists finds the combined price paid by their families and society is about $41 per pack of cigarettes.[121]
Substantial scientific evidence shows that higher cigarette prices result in lower overall cigarette consumption. Most studies indicate that a 10% increase in price will reduce overall cigarette consumption by 3% to 5%. Youth, minorities, and low-income smokers are two to three times more likely to quit or smoke less than other smokers in response to price increases.[122][123] Smoking is often cited[citation needed] as an example of an inelastic good, however, i.e. a large rise in price will only result in a small decrease in consumption.
Many nations have implemented some form of tobacco taxation. As of 1997, Denmark had the highest cigarette tax burden of $4.02 per pack. Taiwan only had a tax burden of $0.62 per pack. The federal government of the United States charges $1.01 per pack.[124]
Cigarette taxes vary widely from state to state in the United States. For example, Missouri has a cigarette tax of only 17 cents per pack, the nation's lowest, while New York has the highest cigarette tax in the U.S.: $4.35 per pack. In Alabama, Illinois, Missouri, New York City, Tennessee, and Virginia, counties and cities may impose an additional limited tax on the price of cigarettes.[125] Sales taxes are also levied on tobacco products in most jurisdictions.
In the United Kingdom, a packet of 20 cigarettes typically costs between £8.00 to £12.00 according to 2018 prices, depending on the brand purchased and where the purchase was made.[126] The UK has a significant black market for tobacco, and it has been estimated by the tobacco industry that 27% of cigarette and 68% of handrolling tobacco consumption is non-UK duty paid (NUKDP).[127]
In Australia total taxes account for 62.5% of the final price of a packet of cigarettes (2011 figures). These taxes include federal excise or customs duty and  Goods and Services Tax.[128]
In June 1967, the US Federal Communications Commission ruled that programmes broadcast on a television station which discussed smoking and health were insufficient to offset the effects of paid advertisements that were broadcast for five to ten minutes each day. In April 1970, the US Congress passed the Public Health Cigarette Smoking Act banning the advertising of cigarettes on television and radio starting on 2 January 1971.[129]
The Tobacco Advertising Prohibition Act 1992 expressly prohibited almost all forms of Tobacco advertising in Australia, including the sponsorship of sporting or other cultural events by cigarette brands.
All tobacco advertising and sponsorship on television has been banned within the European Union since 1991 under the Television Without Frontiers Directive (1989).[130] This ban was extended by the Tobacco Advertising Directive, which took effect in July 2005 to cover other forms of media such as the internet, print media, and radio. The directive does not include advertising in cinemas and on billboards or using merchandising – or tobacco sponsorship of cultural and sporting events which are purely local, with participants coming from only one Member State[131] as these fall outside the jurisdiction of the European Commission. However, most member states have transposed the directive with national laws that are wider in scope than the directive and cover local advertising. A 2008 European Commission report concluded that the directive had been successfully transposed into national law in all EU member states, and that these laws were well implemented.[132]
Some countries also impose legal requirements on the packaging of tobacco products. For example, in the countries of the European Union, Turkey, Australia[133] and South Africa, cigarette packs must be prominently labeled with the health risks associated with smoking.[134] Canada, Australia, Thailand, Iceland and Brazil have also imposed labels upon cigarette packs warning smokers of the effects, and they include graphic images of the potential health effects of smoking. Cards are also inserted into cigarette packs in Canada. There are sixteen of them, and only one comes in a pack. They explain different methods of quitting smoking. Also, in the United Kingdom, there have been a number of graphic NHS advertisements, one showing a cigarette filled with fatty deposits, as if the cigarette is symbolizing the artery of a smoker.
Many countries have a smoking age. In many countries, including the United States, most European Union member states, New Zealand, Canada, South Africa, Israel, India,[16] Brazil, Chile, Costa Rica and Australia, it is illegal to sell tobacco products to minors and in the Netherlands, Austria, Belgium, Denmark and South Africa it is illegal to sell tobacco products to people under the age of 16. On 1 September 2007 the minimum age to buy tobacco products in Germany rose from 16 to 18, as well as in the United Kingdom where on 1 October 2007 it rose from 16 to 18.[135] Underlying such laws is the belief that people should make an informed decision regarding the risks of tobacco use. These laws have a lax enforcement in some nations and states. In China, Turkey, and many other countries usually a child will have little problem buying tobacco products, because they are often told to go to the store to buy tobacco for their parents.
Several countries such as Ireland, Latvia, Estonia, the Netherlands, Finland, Norway, Canada, Australia, Sweden, Portugal, Singapore, Italy, Indonesia, India, Lithuania, Chile, Spain, Iceland, United Kingdom, Slovenia, Turkey and Malta have legislated against smoking in public places, often including bars and restaurants. Restaurateurs have been permitted in some jurisdictions to build designated smoking areas (or to prohibit smoking). In the United States, many states prohibit smoking in restaurants, and some also prohibit smoking in bars. In provinces of Canada, smoking is illegal in indoor workplaces and public places, including bars and restaurants. As of 31 March 2008 Canada has introduced a smoke-free law ban in all public places, as well as within 10 metres of an entrance to any public place. In Australia, smoke-free laws vary from state to state. Currently, Queensland has completely smoke-free indoor public places (including workplaces, bars, pubs and eateries) as well as patrolled beaches and some outdoor public areas. There are, however, exceptions for designated smoking areas. In Victoria, smoking is restricted in railway stations, bus stops and tram stops as these are public locations where second-hand smoke can affect non-smokers waiting for public transport, and since 1 July 2007 is now extended to all indoor public places. In New Zealand and Brazil, smoking is restricted in enclosed public places including bars, restaurants and pubs. Hong Kong restricted smoking on 1 January 2007 in the workplace, public spaces such as restaurants, karaoke rooms, buildings, and public parks (bars which do not admit minors were exempt until 2009). In Romania smoking is illegal in trains, metro stations, public institutions (except where designated, usually outside) and public transport.
In Germany, additionally to smoking bans in public buildings and transports, an anti-smoking ordinance for bars and restaurants was implemented in late 2007. A study by the University of Hamburg (Ahlfeldt and Maennig 2010) demonstrates, that the smoking ban had, if any, only short run impacts on bar and restaurant revenues. In the medium and long run no negative effect was measurable. The results suggest either, that the consumption in bars and restaurants is not affected by smoking bans in the long run, or, that negative revenue impacts by smokers are compensated by increasing revenues through non-smokers.[136]
An indirect public health problem posed by cigarettes is that of accidental fires, usually linked with consumption of alcohol. Enhanced combustion using nitrates was traditionally used but cigarette manufacturers have been silent on this subject claiming at first that a safe cigarette was technically impossible, then that it could only be achieved by modifying the paper. Roll your own cigarettes contain no additives and are fire safe. Numerous fire safe cigarette designs have been proposed, some by tobacco companies themselves, which would extinguish a cigarette left unattended for more than a minute or two, thereby reducing the risk of fire. Among American tobacco companies, some have resisted this idea, while others have embraced it. RJ Reynolds was a leader in making prototypes of these cigarettes in 1983[137] and will make all of their U.S. market cigarettes to be fire-safe by 2010.[138] Phillip Morris is not in active support of it.[139] Lorillard (purchased by RJ Reynolds), the US' 3rd-largest tobacco company, seems to be ambivalent.[139]
The relationship between tobacco and other drug use has been well-established, however the nature of this association remains unclear. The two main theories are the phenotypic causation (gateway) model and the correlated liabilities model. The causation model argues that smoking is a primary influence on future drug use,[140] while the correlated liabilities model argues that smoking and other drug use are predicated on genetic or environmental factors.[141]
Smoking cessation, referred to as "quitting", is the action leading towards abstinence of tobacco smoking. Methods of "quitting" include advice from physicians or social workers,[16] cold turkey, nicotine replacement therapy, contingent vouchers,[142] antidepressants, hypnosis, self-help (mindfulness meditation),[143] and support groups. A meta-analysis from 2018, conducted on 61 RCT, showed that one year after people quit smoking with the assistance of first‐line smoking cessation medications (and some behavioral help), only a little under 20% of smokers remained sustained abstinence.[144]



Alcohol (drug) - Wikipedia

Alcohol, also known by its chemical name ethanol, is a psychoactive substance that is the active ingredient in drinks such as beer, wine, and distilled spirits (hard liquor).[13] It is one of the oldest and most common recreational substances, causing the characteristic effects of alcohol intoxication ("drunkenness").[14] Among other effects, alcohol produces a mood lift and euphoria, decreased anxiety, increased sociability, sedation, impairment of cognitive, memory, motor, and sensory function, and generalized depression of central nervous system function. Ethanol is a type of chemical compound known as an alcohol, and is the only type of alcohol that is found in alcoholic beverages or is commonly used for recreational purposes; other alcohols such as methanol and isopropyl alcohol are toxic.[13]
Alcohol has a variety of short-term and long-term adverse effects. Short-term adverse effects include generalized impairment of neurocognitive function, dizziness, nausea, vomiting, and hangover-like symptoms. Alcohol can be addictive to humans, as in alcoholism, and can result in dependence and withdrawal. It can have a variety of long-term adverse effects on health, for instance liver damage,[15] brain damage,[16][17] and increased risk of cancer.[18] The adverse effects of alcohol on health are most important when it is used in excessive quantities or with heavy frequency. However, some of them, such as increased risk of certain cancers, may occur even with light or moderate alcohol consumption.[19] In high amounts, alcohol may cause loss of consciousness or, in severe cases, death.
Alcohol works in the brain primarily by increasing the effects of a neurotransmitter called γ-aminobutyric acid, or GABA.[20] This is the major inhibitory neurotransmitter in the brain, and by facilitating its actions, alcohol suppresses the activity of the central nervous system.[20] The substance also directly affects a number of other neurotransmitter systems including those of glutamate, glycine, acetylcholine, and serotonin.[21][22] The pleasurable effects of alcohol ingestion are the result of increased levels of dopamine and endogenous opioids in the reward pathways of the brain.[23][24] Alcohol also has toxic and unpleasant actions in the body, many of which are mediated by its byproduct acetaldehyde.[25]
Alcohol has been produced and consumed by humans for its psychoactive effects for almost 10,000 years.[26] Drinking alcohol is generally socially acceptable and is legal in most countries, unlike with many other recreational substances. However, there are often restrictions on alcohol sale and use, for instance a minimum age for drinking and laws against public drinking and drinking and driving.[27] Alcohol has considerable societal and cultural significance and has important social roles in much of the world. Drinking establishments, such as bars and nightclubs, revolve primarily around the sale and consumption of alcoholic beverages, and parties, festivals, and social gatherings commonly feature alcohol consumption as well. Alcohol use is also related to various societal problems, including driving accidents and fatalities, accidental injuries, sexual assaults, domestic abuse, and violent crime.[28] Currently, alcohol is illegal for sale and consumption in Iran, Libya, Sudan, Saudi Arabia, Kuwait, Afghanistan, Somalia, Yemen, and Bangladesh.
Ethanol is typically consumed as a recreational substance by mouth in the form of alcoholic beverages such as beer, wine, and spirits. It is commonly used in social settings due to its capacity to enhance sociability.
The amount of ethanol in the body is typically quantified by blood alcohol content (BAC); weight of ethanol per unit volume of blood. Small doses of ethanol, in general, are stimulant-like[29] and produce euphoria and relaxation; people experiencing these symptoms tend to become talkative and less inhibited, and may exhibit poor judgement. At higher dosages (BAC > 1 g/L), ethanol acts as a central nervous system depressant,[29] producing at progressively higher dosages, impaired sensory and motor function, slowed cognition, stupefaction, unconsciousness, and possible death. Ethanol is commonly consumed as a recreational substance, especially while socializing, due to its psychoactive effects.
Ethanol, alcohol increases levels of high-density lipoproteins (HDLs), which carry cholesterol through the blood.[30][better source needed] Alcohol is known to make blood less likely to clot, reducing risk of heart attack and stroke. This could be the reason that alcohol produces health benefits when consumed in moderate amounts.[31][better source needed] Also, alcohol dilates blood vessels. Consequently, a person feels warmer, and their skin may flush and appear pink.[30][better source needed]
Ethanol is a source of energy and pure ethanol provides 7 calories per gram. For distilled spirits, a standard serving in the United States is 44 ml (1.5 US fl oz), which at 40% ethanol (80 proof), would be 14 grams and 98 calories. Wine and beer contain a similar range of ethanol for servings of 150 ml (5 US fl oz) and 350 ml (12 US fl oz), respectively, but these beverages also contain non-ethanol calories. A 150 ml serving of wine contains 100 to 130 calories. A 350 ml serving of beer contains 95 to 200 calories. According to the U.S. Department of Agriculture, based on NHANES 2013–2014 surveys, women in the US ages 20 and up consume on average 6.8 grams/day and men consume on average 15.5 grams/day.[32] Ignoring the non-alcohol contribution of those beverages, the average ethanol calorie contributions are 48 and 108 cal/day, respectively. Alcoholic beverages are considered empty calorie foods because other than calories, these contribute no essential nutrients.
Alcohol has a variety of short-term and long-term adverse effects. It also has reinforcement-related adverse effects, including addiction, dependence, and withdrawal.
Alcohol causes generalized central nervous system depression and associated cognitive, memory, motor, and sensory impairment. It slows and impairs cognition and reaction time, reduces inhibition and impairs judgement, interferes with motor function resulting in motor incoordination, loss of balance, and slurred speech, impairs memory formation, and causes sensory impairment. At high concentrations, amnesia, stupor, and unconsciousness result.
At very high concentrations, markedly decreased heart rate, respiratory depression, and death can result due to profound suppression of central nervous system function and consequent shutdown of vegetative functions.
Alcohol can cause nausea and vomiting in sufficiently high amounts (varies by person).
Alcohol stimulates gastric juice production, even when food is not present, and as a result, its consumption stimulates acidic secretions normally intended to digest protein molecules. Consequently, the excess acidity may harm the inner lining of the stomach. The stomach lining is normally protected by a mucosal layer that prevents the stomach from, essentially, digesting itself. However, in patients who have a peptic ulcer disease (PUD), this mucosal layer is broken down. PUD is commonly associated with the bacteria H. pylori. H. pylori secrete a toxin that weakens the mucosal wall, which as a result lead to acid and protein enzymes penetrating the weakened barrier. Because alcohol stimulates a person's stomach to secrete acid, a person with PUD should avoid drinking alcohol on an empty stomach. Drinking alcohol causes more acid release, which further damages the already-weakened stomach wall.[34] Complications of this disease could include a burning pain in the abdomen, bloating and in severe cases, the presence of dark black stools indicate internal bleeding.[35] A person who drinks alcohol regularly is strongly advised to reduce their intake to prevent PUD aggravation.[35]
Ingestion of alcohol can initiate systemic pro-inflammatory changes through two intestinal routes: (1) altering intestinal microbiota composition (dysbiosis), which increases lipopolysaccharide (LPS) release, and (2) degrading intestinal mucosal barrier integrity – thus allowing this (LPS) to enter the circulatory system. The major portion of the blood supply to the liver is provided by the portal vein. Therefore, while the liver is continuously fed nutrients from the intestine, it is also exposed to any bacteria and/or bacterial derivatives that breach the intestinal mucosal barrier. Consequently, LPS levels increase in the portal vein, liver and systemic circulation after alcohol intake. Immune cells in the liver respond to LPS with the production of reactive oxygen species (ROS), leukotrienes, chemokines and cytokines. These factors promote tissue inflammation and contribute to organ pathology.[36]
Ethanol-containing beverages can cause urticarial skin eruptions, systemic dermatitis, alcohol flush reactions, exacerbations of rhinitis and, more seriously and commonly, bronchoconstriction in patients with a history of asthma. These reactions occur within 1–60 minutes of ethanol ingestion and are due to: 1) genetic abnormalities in the metabolism of ethanol, which cause the ethanol metabolite, acetaldehyde, to accumulate in tissues and trigger the release of histamine, the evoker of these symptoms; 2) true allergy reactions to allergens occurring naturally in, or contaminating, alcoholic beverages, particularly wines and beers, and 3) unknown causes.[37]
Prolonged heavy consumption of alcohol can cause significant permanent damage to the brain and other organs.
Alcohol can cause permanent brain damage. An extreme example is Wernicke–Korsakoff syndrome.
During the metabolism of alcohol via the respective dehydrogenases, NAD (nicotinamide adenine dinucleotide) is converted into reduced NAD. Normally, NAD is used to metabolize fats in the liver, and as such alcohol competes with these fats for the use of NAD. Prolonged exposure to alcohol means that fats accumulate in the liver, leading to the term 'fatty liver'. Continued consumption (such as in alcoholism) then leads to cell death in the hepatocytes as the fat stores reduce the function of the cell to the point of death. These cells are then replaced with scar tissue, leading to the condition called cirrhosis.
Ethanol is classified as a teratogen.[medical citation needed] According to the CDC, alcohol consumption by women of child-bearing age who are not using birth control increases the risk of fetal alcohol syndrome. The CDC currently recommends complete abstinence from alcoholic beverages.[38]
IARC list ethanol in alcoholic beverages as Group 1 carcinogens and argues that "There is sufficient evidence for the carcinogenicity of acetaldehyde (the major metabolite of ethanol) in experimental animals."[39]
Frequent drinking of alcoholic beverages is a major contributing factor in cases of elevated blood levels of triglycerides.[40]
Alcohol consumption is rewarding and reinforcing and can result in addiction to alcohol, which is termed alcoholism.
Discontinuation of alcohol after extended heavy use and associated tolerance development (resulting in dependence) can result in withdrawal. Alcohol withdrawal can cause confusion, anxiety, insomnia, agitation, tremors, fever, nausea, vomiting, autonomic dysfunction, seizures, and hallucinations. In severe cases, death can result. Delirium tremens is a condition that requires people with a long history of heavy drinking to undertake an alcohol detoxification regimen.
Death from ethanol consumption is possible when blood alcohol levels reach 0.4%. A blood level of 0.5% or more is commonly fatal. Levels of even less than 0.1% can cause intoxication, with unconsciousness often occurring at 0.3–0.4%.[41]
The oral median lethal dose (LD50) of ethanol in rats is 5,628 mg/kg. Directly translated to human beings, this would mean that if a person who weighs 70 kg (150 lb) drank a 500 mL (17 US fl oz) glass of pure ethanol, they would theoretically have a 50% chance of dying. Symptoms of ethanol overdose may include nausea, vomiting, central nervous system depression, coma, acute respiratory failure, or death.
Alcohol can intensify the sedation caused by other central nervous system depressants such as barbiturates, benzodiazepines, opioids, nonbenzodiazepines/Z-drugs (such as zolpidem and zopiclone), antipsychotics, sedative antihistamines, and certain antidepressants.[41] It interacts with cocaine in vivo to produce cocaethylene, another psychoactive substance.[42] Ethanol enhances the bioavailability of methylphenidate (elevated plasma dexmethylphenidate).[43] In combination with cannabis, ethanol increases plasma tetrahydrocannabinol levels, which suggests that ethanol may increase the absorption of tetrahydrocannabinol.[44]
Disulfiram inhibits the enzyme acetaldehyde dehydrogenase, which in turn results in buildup of acetaldehyde, a toxic metabolite of ethanol with unpleasant effects. The medication is used to treat alcoholism, and results in immediate hangover-like symptoms upon consumption of alcohol.
One of the most important drug/food interactions that should be noted is between alcohol and metronidazole.
Metronidazole is an antibacterial agent that kills bacteria by damaging cellular DNA and hence cellular function.[45] Metronidazole is usually given to people who have diarrhea caused by Clostridium difficile bacteria. C. difficile is one of the most common microorganisms that cause diarrhea and can lead to complications such as colon inflammation and even more severely, death.
Patients who are taking metronidazole are strongly advised to avoid alcohol, even after 1 hour following the last dose. The reason is that alcohol and metronidazole can lead to side effects such as flushing, headache, nausea, vomiting, abdominal cramps, and sweating.[46][47] These symptoms are often called the disulfiram-like reaction. The proposed mechanism of action for this interaction is that metronidazole can bind to an enzyme that normally metabolizes alcohol. Binding to this enzyme may impair the liver's ability to process alcohol for proper excretion.[48]
The rate-limiting steps for the elimination of ethanol are in common with certain other substances. As a result, the blood alcohol concentration can be used to modify the rate of metabolism of methanol and ethylene glycol. Methanol itself is not highly toxic, but its metabolites formaldehyde and formic acid are; therefore, to reduce the rate of production and concentration of these harmful metabolites, ethanol can be ingested.[49] Ethylene glycol poisoning can be treated in the same way.
Despite extensive research, the precise mechanism of action of ethanol has proven elusive and remains not fully understood.[20][50] Identifying molecular targets for ethanol has proven unusually difficult, in large part due to its unique biochemical properties.[50] Specifically, ethanol is a very low molecular weight compound and is of exceptionally low potency in its actions, causing effects only at very high (millimolar (mM)) concentrations.[50][51] For these reasons, unlike with most drugs, it has not yet been possible to employ traditional biochemical techniques to directly assess the binding of ethanol to receptors or ion channels.[50][51] Instead, researchers have had to rely on functional studies to elucidate the actions of ethanol.[50] Moreover, although it has been established that ethanol modulates ion channels to mediate its effects,[22] ion channels are complex proteins, and their interactions and functions are complicated by diverse subunit compositions.[20][50]
In spite of the preceding however, much progress has been made in understanding the pharmacodynamics of ethanol over the last few decades.[21][50] While no binding sites have been identified and established unambiguously for ethanol at present, it appears that it affects ion channels, in particular ligand-gated ion channels, to mediate its effects in the central nervous system.[20][21][22][50] Ethanol has specifically been found in functional assays to enhance or inhibit the activity of a variety of ion channels, including the GABAA receptor, the ionotropic glutamate AMPA, kainate, and NMDA receptors, the glycine receptor,[52] the nicotinic acetylcholine receptors,[53] the serotonin 5-HT3 receptor, voltage-gated calcium channels, and BK channels, among others.[20][21][22][54][55] However, many of these actions have been found to occur only at very high concentrations that may not be pharmacologically significant at recreational doses of ethanol, and it is unclear how or to what extent each of the individual actions is involved in the effects of ethanol.[50] In any case, ethanol has long shown a similarity in its effects to positive allosteric modulators of the GABAA receptor like benzodiazepines, barbiturates, and various general anesthetics.[20][50] Indeed, ethanol has been found to enhance GABAA receptor-mediated currents in functional assays.[20][50] In accordance, it is theorized and widely believed that the primary mechanism of action is as a GABAA receptor positive allosteric modulator.[20][50] However, the diverse actions of ethanol on other ion channels may be and indeed likely are involved in its effects as well.[21][50]
In 2007, it was discovered that ethanol directly binds to and potentiates extrasynaptic δ subunit-containing GABAA receptors at behaviorally relevant (as low as 3 mM) concentrations.[20][50][56] This is in contrast to previous functional assays of ethanol on γ subunit-containing GABAA receptors, which it enhances only at far higher concentrations (> 100 mM) that are in excess of recreational concentrations (up to 50 mM).[20][50][57] Ro15-4513, a close analogue of the benzodiazepine antagonist flumazenil (Ro15-1788), has been found to bind to the same site as ethanol and to competitively displace it in a saturable manner.[50][56] In addition, Ro15-4513 blocked the enhancement of δ subunit-containing GABAA receptor currents by ethanol in vitro.[50] In accordance, the drug has been found to reverse many of the behavioral effects of low-to-moderate doses of ethanol in rodents, including its effects on anxiety, memory, motor behavior, and self-administration.[50][56] Taken together, these findings suggest a binding site for ethanol on subpopulations of the GABAA receptor with specific subunit compositions via which it interacts with and potentiates the receptor.[20][50][56][58]
The reinforcing effects of alcohol consumption are mediated by acetaldehyde generated by catalase and other oxidizing enzymes such as cytochrome P-4502E1 in the brain.[59] Although acetaldehyde has been associated with some of the adverse and toxic effects of ethanol, it appears to play a central role in the activation of the mesolimbic dopamine system.[60]
Ethanol's rewarding and reinforcing (i.e., addictive) properties are mediated through its effects on dopamine neurons in the mesolimbic reward pathway, which connects the ventral tegmental area to the nucleus accumbens (NAcc).[61][62] One of ethanol's primary effects is the allosteric inhibition of NMDA receptors and facilitation of GABAA receptors (e.g., enhanced GABAA receptor-mediated chloride flux through allosteric regulation of the receptor).[63] At high doses, ethanol inhibits most ligand-gated ion channels and voltage-gated ion channels in neurons as well.[63]
With acute alcohol consumption, dopamine is released in the synapses of the mesolimbic pathway, in turn heightening activation of postsynaptic D1 receptors.[61][62] The activation of these receptors triggers postsynaptic internal signaling events through protein kinase A, which ultimately phosphorylate cAMP response element binding protein (CREB), inducing CREB-mediated changes in gene expression.[61][62]
With chronic alcohol intake, consumption of ethanol similarly induces CREB phosphorylation through the D1 receptor pathway, but it also alters NMDA receptor function through phosphorylation mechanisms;[61][62] an adaptive downregulation of the D1 receptor pathway and CREB function occurs as well.[61][62] Chronic consumption is also associated with an effect on CREB phosphorylation and function via postsynaptic NMDA receptor signaling cascades through a MAPK/ERK pathway and CAMK-mediated pathway.[62] These modifications to CREB function in the mesolimbic pathway induce expression (i.e., increase gene expression) of ΔFosB in the NAcc,[62] where ΔFosB is the "master control protein" that, when overexpressed in the NAcc, is necessary and sufficient for the development and maintenance of an addictive state (i.e., its overexpression in the nucleus accumbens produces and then directly modulates compulsive alcohol consumption).[62][64][65][66]
Recreational concentrations of ethanol are typically in the range of 1 to 50 mM.[57][20] Very low concentrations of 1 to 2 mM ethanol produce zero or undetectable effects except in alcohol-naive individuals.[57] Slightly higher levels of 5 to 10 mM, which are associated with light social drinking, produce measurable effects including changes in visual acuity, decreased anxiety, and modest behavioral disinhibition.[57] Further higher levels of 15 to 20 mM result in a degree of sedation and motor incoordination that is contraindicated with the operation of motor vehicles.[57] In jurisdictions in the United States, maximum blood alcohol levels for legal driving are about 17 to 22 mM.[68][69] In the upper range of recreational ethanol concentrations of 20 to 50 mM, depression of the central nervous system is more marked, with effects including complete drunkenness, profound sedation, amnesia, emesis, hypnosis, and eventually unconsciousness.[57][68] Levels of ethanol above 50 mM are not typically experienced by normal individuals and hence are not usually physiologically relevant; however, such levels – ranging from 50 to 100 mM – may be experienced by alcoholics with high tolerance to ethanol.[57] Concentrations above this range, specifically in the range of 100 to 200 mM, would cause death in all people except alcoholics.[57]
Ethanol has been reported to possess the following actions in functional assays at varying concentrations:[51]
Some of the actions of ethanol on ligand-gated ion channels, specifically the nicotinic acetylcholine receptors and the glycine receptor, are dose-dependent, with potentiation or inhibition occurring dependent on ethanol concentration.[51] This seems to be because the effects of ethanol on these channels are a summation of positive and negative allosteric modulatory actions.[51]
Ethanol can be taken orally, by inhalation, rectally, or by injection (e.g., intravenous),[5][73] though it is typically ingested simply via oral administration.[7] The oral bioavailability of ethanol is around 80% or more.[7][8] In fasting volunteers, blood levels of ethanol increase proportionally with the dose of ethanol administered.[73] Blood alcohol concentrations may be estimated by dividing the amount of ethanol ingested by the body weight of the individual and correcting for water dilution.[5] Peak circulating levels of ethanol are usually reached within a range of 30 to 90 minutes of ingestion, with an average of 45 to 60 minutes.[5][7]
Food in the gastrointestinal system and hence gastric emptying is the most important factor that influences the absorption of orally ingested ethanol.[5][73] The absorption of ethanol is much more rapid on an empty stomach than with a full one.[5] The delay in ethanol absorption caused by food is similar regardless of whether food is consumed just before, at the same time, or just after ingestion of ethanol.[5] The type of food, whether fat, carbohydrates, or protein, also is of little importance.[73] Not only does food slow the absorption of ethanol, but it also reduces the bioavailability of ethanol, resulting in lower circulating concentrations.[5] People who have fasted overnight have been found to reach peak ethanol concentrations more rapidly, at within 30 minutes of ingestion.[5]
Upon ingestion, ethanol is rapidly distributed throughout the body.[7] It is distributed most rapidly to tissues with the greatest blood supply.[5] As such, ethanol primarily affects the brain, liver, and kidneys.[7] Other tissues with lower circulation, such as bone, require more time for ethanol to distribute into.[5] Ethanol crosses biological membranes and the blood–brain barrier easily, through a simple process of passive diffusion.[7][73] The volume of distribution of ethanol is around .55 L/kg (0.53 US pt/lb).[7] It is only weakly or not at all plasma protein bound.[7][8]
Approximately 90% of the metabolism of ethanol occurs in the liver.[5][6] This occurs predominantly via the enzyme alcohol dehydrogenase, which transforms ethanol into its metabolite acetaldehyde (ethanal).[5][6] Acetaldehyde is subsequently metabolized by the enzyme aldehyde dehydrogenase into acetate (ethanoate), which in turn is broken down into carbon dioxide and water.[5] Acetate also combines with coenzyme A to form acetyl-CoA, and hence may participate in metabolic pathways.[7] Alcohol dehydrogenase and aldehyde dehydrogenase are present at their highest concentrations in the liver, but are widely expressed throughout the body, and alcohol dehydrogenase may also be present in the stomach and small intestine.[7] Aside from alcohol dehydrogenase, the microsomal ethanol-oxidizing system (MEOS), specifically mediated by the cytochrome P450 enzyme CYP2E1, is the other major route of ethanol metabolism.[5][6] CYP2E1 is inducible by ethanol, so while alcohol dehydrogenase handles acute or low concentrations of ethanol, MEOS is predominant with higher concentrations or with repeated/chronic use.[5][6] A small amount of ethanol undergoes conjugation to form ethyl glucuronide and ethyl sulfate.[7] There may also be another metabolic pathway that metabolizes as much as 25 to 35% of ethanol at typical concentrations.[8]
At even low physiological concentrations, ethanol completely saturates alcohol dehydrogenase.[5] This is because ethanol has high affinity for the enzyme and very high concentrations of ethanol occur when it is used as a recreational substance.[5] For this reason, the metabolism of ethanol follows zero-order kinetics at typical physiological concentrations.[6] That is, ethanol does not have an elimination half-life (i.e., is not metabolized at an exponential rate), and instead, is eliminated from the circulation at a constant rate.[6][9] The mean elimination rates for ethanol are 15 mg/dL per hour for men and 18 mg/dL per hour for women, with a range of 10 to 34 mg/dL per hour.[6][5] At very high concentrations, such as in overdose, it has been found that the rate of elimination of ethanol is increased.[8] In addition, ethanol metabolism follows first-order kinetics at very high concentrations, with an elimination half-life of about 4 or 4.5 hours (which implies a clearance rate of approximately 6 L/hour/70 kg).[8][7] This seems to be because other processes, such as the MEOS/CYP2E1, also become involved in the metabolism of ethanol at higher concentrations.[7] However, the MEOS/CYP2E1 alone does not appear sufficient to fully explain the increase in ethanol metabolism rate.[8]
Some individuals have less effective forms of one or both of the metabolizing enzymes of ethanol, and can experience more marked symptoms from ethanol consumption than others.[74] However, those having acquired alcohol tolerance have a greater quantity of these enzymes, and metabolize ethanol more rapidly.[74]
Ethanol is mainly eliminated from the body via metabolism into carbon dioxide and water.[5] Around 5 to 10% of ethanol that is ingested is eliminated unchanged in urine, breath, and sweat.[7] Ethanol or its metabolites may be detectable in urine for up to 96 hours after ingestion.[7]
Ethanol is also known chemically as alcohol, ethyl alcohol, or drinking alcohol. It is a simple alcohol with a molecular formula of C2H6O and a molecular weight of 46.0684 g/mol. The molecular formula of ethanol may also be written as CH3−CH2−OH or as C2H5−OH. The latter can also be thought of as an ethyl group linked to a hydroxyl (alcohol) group and can be abbreviated as EtOH. Ethanol is a volatile, flammable, colorless liquid with a slight characteristic odor. Aside from its use as a psychoactive and recreational substance, ethanol is also commonly used as an antiseptic and disinfectant, a chemical and medicinal solvent, and a fuel.
Ethanol is produced naturally as a byproduct of the metabolic processes of yeast and hence is present in any yeast habitat, including even endogenously in humans. It is manufactured as a petrochemical through hydration of ethylene or by brewing via fermentation of sugars with yeast (most commonly Saccharomyces cerevisiae). In the case of the latter, the sugars are commonly obtained from sources like steeped cereal grains (e.g., barley), grape juice, and sugarcane products (e.g., molasses, sugarcane juice). Petrochemical and yeast manufacturing routes both produce an ethanol–water mixture which can be further purified via distillation.
Ethanol has a variety of analogues, many of which have similar actions and effects. Methanol (methyl alcohol) and isopropyl alcohol are toxic and are not safe for human consumption.[13] Methanol is the most toxic alcohol; the toxicity of isopropyl alcohol lies between that of ethanol and methanol, and is about twice that of ethanol.[75] In general, higher alcohols are less toxic.[75] n-Butanol is reported to produce similar effects to those of ethanol and relatively low toxicity (one-sixth of that of ethanol in one rat study).[76][77] However, its vapors can produce eye irritation and inhalation can cause pulmonary edema.[75] Acetone (propanone) is a ketone rather than an alcohol, and is reported to produce similar toxic effects; it can be extremely damaging to the cornea.[75]
The tertiary alcohol tert-amyl alcohol (TAA), also known as 2-methylbutan-2-ol (2M2B), has a history of use as a hypnotic and anesthetic, as do other tertiary alcohols such as methylpentynol, ethchlorvynol, and chloralodol. Unlike primary alcohols like ethanol, these tertiary alcohols cannot be oxidized into aldehyde or carboxylic acid metabolites, which are often toxic, and for this reason, these compounds are safer in comparison.[78] Other relatives of ethanol with similar effects include chloral hydrate, paraldehyde, and many volatile and inhalational anesthetics (e.g., chloroform, diethyl ether, and isoflurane).
Alcohol was brewed as early as 7,000 to 6,650 BCE in northern China.[26] The earliest evidence of winemaking was dated at 6,000 to 5,800 BCE in Georgia in the South Caucasus.[79] Beer was likely brewed from barley as early as the 6th century BCE (600–500 BCE) in Egypt.[80] Pliny the Elder wrote about the golden age of winemaking in Rome, the 2nd century BCE (200–100 BCE), when vineyards were planted.[81]
Alcohol is legal in most of the world.[82] However, laws banning alcohol are found in the Middle East and some Indian states as well as some Native American reservations in the United States.[82] In addition, there are strict regulations on alcohol sales and use in many countries throughout the world.[82] For instance, most countries have a minimum legal age for purchase and consumption of alcohol (e.g., 21 years of age in the United States).[82] Also, many countries have bans on public drinking.[82] Drinking while driving or intoxicated driving is frequently outlawed and it may be illegal to have an open container of alcohol in an automobile.[82]
Alcohol causes a plethora of detrimental effects in society, both to the individual and to others.[28] It is highly associated with automobile accidents, sexual assaults, and both violent and non-violent crime.[28] About one-third of arrests in the United States involve alcohol abuse.[28] Many emergency room visits also involve alcohol use.[28] As many as 15% of employees show problematic alcohol-related behaviors in the workplace, such as drinking before going to work or even drinking on the job.[28] Heavy drinking is associated with vulnerability to injury, marital discord, and domestic violence.[28] Alcohol use is directly related to considerable morbidity and mortality, for instance due to overdose and alcohol-related health problems.[83]
A 2002 study found 41% of people fatally injured in traffic accidents were in alcohol-related crashes.[84] Abuse of alcohol is associated with more than 40% of deaths that occur in automobile accidents every year.[28] The risk of a fatal car accident increases exponentially with the level of alcohol in the driver's blood.[85] Most drunk driving laws in the United States governing the acceptable levels in the blood while driving or operating heavy machinery set typical upper limits of legal blood alcohol content (BAC) at 0.08%.[86]
Alcohol is often used to facilitate sexual assault or rape.[87][88] Over 50% of all rapes involve alcohol.[28] It is the most commonly used date rape drug.[89]
Over 40% of all assaults and 40 to 50% of all murders involve alcohol.[28] More than 43% of violent encounters with police involve alcohol.[28] Alcohol is implicated in more than two-thirds of cases of intimate partner violence.[28] In 2002, it was estimated that 1 million violent crimes in the United States were related to alcohol use.[28] Alcohol is more commonly associated with both violent and non-violent crime than are drugs like marijuana.[28]
Alcohol abuse and dependence are major problems and many health problems as well as death can result from excessive alcohol use.[28][83] Alcohol dependence is linked to a lifespan that is reduced by about 12 years relative to the average person.[28] In 2004, it was estimated that 4% of deaths worldwide were attributable to alcohol use.[83] Deaths from alcohol are split about evenly between acute causes (e.g., overdose, accidents) and chronic conditions.[83] The leading chronic alcohol-related condition associated with death is alcoholic liver disease.[83] Alcohol dependence is also associated with cognitive impairment and organic brain damage.[28] Some researchers have found that even one alcoholic drink a day increases an individual's risk of health problems.[90]



Cell growth - Wikipedia
The term cell growth is used in the contexts of biological cell development and cell division (reproduction). When used in the context of cell development, the term refers to increase in cytoplasmic and organelle volume (G1 phase), as well as increase in genetic material (G2 phase) following the replication during S phase.[1]
This is not to be confused with growth in the context of cell division, referred to as proliferation, where a cell, known as the "mother cell", grows and divides to produce two "daughter cells" (M phase). 
Cell populations go through a particular type of exponential growth called doubling. Thus, each generation of cells should be twice as numerous as the previous generation. However, the number of generations only gives a maximum figure as not all cells survive in each generation.
Cell size is highly variable among organisms, with some algae such as Caulerpa taxifolia being a single cell several meters in length.[2] Plant cells are much larger than animal cells, and protists such as Paramecium can be 330 μm long, while a typical human cell might be 10 μm. How these cells "decide" how big they should be before dividing is an open question. Chemical gradients are known to be partly responsible, and it is hypothesized that mechanical stress detection by cytoskeletal structures is involved. Work on the topic generally requires an organism whose cell cycle is well-characterized.
The relationship between cell size and cell division has been extensively studied in yeast. For some cells, there is a mechanism by which cell division is not initiated until a cell has reached a certain size. If the nutrient supply is restricted (after time t = 2 in the diagram, below), and the rate of increase in cell size is slowed, the time period between cell divisions is increased.[3] Yeast cell-size mutants were isolated that begin cell division before reaching a normal/regular size (wee mutants).[4]
Wee1 protein is a tyrosine kinase that normally phosphorylates the Cdc2 cell cycle regulatory protein (the homolog of CDK1 in humans), a cyclin-dependent kinase, on a tyrosine residue.  Cdc2 drives entry into mitosis by phosphorylating a wide range of targets.  This covalent  modification of the molecular structure of Cdc2 inhibits the enzymatic activity of Cdc2 and prevents cell division. Wee1 acts to keep Cdc2 inactive during early G2 when cells are still small.  When cells have reached sufficient size during G2, the phosphatase Cdc25 removes the inhibitory phosphorylation, and thus activates Cdc2 to allow mitotic entry.  A balance of Wee1 and Cdc25 activity with changes in cell size is coordinated by the mitotic entry control system.  It has been shown in Wee1 mutants, cells with weakened Wee1 activity, that Cdc2 becomes active when the cell is smaller.  Thus, mitosis occurs before the yeast reach their normal size.  This suggests that cell division may be regulated in part by dilution of Wee1 protein in cells as they grow larger.
The protein kinase Cdr2 (which negatively regulates Wee1) and the Cdr2-related kinase Cdr1 (which directly phosphorylates and inhibits Wee1 in vitro)[5] are localized to a band of cortical nodes in the middle of interphase cells.  After entry into mitosis, cytokinesis factors such as myosin II are recruited to similar nodes; these nodes eventually condense to form the cytokinetic ring.[6]  A previously uncharacterized protein, Blt1, was found to colocalize with Cdr2 in the medial interphase nodes.  Blt1 knockout cells had increased length at division, which is consistent with a delay in mitotic entry.  This finding connects a physical location, a band of cortical nodes, with factors that have been shown to directly regulate mitotic entry, namely Cdr1, Cdr2, and Blt1.
Further experimentation with GFP-tagged proteins and mutant proteins indicates that the medial cortical nodes are formed by the ordered, Cdr2-dependent assembly of multiple interacting proteins during interphase.  Cdr2 is at the top of this hierarchy and works upstream of Cdr1 and Blt1.[7]  Mitosis is promoted by the negative regulation of Wee1 by Cdr2.  It has also been shown that Cdr2 recruits Wee1 to the medial cortical node.  The mechanism of this recruitment has yet to be discovered.  A Cdr2 kinase mutant, which is able to localize properly despite a loss of function in phosphorylation, disrupts the recruitment of Wee1 to the medial cortex and delays entry into mitosis.  Thus, Wee1 localizes with its inhibitory network, which demonstrates that mitosis is controlled through Cdr2-dependent negative regulation of Wee1 at the medial cortical nodes.[7]
Cell polarity factors positioned at the cell tips provide spatial cues to limit Cdr2 distribution to the cell middle.  In fission yeast Schizosaccharomyces pombe (S. Pombe), cells divide at a defined, reproducible size during mitosis because of the regulated activity of Cdk1.[8]  The cell polarity protein kinase Pom1, a member of the dual-specificity tyrosine-phosphorylation regulated kinase (DYRK) family of kinases, localizes to cell ends.  In Pom1 knockout cells, Cdr2 was no longer restricted to the cell middle, but was seen diffusely through half of the cell.  From this data it becomes apparent that Pom1 provides inhibitory signals that confine Cdr2 to the middle of the cell.  It has been further shown that Pom1-dependent signals lead to the phosphorylation of Cdr2.  Pom1 knockout cells were also shown to divide at a smaller size than wild-type, which indicates a premature entry into mitosis.[7]
Pom1 forms polar gradients that peak at cell ends, which shows a direct link between size control factors and a specific physical location in the cell.[9]  As a cell grows in size, a gradient in Pom1 grows.  When cells are small, Pom1 is spread diffusely throughout the cell body.  As the cell increases in size, Pom1 concentration decreases in the middle and becomes concentrated at cell ends.  Small cells in early G2 which contain sufficient levels of Pom1 in the entirety of the cell have inactive Cdr2 and cannot enter mitosis.  It is not until the cells grow into late G2, when Pom1 is confined to the cell ends that Cdr2 in the medial cortical nodes is activated and able to start the inhibition of Wee1.  This finding shows how cell size plays a direct role in regulating the start of mitosis.  In this model, Pom1 acts as a molecular link between cell growth and mitotic entry through a Cdr2-Cdr1-Wee1-Cdk1 pathway.[7]  The Pom1 polar gradient successfully relays information about cell size and geometry to the Cdk1 regulatory system.  Through this gradient, the cell ensures it has reached a defined, sufficient size to enter mitosis.
Many different types of eukaryotic cells undergo size-dependent transitions during the cell cycle. These transitions are controlled by the cyclin-dependent kinase Cdk1.[10]  Though the proteins that control Cdk1 are well understood, their connection to mechanisms monitoring cell size remains elusive.
A postulated model for mammalian size control situates mass as the driving force of the cell cycle.  A cell is unable to grow to an abnormally large size because at a certain cell size or cell mass, the S phase is initiated.  The S phase starts the sequence of events leading to mitosis and cytokinesis.  A cell is unable to get too small because the later cell cycle events, such as S, G2, and M, are delayed until mass increases sufficiently to begin S phase.[11]
Many of the signal molecules that convey information to cells during the control of cellular differentiation or growth are called growth factors. The protein mTOR is a serine/threonine kinase that regulates
translation and cell division.[12] Nutrient availability influences mTOR
so that when cells are not able to grow to normal size they will not
undergo cell division.
The details of the molecular mechanisms of mammalian cell size control
are currently being investigated. The size of post-mitotic neurons
depends on the size of the cell body, axon and dendrites. In
vertebrates, neuron size is often a reflection of the number of
synaptic contacts onto the neuron or from a neuron onto other cells.
For example, the size of motoneurons usually reflects the size of
the motor unit that is controlled by the motoneuron.
Invertebrates often have giant neurons and axons that provide
special functions such as rapid action potential propagation.
Mammals also use this trick for increasing the speed of signals in the
nervous system, but they can also use myelin to accomplish this, so
most human neurons are relatively small cells.
One common means to produce very large cells is by cell fusion to form syncytia. For example, very long (several inches) skeletal muscle cells are formed by fusion of thousands of myocytes. Genetic studies of the fruit fly Drosophila have revealed several genes that are required for the formation of multinucleated muscle cells by fusion of myoblasts.[13] Some of the key proteins are important for cell adhesion between myocytes and some are involved in adhesion-dependent cell-to-cell signal transduction that allows for a cascade of cell fusion events.
Oocytes can be unusually large cells in species for which embryonic development takes place away from the mother's body. Their large size can be achieved either by pumping in cytosolic components from adjacent cells through cytoplasmic bridges (Drosophila) or by internalization of nutrient storage granules (yolk granules) by endocytosis (frogs).
Increases in the size of plant cells are complicated by the fact that almost all plant cells are inside of a solid cell wall. Under the influence of certain plant hormones the cell wall can be remodeled, allowing for increases in cell size that are important for the growth of some plant tissues.
Most unicellular organisms are microscopic in size, but there are some giant bacteria and protozoa that are visible to the naked eye. See: Table of cell sizes —Dense populations of a giant sulfur bacterium in Namibian shelf sediments[14]— Large protists of the genus Chaos, closely related to the genus Amoeba
In the rod-shaped bacteria E. coli, Caulobacter crescentus and B. subtilis cell size is controlled by a simple mechanisms in which cell division occurs after a constant volume has been added since the previous division.[15][16] By always growing by the same amount, cells born smaller or larger than average naturally converge to an average size equivalent to the amount added during each generation.
Cell reproduction is asexual. For most of the constituents of the cell, growth is a steady, continuous process, interrupted only briefly at M phase when the nucleus and then the cell divide in two.
The process of cell division, called cell cycle, has four major parts called phases. The first part, called G1 phase is marked by synthesis of various enzymes that are required for DNA replication.
The second part of the cell cycle is the S phase, where DNA replication produces two identical sets of  chromosomes. The third part is the G2 phase in which a significant protein synthesis occurs, mainly involving the production of microtubules that are required during the process of division, called mitosis.
The fourth phase, M phase, consists of nuclear division (karyokinesis) and cytoplasmic division (cytokinesis), accompanied by the formation of a new cell membrane. This is the physical division of "mother" and "daughter" cells. The M phase has been broken down into several distinct phases, sequentially known as prophase, prometaphase, metaphase, anaphase and telophase leading to cytokinesis.
Cell division is more complex in eukaryotes than in other organisms. Prokaryotic cells such as bacterial cells reproduce by binary fission, a process that includes DNA replication, chromosome segregation, and cytokinesis. Eukaryotic cell division either involves mitosis or a more complex process called meiosis. Mitosis and meiosis are sometimes called the two "nuclear division" processes. Binary fission is similar to eukaryote cell reproduction that involves mitosis. Both lead to the production of two daughter cells with the same number of chromosomes as the parental cell. Meiosis is used for a special cell reproduction process of diploid organisms. It produces four special daughter cells (gametes) which have half the normal cellular amount of DNA. A male and a female gamete can then combine to produce a zygote, a cell which again has the normal amount of chromosomes.
The rest of this article is a comparison of the main features of the three types of cell reproduction that either involve binary fission, mitosis, or meiosis. The diagram below depicts the similarities and differences of these three types of cell reproduction.
The DNA content of a cell is duplicated at the start of the cell reproduction process. Prior to DNA replication, the DNA content of a cell can be represented as the amount Z (the cell has Z chromosomes). After the DNA replication process, the amount of DNA in the cell is 2Z (multiplication: 2 x Z = 2Z). During Binary fission and mitosis the duplicated DNA content of the reproducing parental cell is separated into two equal halves that are destined to end up in the two daughter cells. The final part of the cell reproduction process is cell division, when  daughter cells physically split apart from a parental cell. During meiosis, there are two cell division steps that together produce the four daughter cells.
After the completion of binary fission or cell reproduction involving mitosis, each daughter cell has the same amount of DNA (Z) as what the parental cell had before it replicated its DNA. These two types of cell reproduction produced two daughter cells that have the same number of chromosomes as the parental cell. Chromosomes duplicate prior to cell division when forming new skin cells for reproduction. After meiotic cell reproduction the four daughter cells have half the number of chromosomes that the parental cell originally had. This is the haploid amount of DNA, often symbolized as N. Meiosis is used by diploid organisms to produce haploid gametes. In a diploid organism such as the human organism, most cells of  the body have the diploid amount of DNA, 2N. Using this notation for counting chromosomes we say that human somatic cells have 46 chromosomes (2N = 46) while human sperm and eggs have 23 chromosomes (N = 23). Humans have 23 distinct types of chromosomes, the 22 autosomes and the special category of sex chromosomes. There are two distinct sex chromosomes, the X chromosome and the Y chromosome. A diploid human cell has 23 chromosomes from that person's father and 23 from the mother. That is, your body has two copies of human chromosome number 2, one from each of your parents.
Immediately after DNA replication a human cell will have 46 "double chromosomes". In each double chromosome there are two copies of that chromosome's DNA molecule. During mitosis the double chromosomes are split to produce 92 "single chromosomes", half of which go into each daughter cell. During meiosis, there are two chromosome separation steps which assure that each of the four daughter cells gets one copy of each of the 23 types of chromosome.
Though cell reproduction that uses mitosis can reproduce eukaryotic cells, eukaryotes bother with the more complicated process of meiosis because sexual reproduction such as meiosis confers a selective advantage. Notice that when meiosis starts, the two copies of sister chromatids number 2 are adjacent to each other. During this time, there can be genetic recombination events. Parts of the chromosome 2 DNA gained from one parent (red) will swap over to the chromosome 2 DNA molecule that received from the other parent (green). Notice that in mitosis the two copies of chromosome number 2 do not interact. It is these new combinations of parts of chromosomes that provide the major advantage for sexually reproducing organisms by allowing for new combinations of genes and more efficient evolution.
However, in organisms with more than one set of chromosomes at the main life cycle stage, sex may also provide an advantage because, under random mating, it produces homozygotes and heterozygotes according to the Hardy-Weinberg ratio.
A series of growth disorders can occur at the cellular level and these consequently underpin much of the subsequent course in cancer, in which a group of cells display uncontrolled growth and division beyond the normal limits, invasion (intrusion on and destruction of adjacent tissues), and sometimes metastasis (spread to other locations in the body via lymph or blood).
The cell growth can be detected by a variety of methods.
The cell size growth can be visualized by microscopy, using suitable stains. But the increase of cells number is usually more significant. It can be measured by manual counting of cells under microscopy observation, using the dye exclusion method (i.e. trypan blue) to count only viable cells. Less fastidious, scalable, methods include the use of cytometers, while flow cytometry allows combining cell counts ('events') with other specific parameters: fluorescent probes for membranes, cytoplasm or nuclei allow distinguishing dead/viable cells, cell types, cell differentiation, expression of a biomarker such as Ki67.
Beside the increasing number of cells, one can be assessed regarding the metabolic activity growth, that is, the CFDA and calcein-AM measure (fluorimetrically) not only the membrane functionality (dye retention), but also the functionality of cytoplasmic enzymes (esterases). The MTT assays (colorimetric) and the resazurin assay (fluorimetric) dose the mitochondrial redox potential.
All these assays may correlate well, or not, depending on cell growth conditions and desired aspects (activity, proliferation). The task is even more complicated with populations of different cells, furthermore when combining cell growth interferences or toxicity.
Image result for cell growth
Cell growth (or interphase) is shorthand for the idea of "growth in cell populations" by means of cell reproduction. It is the stage which cells are preparing for the next division, biochemical activities and reactions are taking place, however no obvious changes can be seen at this stage.



The Hallmarks of Cancer - Wikipedia

"The Hallmarks of Cancer"[1] is a seminal[2][3] peer-reviewed article published in the journal Cell in January 2000 by the cancer researchers Douglas Hanahan and Robert Weinberg.
The authors believe that the complexity of cancer can be reduced to a small number of underlying principles. The paper argues that all cancers share six common traits ("hallmarks") that govern the transformation of normal cells to cancer (malignant or tumor) cells.
The traits ("hallmarks") that the authors highlight in the paper are (1) Cancer cells stimulate their own growth (self-sufficiency in growth signals); (2) They resist inhibitory signals that might otherwise stop their growth (insensitivity to anti-growth signals); (3) They resist their programmed cell death  (evading apoptosis); (4) They can multiply indefinitely (limitless replicative potential) (5) They stimulate the growth of blood vessels to supply nutrients to tumors (sustained angiogenesis); (6) They invade local tissue and spread to distant sites (tissue invasion and metastasis).
By November 2010, the paper had been referenced over 15,000 times by other research papers, and was downloaded 20,000 times a year between 2004 and 2007.[4][self-published source?]  As of March 2011, it was Cell's most cited article.[2]
In an update published in 2011 ("Hallmarks of cancer: the next generation"), Weinberg and Hanahan proposed four new hallmarks: (1) abnormal metabolic pathways, (2) evading the immune system, (3) genome instability, and (4) inflammation.[5]
Cancer cells have defects in the control mechanisms that govern how often they divide, and in the feedback systems that regulate these control mechanisms (i.e. defects in homeostasis).
Normal cells grow and divide, but have many controls on that growth. They only grow when stimulated by growth factors. If they are damaged, a molecular brake stops them from dividing until they are repaired. If they can't be repaired, they commit programmed cell death (apoptosis). They can only divide a limited number of times. They are part of a tissue structure, and remain where they belong. They need a blood supply to grow.
All these mechanisms must be overcome in order for a cell to develop into a cancer. Each mechanism is controlled by several proteins. A critical protein must malfunction in each of those mechanisms. These proteins become non-functional or malfunctioning when the DNA sequence of their genes is damaged through acquired or somatic mutations (mutations that are not inherited but occur after conception). This occurs in a series of steps, which Hanahan and Weinberg refer to as hallmarks.
Typically, cells of the body require hormones and other molecules that act as signals for them to grow and divide. Cancer cells, however, have the ability to grow without these external signals. There are multiple ways in which cancer cells can do this: by producing these signals themselves, known as autocrine signalling; by permanently activating the signalling pathways that respond to these signals; or by destroying 'off switches' that prevents excessive growth from these signals (negative feedback). In addition, cell division in normal, non-cancerous cells is tightly controlled. In cancer cells, these processes are deregulated because the proteins that control them are altered, leading to increased growth and cell division within the tumor.[7][8]
To tightly control cell division, cells have processes within them that prevent cell growth and division. These processes are orchestrated by proteins known as tumor suppressor genes. These genes take information from the cell to ensure that it is ready to divide, and will halt division if not (when the DNA is damaged, for example). In cancer, these tumour suppressor proteins are altered so that they don't effectively prevent cell division, even when the cell has severe abnormalities. Another way cells prevent over-division is that normal cells will also stop dividing when the cells fill up the space they are in and touch other cells; known as contact inhibition. Cancer cells do not have contact inhibition, and so will continue to grow and divide, regardless of their surroundings.[7][9]
Cells have the ability to 'self-destruct'; a process known as apoptosis. This is required for organisms to grow and develop properly, for maintaining tissues of the body, and is also initiated when a cell is damaged or infected. Cancer cells, however, lose this ability; even though cells may become grossly abnormal, they do not apoptose. The cancer cells may do this by altering the mechanisms that detect the damage or abnormalities. This means that proper signalling cannot occur, thus apoptosis cannot activate. They may also have defects in the downstream signalling itself, or the proteins involved in apoptosis, each of which will also prevent proper apoptosis.[7][10]
Cells of the body don't normally have the ability to divide indefinitely. They have a limited number of divisions before the cells become unable to divide (senescence), or die (crisis). The cause of these barriers is primarily due to the DNA at the end of chromosomes, known as telomeres. Telomeric DNA shortens with every cell division, until it becomes so short it activates senescence, so the cell stops dividing. Cancer cells bypass this barrier by manipulating enzymes that increase the length of telomeres. Thus, they can divide indefinitely, without initiating senescence.[7][11]
Mammalian cells have an intrinsic program, the Hayflick limit, that limits their multiplication to about 60–70 doublings, at which point they reach a stage of senescence.
This limit can be overcome by disabling their pRB and p53 tumor suppressor proteins, which allows them to continue doubling until they reach a stage called crisis, with apoptosis, karyotypic disarray, and the occasional (10−7) emergence of an immortalized cell that can double without limit. Most tumor cells are immortalized.
The counting device for cell doublings is the telomere, which decreases in size (loses nucleotides at the ends of chromosomes) during each cell cycle. About 85% of cancers upregulate telomerase to extend their telomeres and the remaining 15% use a method called the Alternative Lengthening of Telomeres.[12]
Normal tissues of the body have blood vessels running through them that deliver oxygen from the lungs. Cells must be close to the blood vessels to get enough oxygen for them to survive. New blood vessels are formed during the development of embryos, during wound repair and during the female reproductive cycle.  An expanding tumour requires new blood vessels to deliver adequate oxygen to the cancer cells, and thus exploits these normal physiological processes for its benefit. To do this, the cancer cells acquire the ability to orchestrate production of new vasculature by activating the 'angiogenic switch'. In doing so, they control non-cancerous cells that are present in the tumor that can form blood vessels by reducing the production of factors that inhibit blood vessel production, and increasing the production of factors that promote blood vessel formation.[7][13]
One of the most well known properties of cancer cells is their ability to invade neighboring tissues. It is what dictates whether the tumor is benign or malignant, and is the reason for their dissemination around the body. The cancer cells have to undergo a multitude of changes in order for them to acquire the ability to metastasize. It is a multistep process that starts with local invasion of the cells into the surrounding tissues. They then have to invade blood vessels, survive in the harsh environment of the circulatory system, exit this system and then start dividing in the new tissue.[7][14]
In his 2010 NCRI conference talk, Hanahan proposed four new hallmarks. These were later codified in an updated review article entitled "Hallmarks of cancer: the next generation."[5]
Most cancer cells use abnormal metabolic pathways to generate energy, a fact appreciated since the early twentieth century with the postulation of the Warburg hypothesis,[15] but only now gaining renewed research interest.[16] Cancer cells exhibiting the Warburg effect upregulate glycolysis and lactic acid fermentation in the cytosol and prevent mitochondria from completing normal aerobic respiration (oxidation of pyruvate, the citric acid cycle, and the electron transport chain). Instead of completely oxidizing glucose to produce as much ATP as possible, cancer cells would rather convert pyruvate into the building blocks for more cells. In fact, the low ATP:ADP ratio caused by this effect likely contributes to the deactivation of mitochondria. Mitochondrial membrane potential is hyperpolarized to prevent voltage-sensitive permeability transition pores (PTP) from triggering of apoptosis.[17][18]
The ketogenic diet is being investigated as an adjuvant therapy for some cancers,[19][20] including glioma,[21] because of cancer’s inefficiency in metabolizing ketone bodies.
Despite cancer cells causing increased inflammation and angiogenesis, they also appear to be able to avoid interaction with the body’s via a loss of interleukin-33 immune system. (See cancer immunology)
Cancer cells generally have severe chromosomal abnormalities which worsen as the disease progresses. HeLa cells, for example, are extremely prolific and have tetraploidy 12, trisomy 6, 8, and 17, and a modal chromosome number of 82 (rather than the normal diploid number of 46).[22] Small genetic mutations are most likely what begin tumorigenesis, but once cells begin the breakage-fusion-bridge (BFB) cycle, they are able to mutate at much faster rates. (See genome instability)
Recent discoveries have highlighted the role of local chronic inflammation in inducing many types of cancer. Inflammation leads to angiogenesis and more of an immune response. The degradation of extracellular matrix necessary to form new blood vessels increases the odds of metastasis. (See inflammation in cancer)
An article in Nature Reviews Cancer in 2010 pointed out that five of the 'hallmarks' were also characteristic of benign tumours.[23] The only hallmark of malignant disease was its ability to invade and metastasize.[23]
An article in the Journal of Biosciences in 2013 argued that original data for most of these hallmarks is lacking.[24]  It argued that cancer is a tissue-level disease and these cellular-level hallmarks are misleading.



Ulcer (dermatology) - Wikipedia
An ulcer is a sore on the skin or a mucous membrane, accompanied by the disintegration of tissue. Ulcers can result in complete loss of the epidermis and often portions of the dermis and even subcutaneous fat. Ulcers are most common on the skin of the lower extremities and in the gastrointestinal tract. An ulcer that appears on the skin is often visible as an inflamed tissue with an area of reddened skin. A skin ulcer is often visible in the event of exposure to heat or cold, irritation, or a problem with blood circulation. They can also be caused due to a lack of mobility, which causes prolonged pressure on the tissues. This stress in the blood circulation is transformed to a skin ulcer, commonly known as bedsores or decubitus ulcers.[1] Ulcers often become infected, and pus forms.
Skin ulcers appear as open craters, often round, with layers of skin that have eroded. The skin around the ulcer may be red, swollen, and tender. Patients may feel pain on the skin around the ulcer, and fluid may ooze from the ulcer. In some cases, ulcers can bleed and, rarely, patients experience fever. Ulcers sometimes seem not to heal; healing, if it does occur, tends to be slow. Ulcers that heal within 12 weeks are usually classified as acute, and longer-lasting ones as chronic.
Ulcers develop in stages. In stage 1 the skin is red with soft underlying tissue. In the second stage the redness of the skin becomes more pronounced, swelling appears, and there may be some blisters and loss of outer skin layers. During the next stage, the skin may become necrotic down through the deep layers of skin, and the fat beneath the skin may become exposed and visible. In stage 4, deeper necrosis usually occurs, the fat underneath the skin is completely exposed, and the muscle may also become exposed. In the last two stages the sore may cause a deeper loss of fat and necrosis of the muscle; in severe cases it can extend down to bone level, destruction of the bone may begin, and there may be sepsis of joints.
Chronic ulcers may be painful. Most patients complain of constant pain at night and during the day. Chronic ulcer symptoms usually include increasing pain, friable granulation tissue, foul odour, and wound breakdown instead of healing.[2] Symptoms tend to worsen once the wound has become infected.
Venous skin ulcers that may appear on the lower leg, above the calf or on the lower ankle usually cause achy and swollen legs. If these ulcers become infected they may develop an unpleasant odour, increased tenderness and redness. Before the ulcer establishes definitively, there may be a dark red or purple skin over the affected area as well as a thickening, drying, and itchy skin.
Although skin ulcers do not seem of great concern at a first glance, they are worrying conditions especially in people suffering from diabetes, as they are at risk of developing diabetic neuropathy.
Ulcers may also appear on the cheeks, soft palate, the tongue, and on the inside of the lower lip. These ulcers usually last from 7 to 14 days and can be painful.[3]
Different types of discharges from ulcer are:[4]
The wounds from which ulcers arise can be caused by a wide variety of factors, but the main cause is impaired blood circulation. Especially, chronic wounds and ulcers are caused by poor circulation, either through cardiovascular issues or external pressure from a bed or a wheelchair.[5] A very common and dangerous type of skin ulcers are caused by what are called pressure-sensitive sores, more commonly called bed sores and which are frequent in people who are bedridden or who use wheelchairs for long periods. Other causes producing skin ulcers include bacterial or viral infections, fungal infections and cancers. Blood disorders and chronic wounds can result in skin ulcers as well.[6]
Venous leg ulcers due to impaired circulation or a blood flow disorder are more common in the elderly.
Wagner's grading of ulcer follows:[4]
Some of the investigations done for ulcer are:[4]:19
Skin ulcers may take a very long time to heal. Treatment is typically to avoid the ulcer getting infected, remove any excess discharge, maintain a moist wound environment, control the edema, and ease pain caused by nerve and tissue damage.
Topical antibiotics are normally used to prevent the ulcer getting infected, and the wound or ulcer is usually kept clear of dead tissue through surgical debridement.
Commonly, as a part of the treatment, patients are advised to change their lifestyle if possible and to change their diet. Improving the circulation is important in treating skin ulcers, and patients are consequently usually recommended to exercise, stop smoking, and lose weight.
In recent years, advances have been made in accelerating healing of chronic wounds and ulcers. Chronic wounds produce fewer growth hormones than necessary for healing tissue, and healing may be accelerated by replacing or stimulating growth factors while controlling the formation of other substances that work against them.[7]
Leg ulcers can be prevented by using compression stockings to prevent blood pooling and back flow. It is likely that a person who has had a skin ulcer will have it again; use of compression stockings every day for at least 5 years after the skin ulcer has healed may help to prevent recurrence.



Bronchus - Wikipedia
A bronchus, is a passage of airway in the respiratory system that conducts air into the lungs. The first bronchi to branch from the trachea are the right main bronchus and the left main bronchus. These are the widest and enter the lungs at each hilum, where they branch into narrower secondary bronchi known as lobar bronchi, and these branch into narrower tertiary bronchi known as segmental bronchi. Further divisions of the segmental bronchi are known as 4th order, 5th order, and 6th order segmental bronchi, or grouped together as subsegmental bronchi.[1][2]
The bronchi when too narrow to be supported by cartilage are known as bronchioles. No gas exchange takes place in the bronchi.
The trachea (windpipe) divides at the carina into two main or primary bronchi, the left bronchus and the right bronchus. The carina of the trachea is located at the level of the sternal angle  and the fifth thoracic vertebra (at rest).
The right main bronchus is wider, shorter, and more vertical than the left main bronchus.[3] It enters the right lung at approximately the fifth thoracic vertebra. The right main bronchus subdivides into three secondary bronchi (also known as lobar bronchi), which deliver oxygen to the three lobes of the right lung—the superior, middle and inferior lobe. The azygos vein arches over it from behind; and the right pulmonary artery lies at first below and then in front of it. About 2 cm from its commencement it gives off a branch to the superior lobe of the right lung, which is also called the eparterial bronchus.  Eparterial refers to its position above the right pulmonary artery. The right bronchus now passes below the artery, and is known as the hyparterial branch which divides into the two lobar bronchi to the middle and lower lobes.
The left main bronchus is smaller in caliber but longer than the right, being 5 cm long. It enters the root of the left lung opposite the sixth thoracic vertebra. It passes beneath the aortic arch, crosses in front of the esophagus, the thoracic duct, and the descending aorta, and has the left pulmonary artery lying at first above, and then in front of it. The left bronchus has no eparterial branch, and therefore it has been supposed by some that there is no upper lobe to the left lung, but that the so-called upper lobe corresponds to the middle lobe of the right lung. The left main bronchus divides into two secondary bronchi or lobar bronchi, to deliver air to the two lobes of the left lung—the superior and the inferior lobe.
The secondary bronchi divide further into tertiary bronchi, (also known as segmental bronchi), each of which supplies a bronchopulmonary segment.  A bronchopulmonary segment is a division of a lung separated from the rest of the lung by a septum of connective tissue. This property allows a bronchopulmonary segment to be surgically removed without affecting other segments. Initially, there are ten segments in each lung, but during development with the left lung having just two lobes, two pairs of segments fuse to give eight, four for each lobe. The tertiary bronchi divide further in another three branchings known as 4th order, 5th order and 6th order segmental bronchi which are also referred to as subsegmental bronchi. These branch into many smaller bronchioles which divide into terminal bronchioles, each of which then gives rise to several respiratory bronchioles, which go on to divide into two to eleven alveolar ducts. There are five or six alveolar sacs associated with each alveolar duct.  The alveolus is the basic anatomical unit of gas exchange in the lung.
The main bronchi have relatively large lumens that are lined by respiratory epithelium. This cellular lining has cilia departing towards the mouth which removes dust and other small particles. There is a smooth muscle layer below the epithelium arranged as two ribbons of muscle that spiral in opposite directions.  This smooth muscle layer contains seromucous glands, which secrete mucus, in its wall.  Hyaline cartilage is present in the bronchi, surrounding the smooth muscle layer. In the main bronchi, hyaline cartilage forms an incomplete ring, giving a "D"-shaped appearance, while in the smaller bronchi, hyaline cartilage is present in irregularly arranged plates and islands.  These plates give structural support to the bronchi and keep the airway open.
The bronchial wall normally has a thickness of 10% to 20% of the total bronchial diameter.[4]
The cartilage and mucous membrane of the primary bronchi are similar to those in the trachea. They are lined with respiratory epithelium, which is classified as ciliated pseudostratified columnar epithelium.[5] The epithelium in the main bronchi contains goblet cells,  which are glandular, modified simple columnar epithelial cells that produce mucins, the main component of mucus. Mucus plays an important role in keeping the airways clear in the mucociliary clearance process. As branching continues through the bronchial tree, the amount of hyaline cartilage in the walls decreases until it is absent in the bronchioles. As the cartilage decreases, the amount of smooth muscle increases. The mucous membrane also undergoes a transition from ciliated pseudostratified columnar epithelium to simple cuboidal epithelium to simple squamous epithelium.[5]
In 0.1 to 5% of people there is a right superior lobe bronchus arising from the main stem bronchus prior to the carina.  This is known as a tracheal bronchus, and seen as an anatomical variation. It can have multiple variations and, although usually asymptomatic, it can be the root cause of pulmonary disease such as a recurrent infection.  In such cases resection is often curative[6]
[7]
The cardiac bronchus has a prevalence of ≈0.3% and presents as an accessory bronchus arising from the bronchus intermedius between the upper lobar bronchus and the origin of the middle and lower lobar bronchi of the right main bronchus.[8] Accessory cardiac bronchus is usually an asymptomatic condition but may be associated with persistent infection or hemoptysis.[9][10] In about half of observed cases the cardiac bronchus presents as a short dead ending bronchial stump, in the remainder the bronchus may exhibit branching and associated aerated lung parenchyma.
The alveolar ducts and alveoli consist primarily of simple squamous epithelium, which permits rapid diffusion of oxygen and carbon dioxide. Exchange of gases between the air in the lungs and the blood in the capillaries occurs across the walls of the alveolar ducts and alveoli.
Bronchial wall thickening, as can be seen on CT scan, generally (but not always) implies inflammation of the bronchi.[11] Normally, the ratio of the bronchial wall thickness and the bronchial diameter is between 0.17 and 0.23.[12]
Bronchitis is defined as inflammation of the bronchi,  which can either be acute or chronic. Acute bronchitis is usually caused by viral or bacterial infections.  Most sufferers of chronic bronchitis also suffer from chronic obstructive pulmonary disease (COPD), and this is usually associated with smoking or long-term exposure to irritants.
The left main bronchus departs from the trachea at a greater angle than that of the right main bronchus. The right bronchus is also wider than the left and these differences predispose the right lung to aspirational problems. If food, liquids, or foreign bodies are aspirated, they will tend to lodge in the right main bronchus. Bacterial pneumonia and aspiration pneumonia may result.
If a tracheal tube used for intubation is inserted too far, it will usually lodge in the right bronchus, allowing ventilation only of the right lung.
Asthma is marked by hyperresponsiveness of the bronchi with an inflammatory component, often in response to allergens.
In asthma, the constriction of the bronchi can result in a difficulty in breathing giving shortness of breath; this can lead to a lack of oxygen reaching the body for cellular processes. In this case an asthma puffer (or inhaler) can be used to rectify the problem. The puffer administers a bronchodilator, which serves to soothe the constricted bronchi and to re-expand the airways. This effect occurs quite quickly.
Transverse section of thorax, showing relations of pulmonary artery
Illustration depicting respiratory system
Mainstem bronchi seen branching off the trachea (labeled at top), with the left mainstem bronchi passing below the aortic arch before branching into smaller bronchi.



Cachexia - Wikipedia
Cachexia, or wasting syndrome, is loss of weight, muscle atrophy, fatigue, weakness and significant loss of appetite in someone who is not actively trying to lose weight.
Cachexia is seen in people with cancer, AIDS,[1] coeliac disease,[2] chronic obstructive pulmonary disease, multiple sclerosis, rheumatoid arthritis, congestive heart failure, tuberculosis, familial amyloid polyneuropathy, mercury poisoning (acrodynia), Crohn's disease, untreated/severe type 1 diabetes mellitus, anorexia nervosa and hormonal deficiency.[medical citation needed]
It is a positive risk factor for death, meaning if the person has cachexia, the chance of death from the underlying condition is increased dramatically. It can be a sign of various underlying disorders; when a patient presents with cachexia, a doctor will generally consider the possibility of adverse drug reactions, cancer, metabolic acidosis, certain infectious diseases (e.g., tuberculosis, AIDS), chronic pancreatitis and some autoimmune disorders. Cachexia physically weakens patients to a state of immobility stemming from loss of appetite, asthenia and anemia, and response to standard treatment is usually poor.[3][4] Cachexia includes sarcopenia as a part of its pathology. The term is from Greek κακός kakos, "bad", and ἕξις hexis, "condition".
Cachexia is often seen in end-stage cancer, and in that context is called cancer cachexia. Patients with congestive heart failure can have a cachectic syndrome. Also, a cachexia comorbidity is seen in patients who have any of the range of illnesses classified as chronic obstructive pulmonary disease.[5] Cachexia is also associated with advanced stages of chronic kidney disease, cystic fibrosis, multiple sclerosis, motor neuron disease, Parkinson's disease, dementia, HIV/AIDS and other progressive illnesses.[6]
About 50% of all cancer patients suffer from cachexia. Those with upper gastrointestinal and pancreatic cancers have the highest frequency of developing a cachexic symptom. This figure rises to 80% in terminal cancer patients.[7] In addition to increasing morbidity and mortality, aggravating the side effects of chemotherapy, and reducing quality of life, cachexia is considered the immediate cause of death of a large proportion of cancer patients, ranging from 22% to 40% of the patients.[8] Cachexia has been reported in patients with early-stage cancer and its presence is associated with mortality risk.[9]
Symptoms of cancer cachexia include progressive weight loss and depletion of host reserves of adipose tissue and skeletal muscle. Cachexia should be suspected if involuntary weight loss of greater than 5% of premorbid weight occurs within a six-month period. Traditional treatment approaches, such as appetite stimulants, 5-HT3 antagonists, nutrient supplementation, and COX-2 inhibitor, have failed to demonstrate success in reversing the metabolic abnormalities seen in cancer cachexia.[10]
The exact mechanism in which these diseases cause cachexia is poorly understood, but there is probably a role for inflammatory cytokines, such as tumor necrosis factor-alpha (which is also nicknamed 'cachexin' or 'cachectin'), interferon gamma and interleukin 6, as well as the tumor-secreted proteolysis-inducing factor.
Related syndromes include kwashiorkor and marasmus, although these do not always have an underlying causative illness and are most often symptomatic of severe malnutrition.
Those suffering from the eating disorder anorexia nervosa appear to have high plasma levels of ghrelin. Ghrelin levels are also high in patients who have cancer-induced cachexia.[11]
Much research is currently focused on determining the mechanism of the development of cachexia. The two main theories of the development of cancer cachexia are:
Although the pathogenesis of cancer cachexia is poorly understood, multiple biologic pathways are known to be involved, including proinflammatory cytokines such as TNF-alpha, neuroendocrine hormones, IGF-1, and tumor-specific factors such as proteolysis-inducing factor.
The inflammatory cytokines involved in wasting diseases are interleukin 6, TNF-alpha, IL1B, and interferon-gamma. Although many different tissues and cell types may be responsible for the increase in circulating cytokines during some types of cancer, evidence indicates the tumors are an important source. Cytokines by themselves are capable of inducing weight loss. TNF-alpha has been shown to have direct catabolic effect on skeletal muscle and adipose tissue and produces muscle atrophy through the ubiquitin–proteasome proteolytic pathway. The mechanism involves the formation of reactive oxygen species leading to upregulation of the transcription factor NF-κB. NF-κB is a known regulator of the genes that encode cytokines, and cytokine receptors. The increased production of cytokines induces proteolysis and breakdown of myofibrillar proteins.[10]
The treatment or management of cachexia depends on the underlying causes, the general prognosis and other person related factors. Reversible causes, underlying diseases and contributing factors are treated if possible and acceptable.[13]
Therapy that includes regular physical exercise is recommended for the treatment of cancer cachexia due to the effects of exercise on skeletal muscle.[14][15] Individuals with cancer cachexia generally report low levels of physical activity and few engage in an exercise routine, owing to low motivation to exercise and a belief that exercising may worsen their symptoms or cause harm.[16][17] As of  2018,[update] the efficacy of resistance and aerobic exercise in cancer cachexia has not been established due to a lack of published evidence;[18] however, trials of the effectiveness of exercise therapy – in combination with nutrition and medication – are underway.[18]
A growing body of evidence supports the efficacy of β-hydroxy β-methylbutyrate (HMB) as a treatment for reducing, or even reversing, the loss of muscle mass, muscle function, and muscle strength that occurs in cachexia;[19][20][21] consequently, as of  June 2016[update] it is recommended that both the prevention and treatment of muscle wasting conditions include supplementation with HMB, regular resistance exercise and consumption of a high-protein diet.[19][20] Progestins such as megestrol acetate are a treatment option in refractory cachexia with anorexia as a major symptom.[22][23]
Cachexia occurs less frequently in HIV/AIDS than in the past due to the advent of highly active antiretroviral therapy (HAART).[24] Treatment involving different combinations for cancer cachexia is recommended in Europe, as a combination of nutrition, medication and non-drug-treatment may be more effective than monotherapy.[22] Non-drug therapies which have been shown to be effective in cancer induced cachexia include nutritional counselling, psychotherapeutic interventions and physical training.[22] Anabolic-androgenic steroids like oxandrolone[25][26] may be beneficial in cancer cachexia but their use is recommended for maximal 2 weeks since a longer duration of treatment increases the burden from side effects.[22]
Other drugs that have been used or are being investigated in cachexia therapy, but which lack conclusive evidence of efficacy or safety, and are not generally recommended include:
Medical marijuana has been allowed for the treatment of cachexia in some US states, such as Illinois, Maryland, Delaware, Nevada, Michigan, Washington, Oregon, California, Colorado, New Mexico, Arizona, Vermont, New Jersey, Rhode Island, Maine, and New York [28][29] Hawaii[30] and Connecticut.[23][31]
There is insufficient evidence to support the use of oral fish oil for the management of cachexia associated with advanced cancer.[32][33]
Only limited treatment options exist for patients with clinical cancer cachexia. Current strategy is to improve appetite by using appetite stimulants to ensure adequate intake of nutrients. Pharmacological interventions with appetite stimulants, nutrient supplementation, 5-HT3 antagonists and Cox-2 inhibitor have been used to treat cancer cachexia, but with limited effect.
Studies using a more calorie-dense (1.5 kcals/ml) and higher protein supplementation have suggested at least weight stabilization can be achieved, although improvements in lean body mass have not been observed in these studies.[10]
Therapeutic strategies have been based on either blocking cytokines synthesis or their action. Thalidomide has been demonstrated to suppress TNF-alpha production in monocytes in vitro and to normalize elevated TNF-alpha levels in vivo. A randomized, placebo-controlled trial in patients with cancer cachexia showed the drug was well tolerated and effective at attenuating loss of weight and lean body mass (LBM) in patients with advanced pancreatic cancer. An improvement in the LBM and improved quality of life were also observed in a randomized, double-blind trial using a protein and energy-dense, omega-3 fatty acids-enriched oral supplement, provided its consumption was equal or superior to 2.2 g of eicosapentaenoic acid per day. It is also through decreasing TNF-alpha production. However, data arising from a large, multicenter, double-blind, placebo-controlled trial indicate EPA administration alone is not successful in the treatment of weight loss in patients with advanced gastrointestinal or lung cancer.[34]
Peripheral muscle proteolysis, as it occurs in cancer cachexia, serves to mobilize amino acids required for the synthesis of liver and tumor protein. Therefore, the administration of exogenous amino acids may theoretically serve as a protein-sparing metabolic fuel by providing substrates for both muscle metabolism and gluconeogenesis. Studies have demonstrated dietary supplementation with a specific combination of high protein, leucine and fish oil improves muscle function and daily activity and the immune response in cachectic tumor-bearing mice. In addition, β-hydroxy-β-methyl butirate derived from leucine catabolism used as a supplement in tumor-bearing rats prevents cachexia by modifying NF-κB expression.[34]
A phase-2 study involving the administration of antioxidants, pharmaconutritional support, progestin (megestrol acetate and medroxyprogesterone acetate), and anticyclooxygenase-2 drugs, showed efficacy and safety in the treatment of patients with advanced cancer of different sites suffering cachexia. These data reinforce the use of the multitargeted therapies (nutritional supplementation, appetite stimulants, and physical activity regimen) in the treatment of cancer cachexia.[34]
New studies indicate NSAIDS, like Sulindac, were found to significantly decrease cachexia.[35]
Also studies have shown branched-chain amino acids can return the metabolism of a cachectic patient from catabolic-losing weight- to anabolic- increasing muscle, in over 55% of patients. Branched-chain amino acids consist primarily of leucine and valine. In a research paper published by the Indian J of Palliat Care, the effects the findings concluded that bcaa's interfere with brain serotonergic activity and inhibit the overexpression of critical muscular proteolytic pathways. The potential role of branched-chain amino acids as antianorexia and anticachexia agents was proposed many years ago, but experimental studies and clinical trials have since tested their ability to stimulate food intake and counteract muscle wasting in anorectic, weight-losing patients. In experimental models of cancer cachexia, BCAAs were able to induce a significant suppression in the loss of body weight, producing a significant increase in skeletal muscle wet weight[30] as well as in muscle performance and total daily activity.
The conditionally essential amino acid glutamine has been used as a component of oral supplementation to reverse cachexia in patients with advanced cancer[36] or HIV/AIDS.[37]
According to the 2007 AHRQ National Inpatient Sample, in a projected 129,164 hospital encounters in the United States, cachexia was listed as at least one of up to 14 recorded diagnosis codes based on a sample of 26,325 unweighted encounters.[38] A sample of 32,778 unweighted US outpatient visits collected by the CDC's National Ambulatory Medical Care Survey did not list any visits where cachexia was one of up to three recorded diagnoses treated during the visit.[39]



Paraneoplastic syndrome - Wikipedia
A paraneoplastic syndrome is a syndrome (a set of signs and symptoms) that is the consequence of cancer in the body, but unlike mass effect, is not due to the local presence of cancer cells.[1] In contrast, these phenomena are mediated by humoral factors (such as hormones or cytokines) secreted by tumor cells or by an immune response against the tumor.
Paraneoplastic syndromes are typical among middle-aged to older patients, and they most commonly present with cancers of the lung, breast, ovaries, or lymphatic system (a lymphoma).[2] Sometimes, the symptoms of paraneoplastic syndromes show before the diagnosis of a malignancy, which has been hypothesized to relate to the disease pathogenesis. In this paradigm, tumor cells express tissue-restricted antigens (e.g., neuronal proteins), triggering an anti-tumor immune response which may be partially or, rarely, completely effective[3] in suppressing tumor growth and symptoms.[4][5] Patients then come to clinical attention when this tumor immune response breaks immune tolerance and begins to attack the normal tissue expressing that (e.g., neuronal) protein.
The abbreviation PNS is sometimes used for paraneoplastic syndrome, although it is used more often to refer to the peripheral nervous system.
As mentioned above, symptomatic features of paraneoplastic syndrome cultivate in four different ways: endocrine, neurological, mucocutaneous, and hematological. The most common presentation is a fever (release of endogenous pyrogens often related to lymphokines or tissue pyrogens), but the overall picture will often include several clinical cases observed which may specifically simulate more common benign conditions.[6]
The following diseases manifest by means of endocrine dysfunction: Cushing syndrome, syndrome of inappropriate antidiuretic hormone, hypercalcemia, hypoglycemia, carcinoid syndrome, and hyperaldosteronism.[7]
The following diseases manifest by means of neurological dysfunction: Lambert-Eaton myasthenic syndrome, paraneoplastic cerebellar degeneration, encephalomyelitis, limbic encephalitis, brainstem encephalitis, opsoclonus myoclonus ataxia syndrome, anti-NMDA receptor encephalitis, and polymyositis.[7]
The following diseases manifest by means of mucocutaneous dysfunction: acanthosis nigricans, dermatomyositis,  Leser-Trélat sign, necrolytic migratory erythema, Sweet's syndrome, Florid cutaneous papillomatosis, pyoderma gangrenosum, and acquired generalized hypertrichosis. Mucocutaneous dysfunctions of paraneoplastic syndromes can be seen in cases of itching (hypereosinophilia), immune system depression (latent varicella-zoster virus in sensory ganglia), pancreatic tumors (leading to adipose nodular necrosis of subcutaneous tissues, flushes (prostaglandin secretions), and even dermic melanosis (cannot be eliminated via urine and results in grey to black-blueish skin tones).[7]
The following diseases manifest by means of hematological dysfunction: granulocytosis, polycythemia, Trousseau sign, nonbacterial thrombotic endocarditis, and anemia. Hematological dysfunction of paraneoplastic syndromes can be seen from an increase of erythropoietin (EPO), which may occur in response to hypoxia or ectopic EPO production/altered catabolism. Erythrocytosis is common in regions of the liver, kidney, adrenal glands, lung, thymus, and central nervous system (as well as gynecological tumors and myosarcomas).[7]
The following diseases manifest by means of physiological dysfunction besides the categories above: membranous glomerulonephritis, tumor-induced osteomalacia, Stauffer syndrome, Neoplastic fever, and thymoma-associated multiorgan autoimmunity. Rheumatologic (hypertrophic osteoarthropathy), renal (secondary kidney amyloidosis and sedimentation of immunocomplexes in nephrons), and gastrointestinal (production of molecules that affect the motility and secretory activity of the digestive tract) dysfunctions, for example, may relate to paraneoplastic syndromes.[7]
The mechanism for paraneoplastic syndrome varies from case to case. However, pathophysiological outcomes usually arise from when a tumor arises. Paraneoplastic syndrome often occurs alongside associated cancers as a result of activated immune systems. In this scenario, the body may produce antibodies to fight off the tumor by directly binding and destroying the tumor cell. Paraneoplastic disorders may arise in that antibodies would cross-react with normal tissues and destroy them.[8]
Diagnostic testing in a possible paraneoplastic syndrome depends on the symptoms and the suspected underlying cancer.[citation needed]
Diagnosis may be difficult in patients in whom paraneoplastic antibodies cannot be detected. In the absence of these antibodies, other tests that may be helpful include MRI, PET, lumbar puncture and electrophysiology.[9]
A specifically devastating form of (neurological) paraneoplastic syndromes is a group of disorders classified as paraneoplastic neurological disorders (PNDs).[23] These PNDs affect the central or peripheral nervous system; some are degenerative,[24] though others (such as LEMS) may improve with treatment of the condition or the tumor. Symptoms of PNDs may include difficulty with walking and balance, dizziness, rapid uncontrolled eye movements, difficulty swallowing, loss of muscle tone, loss of fine motor coordination, slurred speech, memory loss, vision problems, sleep disturbances, dementia, seizures, and sensory loss in the limbs.
The most common cancers associated with PNDs are breast, ovarian, and lung cancers, but many other cancers can produce paraneoplastic symptoms, as well.
The root cause is extremely difficult to identify for paraneoplastic syndrome, as there are so many ways the disease can manifest (which may eventually lead to cancer). Ideas may relate to age-related diseases (unable to handle environmental or physical stress in combination with genetic pre-dispositions), accumulation of damaged biomolecules (damages signaling pathways in various regions of the body), increased oxygen free radicals in the body (alters metabolic processes in various regions of the body), etc.
However, prophylactic efforts include routine checks with physicians (particularly those that specialize in neurology and oncology) especially when a patient notices subtle changes in his or her own body.
Treatment options include:[citation needed]
A specific prognosis for those with paraneoplastic syndromes links to each unique case presented. Thus, prognosis for paraneoplastic syndromes may vary greatly. For example, paraneoplastic pemphigus often included infection as a major cause of death.[25] Paraneoplastic pemphigus is one of the three major subtypes that affects IgG autoantibodies that are characteristically raised against desmoglein 1 and desmoglein 3 (which are cell-cell adhesion molecules found in desmosomes).[26] Underlying cancer or irreversible system impairment, seen in acute heart failure or kidney failure, may result in death as well.
Prostate cancer is the second most common urological malignancy to be associated with paraneoplastic syndromes after renal cell carcinoma. Paraneoplastic syndromes of this nature tend to occur in the setting of late stage and aggressive tumors with poor overall outcomes (endocrine manifestations, neurological entities, dermatological conditions, and other syndromes). A vast majority of prostate cancer cases (over 70%) document paraneoplastic syndrome as a major clinical manifestation of prostate cancer; and (under 20%), the syndrome as an initial sign of disease progression to the castrate-resistant state.[27] Urologist researchers identify serum markers that are associated with the syndrome in order to specific what type of therapies may work most effectively.
Paraneoplastic neurological syndromes may be related immune checkpoint inhibitors (ICIs), one of the underlying causes in inflammatory central nervous system diseases (CNS). The central idea around such research pinpoints treatment strategies to combat cancer related outcomes in the clinical arena, specifically ICIs. Research suggests that patients who are treated with ICIs are more susceptible to CNS disease (since the mechanism of ICIs induces adverse effects on the CNS due to augmented immune responses and neurotoxicity).[28] The purpose of this exploration was to shed light on immunotherapies and distinguishing between neurotoxicity and brain metastasis in the early stages of treatment. In other research, scientists have found that paraneoplastic peripheral nerve disorders (autoantibodies linked to multifocal motor neuropathy) may provide important clinical manifestations.[29] This is especially important for patients who experience inflammatory neuropathies since solid tumors are often associated with peripheral nerve disorders. CV2 autoantibodies, which target dihydropyriminase-related protein 5 (DRP5, or CRMP5) are also associated with a variety of paraneoplastic neurological syndromes, including sensorimotor polyneuropathies.[30][31] Patients undergoing immune therapies or tumor removal respond very well to antibodies that target CASPR2 (to treat nerve hyperexcitability and neuromyotonia).[32][33]



Lymphadenopathy - Wikipedia
Lymphadenopathy or adenopathy is disease of the lymph nodes, in which they are abnormal in size, number, or consistency.[1] Lymphadenopathy of an inflammatory type (the most common type) is lymphadenitis,[2]  producing swollen or enlarged lymph nodes. In clinical practice, the distinction between lymphadenopathy and lymphadenitis is rarely made and the words are usually treated as synonymous. Inflammation of the lymphatic vessels is known as lymphangitis.[3] Infectious lymphadenitides affecting lymph nodes in the neck are often called scrofula.
The term comes from the word lymph and a combination of the Greek words αδένας, adenas ("gland") and παθεία, patheia ("act of suffering" or "disease").
Lymphadenopathy is a common and nonspecific sign. Common causes include infections (from minor ones such as the common cold to serious ones such as HIV/AIDS), autoimmune diseases, and cancers. Lymphadenopathy is also frequently idiopathic and self-limiting.
Lymphadenopathy may be classified by:
Lymphadenopathy of the axillary lymph nodes can be defined as solid nodes measuring more than 15 mm without fatty hilum.[11] Axillary lymph nodes may be normal up to 30 mm if consisting largely of fat.[11]
Lymphadenopathy of more than 1.5 cm - 2 cm increases the risk of cancer or granulomatous disease as the cause rather than only inflammation or infection. Still, an increasing size and persistence over time are more indicative of cancer.[14]
Lymph node enlargement is recognized as a common sign of infectious, autoimmune, or malignant disease. Examples may include:
Less common infectious causes of lymphadenopathy may include bacterial infections such as cat scratch disease, tularemia, brucellosis, or prevotella.[citation needed]
Benign lymphadenopathy is a common biopsy finding, and may often be confused with malignant lymphoma. It may be separated into major morphologic patterns, each with its own differential diagnosis with certain types of lymphoma. Most cases of reactive follicular hyperplasia are easy to diagnose, but some cases may be confused with follicular lymphoma. There are seven distinct patterns of benign lymphadenopathy:[18]
These morphological patterns are never pure. Thus, reactive follicular hyperplasia can have a component of paracortical hyperplasia. However, this distinction is important for the differential diagnosis of the cause.
In cervical lymphadenopathy, it is routine to perform a throat examination including mirror and/or endoscopy.[36]
On ultrasound, B-mode imaging depicts lymph node morphology, whilst power Doppler can assess the vascular pattern.[37] B-mode imaging features that can distinguish metastasis and lymphoma include size, shape, calcification, loss of hilar architecture, as well as intranodal necrosis.[37] Soft tissue edema and nodal matting on B-mode imaging suggests tuberculous cervical lymphadenitis or previous radiation therapy.[37] Serial monitoring of nodal size and vascularity are useful in assessing treatment response.[37]
Fine needle aspiration cytology (FNAC) has a sensitivity and specificity percentages of 81% and 100%, respectively, in the histopathology of malignant cervical lymphadenopathy.[36] PET-CT has proven to be helpful in identifying occult primary carcinomas of the head and neck, especially when applied as a guiding tool prior to panendoscopy, and may induce treatment related clinical decisions in up to 60% of cases.[36]



Environmental disease - Wikipedia
In epidemiology, environmental diseases are diseases that can be directly attributed to environmental factors (as distinct from genetic factors or infection). Apart from the true monogenic genetic disorders, environmental diseases may determine the development of disease in those genetically predisposed to a particular condition. Stress, physical and mental abuse, diet, exposure to toxins, pathogens, radiation, and chemicals found in almost all personal care products and household cleaners are possible causes of a large segment of non-hereditary disease. If a disease process is concluded to be the result of a combination of genetic and environmental factor influences, its etiological origin can be referred to as having a multifactorial pattern.
There are many different types of environmental disease including:[1]
Environmental diseases are a direct result from the environment. This includes diseases caused by substance abuse, exposure to toxic chemicals, and physical factors in the environment, like UV radiation from the sun, as well as genetic predisposition. Meanwhile, pollution-related diseases are attributed to exposure to toxins in the air, water, and soil. Therefore all pollution-related disease are environmental diseases, but not all environmental diseases are pollution-related diseases.
Poisoning by lead and mercury has been known since antiquity. Other toxic metals or metals that are known to evoke adverse immune reactions are arsenic, phosphorus, zinc, beryllium, cadmium, chromium, manganese, nickel, cobalt, osmium, platinum,[2] selenium, tellurium, thallium, uranium, and vanadium.
There are many other diseases likely to have been caused by common anions found in natural drinking water. Fluoride is one of the most common found in drier climates where the geology favors release of fluoride ions to soil as the rocks decompose. In Sri Lanka, 90% of the country is underlain by crystalline metamorphic rocks of which most carry mica as a major mineral. Mica carries fluoride in their structure and releases to soil when decomposes. In the dry and arid climates, fluoride concentrates on top soil and slowly dissolves in shallow groundwater. This has been the cause of high fluoride levels in drinking water where the majority of the rural Sri Lankans obtain their drinking water from backyard wells. High fluoride in drinking water has caused a high incidence of fluorosis among dry zone population in Sri Lanka. However, in the wet zone, high rainfall effectively removes fluoride from soils where no fluorosis is evident. In some parts of Sri Lanka  iodine deficiency has also been noted which has been identified as a result of iodine fixation by hydrated iron oxide found in lateritic soils in wet coastal lowlands.
Additionally, there are environmental diseases caused by the aromatic carbon compounds including :  benzene,  hexachlorocyclohexane, toluene diisocyanate, phenol, pentachlorophenol, quinone and hydroquinone.
Also included are the aromatic nitro-, amino-, and pyridilium-deratives:  nitrobenzene, dinitrobenzene, trinitrotoluene, paramethylaminophenol sulfate (Metol), dinitro-ortho-cresol, aniline, trinitrophenylmethylnitramine (tetryl), hexanitrodiphenylamine (aurantia), phenylenediamines, and paraquat.
The aliphatic carbon compounds can also cause environmental disease.  Included in these are methanol, nitroglycerine, nitrocellulose, dimethylnitrosamine, and the halogenated hydrocarbons:  methyl chloride, methyl bromide, trichloroethylene, carbon tetrachloride, and the chlorinated naphthalenes.  Also included are glycols:  ethylene chlorhydrin and diethylene dioxide as well as carbon disulfide, acrylonitrile, acrylamide, and vinyl chloride.
Noxious gases can be categorized as :  Simple asphyxiants, chemical asphyxiants, and irritant gases.  The simple asphixiants are nitrogen, methane, and carbon dioxide.
The chemical asphyxiants are carbon monoxide, sulfuretted hydrogen and hydrogen cyanide.
The irritant gases are sulfur dioxide, ammonia, nitrogen dioxide, chlorine, phosgene, and fluorine and its compounds, which include luroine and hydrofluoric acid, fluorspar, fluorapatite, cryolite, and organic fluorine compounds.
The U.S. Coast Guard has developed a Coast Guard-wide comprehensive system for surveillance of workplace diseases.
The American Medical Association's fifth edition of the Current Medical Information and Terminology (CMIT) was used as a reference to expand the basic list of 50 Sentinel Health Events (Occupational) [SHE(O)] published by the National Institute of Occupational Health and Safety (NIOSH), September, 1983.



Organ donation - Wikipedia

Organ donation is when a person allows an organ of theirs to be removed, legally, either by consent while the donor is alive or after death with the assent of the next of kin.
Donation may be for research, or, more commonly healthy transplantable organs and tissues may be donated to be transplanted into another person.[1][2]
Common transplantations include: kidneys, heart, liver, pancreas, intestines, lungs, bones, bone marrow, skin, and corneas.[1] Some organs and tissues can be donated by living donors, such as a kidney or part of the liver, part of the pancreas, part of the lungs or part of the intestines,[3] but most donations occur after the donor has died.[1]
As of February 2, 2018, there were 115,085 people waiting for life-saving organ transplants in the US.[4] Of these, 74,897 people were active candidates waiting for a donor.[4] While views of organ donation are positive there is a large gap between the numbers of registered donors compared to those awaiting organ donations on a global level.[citation needed]
Organ donors are usually dead at the time of donation, but may be living. For living donors, organ donation typically involves extensive testing before the donation, including psychological evaluation to determine whether the would-be donor understands and consents to the donation. On the day of the donation, the donor and the recipient arrive at the hospital, just like they would for any other major surgery.
By the process of transplantation of the organs of the person by well developed doctors and professors such that they can save the life of living organisms even humans also
For dead donors, the process begins with verifying that the person is undoubtedly deceased, determining whether any organs could be donated, and obtaining consent for the donation of any usable organs. Normally, nothing is done until the person has already died, although if death is inevitable, it is possible to check for consent and to do some simple medical tests shortly beforehand, to help find a matching recipient. The verification of death is normally done by a neurologist (a physician specializing in brain function) that is not involved in the previous attempts to save the patient's life. This physician has nothing to do with the transplantation process.[5] Verification of death is often done multiple times, to prevent doctors from overlooking any remaining sign of life, however small.[6] After death, the hospital may keep the body on a mechanical ventilator and use other methods to keep the organs in good condition.[6] Donors and their families are not charged for any expenses related to the donation.
The surgical process depends upon which organs are being donated. After the surgeons remove the organs, they are transported as quickly as possible to the recipient, for immediate transplantation. Most organs only survive outside the body for a few hours, so recipients in the same region are usually chosen. In the case of a dead donor, after the organs are removed, the body is normally restored to as normal an appearance as possible, so that the family can proceed with funeral rites and either cremation or burial.
The Ten Major ideal donor management goals (DMGs):[7]
-  MAP 60-120 mmHg
-  CVP 4-12 (or < 12)
-  Final Na ≤ 155, or 135-160 mmol/L
-  Pressors < 1 ideal, or low dose pressor
-  PaO2/FiO2 ratio > 300 (PaO2 > 300 on 100% FiO2, 5 PEEP)
-  pH on ABG 7.25-7.5
-  Glucose < 150
-  Urine Output 0.5-3 mL/kg/h
-  LV EF (%) > 50
-  Hgb > 10 mg/dL
-The lungs are highly vulnerable to injury and thus the most difficult to preserve, with only 15-25% of donated organs utilized. Suggested management includes ARDS goals; low tidal volume ventilation (6-8 mL/kg), low FiO2, and relatively high PEEP. PaO2 ratio should be > 300 in preparation for organ donation and/or a PaO2 > 300 on 100% FiO2 and 5 cm H2O PEEP. While a lower PaO2/FiO2 will not always result in exclusion, this should still be the goal.
The first living organ donor in a successful transplant was Ronald Lee Herrick (1931–2010), who donated a kidney to his identical twin brother in 1954.[8] The lead surgeon, Joseph Murray, won the Nobel Prize in Physiology or Medicine in 1990 for advances in organ transplantation.
The youngest organ donor was a baby with anencephaly, born in 2015, who lived for only 100 minutes and donated his kidneys to an adult with renal failure.[9] The oldest known organ donor was a 107-year-old Scottish woman, whose corneas were donated after her death in 2016.[10] The oldest known organ donor for an internal organ was a 92-year-old Texas man, whose family chose to donate his liver after he died of a brain hemorrhage.[11]
The oldest altruistic living organ donor was an 85-year-old woman in Britain, who donated her kidney to a stranger in 2014 after hearing how many people needed to receive a transplant.[12]
Researchers were able to develop a novel way to transplant human fetal kidneys into anephric rats to overcome a significant obstacle in impeding human fetal organ transplantations.[13] The human fetal kidneys demonstrated both growth and function within the rats.[13]
Because there are no known cures for many brain disorders, a high priority is given to research designed to improve the scientific understanding of healthy brain tissue to try to find new treatments.
This is to  ensure research is thorough, as it is important to have access to brain tissues from people who did not have the diseases being studied for comparison. These unaffected tissues are known as ‘control tissues’.[14] a short A BBC video  appeal was published in early 2017
The laws of different countries allow potential donors to permit or refuse donation, or give this choice to relatives. The frequency of donations varies among countries.
The term consent is typically defined as a subject adhering to an agreement of principals and regulations; however, the definition becomes difficult to execute concerning the topic of organ donation, mainly because the subject is incapable of consent due to death or mental impairment.[15] There are two types of consent being reviewed; explicit consent and presumed consent. Explicit consent consists of the donor giving direct consent through proper registration depending on the country.[16] The second consent process is presumed consent, which does not need direct consent from the donor or the next of kin.[16] Presumed consent assumes that donation would have been permitted by the potential donor if permission was pursued.[16] Of possible donors an estimated twenty-five percent of families refuse to donate a loved one's organs.[17] Consent is defined as adhering to an agreement of principals. However, this definition is hard to enforce in accordance with organ donation because, in most cases, organs are donated from the deceased, and can no longer provide consent for themselves.
As medical science advances, the number of people who could be helped by organ donors increases continuously. As opportunities to save lives increase with new technologies and procedures, the number of people willing to donate organs needs to increase as well.[18] In order to respect individual autonomy, voluntary consent must be determined for the individual's disposition of their remains following death.[19] There are two main methods for determining voluntary consent: "opt in" (only those who have given explicit consent are donors) and "opt out" (anyone who has not refused consent to donate is a donor). In terms of an opt-out or presumed consent system, it is assumed that individuals do intend to donate their organs to medical use when they expire.[19] Opt-out legislative systems dramatically increase effective rates of consent for donation as a consequence of the default effect.[20] For example, Germany, which uses an opt-in system, has an organ donation consent rate of 12% among its population, while Austria, a country with a very similar culture and economic development, but which uses an opt-out system, has a consent rate of 99.98%.[20][21]
Opt-out consent, otherwise known as "deemed" consent, support refers to the notion that the majority of people support organ donation, but only a small percentage of the population are actually registered, because they fail to go through the actual step of registration, even if they want to donate their organs at the time of death. This could be resolved with an opt-out system, where many more people would be registered as donors when only those who object consent to donation have to register to be on the non-donation list.[19] For this reasons, countries, such as Wales, have adopted a "soft opt-out" consent, meaning if a citizen has not clearly made a decision to register, then they will be treated as a registered citizen and participate in the organ donation process. Likewise, opt-in consent refers to the consent process of only those who are registered to participate in organ donation. Currently, the United States has an opt-in system, but studies show that countries with an opt-out system save more lives due to more availability of donated organs. The current opt-in consent policy assumes that individuals are not willing to become organ donors at the time of their death, unless they have documented otherwise through organ donation registration.[19] Registering to become an organ donor heavily depends on the attitude of the individual; those with a positive outlook might feel a sense of altruism towards organ donation, while others may have a more negative perspective, such as not trusting doctors to work as hard to save the lives of registered organ donors. Some common concerns regarding a presumed consent ("opt-out") system are sociologic fears of a new system, moral objection, sentimentality, and worries of the management of the objection registry for those who do decide to opt-out of donation.[19] Additional concerns exist with views of compromising the freedom of choice to donate [22] and conflicts with religious beliefs which exist.[23] Even though concerns exist, the United States still has a 95 percent organ donation approval rate. This level of nationwide acceptance may foster an environment where moving to a policy of presumed consent may help solve some of the organ shortage problem, where individuals are assumed to be willing organ donors unless they document a desire to "opt-out", which must be respected.[23]
Because of public policies, cultural, infrastructural and other factors, presumed consent or opt-out models do not always translate directly into increased effective rates of donation. The United Kingdom has several different laws and policies for the organ donation process, such as consent of a witness or guardian must be provided to participate in organ donation. This policy is currently being consulted on by Department of Health and Social Care. In terms of effective organ donations, in some systems like Australia (14.9 donors per million, 337 donors in 2011), family members are required to give consent or refusal, or may veto a potential recovery even if the donor has consented.[24] Some countries with an opt-out system like Spain (36 effective donors per million inhabitants)[25] or Austria (21 donors/million) have high donor rates and some countries with opt-in systems like Germany (16 donors/million) or Greece (6 donors/million) have lower effective donation rates.[26] The president of the Spanish National Transplant Organisation has acknowledged Spain's legislative approach is likely not the primary reason for the country's success in increasing the donor rates, starting in the 1990s.[26] Looking to the example of Spain, which has successfully adopted the presumed consent donation system, intensive care units (ICUs) must be equipped with enough doctors to maximize the recognition of potential donors and maintain organs while families are consulted for donation. The characteristic that enables the Spanish presumed consent model to be successful is the resource of transplant coordinators; it is recommended to have at least one at each hospital where opt-out donation is practiced to authorize organ procurement efficiently.[27]
Public views are crucial to the success of opt-out or presumed consent donation systems. In a study done to determine if health policy change to a presumed consent or opt-out system would help to increase donors, an increase of 20 to 30 percent was seen among countries who changed their policies from some type of opt-in system to an opt-out system. Of course, this increase must have a great deal to do with the health policy change, but also may be influenced by other factors that could have impacted donor increases.[28]
Transplant Priority for Willing Donors is a newer method and the first to incorporate a "non-medical" criteria into the priority system to encourage higher donation rates in the opt-in system.[29] Initially implemented in Israel, it allows an individual in need of an organ to move up the recipient list. Moving up the list is contingent on the individual opting-in prior to their need for an organ donation. The policy applies nonmedical criteria when allowing the individual who has previously registered as an organ donor, or family has previously donated an organ, priority over another possible recipient. It must be determined that both recipients have identical medical needs prior to moving a recipient up the list. While incentives like this in the opt-in system do help raise donation rates, they are not as successful in doing so as the opt-out, presumed consent default policies for donation.[24]
On 30 November 2005, the Congress introduced an opt-out policy on organ donation, where all people over 18 years of age will be organ donors unless they or their family state their negative. The law was promulgated on 22 December 2005 as "Law 26,066".[30]
On 4 July 2018 the Congress passed a law removing the family requirement, making the organ donor the only person that can state their negative. It was promulgated on 26 July 2018 as "Law 27,447".[31]
A campaign by Sport Club Recife has led to waiting lists for organs in north-east Brazil to drop almost to zero; while according to the Brazilian law the family has the ultimate authority, the issuance of the organ donation card and the ensuing discussions have however eased the process.[32]
On 6 January 2010 the "Law 20,413" was promulgated, introducing an opt-out policy on organ donation, where all people over 18 years of age will be organ donors unless they state their negative.[33][34]
On 4 August 2016, the Congress passed the "Law 1805", which introduced an opt-out policy on organ donation where all people will be organ donors unless they state their negative.[35] The law came into force on 4 February 2017.[36]
Within the European Union, organ donation is regulated by member states. As of 2010, 24 European countries have some form of presumed consent (opt-out) system, with the most prominent and limited opt-out systems in Spain, Austria, and Belgium yielding high donor rates.[37] Spain had the highest donor rate in the world, 46.9 per million people in the population, in 2017.[38]
In England organ donation is voluntary and no consent is presumed. Individuals who wish to donate their organs after death can use the Organ Donation Register, a national database. The government of Wales became the first constituent country in the UK to adopt presumed consent in July 2013.[39] The opt-out organ donation scheme in Wales went live on December 1, 2015 and is expected to increase the amount of donors by 25%.[40] In 2008, the UK discussed whether to switch to an opt-out system in light of the success in other countries and a severe British organ donor shortfall.[41] In Italy if the deceased neither allowed nor refused donation while alive, relatives will pick the decision on his or her behalf despite a 1999 act that provided for a proper opt-out system.[42] In 2008, the European Parliament overwhelmingly voted for an initiative to introduce an EU organ donor card in order to foster organ donation in Europe.[citation needed]
Landstuhl Regional Medical Center (LRMC) has become one of the most active organ donor hospitals in all of Germany, which otherwise has one of the lowest organ donation participation rates in the Eurotransplant organ network. LRMC, the largest U.S. military hospital outside the United States, is one of the top hospitals for organ donation in the Rhineland-Palatinate state of Germany, even though it has relatively few beds compared to many German hospitals. According to the German organ transplantation organization, Deutsche Stiftung Organtransplantation (DSO), 34 American military service members who died at LRMC (roughly half of the total number who died there) donated a total of 142 organs between 2005 and 2010. In 2010 alone, 10 of the 12 American service members who died at LRMC were donors, donating a total of 45 organs. Of the 205 hospitals in the DSO's central region—which includes the large cities of Frankfurt and Mainz—only six had more organ donors than LRMC in 2010.[43]
Scotland conforms to the Human Tissue Authority Code of Practice, which grants authority to donate organs, instead of consent of the individual.[44] This helps to avoid conflict of implications and contains several requirements. In order to participate in organ donation, one must be listed on the Organ Donor Registry (ODR). If the subject is incapable of providing consent, and is not on the ODR, then an acting representative, such as a legal guardian or family member can give legal consent for organ donation of the subject, along with a presiding witness, according to the Human Tissue Authority Code of Practice. Consent or refusal from a spouse, family member, or relative is necessary for a subject is incapable.
Austria participates in the "opt-out" consent process, and have laws that make organ donation the default option at the time of death. In this case, citizens must explicitly "opt out" of organ donation. "In these so-called opt-out countries, more than 90% of people donate their organs. Yet in countries such as U.S. and Germany, people must explicitly "opt in" if they want to donate their organs when they die. In these opt-in countries, fewer than 15% of people donate their organs at death."[9]
In May 2017, Ireland began the process of introducing an "opt-out" system for organ donation. Minister for Health, Simon Harris, outlined his expectations to have the Human Tissue Bill passed by the end of 2017. This bill would put in place the system of "presumed consent".[45]
The Mental Capacity Act is another legal policy in place for organ donation in the UK. The act is used by medical professionals to declare a patient's mental capacity. The act claims that medical professionals are to "act in a patient's best interest", when the patient is unable to do so.[44]
India has a fairly well developed corneal donation programme; however, donation after brain death has been relatively slow to take off. Most of the transplants done in India are living related or unrelated transplants. To curb organ commerce and promote donation after brain death the government enacted a law called "The Transplantation of Human Organs Act" in 1994 that brought about a significant change in the organ donation and transplantation scene in India.
[46][47][48][49]
[50][51][52][53][54] Many Indian states have adopted the law and in 2011 further amendment of the law took place.[55][56][57][58]
[59] Despite the law there have been stray instances of organ trade in India and these have been widely reported in the press. This resulted in the amendment of the law further in 2011. Deceased donation after brain death have slowly started happening in India and 2012 was the best year for the programme.
The year 2013 has been the best yet for deceased organ donation in India. A total of 845 organs were retrieved from 310 multi-organ donors resulting in a national organ donation rate of 0.26 per million population(Table 2).
* ODR (pmp) – Organ Donation Rate (per million population)
In the year 2000 through the efforts of an NGO called MOHAN Foundation state of Tamil Nadu started an organ sharing network between a few hospitals.[61][62] This NGO also set up similar sharing network in the state of Andhra Pradesh and these two states were at the forefront of deceased donation and transplantation programme for many years.[63][64] As a result, retrieval of 1033 organs and tissues were facilitated in these two states by the NGO.[65] Similar sharing networks came up in the states of Maharashtra and Karnataka; however, the numbers of deceased donation happening in these states were not sufficient to make much impact.In 2008, the Government of Tamil Nadu put together government orders laying down procedures and guidelines for deceased organ donation and transplantation in the state.[66] These brought in almost thirty hospitals in the programme and has resulted in significant increase in the donation rate in the state. With an organ donation rate of 1.15 per million population, Tamil Nadu is the leader in deceased organ donation in the country. The small success of Tamil Nadu model has been possible due to the coming together of both government and private hospitals, NGOs and the State Health department. Most of the deceased donation programmes have been developed in southern states of India.[67] The various such programmes are as follows- 
In the year 2012 besides Tamil Nadu other southern states too did deceased donation transplants more frequently. An online organ sharing registry for deceased donation and transplantation is used by the states of Tamil Nadu and Kerala. Both these registries have been developed, implemented and maintained by MOHAN Foundation. However. National Organ and Tissue Transplant Organization (NOTTO) is a National level organization set up under Directorate General of Health Services, Ministry of Health and Family Welfare, Government of India and only official organization.
Organ selling is legally banned in Asia. Numerous studies have documented that organ vendors have a poor quality of life (QOL) following kidney donation. However, a study done by Vemuru reddy et al shows a significant improvement in Quality of life contrary to the earlier belief.[68] Live related renal donors have a significant improvement in the QOL following renal donation using the WHO QOL BREF in a study done at the All India Institute of Medical Sciences from 2006 to 2008. The quality of life of the donor was poor when the graft was lost or the recipient died.[68]
Only one country, Iran has eliminated the shortage of transplant organs – and only Iran has a working and legal payment system for organ donation. [54] It is also the only country where organ trade is legal. The way their system works is, if a patient does not have a living relative or who are not assigned an organ from a deceased donor, apply to the nonprofit Dialysis and Transplant Patients Association (Datpa). The association establishes potential donors, those donors are assessed by transplant doctors who are not affiliated with the Datpa association. The government gives a compensation of $1,200 to the donors and aid them a year of limited health-insurance. Additionally, working through Datpa, kidney recipients pay donors between $2,300 and $4,500.[69] Importantly, it is illegal for the medical and surgical teams involved or any ‘middleman’ to receive payment.[70] Charity donations are made to those donors whose recipients are unable to pay. The Iranian system began in 1988 and eliminated the shortage of kidneys by 1999. Within the first year of the establishment of this system, the number of transplants had almost doubled; nearly four fifths were from living unrelated sources.[70][55] Nobel Laureate economist Gary Becker and Julio Elias estimated that a payment of $15,000 for living donors would alleviate the shortage of kidneys in the U.S.[69]
Since 2008, signing an organ donor card in Israel has provided a potential medical benefit to the signer. If two patients require an organ donation and have the same medical need, preference will be given to the one that had signed an organ donation card. This policy was nicknamed "dont give, don't get". Organ donation in Israel increased after 2008.
The rate of organ donation in Japan is significantly lower than in Western countries.[71] This is attributed to cultural reasons, some distrust of western medicine, and a controversial organ transplantation in 1968 that provoked a ban on cadaveric organ donation that would last thirty years.[71] Organ donation in Japan is regulated by a 1997 organ transplant law, which defines "brain death" and legalized organ procurement from brain dead donors.
New Zealand law allows live donors to participate in altruistic organ donation only. In 2013 there were 3 cases of liver donation by live donors and 58 cases of kidney donation by live donors.[72] New Zealand has low rates of live donation, which could be due to the fact that it is illegal to pay someone for their organs. The Human Tissue Act 2008 states that trading in human tissue is prohibited, and is punishable by a fine of up to $50,000 or a prison term of up to 1 year.[73]
New Zealand law also allows for organ donation from deceased individuals. In 2013 organs were taken from 36 deceased individuals.[74] Everyone who applies for a driver's licence in New Zealand indicates whether or not they wish to be a donor if they die in circumstances that would allow for donation.[75] The question is required to be answered for the application to be processed, meaning that the individual must answer yes or no, and does not have the option of leaving it unanswered.[75] However, the answer given on the drivers license does not constitute informed consent, because at the time of drivers license application not all individuals are equipped to make an informed decision regarding whether to be a donor, and it is therefore not the deciding factor in whether donation is carried out or not.[75] It is there to simply give indication of the person's wishes.[75] Family must agree to the procedure for donation to take place.[75][76]
A 2006 bill proposed setting up an organ donation register where people can give informed consent to organ donations and clearly state their legally binding wishes.[77] However, the bill did not pass, and there was condemnation of the bill from some doctors, who said that even if a person had given express consent for organ donation to take place, they would not carry out the procedure in the presence of any disagreement from grieving family members.[78]
The indigenous population of New Zealand also have strong views regarding organ donation. Many Maori people believe organ donation is morally unacceptable due to the cultural need for a dead body to remain fully intact.[79] However, because there is not a universally recognised cultural authority, no one view on organ donation is universally accepted in the Maori population.[79] They are, however, less likely to accept a kidney transplant than other New Zealanders, despite being overrepresented in the population receiving dialysis.[79]
Organ donation in Sri Lanka was ratified by the Human Tissue Transplantation Act No. 48 of 1987. Sri Lanka Eye Donation Society, a non-governmental organization established in 1961 has provided over 60,000 corneas for corneal transplantation, for patients in 57 countries. It is one of the major suppliers of human eyes to the world, with a supply of approximately 3,000 corneas per year.[80]
Over 121,000 people in need of an organ are on the U.S. government waiting list.[81] This crisis within the United States is growing rapidly because on average there are only 30,000 transplants performed each year. More than 8,000 people die each year from lack of a donor organ, an average of 22 people a day.[82] Between the years 1988 and 2006 the number of transplants doubled, but the number of patients waiting for an organ grew six times as large.[83] It has been estimated that the number of organs donated would double if every person with suitable organs decided to donate. In the past presumed consent was urged to try to decrease the need for organs. The Uniform Anatomical Gift Act of 1987 was adopted in several states, and allowed medical examiners to determine if organs and tissues of cadavers could be donated. By the 1980s, several states adopted different laws that allowed only certain tissues or organs to be retrieved and donated, some allowed all, and some did not allow any without consent of the family. In 2006 when the UAGA was revised, the idea of presumed consent was abandoned. In the United States today, organ donation is done only with consent of the family or donator themselves.[84] According to economist Alex Tabarrok, the shortage of organs has increased the use of so-called expanded criteria organs, or organs that used to be considered unsuitable for transplant.[69] Five patients that received kidney transplants at the University of Maryland School of Medicine developed cancerous or benign tumors which had to be removed. The head surgeon, Dr. Michael Phelan, explained that "the ongoing shortage of organs from deceased donors, and the high risk of dying while waiting for a transplant, prompted five donors and recipients to push ahead with the surgery."[69] Several organizations such as the American Kidney Fund are pushing for opt-out organ donation in the United States.[85]
In addition to their sick and annual leave, federal executive agency employees are entitled to 30 days paid leave for organ donation.[86] Thirty-two states (excluding only Alabama, Connecticut, Florida, Kentucky, Maine, Michigan, Montana, Nebraska, Nevada, New Hampshire, New Jersey, North Carolina, Pennsylvania, Rhode Island, South Dakota, Tennessee, Vermont, and Wyoming) and the District of Columbia also offer paid leave for state employees.[87] Five states (California, Hawaii, Louisiana, Minnesota, and Oregon) require certain private employers to provide paid leave for employees for organ or bone marrow donation, and seven others (Arkansas, Connecticut, Maine, Nebraska, New York, South Carolina, and West Virginia) either require employers to provide unpaid leave, or encourage employers to provide leave, for organ or bone marrow donation.[87]
A bill in the US House of Representatives, the Living Donor Protection Act (introduced in 2016, then reintroduced in 2017[88]), would amend the Family and Medical Leave Act of 1993 to provide leave under the act for an organ donor. If successful, this new law would permit "eligible employee" organ donors to receive up to 12 work weeks of leave in a 12-month period.[89][90]
Nineteen US states and the District of Columbia provide tax incentives for organ donation.[87] The most generous state tax incentive is Utah's tax credit, which covers up to $10,000 of unreimbursed expenses (travel, lodging, lost wages, and medical expenses) associated with organ or tissue donation.[87] Idaho (up to $5,000 of unreimbursed expenses) and Louisiana (up to $7,500 of 72% of unreimbursed expenses) also provide donor tax credits.[87] Arkansas, the District of Columbia, Louisiana and Pennsylvania provide tax credits to employers for wages paid to employees on leave for organ donation.[87] Thirteen states (Arkansas, Georgia, Iowa, Massachusetts, Mississippi, New Mexico, New York, North Dakota, Ohio, Oklahoma, Rhode Island and Wisconsin) have a tax deduction for up to $10,000 of unreimbursed costs, and Kansas and Virginia offer a tax deduction for up to $5,000 of unreimbursed costs.[87]
States have focused their tax incentives on unreimbursed costs associated with organ donation to ensure compliance with the National Organ Transplant Act of 1984.[91] NOTA prohibits, "any person to knowingly acquire, receive, or otherwise transfer any human organ for valuable consideration for use in human transplantation."[92] However, NOTA exempts, "the expenses of travel, housing, and lost wages incurred by the donor of a human organ in connection with the donation of the organ," from its definition of "valuable consideration."[92]
While offering income tax deductions has been the preferred method of providing tax incentives, some commentators have expressed concern that these incentives provide disproportionate benefits to wealthier donors.[93] Tax credits, on the other hand, are perceived as more equitable since the after tax benefit of the incentive is not tied to the marginal tax rate of the donor.[93] Additional tax favored approaches have been proposed for organ donation, including providing: tax credits to the families of deceased donors (seeking to encourage consent), refundable tax credits (similar to the earned income credit) to provide greater tax equity among potential donors, and charitable deductions for the donation of blood or organs.[94]
As stated above, under the National Organ Transplant Act of 1984, granting monetary incentives for organ donation is illegal in the United States.[95] However, there has been some discussion about providing fixed payment for potential live donors. In 1988, regulated paid organ donation was instituted in Iran and, as a result, the renal transplant waiting list was eliminated. Critics of paid organ donation argue that the poor and vulnerable become susceptible to transplant tourism. Travel for transplantation becomes transplant tourism if the movement of organs, donors, recipients or transplant professionals occurs across borders and involves organ trafficking or transplant commercialism. Poor and underserved populations in underdeveloped countries are especially vulnerable to the negative consequences of transplant tourism because they have become a major source of organs for the 'transplant tourists' that can afford to travel and purchase organs.[96]
In 1994 a law was passed in Pennsylvania which proposed to pay $300 for room and board and $3,000 for funeral expenses to an organ donor's family. Developing the program was an eight-year process; it is the first of its kind. Procurement directors and surgeons across the nation await the outcomes of Pennsylvania's program.[97] There have been at least nineteen families that have signed up for the benefit. Due to investigation of the program, however, there has been some concern whether the money collected is being used to assist families.[98] Some organizations, such as the National Kidney Foundation, oppose financial incentives associated with organ donation claiming, "Offering direct or indirect economic benefits in exchange for organ donation is inconsistent with our values as a society."[99] One argument is it will disproportionately affect the poor.[100] The $300–3,000 reward may act as an incentive for poorer individuals, as opposed to the wealthy who may not find the offered incentives significant. The National Kidney Foundation has noted that financial incentives, such as this Pennsylvania statute, diminish human dignity.[99]
Deontological issues are issues about whether a person has an ethical duty or responsibility to take an action. Nearly all scholars and societies around the world agree that voluntarily donating organs to sick people is ethically permissible. Although nearly all scholars encourage organ donation, fewer scholars believe that all people are ethically required to donate their organs after death. Similarly, nearly all religions support voluntary organ donation as a charitable act of great benefit to the community, although a few small groups, like the Roma (gypsies), oppose organ donation on religious grounds.[101] Issues surrounding patient autonomy, living wills, and guardianship make it nearly impossible for involuntary organ donation to occur.
From the standpoint of deontological ethics, the primary issues surrounding the morality of organ donation are semantic in nature. The debate over the definitions of life, death, human, and body is ongoing. For example, whether or not a brain-dead patient ought to be kept artificially animate in order to preserve organs for donation is an ongoing problem in clinical bioethics. In addition, some have argued that organ donation constitutes an act of self-harm, even when an organ is donated willingly.
Further, the use of cloning to produce organs with a genotype identical to the recipient is a controversial topic, especially considering the possibility for an entire person to be brought into being for the express purpose of being destroyed for organ procurement. While the benefit of such a cloned organ would be a zero-percent chance of transplant rejection, the ethical issues involved with creating and killing a clone may outweigh these benefits. However, it may be possible in the future to use cloned stem-cells to grow a new organ without creating a new human being.[citation needed]
A relatively new field of transplantation has reinvigorated the debate. Xenotransplantation, or the transfer of animal (usually pig) organs into human bodies, promises to eliminate many of the ethical issues, while creating many of its own.[citation needed] While xenotransplantation promises to increase the supply of organs considerably, the threat of organ transplant rejection and the risk of xenozoonosis, coupled with general anathema to the idea, decreases the functionality of the technique. Some animal rights groups oppose the sacrifice of an animal for organ donation and have launched campaigns to ban them.[citation needed]
On teleological or utilitarian grounds, the moral status of "black market organ donation" relies upon the ends, rather than the means.[citation needed] In so far as those who donate organs are often impoverished[citation needed] and those who can afford black market organs are typically well-off,[citation needed] it would appear that there is an imbalance in the trade. In many cases, those in need of organs are put on waiting lists for legal organs for indeterminate lengths of time — many die while still on a waiting list.
Organ donation is fast becoming an important bioethical issue from a social perspective as well. While most first-world nations have a legal system of oversight for organ transplantation, the fact remains that demand far outstrips supply. Consequently, there has arisen a black market trend often referred to as transplant tourism.[citation needed] The issues are weighty and controversial. On the one hand are those who contend that those who can afford to buy organs are exploiting those who are desperate enough to sell their organs. Many suggest this results in a growing inequality of status between the rich and the poor. On the other hand, are those who contend that the desperate should be allowed to sell their organs and that preventing them from doing so is merely contributing to their status as impoverished. Further, those in favor of the trade hold that exploitation is morally preferable to death, and in so far as the choice lies between abstract notions of justice on the one hand and a dying person whose life could be saved on the other hand, the organ trade should be legalized. Conversely, surveys conducted among living donors postoperatively and in a period of five years following the procedure have shown extreme regret in a majority of the donors, who said that given the chance to repeat the procedure, they would not.[102] Additionally, many study participants reported a decided worsening of economic condition following the procedure.[103] These studies looked only at people who sold a kidney in countries where organ sales are already legal.
A consequence of the black market for organs has been a number of cases and suspected cases of organ theft,[104][105] including murder for the purposes of organ theft.[106][107] Proponents of a legal market for organs say that the black-market nature of the current trade allows such tragedies and that regulation of the market could prevent them. Opponents say that such a market would encourage criminals by making it easier for them to claim that their stolen organs were legal.
Legalization of the organ trade carries with it its own sense of justice as well[citation needed]. Continuing black-market trade creates further disparity on the demand side: only the rich can afford such organs. Legalization of the international organ trade could lead to increased supply, lowering prices so that persons outside the wealthiest segments could afford such organs as well.
Exploitation arguments generally come from two main areas:
The New Cannibalism is a phrase coined by anthropologist Nancy Scheper-Hughes in 1998 for an article written for The New Internationalist. Her argument was that the actual exploitation is an ethical failing, a human exploitation; a perception of the poor as organ sources which may be used to extend the lives of the wealthy.[112]
Economic drivers leading to increased donation are not limited to areas such as India and Africa, but also are emerging in the United States. Increasing funeral expenses combined with decreasing real value of investments such as homes and retirement savings which took place in the 2000s have purportedly led to an increase in citizens taking advantage of arrangements where funeral costs are reduced or eliminated.[113]
Brain death may result in legal death, but still with the heart beating and with mechanical ventilation, keeping all other vital organs alive and functional for a certain period of time. Given long enough, patients who do not fully die in the complete biological sense, but who are declared brain dead, will usually start to build up toxins and wastes in the body. In this way, the organs can eventually dysfunction due to coagulopathy, fluid or electrolyte and nutrient imbalances, or even fail. Thus, the organs will usually only be sustainable and viable for acceptable use up until a certain length of time. This may depend on factors such as how well the patient is maintained, any comorbidities, the skill of the healthcare teams and the quality their facilities.[114][unreliable medical source?] A major point of contention is whether transplantation should be allowed at all if the patient is not yet fully biologically dead, and if brain death is acceptable, whether the person's whole brain needs to have died, or if the death of a certain part of the brain is enough for legal and ethical and moral purposes.
Most organ donation for organ transplantation is done in the setting of brain death. However, in Japan this is a fraught point, and prospective donors may designate either brain death or cardiac death – see organ transplantation in Japan. In some nations such as Belgium, Poland, Portugal, Spain and France, everyone is automatically an organ donor, although some jurisdictions, such as Singapore, Portugal, Poland, New Zealand, and Netherlands, allow opting out of the system. Elsewhere, consent from family members or next-of-kin is required for organ donation. The non-living donor is kept on ventilator support until the organs have been surgically removed. If a brain-dead individual is not an organ donor, ventilator and drug support is discontinued and cardiac death is allowed to occur.
In the United States, where since the 1980s the Uniform Determination of Death Act has defined death as the irreversible cessation of the function of either the brain or the heart and lungs,[115] the 21st century has seen an order-of-magnitude increase of donation following cardiac death. In 1995, only one out of 100 dead donors in the nation gave their organs following the declaration of cardiac death. That figure grew to almost 11 percent in 2008, according to the Scientific Registry of Transplant Recipients.[115] That increase has provoked ethical concerns about the interpretation of "irreversible" since "patients may still be alive five or even 10 minutes after cardiac arrest because, theoretically, their hearts could be restarted, [and thus are] clearly not dead because their condition was reversible."[115]
There are also controversial issues regarding how organs are allocated to recipients.[clarify] For example, some believe that livers should not be given to alcoholics in danger of reversion, while others view alcoholism as a medical condition like diabetes.[citation needed] Faith in the medical system is important to the success of organ donation. Brazil switched to an opt-out system and ultimately had to withdraw it because it further alienated patients who already distrusted the country's medical system.[116] Adequate funding, strong political will to see transplant outcomes improve, and the existence of specialized training, care and facilities also increase donation rates. Expansive legal definitions of death, such as Spain uses, also increase the pool of eligible donors by allowing physicians to declare a patient to be dead at an earlier stage, when the organs are still in good physical condition.Allowing or forbidding payment for organs affects the availability of organs. Generally, where organs cannot be bought or sold, quality and safety are high, but supply is not adequate to the demand. Where organs can be purchased, the supply increases.[117]
Iran adopted a system of paying kidney donors in 1988 and within 11 years it became the only country in the world to clear its waiting list for transplants.Healthy humans have two kidneys, a redundancy that enables living donors (inter vivos) to give a kidney to someone who needs it. The most common transplants are to close relatives, but people have given kidneys to other friends. The rarest type of donation is the undirected donation whereby a donor gives a kidney to a stranger. Less than a few hundred of such kidney donations have been performed. In recent years, searching for altruistic donors via the internet has also become a way to find life saving organs. However, internet advertising for organs is a highly controversial practice, as some scholars believe it undermines the traditional list-based allocation system.[118]
The National Transplant Organization of Spain is one of the most successful in the world (Spain has been the world leader in organ donation for decades),[119] but it still cannot meet the demand, as 10% of those needing a transplant die while still on the transplant list.[120] Donations from corpses are anonymous, and a network for communication and transport allows fast extraction and transplant across the country. Under Spanish law, every corpse can provide organs unless the deceased person had expressly rejected it. Because family members still can forbid the donation,[121] carefully trained doctors ask the family for permission, making it very similar in practice to the United States system.[122]
In the overwhelming majority of cases, organ donation is not possible for reasons of recipient safety, match failures, or organ condition. Even in Spain, which has the highest organ donation rate in the world, there are only 35.1 actual donors per million people, and there are hundreds of patients on the waiting list.[116] This rate compares to 24.8 per million in Austria, where families are rarely asked to donate organs, and 22.2 per million in France, which—like Spain—has a presumed-consent system.
In the United States, prisoners are not discriminated against as organ recipients and are equally eligible for organ transplants along with the general population. A 1976 U.S. Supreme Court case[123] ruled that withholding health care from prisoners constituted "cruel and unusual punishment". United Network for Organ Sharing, the organization that coordinates available organs with recipients, does not factor a patient's prison status when determining suitability for a transplant.[124][125]
An organ transplant and follow-up care can cost the prison system up to one million dollars.[125][126] If a prisoner qualifies, a state may allow compassionate early release to avoid high costs associated with organ transplants.[125] However, an organ transplant may save the prison system substantial costs associated with dialysis and other life-extending treatments required by the prisoner with the failing organ. For example, the estimated cost of a kidney transplant is about $111,000.[127] A prisoner's dialysis treatments are estimated to cost a prison $120,000 per year.[128]
Because donor organs are in short supply, there are more people waiting for a transplant than available organs. When a prisoner receives an organ, there is a high probability that someone else will die waiting for the next available organ. A response to this ethical dilemma states that felons who have a history of violent crime, who have violated others’ basic rights, have lost the right to receive an organ transplant, though it is noted that it would be necessary "to reform our justice system to minimize the chance of an innocent person being wrongly convicted of a violent crime and thus being denied an organ transplant"[129]
Prisons typically do not allow inmates to donate organs to anyone but immediate family members. There is no law against prisoner organ donation; however, the transplant community has discouraged use of prisoner's organs since the early 1990s due to concern over prisons' high-risk environment for infectious diseases.[130] Physicians and ethicists also criticize the idea because a prisoner is not able to consent to the procedure in a free and non-coercive environment,[131] especially if given inducements to participate. However, with modern testing advances to more safely rule out infectious disease and by ensuring that there are no incentives offered to participate, some have argued that prisoners can now voluntarily consent to organ donation just as they can now consent to medical procedures in general. With careful safeguards, and with over 2 million prisoners in the U.S., they reason that prisoners can provide a solution for reducing organ shortages in the U.S.[132]
While some have argued that prisoner participation would likely be too low to make a difference, one Arizona program started by former Maricopa County Sheriff Joe Arpaio encourages inmates to voluntarily sign up to donate their heart and other organs.[133] As of mid-2012, over 10,000 inmates had signed up in that one county alone.[134]
There are several different religions that have different perspectives. Islam has a conflicting view regarding the issue, with half believing that it is against the religion. However, Muhammed claims his believers should seek medical attention when in need and saving life is a very important factor of the Islamic religion. Buddhism is mostly against the practice, because it disrespects the bodies of ancestors and nature. Christianity is the most lenient on the topic of organ donation, and believe it is a service of life.[135]
All major religions accept organ donation in at least some form[136] on either utilitarian grounds (i.e., because of its life-saving capabilities) or deontological grounds (e.g., the right of an individual believer to make his or her own decision).[citation needed] Most religions, among them the Roman Catholic Church, support organ donation on the grounds that it constitutes an act of charity and provides a means of saving a life, consequently Pope Francis is an organ donor. One religious group, The Jesus Christians, became known as "The Kidney Cult" because more than half its members had donated their kidneys altruistically. Jesus Christians claim altruistic kidney donation is a great way to "Do unto others what they would want you to do unto them."[137] Some religions impose certain restrictions on the types of organs that may be donated and/or on the means by which organs may be harvested and/or transplanted.[138] For example, Jehovah's Witnesses require that organs be drained of any blood due to their interpretation of the Hebrew Bible/Christian Old Testament as prohibiting blood transfusion,[139] and Muslims require that the donor have provided written consent in advance.[139] A few groups disfavor organ transplantation or donation; notably, these include Shinto[140] and those who follow the customs of the Gypsies.[139]
Orthodox Judaism considers organ donation obligatory if it will save a life, as long as the donor is considered dead as defined by Jewish law.[139] In both Orthodox Judaism and non-Orthodox Judaism, the majority view holds that organ donation is permitted in the case of irreversible cardiac rhythm cessation. In some cases, rabbinic authorities believe that organ donation may be mandatory, whereas a minority opinion considers any donation of a live organ as forbidden.[141]
The demand for organs significantly surpasses the number of donors everywhere in the world. There are more potential recipients on organ donation waiting lists than organ donors.[142] In particular, due to significant advances in dialysis techniques, patients suffering from end-stage renal disease (ESRD) can survive longer than ever before.[143] Because these patients don't die as quickly as they used to, and as kidney failure increases with the rising age and prevalence of high blood pressure and diabetes in a society, the need especially for kidneys rises every year.[144]
As of  March 2014[update], about 121,600 people in the United States are on the waiting list, although about a third of those patients are inactive and could not receive a donated organ.[145][146] Wait times and success rates for organs differ significantly between organs due to demand and procedure difficulty. As of  2007[update], three-quarters of patients in need of an organ transplant were waiting for a kidney,[147] and as such kidneys have much longer waiting times. As stated by the Gift of Life Donor Program website, the median patient who ultimately received an organ waited 4 months for a heart or lung — but 18 months for a kidney, and 18–24 months for a pancreas because demand for these organs substantially outstrips supply.[148]
In Australia, there are 10.8 transplants per million people,[149] about a third of the Spanish rate. The Lions Eye Institute, in Western Australia, houses the Lions Eye Bank. The Bank was established in 1986 and coordinates the collection, processing and distribution of eye tissue for transplantation. The Lions Eye Bank also maintains a waitlist of patients who require corneal graft operations.
About 100 corneas are provided by the Bank for transplant each year, but there is still a waiting list for corneas.[150]"To an economist, this is a basic supply-and-demand gap with tragic consequences."[151] Approaches to addressing this shortfall include:
In hospitals, organ network representatives routinely screen patient records to identify potential donors shortly in advance of their deaths.[157] In many cases, organ-procurement representatives will request screening tests (such as blood typing) or organ-preserving drugs (such as blood pressure drugs) to keep potential donors' organs viable until their suitability for transplants can be determined and family consent (if needed) can be obtained.[157] This practice increases transplant efficiency, as potential donors who are unsuitable due to infection or other causes are removed from consideration before their deaths, and decreases the avoidable loss of organs.[157] It may also benefit families indirectly, as the families of unsuitable donors are not approached to discuss organ donation.[157]
The Center for Ethical Solutions, an American bioethics think tank, is currently working on a project called "Solving the Organ Shortage", in which it is studying the Iranian kidney procurement system in order to better inform the debate over solving the organ shortfall in the United States.[158]
The United States has two agencies that govern organ procurement and distribution within the country. The United Network for Organ Sharing and the Organ Procurement and Transplant Network (OPTN) regulate Organ Procurement Organizations (OPO) with regard to procurement and distribution ethics and standards. OPOs are non-profit organizations charged with the evaluation, procurement and allocation of organs within their Designated Service Area (DSA). Once a donor has been evaluated and consent obtained, provisional allocation of organs commences. UNOS developed a computer program that automatically generates donor specific match lists for suitable recipients based on the criteria that the patient was listed with. OPO coordinators enter donor information into the program and run the respective lists. Organ offers to potential recipients are made to transplant centers to make them aware of a potential organ. The surgeon will evaluate the donor information and make a provisional determination of medical suitability to their recipient. Distribution varies slightly between different organs but is essentially very similar. When lists are generated many factors are taken into consideration; these factors include: distance of transplant center from the donor hospital, blood type, medical urgency, wait time, donor size and tissue typing. For heart recipients medical urgency is denoted by a recipients "Status" (Status 1A, 1B and status 2). Lungs are allocated based on a recipients Lung Allocation Score (LAS) that is determined based on the urgency of clinical need as well as the likelihood of benefit from the transplant. Livers are allocated using both a status system and MELD/PELD score (Model for End-stage Liver Disease/Pediatric End-stage Liver Disease). Kidney and pancreas lists are based on location, blood type, Human Leukocyte Antigen (HLA) typing and wait time. When a recipient for a kidney or pancreas has no direct antibodies to the donor HLA the match is said to be a 0 ABDR mismatch or zero antigen mismatch. A zero mismatch organ has a low rate of rejection and allows a recipient to be on lower doses of immunosuppressive drugs. Since zero mismatches have such high graft survival these recipients are afforded priority regardless of location and wait time. UNOS has in place a "Payback" system to balance organs that are sent out of a DSA because of a zero mismatch.
Location of a transplant center with respect to a donor hospital is given priority due to the effects of Cold Ischemic Time (CIT). Once the organ is removed from the donor, blood no longer perfuses through the vessels and begins to starve the cells of oxygen (ischemia). Each organ tolerates different ischemic times. Hearts and lungs need to be transplanted within 4–6 hours from recovery, liver about 8–10 hours and pancreas about 15 hours; kidneys are the most resilient to ischemia.[citation needed] Kidneys packaged on ice can be successfully transplanted 24–36 hours after recovery. Developments in kidney preservation have yielded a device that pumps cold preservation solution through the kidneys vessels to prevent Delayed Graft Function (DGF) due to ischemia. Perfusion devices, often called kidney pumps, can extend graft survival to 36–48 hours post recovery for kidneys. Recently similar devices have been developed for the heart and lungs, in an effort to increase distances procurement teams may travel to recover an organ.
People committing suicide have a higher rate of donating organs than average. One reason is lower negative response or refusal rate by the family and relatives, but the explanation for this remains to be clarified.[159] In addition, donation consent is higher than average from people committing suicide.[160]
Attempted suicide is a common cause of brain death (3.8%), mainly among young men.[159] Organ donation is more common in this group compared to other causes of death. Brain death may result in legal death, but still with the heart beating, and with mechanical ventilation all other vital organs may be kept completely alive and functional,[114] providing optimal opportunities for organ transplantation.
In 2008, California transplant surgeon Hootan Roozrokh was charged with dependent adult abuse for prescribing what prosecutors alleged were excessive doses of morphine and sedatives to hasten the death of a man with adrenal leukodystrophy and irreversible brain damage, in order to procure his organs for transplant.[161] The case brought against Roozrokh was the first criminal case against a transplant surgeon in the US, and resulted in his acquittal. Further, Dr. Roozrokh successfully sued for defamation stemming from the incident.[162]
At California's Emanuel Medical Center, neurologist Narges Pazouki, MD, said an organ-procurement organization representative pressed her to declare a patient brain-dead before the appropriate tests had been done.[157] In September 1999, eBay blocked an auction for "one functional human kidney" which had reached a highest bid of $5.7 million. Under United States federal laws, eBay was obligated to dismiss the auction for the selling of human organs which is punishable by up to five years in prison and a $50,000 fine.[163]
On June 27, 2008, Indonesian Sulaiman Damanik, 26, pleaded guilty in a Singapore court for sale of his kidney to CK Tang's executive chair, Mr. Tang Wee Sung, 55, for 150 million rupiah (US$17,000). The Transplant Ethics Committee must approve living donor kidney transplants. Organ trading is banned in Singapore and in many other countries to prevent the exploitation of "poor and socially disadvantaged donors who are unable to make informed choices and suffer potential medical risks." Toni, 27, the other accused, donated a kidney to an Indonesian patient in March, alleging he was the patient's adopted son, and was paid 186 million rupiah (US$21,000).
Marketing for organ donation must walk a fine line between stressing the need for organ donation and not being too forceful.[164] If the marketing agent is too forceful, then the target of the message will react defensively to the request. According to psychological reactance theory, a person will perceive their freedom threatened and will react to restore the freedom. According to Ashley Anker, the use of transportation theory has a positive effect on target reactions by marketing attempts.[164] When public service announcements use recipient-focused messages, targets were more transported. Individuals who watched recipient-focused messages were more transported because potential donors experience empathy for the potential recipient. Future public service announcements should use recipient-focused stories to elicit relationship formation between potential donors and recipients.
Awareness about organ donation leads to greater social support for organ donation, in turn leading to greater registration. By starting with promoting college students’ awareness of organ donation and moving to increasing social support for organ donation, the more likely people will be to register as organ donors.[165]
The United States Department of Health funded a study by the University of Wisconsin Hospital to increase efforts to increase awareness and the amount of registered donors by pursuing members of the university and their family and friends through social media.[166] The results of the study showed a 20% increase in organ donation by creating support and awareness through social media.[166]



Carcinogen - Wikipedia
A carcinogen is any substance, radionuclide, or radiation that promotes carcinogenesis, the formation of cancer. This may be due to the ability to damage the genome or to the disruption of cellular metabolic processes. Several radioactive substances are considered carcinogens, but their carcinogenic activity is attributed to the radiation, for example gamma rays and alpha particles, which they emit. Common examples of non-radioactive carcinogens are inhaled asbestos, certain dioxins, and tobacco smoke.  Although the public generally associates carcinogenicity with synthetic chemicals, it is equally likely to arise in both natural and synthetic substances.[1] Carcinogens are not necessarily immediately toxic; thus, their effect can be insidious.
Cancer is any disease in which normal cells are damaged and do not undergo programmed cell death as fast as they divide via mitosis. Carcinogens may increase the risk of cancer by altering cellular metabolism or damaging DNA directly in cells, which interferes with biological processes, and induces the uncontrolled, malignant division, ultimately leading to the formation of tumors. Usually, severe DNA damage leads to programmed cell death, but if the programmed cell death pathway is damaged, then the cell cannot prevent itself from becoming a cancer cell.
There are many natural carcinogens. Aflatoxin B1, which is produced by the fungus Aspergillus flavus growing on stored grains, nuts and peanut butter, is an example of a potent, naturally occurring microbial carcinogen. Certain viruses such as hepatitis B and human papilloma virus have been found to cause cancer in humans. The first one shown to cause cancer in animals is Rous sarcoma virus, discovered in 1910 by Peyton Rous.  Other infectious organisms which cause cancer in humans include some bacteria (e.g. Helicobacter pylori [2][3]) and helminths (e.g. Opisthorchis viverrini [4] and Clonorchis sinensis [5].
Dioxins and dioxin-like compounds, benzene, kepone, EDB, and asbestos have all been classified as carcinogenic.[6] As far back as the 1930s, Industrial smoke and tobacco smoke were identified as sources of dozens of carcinogens, including benzo[a]pyrene, tobacco-specific nitrosamines such as nitrosonornicotine, and reactive aldehydes such as formaldehyde, which is also a hazard in embalming and making plastics. Vinyl chloride, from which PVC is manufactured, is a carcinogen and thus a hazard in PVC production.
Co-carcinogens are chemicals that do not necessarily cause cancer on their own, but promote the activity of other carcinogens in causing cancer.
After the carcinogen enters the body, the body makes an attempt to eliminate it through a process called biotransformation. The purpose of these reactions is to make the carcinogen more water-soluble so that it can be removed from the body. However, in some cases, these reactions can also convert a less toxic carcinogen into a more toxic carcinogen.
DNA is nucleophilic; therefore, soluble carbon electrophiles are carcinogenic, because DNA attacks them. For example, some alkenes are toxicated by human enzymes to produce an electrophilic epoxide. DNA attacks the epoxide, and is bound permanently to it. This is the mechanism behind the carcinogenicity of benzo[a]pyrene in tobacco smoke, other aromatics, aflatoxin and mustard gas.
CERCLA identifies all radionuclides as carcinogens, although the nature of the emitted radiation (alpha, beta, gamma, or neutron and the radioactive strength), its consequent capacity to cause ionization in tissues, and the magnitude of radiation exposure, determine the potential hazard. Carcinogenicity of radiation depends on the type of radiation, type of exposure, and penetration. For example, alpha radiation has low penetration and is not a hazard outside the body, but emitters are carcinogenic when inhaled or ingested.  For example, Thorotrast, a (incidentally radioactive) suspension previously used as a contrast medium in x-ray diagnostics, is a potent human carcinogen known because of its retention within various organs and persistent emission of alpha particles.  Low-level ionizing radiation may induce irreparable DNA damage (leading to replicational and transcriptional errors needed for neoplasia or may trigger viral interactions) leading to pre-mature aging and cancer.[8][9][10]
Not all types of electromagnetic radiation are carcinogenic. Low-energy waves on the electromagnetic spectrum including radio waves, microwaves, infrared radiation and visible light are thought not to be, because they have insufficient energy to break chemical bonds. Evidence for carcinogenic effects of non-ionizing radiation is generally inconclusive, though there are some documented cases of radar technicians with prolonged high exposure experiencing significantly higher cancer incidence.[11]
Higher-energy radiation, including ultraviolet radiation (present in sunlight), x-rays, and gamma radiation, generally is carcinogenic, if received in sufficient doses.   For most people, ultraviolet radiations from sunlight is the most common cause of skin cancer.  In Australia, where people with pale skin are often exposed to strong sunlight, melanoma is the most common cancer diagnosed in people aged 15–44 years.[12][13]
Substances or foods irradiated with electrons or electromagnetic radiation (such as microwave, X-ray or gamma) are not carcinogenic.[citation needed] In contrast, non-electromagnetic neutron radiation produced inside nuclear reactors can produce secondary radiation through nuclear transmutation.
Chemicals used in processed and cured meat such as some brands of bacon, sausages and ham may or may not produce carcinogens.[14] For example, nitrites used as food preservatives in cured meat such as bacon have also been noted as being carcinogenic with demographic links, but not causation, to colon cancer.[15] Cooking food at high temperatures, for example grilling or barbecuing meats, can, or can not, also lead to the formation of minute quantities of many potent carcinogens that are comparable to those found in cigarette smoke (i.e., benzo[a]pyrene).[16] Charring of food looks like coking and tobacco pyrolysis, and produces carcinogens. There are several carcinogenic pyrolysis products, such as polynuclear aromatic hydrocarbons, which are converted by human enzymes into epoxides, which attach permanently to DNA. Pre-cooking meats in a microwave oven for 2–3 minutes before grilling shortens the time on the hot pan, and removes heterocyclic amine (HCA) precursors, which can help minimize the formation of these carcinogens.[17]
Reports from the Food Standards Agency have found that the known animal carcinogen acrylamide is generated in fried or overheated carbohydrate foods (such as french fries and potato chips).[18] Studies are underway at the FDA and European regulatory agencies to assess its potential risk to humans.
There is a strong association of smoking with lung cancer; the lifetime risk of developing lung cancer increases significantly in smokers.[19] A large number of known carcinogens are found in cigarette smoke. Potent carcinogens found in cigarette smoke include polycyclic aromatic hydrocarbons (PAH, such as benzo[a]pyrene), Benzene, and Nitrosamine.[20] The tar from cigarette smoke is similar to that of marijuana smoke and  contains similar carcinogens.[21]
Carcinogens can be classified as genotoxic or nongenotoxic. Genotoxins cause irreversible genetic damage or mutations by binding to DNA. Genotoxins include chemical agents like N-nitroso-N-methylurea (NMU) or non-chemical agents such as ultraviolet light and ionizing radiation. Certain viruses can also act as carcinogens by interacting with DNA.
Nongenotoxins do not directly affect DNA but act in other ways to promote growth. These include hormones and some organic compounds.[22]
The International Agency for Research on Cancer (IARC) is an intergovernmental agency established in 1965, which forms part of the World Health Organization of the United Nations. It is based in Lyon, France. Since 1971 it has published a series of Monographs on the Evaluation                          of Carcinogenic Risks to Humans[23] that have been highly influential in the classification of possible carcinogens.
The Globally Harmonized System of Classification and Labelling of Chemicals (GHS) is a United Nations initiative to attempt to harmonize the different systems of assessing chemical risk which currently exist (as of March 2009) around the world. It classifies carcinogens into two categories, of which the first may be divided again into subcategories if so desired by the competent regulatory authority:
The National Toxicology Program of the U.S. Department of Health and Human Services is mandated to produce a biennial Report on Carcinogens.[24] As of June 2011, the latest edition was the 12th report (2011).[6] It classifies carcinogens into two groups:
The American Conference of Governmental Industrial Hygienists (ACGIH) is a private organization best known for its publication of threshold limit values (TLVs) for occupational exposure and monographs on workplace chemical hazards. It assesses carcinogenicity as part of a wider assessment of the occupational hazards of chemicals.
The European Union classification of carcinogens is contained in the Dangerous Substances Directive and the Dangerous Preparations Directive. It consists of three categories:
This assessment scheme is being phased out in favor of the GHS scheme (see above), to which it is very close in category definitions.
Under a previous name, the NOHSC, in 1999 Safe Work Australia published the Approved Criteria for Classifying Hazardous Substances [NOHSC:1008(1999)].[25]
Section 4.76 of this document outlines the criteria for classifying carcinogens as approved by the Australian government. This classification consists of three categories:
A procarcinogen is a precursor to a carcinogen. One example is nitrites when taken in by the diet. They are not carcinogenic themselves, but turn into nitrosamines in the body, which can be carcinogenic.[26]
Occupational carcinogens are agents that pose a risk of cancer in several specific work-locations:
Not in widespread use, but found in:
circadian disruption[29]
In this section, the carcinogens implicated as the main causative agents of the four most common cancers worldwide are briefly described. These four cancers are lung, breast, colon, and stomach cancers.  Together they account for about 41% of worldwide cancer incidence and 42% of cancer deaths (for more detailed information on the carcinogens implicated in these and other cancers, see references[33][34]).
Lung cancer (pulmonary carcinoma) is the most common cancer in the world, both in terms of cases (1.6 million cases; 12.7% of total cancer cases) and deaths (1.4 million deaths; 18.2% of total cancer deaths).[35]  Lung cancer is largely caused by tobacco smoke. Risk estimates for lung cancer in the United States indicate that tobacco smoke is responsible for 90% of lung cancers. Other factors are implicated in lung cancer, and these factors can interact synergistically with smoking so that total attributable risk adds up to more than 100%. These factors include occupational exposure to carcinogens (about 9-15%), radon (10%) and outdoor air pollution (1-2%).[36] Tobacco smoke is a complex mixture of more than 5,300 identified chemicals. The most important carcinogens in tobacco smoke have been determined by a “Margin of Exposure” approach.[37] Using this approach, the most important tumorigenic compounds in tobacco smoke were, in order of importance, acrolein, formaldehyde, acrylonitrile, 1,3-butadiene, cadmium, acetaldehyde, ethylene oxide, and isoprene. Most of these compounds cause DNA damage by forming DNA adducts or by inducing other alterations in DNA.[34] DNA damages are subject to error-prone DNA repair or can cause replication errors. Such errors in repair or replication can result in mutations in tumor suppressor genes or oncogenes leading to cancer.
Breast cancer is the second most common cancer [(1.4 million cases, 10.9%), but ranks 5th as cause of death (458,000, 6.1%)].[35]  Increased risk of breast cancer is associated with persistently elevated blood levels of estrogen.[38]  Estrogen appears to contribute to breast carcinogenesis by three processes; (1) the metabolism of estrogen to genotoxic, mutagenic carcinogens, (2) the stimulation of tissue growth, and (3) the repression of phase II detoxification enzymes that metabolize ROS leading to increased oxidative DNA damage.[39][40][41]  The major estrogen in humans, estradiol, can be metabolized to quinone derivatives that form adducts with DNA.[42]  These derivatives can cause dupurination, the removal of bases from the phosphodiester backbone of DNA, followed by inaccurate repair or replication of the apurinic site leading to mutation and eventually cancer.  This genotoxic mechanism may interact in synergy with estrogen receptor-mediated, persistent cell proliferation to ultimately cause breast cancer.[42] Genetic background, dietary practices and environmental factors also likely contribute to the incidence of DNA damage and breast cancer risk.
Colorectal cancer is the third most common cancer [1.2 million cases (9.4%), 608,000 deaths (8.0%)].[35] Tobacco smoke may be responsible for up to 20% of colorectal cancers in the United States.[43]  In addition, substantial evidence implicates bile acids as an important factor in colon cancer.  Twelve studies (summarized in Bernstein et al.[44]) indicate that the bile acids deoxycholic acid (DCA) and/or lithocholic acid (LCA) induce production of DNA-damaging reactive oxygen species and/or reactive nitrogen species in human or animal colon cells.  Furthermore, 14 studies showed that DCA and LCA induce DNA damage in colon cells.  Also 27 studies reported that bile acids cause programmed cell death (apoptosis).  Increased apoptosis can result in selective survival of cells that are resistant to induction of apoptosis.[44]  Colon cells with reduced ability to undergo apoptosis in response to DNA damage would tend to accumulate mutations, and such cells may give rise to colon cancer.[44]  Epidemiologic studies have found that fecal bile acid concentrations are increased in populations with a high incidence of colon cancer.  Dietary increases in total fat or saturated fat result in elevated DCA and LCA in feces and elevated exposure of the colon epithelium to these bile acids.  When the bile acid DCA was added to the standard diet of wild-type mice invasive colon cancer was induced in 56% of the mice after 8 to 10 months.[45]  Overall, the available evidence indicates that DCA and LCA are centrally important DNA-damaging carcinogens in colon cancer.
Stomach cancer is the fourth most common cancer [990,000 cases (7.8%), 738,000 deaths (9.7%)].[35] Helicobacter pylori infection is the main causative factor in stomach cancer.  Chronic gastritis (inflammation) caused by H. pylori is often long-standing if not treated.  Infection of gastric epithelial cells with H. pylori results in increased production of reactive oxygen species (ROS).[46][47]  ROS cause oxidative DNA damage including the major base alteration 8-hydroxydeoxyguanosine (8-OHdG).  8-OHdG resulting from ROS is increased in chronic gastritis.  The altered DNA base can cause errors during DNA replication that have mutagenic and carcinogenic potential.  Thus H. pylori-induced ROS appear to be the major carcinogens in stomach cancer because they cause oxidative DNA damage leading to carcinogenic mutations.  Diet is thought to be a contributing factor in stomach cancer - in Japan where very salty pickled foods are popular, the incidence of stomach cancer is high.  Preserved meat such as bacon, sausages, and ham increases the risk while a diet high in fresh fruit and vegetables may reduce the risk. The risk also increases with age.[48]



Tobacco smoking - Wikipedia

Tobacco smoking is the practice of smoking tobacco and inhaling tobacco smoke (consisting of particle and gaseous phases). (A more broad definition may include simply taking tobacco smoke into the mouth, and then releasing it, as is done by some with tobacco pipes and cigars.) The practice is believed to have begun as early as 5000–3000 BC in Mesoamerica and South America.[1] Tobacco was introduced to Eurasia in the late 17th century by European colonists, where it followed common trade routes. The practice encountered criticism from its first import into the Western world onwards but embedded itself in certain strata of a number of societies before becoming widespread upon the introduction of automated cigarette-rolling apparatus.[2][3]
German scientists identified a link between smoking and lung cancer in the late 1920s, leading to the first anti-smoking campaign in modern history, albeit one truncated by the collapse of Nazi Germany at the end of World War II.[4] In 1950, British researchers demonstrated a clear relationship between smoking and cancer.[5] Evidence continued to mount in the 1980s, which prompted political action against the practice. Rates of consumption since 1965 in the developed world have either peaked or declined.[6] However, they continue to climb in the developing world.[7]
Smoking is the most common method of consuming tobacco, and tobacco is the most common substance smoked. The agricultural product is often mixed with additives[8] and then combusted. The resulting smoke is then inhaled and the active substances absorbed through the alveoli in the lungs or the oral mucosa.[9] Combustion was traditionally enhanced by addition of potassium or nitrates.[citation needed] Many substances in cigarette smoke trigger chemical reactions in nerve endings, which heighten heart rate, alertness[10] and reaction time, among other things.[11] Dopamine and endorphins are released, which are often associated with pleasure.[12] As of 2008 to 2010, tobacco is used by about 49% of men and 11% of women aged 15 or older in fourteen low-income and middle-income countries (Bangladesh, Brazil, China, Egypt, India, Mexico, Philippines, Poland, Russia, Thailand, Turkey, Ukraine, Uruguay and Vietnam), with about 80% of this usage in the form of smoking.[13] The gender gap tends to be less pronounced in lower age groups.[14][15]
Many smokers begin during adolescence or early adulthood.[16] During the early stages, a combination of perceived pleasure acting as positive reinforcement and desire to respond to social peer pressure may offset the unpleasant symptoms of initial use, which typically include nausea and coughing. After an individual has smoked for some years, the avoidance of withdrawal symptoms and negative reinforcement become the key motivations to continue.
A study of first smoking experiences of seventh-grade students found out that the most common factor leading students to smoke is cigarette advertisements. Smoking by parents, siblings and friends also encourages students to smoke.[17]
Smoking's history dates back to as early as 5000–3000 BC, when the agricultural product began to be cultivated in Mesoamerica and South America; consumption later evolved into burning the plant substance either by accident or with intent of exploring other means of consumption.[1] The practice worked its way into shamanistic rituals.[18] Many ancient civilizations — such as the Babylonians, the Indians, and the Chinese — burnt incense during religious rituals. Smoking in the Americas probably had its origins in the incense-burning ceremonies of shamans but was later adopted for pleasure or as a social tool.[19] The smoking of tobacco and various hallucinogenic drugs was used to achieve trances and to come into contact with the spirit world.
Eastern North American tribes would carry large amounts of tobacco in pouches as a readily accepted trade item and would often smoke it in ceremonial pipes, either in sacred ceremonies or to seal bargains.[20] Adults as well as children enjoyed the practice.[21] It was believed that tobacco was a gift from the Creator and that the exhaled tobacco smoke was capable of carrying one's thoughts and prayers to heaven.[22]
Apart from smoking, tobacco had a number of uses as medicine. As a pain killer it was used for earache and toothache and occasionally as a poultice. Smoking was said by the desert Indians to be a cure for colds, especially if the tobacco was mixed with the leaves of the small desert Sage, Salvia dorrii, or the root of Indian balsam or cough root, Leptotaenia multifida, the addition of which was thought to be particularly good for asthma and tuberculosis.[23]
In 1612, six years after the settlement of Jamestown, Virginia, John Rolfe was credited as the first settler to successfully raise tobacco as a cash crop. The demand quickly grew as tobacco, referred to as "brown gold", revived the Virginia joint stock company from its failed gold expeditions.[24] In order to meet demands from the Old World, tobacco was grown in succession, quickly depleting the soil. This became a motivator to settle west into the unknown continent, and likewise an expansion of tobacco production.[25] Indentured servitude became the primary labor force up until Bacon's Rebellion, from which the focus turned to slavery.[26] This trend abated following the American Revolution as slavery became regarded as unprofitable. However, the practice was revived in 1794 with the invention of the cotton gin.[27]
Frenchman Jean Nicot (from whose name the word nicotine is derived) introduced tobacco to France in 1560, and tobacco then spread to England. The first report of a smoking Englishman is of a sailor in Bristol in 1556, seen "emitting smoke from his nostrils".[2] Like tea, coffee and opium, tobacco was just one of many intoxicants that was originally used as a form of medicine.[28] Tobacco was introduced around 1600 by French merchants in what today is modern-day Gambia and Senegal. At the same time, caravans from Morocco brought tobacco to the areas around Timbuktu, and the Portuguese brought the commodity (and the plant) to southern Africa, establishing the popularity of tobacco throughout all of Africa by the 1650s.
Soon after its introduction to the Old World, tobacco came under frequent criticism from state and religious leaders. James VI and I, King of Scotland and England, produced the treatise A Counterblaste to Tobacco in 1604, and also introduced excise duty on the product. Murad IV, sultan of the Ottoman Empire 1623–40 was among the first to attempt a smoking ban by claiming it was a threat to public morals and health. The Chongzhen Emperor of China issued an edict banning smoking two years before his death and the overthrow of the Ming dynasty. Later, the Manchu rulers of the Qing dynasty, would proclaim smoking "a more heinous crime than that even of neglecting archery". In Edo period Japan, some of the earliest tobacco plantations were scorned by the shogunate as being a threat to the military economy by letting valuable farmland go to waste for the use of a recreational drug instead of being used to plant food crops.[29]
Religious leaders have often been prominent among those who considered smoking immoral or outright blasphemous. In 1634, the Patriarch of Moscow forbade the sale of tobacco, and sentenced men and women who flouted the ban to have their nostrils slit and their backs flayed. Pope Urban VIII likewise condemned smoking on holy places in a papal bull of 1624. Despite some concerted efforts, restrictions and bans were largely ignored. When James I of England, a staunch anti-smoker and the author of A Counterblaste to Tobacco, tried to curb the new trend by enforcing a 4000% tax increase on tobacco in 1604 it was unsuccessful, as suggested by the presence of around 7,000 tobacco outlets in London by the early 17th century. From this point on for some centuries, several administrations withdrew from efforts at discouragement and instead turned tobacco trade and cultivation into sometimes lucrative government monopolies.[30][31]
By the mid-17th century most major civilizations had been introduced to tobacco smoking and in many cases had already assimilated it into the native culture, despite some continued attempts upon the parts of rulers to eliminate the practice with penalties or fines. Tobacco, both product and plant, followed the major trade routes to major ports and markets, and then on into the hinterlands. The English language term smoking appears to have entered currency in the late 18th century, before which less abbreviated descriptions of the practice such as drinking smoke were also in use.[2]
Growth in the US remained stable until the American Civil War in 1860s, when the primary agricultural workforce shifted from slavery to sharecropping. This, along with a change in demand, accompanied the industrialization of cigarette production as craftsman James Bonsack created a machine in 1881 to partially automate their manufacture.[32]
In Germany, anti-smoking groups, often associated with anti-liquor groups,[33] first published advocacy against the consumption of tobacco in the journal Der Tabakgegner (The Tobacco Opponent) in 1912 and 1932. In 1929, Fritz Lickint of Dresden, Germany, published a paper containing formal statistical evidence of a lung cancer–tobacco link. During the Great Depression Adolf Hitler condemned his earlier smoking habit as a waste of money,[34] and later with stronger assertions. This movement was further strengthened with Nazi reproductive policy as women who smoked were viewed as unsuitable to be wives and mothers in a German family.[35]
The anti-tobacco movement in Nazi Germany did not reach across enemy lines during the Second World War, as anti-smoking groups quickly lost popular support. By the end of the Second World War, American cigarette manufacturers quickly reentered the German black market. Illegal smuggling of tobacco became prevalent,[36] and leaders of the Nazi anti-smoking campaign were silenced.[37] As part of the Marshall Plan, the United States shipped free tobacco to Germany; with 24,000 tons in 1948 and 69,000 tons in 1949.[36] Per capita yearly cigarette consumption in post-war Germany steadily rose from 460 in 1950 to 1,523 in 1963.[4] By the end of the 20th century, anti-smoking campaigns in Germany were unable to exceed the effectiveness of the Nazi-era climax in the years 1939–41 and German tobacco health research was described by Robert N. Proctor as "muted".[4]
In 1950, Richard Doll published research in the British Medical Journal showing a close link between smoking and lung cancer.[38]  Beginning in December 1952, the magazine Reader's Digest published "Cancer by the Carton", a series of articles that linked smoking with lung cancer.[39]
In 1954, the British Doctors Study, a prospective study of some 40 thousand doctors for about 2.5 years, confirmed the suggestion, based on which the government issued advice that smoking and lung cancer rates were related.[5] In January 1964, the United States Surgeon General's Report on Smoking and Health likewise began suggesting the relationship between smoking and cancer.[40]
As scientific evidence mounted in the 1980s, tobacco companies claimed contributory negligence as the adverse health effects were previously unknown or lacked substantial credibility. Health authorities sided with these claims up until 1998, from which they reversed their position. The Tobacco Master Settlement Agreement, originally between the four largest US tobacco companies and the Attorneys General of 46 states, restricted certain types of tobacco advertisement and required payments for health compensation; which later amounted to the largest civil settlement in United States history.[41]
Social campaigns have been instituted in many places to discourage smoking, such as Canada's National Non-Smoking Week.
From 1965 to 2006, rates of smoking in the United States declined from 42% to 20.8%.[6] The majority of those who quit were professional, affluent men. Although the per-capita number of smokers decreased, the average number of cigarettes consumed per person per day increased from 22 in 1954 to 30 in 1978. This paradoxical event suggests that those who quit smoked less, while those who continued to smoke moved to smoke more light cigarettes.[42] The trend has been paralleled by many industrialized nations as rates have either leveled-off or declined. In the developing world, however, tobacco consumption continues to rise at 3.4% in 2002.[7] In Africa, smoking is in most areas considered to be modern, and many of the strong adverse opinions that prevail in the West receive much less attention.[43] Today Russia leads as the top consumer of tobacco followed by Indonesia, Laos, Ukraine, Belarus, Greece, Jordan, and China.[44]
Tobacco is an agricultural product processed from the fresh leaves of plants in the genus Nicotiana. The genus contains a number of species, however, Nicotiana tabacum is the most commonly grown. Nicotiana rustica follows as second containing higher concentrations of nicotine. These leaves are harvested and cured to allow for the slow oxidation and degradation of carotenoids in tobacco leaf. This produces certain compounds in the tobacco leaves which can be attributed to sweet hay, tea, rose oil, or fruity aromatic flavors. Before packaging, the tobacco is often combined with other additives in order to enhance the addictive potency, shift the products pH, or improve the effects of smoke by making it more palatable. In the United States these additives are regulated to 599 substances.[8] The product is then processed, packaged, and shipped to consumer markets.

The active substances in tobacco, especially cigarettes, are administered by burning the leaves and inhaling the vaporized gas that results. This quickly and effectively delivers substances into the bloodstream by absorption through the alveoli in the lungs. The lungs contain some 300 million alveoli, which amounts to a surface area of over 70 m2 (about the size of a tennis court). This method is not completely efficient as not all of the smoke will be inhaled, and some amount of the active substances will be lost in the process of combustion, pyrolysis.[9] Pipe and Cigar smoke are not inhaled because of its high alkalinity, which are irritating to the trachea and lungs. However, because of its higher alkalinity (pH 8.5) compared to cigarette smoke (pH 5.3), non-ionized nicotine is more readily absorbed through the mucous membranes in the mouth.[50] Nicotine absorption from cigar and pipe, however, is much less than that from cigarette smoke.[51] Nicotine and cocaine activate similar patterns of neurons, which supports the existence of common substrates among these drugs.[52]
The inhaled nicotine mimics nicotinic acetylcholine which when bound to nicotinic acetylcholine receptors prevents the reuptake of acetylcholine thereby increasing that neurotransmitter in those areas of the body.[53] These nicotinic acetylcholine receptors are located in the central nervous system and at the nerve-muscle junction of skeletal muscles; whose activity increases heart rate, alertness,[10] and faster reaction times.[11] Nicotine acetylcholine stimulation is not directly addictive. However, since dopamine-releasing neurons are abundant on nicotine receptors, dopamine is released; and, in the nucleus accumbens, dopamine is associated with motivation causing reinforcing behavior.[54] Dopamine increase, in the prefrontal cortex, may also increase working memory.[55]
When tobacco is smoked, most of the nicotine is pyrolyzed. However, a dose sufficient to cause mild somatic dependency and mild to strong psychological dependency remains. There is also a formation of harmane (a MAO inhibitor) from the acetaldehyde in tobacco smoke. This may play a role in nicotine addiction, by facilitating a dopamine release in the nucleus accumbens as a response to nicotine stimuli.[56] Using rat studies, withdrawal after repeated exposure to nicotine results in less responsive nucleus accumbens cells, which produce dopamine responsible for reinforcement.[57]
As of 2000, smoking was practiced by around 1.22 billion people. At current rates of 'smoker replacement' and market growth, this may reach around 1.9 billion in 2025.[58]
Smoking may be up to five times more prevalent among men than women in some communities,[58] although the gender gap usually declines with younger age.[14][15] In some developed countries smoking rates for men have peaked and begun to decline, while for women they continue to climb.[59]
As of 2002, about twenty percent of young teenagers (13–15) smoked worldwide. From which 80,000 to 100,000 children begin smoking every day, roughly half of whom live in Asia. Half of those who begin smoking in adolescent years are projected to go on to smoke for 15 to 20 years.[7]
The World Health Organization (WHO) states that "Much of the disease burden and premature mortality attributable to tobacco use disproportionately affect the poor". Of the 1.22 billion smokers, 1 billion of them live in developing or transitional economies. Rates of smoking have leveled off or declined in the developed world.[60] In the developing world, however, tobacco consumption is rising by 3.4% per year as of 2002.[7]
The WHO in 2004 projected 58.8 million deaths to occur globally,[61] from which 5.4 million are tobacco-attributed,[62] and 4.9 million as of 2007.[63] As of 2002, 70% of the deaths are in developing countries.[63] As of 2017, smoking causes one in ten deaths worldwide, with half of those deaths in the US, China, India and Russia.[64]
Most smokers begin smoking during adolescence or early adulthood. Some studies also show that smoking can also be linked to various mental health complications.[66] Smoking has elements of risk-taking and rebellion, which often appeal to young people. The presence of peers that smoke and media featuring high-status models smoking may also encourage smoking. Because teenagers are influenced more by their peers than by adults, attempts by parents, schools, and health professionals at preventing people from trying cigarettes are often unsuccessful.[67][68]
Children of smoking parents are more likely to smoke than children with non-smoking parents. Children of parents who smoke are less likely to quit smoking.[16] One study found that parental smoking cessation was associated with less adolescent smoking, except when the other parent currently smoked.[69] A current study tested the relation of adolescent smoking to rules regulating where adults are allowed to smoke in the home. Results showed that restrictive home smoking policies were associated with lower likelihood of trying smoking for both middle and high school students.[70]
Behavioural research generally indicates that teenagers begin their smoking habits due to peer pressure, and cultural influence portrayed by friends. However, one study found that direct pressure to smoke cigarettes played a less significant part in adolescent smoking, with adolescents also reporting low levels of both normative and direct pressure to smoke cigarettes.[71] Mere exposure to tobacco retailers may motivate smoking behaviour in adults.[72] A similar study suggested that individuals may play a more active role in starting to smoke than has previously been thought and that social processes other than peer pressure also need to be taken into account.[73] Another study's results indicated that peer pressure was significantly associated with smoking behavior across all age and gender cohorts, but that intrapersonal factors were significantly more important to the smoking behavior of 12- to 13-year-old girls than same-age boys. Within the 14- to 15-year-old age group, one peer pressure variable emerged as a significantly more important predictor of girls' than boys' smoking.[74] It is debated whether peer pressure or self-selection is a greater cause of adolescent smoking.
Psychologists such as Hans Eysenck have developed a personality profile for the typical smoker. Extraversion is the trait that is most associated with smoking, and smokers tend to be sociable, impulsive, risk taking, and excitement seeking individuals.[75] Although personality and social factors may make people likely to smoke, the actual habit is a function of operant conditioning. During the early stages, smoking provides pleasurable sensations (because of its action on the dopamine system) and thus serves as a source of positive reinforcement.
The reasons given by some smokers for this activity have been categorized as addictive smoking, pleasure from smoking, tension reduction/relaxation, social smoking, stimulation, habit/automatism, and handling. There are gender differences in how much each of these reasons contribute, with females more likely than males to cite tension reduction/relaxation, stimulation and social smoking.[76]
Some smokers argue that the depressant effect of smoking allows them to calm their nerves, often allowing for increased concentration. However, according to the Imperial College London, "Nicotine seems to provide both a stimulant and a depressant effect, and it is likely that the effect it has at any time is determined by the mood of the user, the environment and the circumstances of use. Studies have suggested that low doses have a depressant effect, while higher doses have stimulant effect."[77]
A number of studies have established that cigarette sales and smoking follow distinct time-related patterns. For example, cigarette sales in the United States of America have been shown to follow a strongly seasonal pattern, with the high months being the months of summer, and the low months being the winter months.[78]
Similarly, smoking has been shown to follow distinct circadian patterns during the waking day—with the high point usually occurring shortly after waking in the morning, and shortly before going to sleep at night.[79]
In countries where there is a universally funded healthcare system, the government covers the cost of medical care for smokers who become ill through smoking in the form of increased taxes. Two broad debating positions exist on this front, the "pro-smoking" argument suggesting that heavy smokers generally don't live long enough to develop the costly and chronic illnesses which affect the elderly, reducing society's healthcare burden, and the "anti-smoking" argument suggests that the healthcare burden is increased because smokers get chronic illnesses younger and at a higher rate than the general population.  Data on both positions has been contested. The Centers for Disease Control and Prevention published research in 2002 claiming that the cost of each pack of cigarettes sold in the United States was more than $7 in medical care and lost productivity.[80] The cost may be higher, with another study putting it as high as $41 per pack, most of which however is on the individual and his/her family.[81] This is how one author of that study puts it when he explains the very low cost for others: "The reason the number is low is that for private pensions, Social Security, and Medicare — the biggest factors in calculating costs to society — smoking actually saves money. Smokers die at a younger age and don't draw on the funds they've paid into those systems."[81] Other research demonstrates that premature death caused by smoking may redistribute Social Security income in unexpected ways that affect behavior and reduce the economic well-being of smokers and their dependents.[82] To further support this, whatever the rate of smoking  consumption  is per day, smokers have a greater lifetime medical cost on average compared to a non smoker by an estimated $6000 [83] Between the cost for lost productivity and health care expenditures combined, cigarette smoking costs at least 193 billion dollars (Research also shows that smokers earn less money than nonsmokers[84]). As for secondhand smoke, the cost is over 10 billion dollars.[85]
By contrast, some non-scientific studies, including one conducted by Philip Morris in the Czech Republic called Public Finance Balance of Smoking in the Czech Republic[86] and another by the Cato Institute,[87] support the opposite position. Philip Morris has explicitly apologised for the former study, saying: "The funding and public release of this study which, among other things, detailed purported cost savings to the Czech Republic due to premature deaths of smokers, exhibited terrible judgment as well as a complete and unacceptable disregard of basic human values. For one of our tobacco companies to commission this study was not just a terrible mistake, it was wrong. All of us at Philip Morris, no matter where we work, are extremely sorry for this. No one benefits from the very real, serious and significant diseases caused by smoking."[86]
Between 1970 and 1995, per-capita cigarette consumption in poorer developing countries increased by 67 percent, while it dropped by 10 percent in the richer developed world. Eighty percent of smokers now live in less developed countries. By 2030, the World Health Organization (WHO) forecasts that 10 million people a year will die of smoking-related illness, making it the single biggest cause of death worldwide, with the largest increase to be among women. WHO forecasts the 21st century's death rate from smoking to be ten times the 20th century's rate ("Washingtonian" magazine, December 2007).
Cigarette smoking is the leading cause of preventable death and a major public health concern.[88]
There are 1.1 billion tobacco users in the world. One person dies every six seconds from a tobacco related disease.[89]
Tobacco use leads most commonly to diseases affecting the heart and lungs, with smoking being a major risk factor for heart attacks, strokes, chronic obstructive pulmonary disease (COPD), Idiopathic Pulmonary Fibrosis (IPF), emphysema, and cancer (particularly lung cancer, cancers of the larynx and mouth, esophageal cancer and pancreatic cancer).[16] Cigarette smoking increases the risk of Crohn's disease as well as the severity of the course of the disease.[91] It is also the number one cause of bladder cancer. The smoke from tobacco elicits carcinogenic effects on the tissues of the body that are exposed to the smoke.[92]
Tobacco smoke is a complex mixture of over 5,000 identified chemicals, of which 98 are known to have specific toxicological properties.[16][93] The most important chemicals causing cancer are those that produce DNA damage since such damage appears to be the primary underlying cause of cancer.[94][95]  Cunningham et al.[96] combined the microgram weight of the compound in the smoke of one cigarette with the known genotoxic effect per microgram to identify the most carcinogenic compounds in cigarette smoke.  The seven most important carcinogens in tobacco smoke are shown in the table, along with DNA alterations they cause. 
Tobacco smoke can combine with other carcinogens present within the environment in order to produce elevated degrees of lung cancer.
Cigarette smoking has also been associated with sarcopenia, the age-related loss of muscle mass and strength.[104]
The World Health Organization estimates that tobacco caused 5.4 million deaths in 2004[105] and 100 million deaths over the course of the 20th century.[106] Similarly, the United States Centers for Disease Control and Prevention describes tobacco use as "the single most important preventable risk to human health in developed countries and an important cause of premature death worldwide."[107] Although 70% of smokers state their intention to quit only 3-5% are actually successful in doing so.[83]
The probabilities of death from lung cancer before age 75 in the United Kingdom are 0.2% for men who never smoked (0.4% for women), 5.5% for male former smokers (2.6% in women), 15.9% for current male smokers (9.5% for women) and 24.4% for male “heavy smokers” defined as smoking more than 5 cigarettes per day (18.5% for women).[108] Tobacco smoke can combine with other carcinogens present within the environment in order to produce elevated degrees of lung cancer.
Rates of smoking have generally leveled-off or declined in the developed world. Smoking rates in the United States have dropped by half from 1965 to 2006, falling from 42% to 20.8% in adults.[109] In the developing world, tobacco consumption is rising by 3.4% per year.[110]
Second-hand smoke presents a known health risk, to which six hundred thousand deaths were attributed in 2004. It also has been known to produce skin conditions such as freckles and dryness.[111]
In 2015, a meta-analysis found that smokers were at greater risk of developing psychotic illness.[112] Tobacco has also been described an anaphrodisiac due to its propensity for causing erectile dysfunction.[113]
Famous smokers of the past used cigarettes or pipes as part of their image, such as Jean-Paul Sartre's Gauloises-brand cigarettes; Albert Einstein's, Douglas MacArthur's, Bertrand Russell's, and Bing Crosby's pipes; or the news broadcaster Edward R. Murrow's cigarette. Writers in particular seem to be known for smoking, for example, Cornell Professor Richard Klein's book Cigarettes are Sublime for the analysis, by this professor of French literature, of the role smoking plays in 19th and 20th century letters. The popular author Kurt Vonnegut addressed his addiction to cigarettes within his novels. British Prime Minister Harold Wilson was well known for smoking a pipe in public as was Winston Churchill for his cigars. Sherlock Holmes, the fictional detective created by Sir Arthur Conan Doyle smoked a pipe, cigarettes, and cigars. The DC Vertigo comic book character, John Constantine, created by Alan Moore, is synonymous with smoking, so much so that the first storyline by Preacher creator, Garth Ennis, centered around John Constantine contracting lung cancer. Professional wrestler James Fullington, while in character as "The Sandman", is a chronic smoker in order to appear "tough".
The problem of smoking at home is particularly difficult for women in many cultures (especially Arab cultures), where it may not be acceptable for a woman to ask her husband not to smoke at home or in the presence of her children. Studies have shown that pollution levels for smoking areas indoors are higher than levels found on busy roadways, in closed motor garages, and during fire storms.[clarification needed] Furthermore, smoke can spread from one room to another, even if doors to the smoking area are closed.[114]
The ceremonial smoking of tobacco, and praying with a sacred pipe, is a prominent part of the religious ceremonies of a number of Native American Nations. Sema, the Anishinaabe word for tobacco, is grown for ceremonial use and considered the ultimate sacred plant since its smoke is believed to carry prayers to the spirits. In most major religions, however, tobacco smoking is not specifically prohibited, although it may be discouraged as an immoral habit. Before the health risks of smoking were identified through controlled study, smoking was considered an immoral habit by certain Christian preachers and social reformers. The founder of the Latter Day Saint movement, Joseph Smith, recorded that on 27 February 1833, he received a revelation which discouraged tobacco use. This "Word of Wisdom" was later accepted as a commandment, and faithful Latter-day Saints abstain completely from tobacco.[115] Jehovah's Witnesses base their stand against smoking on the Bible's command to "clean ourselves of every defilement of flesh" (2 Corinthians 7:1). The Jewish Rabbi Yisrael Meir Kagan (1838–1933) was one of the first Jewish authorities to speak out on smoking. In Ahmadiyya Islam, smoking is highly discouraged, although not forbidden. During the month of fasting however, it is forbidden to smoke tobacco.[116] In the Bahá'í Faith, smoking tobacco is discouraged though not forbidden.[117]
One of the largest global enterprises in the world is known to be the tobacco industry. The six biggest tobacco companies made a combined profit of $35.1 billion (Jha et al., 2014) in 2010.[118] Tobacco smoking causes millions of deaths globally each year. According to The Tobacco Atlas, the use of tobacco has led 6 million deaths in 2011, 80% of these deaths occurred in low and middle-income countries.[119] Research has shown that there are many negative effects of smoking; some of these factors are health, social and psychological factors which can harm the life of a person.
On 27 February 2005 the WHO Framework Convention on Tobacco Control, took effect. The FCTC is the world's first public health treaty. Countries that sign on as parties agree to a set of common goals, minimum standards for tobacco control policy, and to cooperate in dealing with cross-border challenges such as cigarette smuggling. Currently the WHO declares that 4 billion people will be covered by the treaty, which includes 168 signatories.[120] Among other steps, signatories are to put together legislation that will eliminate secondhand smoke in indoor workplaces, public transport, indoor public places and, as appropriate, other public places.
Many governments have introduced excise taxes on cigarettes in order to reduce the consumption of cigarettes.
In 2002, the Centers for Disease Control and Prevention said that each pack of cigarettes[quantify] sold in the United States costs the nation more than $7 in medical care and lost productivity,[80] around $3400 per year per smoker. Another study by a team of health economists finds the combined price paid by their families and society is about $41 per pack of cigarettes.[121]
Substantial scientific evidence shows that higher cigarette prices result in lower overall cigarette consumption. Most studies indicate that a 10% increase in price will reduce overall cigarette consumption by 3% to 5%. Youth, minorities, and low-income smokers are two to three times more likely to quit or smoke less than other smokers in response to price increases.[122][123] Smoking is often cited[citation needed] as an example of an inelastic good, however, i.e. a large rise in price will only result in a small decrease in consumption.
Many nations have implemented some form of tobacco taxation. As of 1997, Denmark had the highest cigarette tax burden of $4.02 per pack. Taiwan only had a tax burden of $0.62 per pack. The federal government of the United States charges $1.01 per pack.[124]
Cigarette taxes vary widely from state to state in the United States. For example, Missouri has a cigarette tax of only 17 cents per pack, the nation's lowest, while New York has the highest cigarette tax in the U.S.: $4.35 per pack. In Alabama, Illinois, Missouri, New York City, Tennessee, and Virginia, counties and cities may impose an additional limited tax on the price of cigarettes.[125] Sales taxes are also levied on tobacco products in most jurisdictions.
In the United Kingdom, a packet of 20 cigarettes typically costs between £8.00 to £12.00 according to 2018 prices, depending on the brand purchased and where the purchase was made.[126] The UK has a significant black market for tobacco, and it has been estimated by the tobacco industry that 27% of cigarette and 68% of handrolling tobacco consumption is non-UK duty paid (NUKDP).[127]
In Australia total taxes account for 62.5% of the final price of a packet of cigarettes (2011 figures). These taxes include federal excise or customs duty and  Goods and Services Tax.[128]
In June 1967, the US Federal Communications Commission ruled that programmes broadcast on a television station which discussed smoking and health were insufficient to offset the effects of paid advertisements that were broadcast for five to ten minutes each day. In April 1970, the US Congress passed the Public Health Cigarette Smoking Act banning the advertising of cigarettes on television and radio starting on 2 January 1971.[129]
The Tobacco Advertising Prohibition Act 1992 expressly prohibited almost all forms of Tobacco advertising in Australia, including the sponsorship of sporting or other cultural events by cigarette brands.
All tobacco advertising and sponsorship on television has been banned within the European Union since 1991 under the Television Without Frontiers Directive (1989).[130] This ban was extended by the Tobacco Advertising Directive, which took effect in July 2005 to cover other forms of media such as the internet, print media, and radio. The directive does not include advertising in cinemas and on billboards or using merchandising – or tobacco sponsorship of cultural and sporting events which are purely local, with participants coming from only one Member State[131] as these fall outside the jurisdiction of the European Commission. However, most member states have transposed the directive with national laws that are wider in scope than the directive and cover local advertising. A 2008 European Commission report concluded that the directive had been successfully transposed into national law in all EU member states, and that these laws were well implemented.[132]
Some countries also impose legal requirements on the packaging of tobacco products. For example, in the countries of the European Union, Turkey, Australia[133] and South Africa, cigarette packs must be prominently labeled with the health risks associated with smoking.[134] Canada, Australia, Thailand, Iceland and Brazil have also imposed labels upon cigarette packs warning smokers of the effects, and they include graphic images of the potential health effects of smoking. Cards are also inserted into cigarette packs in Canada. There are sixteen of them, and only one comes in a pack. They explain different methods of quitting smoking. Also, in the United Kingdom, there have been a number of graphic NHS advertisements, one showing a cigarette filled with fatty deposits, as if the cigarette is symbolizing the artery of a smoker.
Many countries have a smoking age. In many countries, including the United States, most European Union member states, New Zealand, Canada, South Africa, Israel, India,[16] Brazil, Chile, Costa Rica and Australia, it is illegal to sell tobacco products to minors and in the Netherlands, Austria, Belgium, Denmark and South Africa it is illegal to sell tobacco products to people under the age of 16. On 1 September 2007 the minimum age to buy tobacco products in Germany rose from 16 to 18, as well as in the United Kingdom where on 1 October 2007 it rose from 16 to 18.[135] Underlying such laws is the belief that people should make an informed decision regarding the risks of tobacco use. These laws have a lax enforcement in some nations and states. In China, Turkey, and many other countries usually a child will have little problem buying tobacco products, because they are often told to go to the store to buy tobacco for their parents.
Several countries such as Ireland, Latvia, Estonia, the Netherlands, Finland, Norway, Canada, Australia, Sweden, Portugal, Singapore, Italy, Indonesia, India, Lithuania, Chile, Spain, Iceland, United Kingdom, Slovenia, Turkey and Malta have legislated against smoking in public places, often including bars and restaurants. Restaurateurs have been permitted in some jurisdictions to build designated smoking areas (or to prohibit smoking). In the United States, many states prohibit smoking in restaurants, and some also prohibit smoking in bars. In provinces of Canada, smoking is illegal in indoor workplaces and public places, including bars and restaurants. As of 31 March 2008 Canada has introduced a smoke-free law ban in all public places, as well as within 10 metres of an entrance to any public place. In Australia, smoke-free laws vary from state to state. Currently, Queensland has completely smoke-free indoor public places (including workplaces, bars, pubs and eateries) as well as patrolled beaches and some outdoor public areas. There are, however, exceptions for designated smoking areas. In Victoria, smoking is restricted in railway stations, bus stops and tram stops as these are public locations where second-hand smoke can affect non-smokers waiting for public transport, and since 1 July 2007 is now extended to all indoor public places. In New Zealand and Brazil, smoking is restricted in enclosed public places including bars, restaurants and pubs. Hong Kong restricted smoking on 1 January 2007 in the workplace, public spaces such as restaurants, karaoke rooms, buildings, and public parks (bars which do not admit minors were exempt until 2009). In Romania smoking is illegal in trains, metro stations, public institutions (except where designated, usually outside) and public transport.
In Germany, additionally to smoking bans in public buildings and transports, an anti-smoking ordinance for bars and restaurants was implemented in late 2007. A study by the University of Hamburg (Ahlfeldt and Maennig 2010) demonstrates, that the smoking ban had, if any, only short run impacts on bar and restaurant revenues. In the medium and long run no negative effect was measurable. The results suggest either, that the consumption in bars and restaurants is not affected by smoking bans in the long run, or, that negative revenue impacts by smokers are compensated by increasing revenues through non-smokers.[136]
An indirect public health problem posed by cigarettes is that of accidental fires, usually linked with consumption of alcohol. Enhanced combustion using nitrates was traditionally used but cigarette manufacturers have been silent on this subject claiming at first that a safe cigarette was technically impossible, then that it could only be achieved by modifying the paper. Roll your own cigarettes contain no additives and are fire safe. Numerous fire safe cigarette designs have been proposed, some by tobacco companies themselves, which would extinguish a cigarette left unattended for more than a minute or two, thereby reducing the risk of fire. Among American tobacco companies, some have resisted this idea, while others have embraced it. RJ Reynolds was a leader in making prototypes of these cigarettes in 1983[137] and will make all of their U.S. market cigarettes to be fire-safe by 2010.[138] Phillip Morris is not in active support of it.[139] Lorillard (purchased by RJ Reynolds), the US' 3rd-largest tobacco company, seems to be ambivalent.[139]
The relationship between tobacco and other drug use has been well-established, however the nature of this association remains unclear. The two main theories are the phenotypic causation (gateway) model and the correlated liabilities model. The causation model argues that smoking is a primary influence on future drug use,[140] while the correlated liabilities model argues that smoking and other drug use are predicated on genetic or environmental factors.[141]
Smoking cessation, referred to as "quitting", is the action leading towards abstinence of tobacco smoking. Methods of "quitting" include advice from physicians or social workers,[16] cold turkey, nicotine replacement therapy, contingent vouchers,[142] antidepressants, hypnosis, self-help (mindfulness meditation),[143] and support groups. A meta-analysis from 2018, conducted on 61 RCT, showed that one year after people quit smoking with the assistance of first‐line smoking cessation medications (and some behavioral help), only a little under 20% of smokers remained sustained abstinence.[144]



Sedentary lifestyle - Wikipedia
A sedentary lifestyle is a type of lifestyle with little or no physical activity. A person living a sedentary lifestyle is often sitting or lying down while engaged in an activity like reading, socializing, watching television, playing video games, or using a mobile phone/computer for much of the day. A sedentary lifestyle can potentially contribute to ill health and many preventable causes of death.
Screen time is a modern term for the amount of time a person spends looking at a screen such as a television, computer monitor, or mobile device. Excessive screen time is linked to negative health consequences.[1][2][3][4]
The term couch potato was coined by a friend of underground comics artist Robert Armstrong in the 1970s; Armstrong featured a group of couch potatoes in a series of comics featuring sedentary characters and with Jack Mingo and Allan Dodge created a satirical organization that purported to watch television as a form of meditation. With two books and endless promotion through the 1980s, the Couch Potatoes appeared in hundreds of newspapers, magazines and broadcasts, spreading its "turn on, tune in, veg out" message, garnering 7,000 members, and popularizing the term.
The condition, which predates the term, is characterized by sitting or remaining inactive for most of the day with little or no exercise.
Lack of exercise causes muscle atrophy, i.e. shrinking and weakening of the muscles, and accordingly increases susceptibility to physical injury. Additionally, physical fitness is correlated with immune system function;[5] a reduction in physical fitness is generally accompanied by a weakening of the immune system. A review in Nature Reviews Cardiology suggests that since illness or injury are associated with prolonged periods of enforced rest, such sedentariness has physiologically become linked to life-preserving metabolic and stress related responses such as inflammation that aid recovery during illness and injury but which due to being nonadaptive during health now lead to chronic diseases.[6]
Despite the well-known benefits of physical activity, many adults and many children lead a relatively sedentary lifestyle[7][8] and are not active enough to achieve these health benefits.
In the 2008 United States American National Health Interview Survey (NHIS) 36% of adults were considered inactive. 59% of adult respondents never participated in vigorous physical activity lasting more than 10 minutes per week.[9]
A lack of physical activity is one of the leading causes of preventable death worldwide.[10]
Sitting still may cause premature death. The risk is higher among those that sit still more than 5 hours per day. It is shown to be a risk factor on its own independent of hard exercise and BMI. The more still, the higher risk of chronic diseases. People that sit still more than 4 hours per day have a 40 percent higher risk than those that sit fewer than 4 hours per day. However, those that exercise at least 4 hours per week are as healthy as those that sit fewer than 4 hours per day.[11][12]
A sedentary lifestyle and lack of physical activity can contribute to or be a risk factor for:
As a response to concerns over health and environmental issues, some organizations have promoted active travel, which seeks to promote walking and cycling as safe and attractive alternatives to motorized transport.[25] Additionally, some organizations have implemented exercise classes at lunch, walking challenges among co-workers, or allowing employees to stand rather than sit at their desk during the workday. Workplace interventions such as alternative activity workstations, sit-stand desks, promotion of stair use are among measures being implemented to counter the harms of sedentary workplace environments.[26] A Cochrane systematic review published in 2016 concluded that "at present there is very low quality evidence that sit-stand desks can reduce sitting at work at the short term. There is no evidence for other types of interventions." Also, evidence was lacking on the long term health benefits of such interventions.[27][needs update] Similarly a recently published review concluded that interventions aimed at reducing sitting outside of work were only modestly effective.[28] Organizations may also offer cholesterol or blood pressure screenings to employees.[citation needed]



Stomach cancer - Wikipedia
Stomach cancer, also known as gastric cancer, is a cancer which develops from the lining of the stomach.[10] Early symptoms may include heartburn, upper abdominal pain, nausea and loss of appetite.[1] Later signs and symptoms may include weight loss, yellowing of the skin and whites of the eyes, vomiting, difficulty swallowing and blood in the stool among others.[1] The cancer may spread from the stomach to other parts of the body, particularly the liver, lungs, bones, lining of the abdomen and lymph nodes.[11]
The most common cause is infection by the bacterium Helicobacter pylori, which accounts for more than 60% of cases.[2][3][12] Certain types of H. pylori have greater risks than others.[2] Smoking, dietary factors such as pickled vegetables and obesity are other risk factors.[2][4] About 10% of cases run in families, and between 1% and 3% of cases are due to genetic syndromes inherited from a person's parents such as hereditary diffuse gastric cancer.[2] Most cases of stomach cancers are gastric carcinomas.[2] This type can be divided into a number of subtypes.[2] Lymphomas and mesenchymal tumors may also develop in the stomach.[2] Most of the time, stomach cancer develops in stages over years.[2] Diagnosis is usually by biopsy done during endoscopy.[1] This is followed by medical imaging to determine if the disease has spread to other parts of the body.[1] Japan and South Korea, two countries that have high rates of the disease, screen for stomach cancer.[2]
A Mediterranean diet lowers the risk of cancer as does the stopping of smoking.[2][5] There is tentative evidence that treating H. pylori decreases the future risk.[2][5] If cancer is treated early, many cases can be cured.[2] Treatments may include some combination of surgery, chemotherapy, radiation therapy and targeted therapy.[1][13] If treated late, palliative care may be advised.[2] Outcomes are often poor with a less than 10% five-year survival rate globally.[6] This is largely because most people with the condition present with advanced disease.[6] In the United States, five-year survival is 28%,[7] while in South Korea it is over 65%, partly due to screening efforts.[2]
Globally, stomach cancer is the fifth leading cause of cancer and the third leading cause of death from cancer, making up 7% of cases and 9% of deaths.[14] In 2012, it newly occurred in 950,000 people and caused 723,000 deaths.[14] Before the 1930s, in much of the world, including most Western developed countries, it was the most common cause of death from cancer.[15][16][17] Rates of death have been decreasing in many areas of the world since then.[2] This is believed to be due to the eating of less salted and pickled foods as a result of the development of refrigeration as a method of keeping food fresh.[18] Stomach cancer occurs most commonly in East Asia and Eastern Europe.[2] It occurs twice as often in males as in females.[2]
Stomach cancer is often either asymptomatic (producing no noticeable symptoms) or it may cause only nonspecific symptoms (symptoms that may also be present in other related or unrelated disorders) in its early stages. By the time symptoms occur, the cancer has often reached an advanced stage (see below) and may have metastasized (spread to other, perhaps distant, parts of the body), which is one of the main reasons for its relatively poor prognosis.[19] Stomach cancer can cause the following signs and symptoms:
Early cancers may be associated with indigestion or a burning sensation (heartburn). However, less than 1 in every 50 people referred for endoscopy due to indigestion has cancer.[20] Abdominal discomfort and loss of appetite, especially for meat, can occur.
Gastric cancers that have enlarged and invaded normal tissue can cause weakness, fatigue, bloating of the stomach after meals, abdominal pain in the upper abdomen, nausea and occasional vomiting, diarrhea or constipation. Further enlargement may cause weight loss or bleeding with vomiting blood or having blood in the stool, the latter apparent as black discolouration (melena) and sometimes leading to anemia. Dysphagia suggests a tumour in the cardia or extension of the gastric tumour into the esophagus.
These can be symptoms of other problems such as a stomach virus, gastric ulcer, or tropical sprue.
Gastric cancer occurs as a result of many factors.[21] It occurs twice as commonly in males as females. Estrogen may protect women against the development of this form of cancer.[22][23]
Helicobacter pylori infection is an essential risk factor in 65–80% of gastric cancers, but only 2% of people with Helicobacter infections develop stomach cancer.[24][4] The mechanism by which H. pylori induces stomach cancer potentially involves chronic inflammation, or the action of H. pylori virulence factors such as CagA.[25] It was estimated that Epstein–Barr virus is responsible for 84,000 cases per year.[26] AIDS is also associated with elevated risk.[4]
Smoking increases the risk of developing gastric cancer significantly, from 40% increased risk for current smokers to 82% increase for heavy smokers. Gastric cancers due to smoking mostly occur in the upper part of the stomach near the esophagus.[27][28][29] Some studies show increased risk with alcohol consumption as well.[4][30]
Dietary factors are not proven causes and the association between stomach cancer and various foods and beverages is weak.[32] Some foods including smoked foods, salt and salt-rich foods, red meat, processed meat, pickled vegetables, and bracken are associated with a higher risk of stomach cancer.[33][4][34] Nitrates and nitrites in cured meats can be converted by certain bacteria, including H. pylori, into compounds that have been found to cause stomach cancer in animals.
Fresh fruit and vegetable intake, citrus fruit intake, and antioxidant intake are associated with a lower risk of stomach cancer.[4][27] A Mediterranean diet is associated with lower rates of stomach cancer,[35] as is regular aspirin use.[4]
Obesity is a physical risk factor that has been found to increase the risk of gastric adenocarcinoma by contributing to the development of gastroesophageal reflux disease (GERD).[36] The exact mechanism by which obesity causes GERD is not completely known. Studies hypothesize that increased dietary fat leading to increased pressure on the stomach and the lower esophageal sphincter, due to excess adipose tissue, could play a role, yet no statistically significant data has been collected.[37] However, the risk of gastric cardia adenocarcinoma, with GERD present, has been found to increase more than 2 times for an obese person.[36] There is a correlation between iodine deficiency and gastric cancer.[38][39][40]
About 10% of cases run in families and between 1% and 3% of cases are due to genetic syndromes inherited from a person's parents such as hereditary diffuse gastric cancer.[2]
A genetic risk factor for gastric cancer is a genetic defect of the CDH1 gene known as hereditary diffuse gastric cancer (HDGC). The CDH1 gene, which codes for E-cadherin, lies on the 16th chromosome.[41] When the gene experiences a particular mutation, gastric cancer develops through a mechanism that is not fully understood.[41] This mutation is considered autosomal dominant meaning that half of a carrier’s children will likely experience the same mutation.[41] Diagnosis of hereditary diffuse gastric cancer usually takes place when at least two cases involving a family member, such as a parent or grandparent, are diagnosed, with at least one diagnosed before the age of 50.[41] The diagnosis can also be made if there are at least three cases in the family, in which case age is not considered.[41]
The International Cancer Genome Consortium is leading efforts to identify genomic changes involved in stomach cancer.[42][43] A very small percentage of diffuse-type gastric cancers (see Histopathology below) arise from an inherited abnormal CDH1 gene. Genetic testing and treatment options are available for families at risk.[44]
Other risks include diabetes,[45]
pernicious anemia,[30] chronic atrophic gastritis,[46] Menetrier's disease (hyperplastic, hypersecretory gastropathy),[47]
and intestinal metaplasia.[48]
To find the cause of symptoms, the doctor asks about the patient's medical history, does a physical exam, and may order laboratory studies. The patient may also have one or all of the following exams:
In 2013, Chinese and Israeli scientists reported a successful pilot study of a breathalyzer-style breath test intended to diagnose stomach cancer by analyzing exhaled chemicals without the need for an intrusive endoscopy.[50] A larger-scale clinical trial of this technology was completed in 2014.[51]
Abnormal tissue seen in a gastroscope examination will be biopsied by the surgeon or gastroenterologist. This tissue is then sent to a pathologist for histological examination under a microscope to check for the presence of cancerous cells. A biopsy, with subsequent histological analysis, is the only sure way to confirm the presence of cancer cells.[30]
Various gastroscopic modalities have been developed to increase yield of detected mucosa with a dye that accentuates the cell structure and can identify areas of dysplasia. Endocytoscopy involves ultra-high magnification to visualise cellular structure to better determine areas of dysplasia. Other gastroscopic modalities such as optical coherence tomography are being tested investigationally for similar applications.[52]
A number of cutaneous conditions are associated with gastric cancer. A condition of darkened hyperplasia of the skin, frequently of the axilla and groin, known as acanthosis nigricans, is associated with intra-abdominal cancers such as gastric cancer. Other cutaneous manifestations of gastric cancer include tripe palms (a similar darkening hyperplasia of the skin of the palms) and the Leser-Trelat sign, which is the rapid development of skin lesions known as seborrheic keratoses.[53]
Various blood tests may be done including a complete blood count (CBC) to check for anaemia, and a fecal occult blood test to check for blood in the stool.
If cancer cells are found in the tissue sample, the next step is to stage, or find out the extent of the disease. Various tests determine whether the cancer has spread and, if so, what parts of the body are affected. Because stomach cancer can spread to the liver, the pancreas, and other organs near the stomach as well as to the lungs, the doctor may order a CT scan, a PET scan,[56] an endoscopic ultrasound exam, or other tests to check these areas. Blood tests for tumor markers, such as carcinoembryonic antigen (CEA) and carbohydrate antigen (CA) may be ordered, as their levels correlate to extent of metastasis, especially to the liver, and the cure rate.
Staging may not be complete until after surgery. The surgeon removes nearby lymph nodes and possibly samples of tissue from other areas in the abdomen for examination by a pathologist.
The clinical stages of stomach cancer are:[57][58]
The TNM staging system is also used.[59]
In a study of open-access endoscopy in Scotland, patients were diagnosed 7% in Stage I 17% in Stage II, and 28% in Stage III.[60] A Minnesota population was diagnosed 10% in Stage I, 13% in Stage II, and 18% in Stage III.[61] However, in a high-risk population in the Valdivia Province of southern Chile, only 5% of patients were diagnosed in the first two stages and 10% in stage III.[62]
Getting rid of H. pylori in those who are infected decreases the risk of stomach cancer, at least in those who are Asian.[63] A 2014 meta-analysis of observational studies found that a diet high in fruits, mushrooms, garlic, soybeans, and green onions was associated with a lower risk of stomach cancer in the Korean population.[64] Low doses of vitamins, especially from a healthy diet, decrease the risk of stomach cancer.[65] A previous review of antioxidant supplementation did not find supporting evidence and possibly worse outcomes.[66][67]
Cancer of the stomach is difficult to cure unless it is found at an early stage (before it has begun to spread). Unfortunately, because early stomach cancer causes few symptoms, the disease is usually advanced when the diagnosis is made.[68]
Treatment for stomach cancer may include surgery,[69] chemotherapy,[13] and/or radiation therapy.[70] New treatment approaches such as immunotherapy or gene therapy and improved ways of using current methods are being studied in clinical trials.[71]
Surgery remains the only curative therapy for stomach cancer.[6] Of the different surgical techniques, endoscopic mucosal resection (EMR) is a treatment for early gastric cancer (tumor only involves the mucosa) that was pioneered in Japan and is available in the United States at some centers.[6] In this procedure, the tumor, together with the inner lining of stomach (mucosa), is removed from the wall of the stomach using an electrical wire loop through the endoscope. The advantage is that it is a much smaller operation than removing the stomach.[6] Endoscopic submucosal dissection (ESD) is a similar technique pioneered in Japan, used to resect a large area of mucosa in one piece.[6] If the pathologic examination of the resected specimen shows incomplete resection or deep invasion by tumor, the patient would need a formal stomach resection.[6] A 2016 Cochrane review found low quality evidence of no difference in short-term mortality between laparoscopic and open gastrectomy (removal of stomach), and that benefits or harms of laparoscopic gastrectomy cannot be ruled out.[72]
Those with metastatic disease at the time of presentation may receive palliative surgery and while it remains controversial, due to the possibility of complications from the surgery itself and the fact that it may delay chemotherapy the data so far is mostly positive, with improved survival rates being seen in those treated with this approach.[6][73]
The use of chemotherapy to treat stomach cancer has no firmly established standard of care.[13] Unfortunately, stomach cancer has not been particularly sensitive to these drugs, and chemotherapy, if used, has usually served to palliatively reduce the size of the tumor, relieve symptoms of the disease and increase survival time. Some drugs used in stomach cancer treatment have included: 5-FU (fluorouracil) or its analog capecitabine, BCNU (carmustine), methyl-CCNU (semustine) and doxorubicin (Adriamycin), as well as mitomycin C, and more recently cisplatin and taxotere, often using drugs in various combinations.[13] The relative benefits of these different drugs, alone and in combination, are unclear.[74][13] Clinical researchers are exploring the benefits of giving chemotherapy before surgery to shrink the tumor, or as adjuvant therapy after surgery to destroy remaining cancer cells.[6]
Recently, treatment with human epidermal growth factor receptor 2 (HER2) inhibitor, trastuzumab, has been demonstrated to increase overall survival in inoperable locally advanced or metastatic gastric carcinoma over-expressing the HER2/neu gene.[6] In particular, HER2 is overexpressed in 13–22% of patients with gastric cancer.[71][75] Of note, HER2 overexpression in gastric neoplasia is heterogeneous and comprises a minority of tumor cells (less than 10% of gastric cancers overexpress HER2 in more than 5% of tumor cells). Hence, this heterogeneous expression should be taken into account for HER2 testing, particularly in small samples such as biopsies, requiring the evaluation of more than one bioptic sample.[75]
Radiation therapy (also called radiotherapy) may be used to treat stomach cancer, often as an adjuvant to chemotherapy and/or surgery.[6]
The prognosis of stomach cancer is generally poor, due to the fact the tumour has often metastasised by the time of discovery and the fact that most people with the condition are elderly (median age is between 70 and 75 years) at presentation.[76] The five-year survival rate for stomach cancer is reported to be less than 10 percent.[6]
Almost 300 genes are related to outcomes in stomach cancer with both unfavorable genes where high expression related to poor survival and favorable genes where high expression associated with longer survival times.[77][78] Examples of poor prognosis genes include ITGAV and DUSP1.
Worldwide, stomach cancer is the fifth most-common cancer with 952,000 cases diagnosed in 2012.[14] It is more common both in men and in developing countries.[79][80] In 2012, it represented 8.5% of cancer cases in men, making it the fourth most-common cancer in men.[81] Also in 2012, the number of deaths was 700,000 having decreased slightly from 774,000 in 1990, making it the third-leading cause of cancer-related death (after lung cancer and liver cancer).[82][83]
Less than 5% of stomach cancers occur in people under 40 years of age with 81.1% of that 5% in the age-group of 30 to 39 and 18.9% in the age-group of 20 to 29.[84]
In 2014, stomach cancer resulted in 0.61% of deaths (13,303 cases) in the U.S.[85] In China, stomach cancer accounted for 3.56% of all deaths (324,439 cases).[86] The highest rate of stomach cancer was in Mongolia, at 28 cases per 100,000 people.[87]
In the United Kingdom, stomach cancer is the fifteenth most-common cancer (around 7,100 people were diagnosed with stomach cancer in 2011), and it is the tenth most-common cause of cancer-related deaths (around 4,800 people died in 2012).[88]
Incidence and mortality rates of gastric cancer vary greatly in Africa. The GLOBOCAN system is currently the most widely-used method to compare these rates between countries, but African incidence and mortality rates are seen to differ among countries, possibly due to the lack of universal access to a registry system for all countries.[89] Variation as drastic as estimated rates from 0.3/100000 in Botswana to 20.3/100000 in Mali have been observed.[89] In Uganda, the incidence of gastric cancer has increased from the 1960s measurement of 0.8/100000 to 5.6/100000.[89] Gastric cancer, though present, is relatively low when compared to countries with high incidence like Japan and China. One suspected cause of the variation within Africa and between other countries is due to different strains of the Helicobacter pylori bacteria. The trend commonly-seen is that H. pylori infection increases the risk for gastric cancer. However, this is not the case in Africa, giving this phenomenon the name the “African enigma.”[90] Although this bacteria is found in Africa, evidence has supported that different strains with mutations in the bacterial genotype may contribute to the difference in cancer development between African countries and others outside the continent.[90] However, increasing access to health care and treatment measures have been commonly-associated with the rising incidence, particularly in Uganda.[89]
The stomach is a muscular organ of the gastrointestinal tract that holds food and begins the digestive process by secreting gastric juice. The most common cancers of the stomach are adenocarcinomas but other histological types have been reported. Signs vary but may include vomiting (especially if blood is present), weight loss, anemia, and lack of appetite. Bowel movements may be dark and tarry in nature. In order to determine whether cancer is present in the stomach, special X-rays and/or abdominal ultrasound may be performed. Gastroscopy, a test using an instrument called endoscope to examine the stomach, is a useful diagnostic tool that can also take samples of the suspected mass for histopathological analysis to confirm or rule out cancer. The most definitive method of cancer diagnosis is through open surgical biopsy.[91] Most stomach tumors are malignant with evidence of spread to lymph nodes or liver, making treatment difficult. Except for lymphoma, surgery is the most frequent treatment option for stomach cancers but it is associated with significant risks.




Infection - Wikipedia
Infection is the invasion of an organism's body tissues by disease-causing agents, their multiplication, and the reaction of host tissues to the infectious agents and the toxins they produce.[1][2] Infectious disease, also known as transmissible disease or communicable disease, is illness resulting from an infection.
Infections are caused by infectious agents including viruses, viroids, prions, bacteria, nematodes such as parasitic roundworms and pinworms, arthropods such as ticks, mites, fleas, and lice, fungi such as ringworm, and other macroparasites such as tapeworms and other helminths.
Hosts can fight infections using their immune system. Mammalian hosts react to infections with an innate response, often involving inflammation, followed by an adaptive response.[3]
Specific medications used to treat infections include antibiotics, antivirals, antifungals, antiprotozoals, and antihelminthics. Infectious diseases resulted in 9.2 million deaths in 2013 (about 17% of all deaths).[4] The branch of medicine that focuses on infections is referred to as infectious disease.[5]
Symptomatic infections are apparent and clinical, whereas an infection that is active but does not produce noticeable symptoms may be called inapparent, silent, subclinical, or occult. An infection that is inactive or dormant is called a latent infection.[6] An example of a latent bacterial infection is latent tuberculosis. Some viral infections can also be latent, examples of latent viral infections are any of those from the Herpesviridae family.
The word infection can denote any presence of a particular pathogen at all (no matter how little) but also is often used in a sense implying a clinically apparent infection (in other words, a case of infectious disease).[7] This fact occasionally creates some ambiguity or prompts some usage discussion. To get around the usage annoyance, it is common for health professionals to speak of colonization (rather than infection) when they mean that some of the pathogens are present but that no clinically apparent infection (no disease) is present.
A short-term infection is an acute infection. A long-term infection is a chronic infection. Infections can be further classified by causative agent (bacterial, viral, fungal, parasitic), and by the presence or absence of systemic symptoms (sepsis).
 
Among the many varieties of microorganisms, relatively few cause disease in otherwise healthy individuals.[8] Infectious disease results from the interplay between those few pathogens and the defenses of the hosts they infect. The appearance and severity of disease resulting from any pathogen, depends upon the ability of that pathogen to damage the host as well as the ability of the host to resist the pathogen. However a host's immune system can also cause damage to the host itself in an attempt to control the infection. Clinicians therefore classify infectious microorganisms or microbes according to the status of host defenses - either as primary pathogens or as opportunistic pathogens:
One way of proving that a given disease is "infectious", is to satisfy Koch's postulates (first proposed by Robert Koch), which demands that the infectious agent be identified only in patients and not in healthy controls, and that patients who contract the agent also develop the disease. These postulates were first used in the discovery that Mycobacteria species cause tuberculosis. Koch's postulates cannot be applied ethically for many human diseases because they require experimental infection of a healthy individual with a pathogen produced as a pure culture. Often, even clearly infectious diseases do not meet the infectious criteria. For example, Treponema pallidum, the causative spirochete of syphilis, cannot be cultured in vitro – however the organism can be cultured in rabbit testes. It is less clear that a pure culture comes from an animal source serving as host than it is when derived from microbes derived from plate culture.
Epidemiology is another important tool used to study disease in a population. For infectious diseases it helps to determine if a disease outbreak is sporadic (occasional occurrence), endemic (regular cases often occurring in a region), epidemic (an unusually high number of cases in a region), or pandemic (a global epidemic).
Infectious diseases are sometimes called contagious disease when they are easily transmitted by contact with an ill person or their secretions (e.g., influenza). Thus, a contagious disease is a subset of infectious disease that is especially infective or easily transmitted. Other types of infectious/transmissible/communicable diseases with more specialized routes of infection, such as vector transmission or sexual transmission, are usually not regarded as "contagious", and often do not require medical isolation (sometimes loosely called quarantine) of victims. However, this specialized connotation of the word "contagious" and "contagious disease" (easy transmissibility) is not always respected in popular use.
Infectious diseases are commonly transmitted from person to person through direct contact. The types of contact are through person to person and droplet spread. Indirect contact such as airborne transmission, contaminated objects, food and drinking water, animal person contact, animal reservoirs, insect bites, and environmental reservoirs  are another way infectious diseases are transmitted,[10]
Infections can be classified by the anatomic location or organ system infected, including:
In addition, locations of inflammation where infection is the most common cause include pneumonia, meningitis and salpingitis.
The symptoms of an infection depends on the type of disease. Some signs of infection affect the whole body generally, such as fatigue, loss of appetite, weight loss, fevers, night sweats, chills, aches and pains. Others are specific to individual body parts, such as skin rashes, coughing, or a runny nose.
In certain cases, infectious diseases may be asymptomatic for much or even all of their course in a given host. In the latter case, the disease may only be defined as a "disease" (which by definition means an illness) in hosts who secondarily become ill after contact with an asymptomatic carrier. An infection is not synonymous with an infectious disease, as some infections do not cause illness in a host.[9]
Bacterial and viral infections can both cause the same kinds of symptoms. It can be difficult to distinguish which is the cause of a specific infection.[11] It's important to distinguish, because viral infections cannot be cured by antibiotics.[12]
There is a general chain of events that applies to infections.[14] The chain of events involves several steps—which include the infectious agent, reservoir, entering a susceptible host, exit and transmission to new hosts. Each of the links must be present in a chronological order for an infection to develop. Understanding these steps helps health care workers target the infection and prevent it from occurring in the first place.[15]
Infection begins when an organism successfully enters the body, grows and multiplies. This is referred to as colonization. Most humans are not easily infected. Those who are weak, sick, malnourished, have cancer or are diabetic have increased susceptibility to chronic or persistent infections. Individuals who have a suppressed immune system are particularly susceptible to opportunistic infections. Entrance to the host at host-pathogen interface, generally occurs through the mucosa in orifices like the oral cavity, nose, eyes, genitalia, anus, or the microbe can enter through open wounds. While a few organisms can grow at the initial site of entry, many migrate and cause systemic infection in different organs. Some pathogens grow within the host cells (intracellular) whereas others grow freely in bodily fluids.
Wound colonization refers to nonreplicating microorganisms within the wound, while in infected wounds, replicating organisms exist and tissue is injured. All multicellular organisms are colonized to some degree by extrinsic organisms, and the vast majority of these exist in either a mutualistic or commensal relationship with the host. An example of the former is the anaerobic bacteria species, which colonizes the mammalian colon, and an example of the latter are the various species of staphylococcus that exist on human skin. Neither of these colonizations are considered infections. The difference between an infection and a colonization is often only a matter of circumstance. Non-pathogenic organisms can become pathogenic given specific conditions, and even the most virulent organism requires certain circumstances to cause a compromising infection. Some colonizing bacteria, such as Corynebacteria sp. and viridans streptococci, prevent the adhesion and colonization of pathogenic bacteria and thus have a symbiotic relationship with the host, preventing infection and speeding wound healing.
The variables involved in the outcome of a host becoming inoculated by a pathogen and the ultimate outcome include:
As an example, several staphylococcal species remain harmless on the skin, but, when present in a normally sterile space, such as in the capsule of a joint or the peritoneum, multiply without resistance and cause harm.
An interesting fact that gas chromatography–mass spectrometry, 16S ribosomal RNA analysis, omics, and other advanced technologies have made more apparent to humans in recent decades is that microbial colonization is very common even in environments that humans think of as being nearly sterile. Because it is normal to have bacterial colonization, it is difficult to know which chronic wounds can be classified as infected and how much risk of progression exists. Despite the huge number of wounds seen in clinical practice, there are limited quality data for evaluated symptoms and signs. A review of chronic wounds in the Journal of the American Medical Association's "Rational Clinical Examination Series" quantified the importance of increased pain as an indicator of infection.[19] The review showed that the most useful finding is an increase in the level of pain [likelihood ratio (LR) range, 11–20] makes infection much more likely, but the absence of pain (negative likelihood ratio range, 0.64–0.88) does not rule out infection (summary LR 0.64–0.88).
Disease can arise if the host's protective immune mechanisms are compromised and the organism inflicts damage on the host. Microorganisms can cause tissue damage by releasing a variety of toxins or destructive enzymes. For example, Clostridium tetani releases a toxin that paralyzes muscles, and staphylococcus releases toxins that produce shock and sepsis. Not all infectious agents cause disease in all hosts. For example, less than 5% of individuals infected with polio develop disease.[20] On the other hand, some infectious agents are highly virulent. The prion causing mad cow disease and Creutzfeldt–Jakob disease invariably kills all animals and people that are infected.
Persistent infections occur because the body is unable to clear the organism after the initial infection. Persistent infections are characterized by the continual presence of the infectious organism, often as latent infection with occasional recurrent relapses of active infection. There are some viruses that can maintain a persistent infection by infecting different cells of the body. Some viruses once acquired never leave the body. A typical example is the herpes virus, which tends to hide in nerves and become reactivated when specific circumstances arise.
Persistent infections cause millions of deaths globally each year.[21] Chronic infections by parasites account for a high morbidity and mortality in many underdeveloped countries.
For infecting organisms to survive and repeat the infection cycle in other hosts, they (or their progeny) must leave an existing reservoir and cause infection elsewhere. Infection transmission can take place via many potential routes:
The relationship between virulence versus transmissibility is complex; if a disease is rapidly fatal, the host may die before the microbe can be passed along to another host.
Diagnosis of infectious disease sometimes involves identifying an infectious agent either directly or indirectly. In practice most minor infectious diseases such as warts, cutaneous abscesses, respiratory system infections and diarrheal diseases are diagnosed by their clinical presentation and treated without knowledge of the specific causative agent. Conclusions about the cause of the disease are based upon the likelihood that a patient came in contact with a particular agent, the presence of a microbe in a community, and other epidemiological considerations. Given sufficient effort, all known infectious agents can be specifically identified. The benefits of identification, however, are often greatly outweighed by the cost, as often there is no specific treatment, the cause is obvious, or the outcome of an infection is benign.
Diagnosis of infectious disease is nearly always initiated by medical history and physical examination. More detailed identification techniques involve the culture of infectious agents isolated from a patient. Culture allows identification of infectious organisms by examining their microscopic features, by detecting the presence of substances produced by pathogens, and by directly identifying an organism by its genotype. Other techniques (such as X-rays, CAT scans, PET scans or NMR) are used to produce images of internal abnormalities resulting from the growth of an infectious agent. The images are useful in detection of, for example, a bone abscess or a spongiform encephalopathy produced by a prion.
The diagnosis is aided by the presenting symptoms in any individual with an infectious disease, yet it usually needs additional diagnostic techniques to confirm the suspicion. Some signs are specifically characteristic and indicative of a disease and are called pathognomonic signs; but these are rare. Not all infections are symptomatic.[25]
In children the presence of cyanosis, rapid breathing, poor peripheral perfusion, or a petechial rash increases the risk of a serious infection by greater than 5 fold.[26] Other important indicators include parental concern, clinical instinct, and temperature greater than 40 °C.[26]
Microbiological culture is a principal tool used to diagnose infectious disease. In a microbial culture, a growth medium is provided for a specific agent. A sample taken from potentially diseased tissue or fluid is then tested for the presence of an infectious agent able to grow within that medium. Most pathogenic bacteria are easily grown on nutrient agar, a form of solid medium that supplies carbohydrates and proteins necessary for growth of a bacterium, along with copious amounts of water. A single bacterium will grow into a visible mound on the surface of the plate called a colony, which may be separated from other colonies or melded together into a "lawn". The size, color, shape and form of a colony is characteristic of the bacterial species, its specific genetic makeup (its strain), and the environment that supports its growth. Other ingredients are often added to the plate to aid in identification. Plates may contain substances that permit the growth of some bacteria and not others, or that change color in response to certain bacteria and not others. Bacteriological plates such as these are commonly used in the clinical identification of infectious bacterium. Microbial culture may also be used in the identification of viruses: the medium in this case being cells grown in culture that the virus can infect, and then alter or kill. In the case of viral identification, a region of dead cells results from viral growth, and is called a "plaque". Eukaryotic parasites may also be grown in culture as a means of identifying a particular agent.
In the absence of suitable plate culture techniques, some microbes require culture within live animals. Bacteria such as Mycobacterium leprae and Treponema pallidum can be grown in animals, although serological and microscopic techniques make the use of live animals unnecessary. Viruses are also usually identified using alternatives to growth in culture or animals. Some viruses may be grown in embryonated eggs. Another useful identification method is Xenodiagnosis, or the use of a vector to support the growth of an infectious agent. Chagas disease is the most significant example, because it is difficult to directly demonstrate the presence of the causative agent, Trypanosoma cruzi in a patient, which therefore makes it difficult to definitively make a diagnosis. In this case, xenodiagnosis involves the use of the vector of the Chagas agent T. cruzi, an uninfected triatomine bug, which takes a blood meal from a person suspected of having been infected. The bug is later inspected for growth of T. cruzi within its gut.
Another principal tool in the diagnosis of infectious disease is microscopy. Virtually all of the culture techniques discussed above rely, at some point, on microscopic examination for definitive identification of the infectious agent. Microscopy may be carried out with simple instruments, such as the compound light microscope, or with instruments as complex as an electron microscope. Samples obtained from patients may be viewed directly under the light microscope, and can often rapidly lead to identification. Microscopy is often also used in conjunction with biochemical staining techniques, and can be made exquisitely specific when used in combination with antibody based techniques. For example, the use of antibodies made artificially fluorescent (fluorescently labeled antibodies) can be directed to bind to and identify a specific antigens present on a pathogen. A fluorescence microscope is then used to detect fluorescently labeled antibodies bound to internalized antigens within clinical samples or cultured cells. This technique is especially useful in the diagnosis of viral diseases, where the light microscope is incapable of identifying a virus directly.
Other microscopic procedures may also aid in identifying infectious agents. Almost all cells readily stain with a number of basic dyes due to the electrostatic attraction between negatively charged cellular molecules and the positive charge on the dye. A cell is normally transparent under a microscope, and using a stain increases the contrast of a cell with its background. Staining a cell with a dye such as Giemsa stain or crystal violet allows a microscopist to describe its size, shape, internal and external components and its associations with other cells. The response of bacteria to different staining procedures is used in the taxonomic classification of microbes as well. Two methods, the Gram stain and the acid-fast stain, are the standard approaches used to classify bacteria and to diagnosis of disease. The Gram stain identifies the bacterial groups Firmicutes and Actinobacteria, both of which contain many significant human pathogens. The acid-fast staining procedure identifies the Actinobacterial genera Mycobacterium and Nocardia.
Biochemical tests used in the identification of infectious agents include the detection of metabolic or enzymatic products characteristic of a particular infectious agent. Since bacteria ferment carbohydrates in patterns characteristic of their genus and species, the detection of fermentation products is commonly used in bacterial identification. Acids, alcohols and gases are usually detected in these tests when bacteria are grown in selective liquid or solid media.
The isolation of enzymes from infected tissue can also provide the basis of a biochemical diagnosis of an infectious disease. For example, humans can make neither RNA replicases nor reverse transcriptase, and the presence of these enzymes are characteristic of specific types of viral infections. The ability of the viral protein hemagglutinin to bind red blood cells together into a detectable matrix may also be characterized as a biochemical test for viral infection, although strictly speaking hemagglutinin is not an enzyme and has no metabolic function.
Serological methods are highly sensitive, specific and often extremely rapid tests used to identify microorganisms. These tests are based upon the ability of an antibody to bind specifically to an antigen. The antigen, usually a protein or carbohydrate made by an infectious agent, is bound by the antibody. This binding then sets off a chain of events that can be visibly obvious in various ways, dependent upon the test. For example, "Strep throat" is often diagnosed within minutes, and is based on the appearance of antigens made by the causative agent, S. pyogenes, that is retrieved from a patients throat with a cotton swab. Serological tests, if available, are usually the preferred route of identification, however the tests are costly to develop and the reagents used in the test often require refrigeration. Some serological methods are extremely costly, although when commonly used, such as with the "strep test", they can be inexpensive.[9]
Complex serological techniques have been developed into what are known as Immunoassays. Immunoassays can use the basic antibody – antigen binding as the basis to produce an electro-magnetic or particle radiation signal, which can be detected by some form of instrumentation. Signal of unknowns can be compared to that of standards allowing quantitation of the target antigen. To aid in the diagnosis of infectious diseases, immunoassays can detect or measure antigens from either infectious agents or proteins generated by an infected organism in response to a foreign agent. For example, immunoassay A may detect the presence of a surface protein from a virus particle. Immunoassay B on the other hand may detect or measure antibodies produced by an organism's immune system that are made to neutralize and allow the destruction of the virus.
Instrumentation can be used to read extremely small signals created by secondary reactions linked to the antibody – antigen binding. Instrumentation can control sampling, reagent use, reaction times, signal detection, calculation of results, and data management to yield a cost effective automated process for diagnosis of infectious disease.
Technologies based upon the polymerase chain reaction (PCR) method will become nearly ubiquitous gold standards of diagnostics of the near future, for several reasons. First, the catalog of infectious agents has grown to the point that virtually all of the significant infectious agents of the human population have been identified. Second, an infectious agent must grow within the human body to cause disease; essentially it must amplify its own nucleic acids in order to cause a disease. This amplification of nucleic acid in infected tissue offers an opportunity to detect the infectious agent by using PCR. Third, the essential tools for directing PCR, primers, are derived from the genomes of infectious agents, and with time those genomes will be known, if they are not already.
Thus, the technological ability to detect any infectious agent rapidly and specifically are currently available. The only remaining blockades to the use of PCR as a standard tool of diagnosis are in its cost and application, neither of which is insurmountable. The diagnosis of a few diseases will not benefit from the development of PCR methods, such as some of the clostridial diseases (tetanus and botulism). These diseases are fundamentally biological poisonings by relatively small numbers of infectious bacteria that produce extremely potent neurotoxins. A significant proliferation of the infectious agent does not occur, this limits the ability of PCR to detect the presence of any bacteria.
Given the wide range of bacteria, viruses, and other pathogens that cause debilitating and life-threatening illness, the ability to quickly identify the cause of infection is important yet often challenging. For example, more than half of cases of encephalitis, a severe illness affecting the brain, remain undiagnosed, despite extensive testing using state-of-the-art clinical laboratory methods. Metagenomics is currently being researched for clinical use, and shows promise as a sensitive and rapid way to diagnose infection using a single all-encompassing test. This test is similar to current PCR tests; however, amplification of genetic material is unbiased rather than using primers for a specific infectious agent. This amplification step is followed by next-generation sequencing and alignment comparisons using large databases of thousands of organismic and viral genomes.
Metagenomic sequencing could prove especially useful for diagnosis when the patient is immunocompromised. An ever-wider array of infectious agents can cause serious harm to individuals with immunosuppression, so clinical screening must often be broader. Additionally, the expression of symptoms is often atypical, making clinical diagnosis based on presentation more difficult. Thirdly, diagnostic methods that rely on the detection of antibodies are more likely to fail. A broad, sensitive test for pathogens that detects the presence of infectious material rather than antibodies is therefore highly desirable.
There is usually an indication for a specific identification of an infectious agent only when such identification can aid in the treatment or prevention of the disease, or to advance knowledge of the course of an illness prior to the development of effective therapeutic or preventative measures. For example, in the early 1980s, prior to the appearance of AZT for the treatment of AIDS, the course of the disease was closely followed by monitoring the composition of patient blood samples, even though the outcome would not offer the patient any further treatment options. In part, these studies on the appearance of HIV in specific communities permitted the advancement of hypotheses as to the route of transmission of the virus. By understanding how the disease was transmitted, resources could be targeted to the communities at greatest risk in campaigns aimed at reducing the number of new infections. The specific serological diagnostic identification, and later genotypic or molecular identification, of HIV also enabled the development of hypotheses as to the temporal and geographical origins of the virus, as well as a myriad of other hypothesis.[9] The development of molecular diagnostic tools have enabled physicians and researchers to monitor the efficacy of treatment with anti-retroviral drugs. Molecular diagnostics are now commonly used to identify HIV in healthy people long before the onset of illness and have been used to demonstrate the existence of people who are genetically resistant to HIV infection. Thus, while there still is no cure for AIDS, there is great therapeutic and predictive benefit to identifying the virus and monitoring the virus levels within the blood of infected individuals, both for the patient and for the community at large.
Techniques like hand washing, wearing gowns, and wearing face masks can help prevent infections from being passed from one person to another. Aseptic technique was introduced in medicine and surgery in the late 19th century and greatly reduced the incidence of infections caused by surgery. Frequent hand washing remains the most important defense against the spread of unwanted organisms.[28] There are other forms of prevention such as avoiding the use of illicit drugs, using a condom, and having a healthy lifestyle with a balanced diet and regular exercise. Cooking foods well and avoiding foods that have been left outside for a long time is also important.
Antimicrobial substances used to prevent transmission of infections include:
One of the ways to prevent or slow down the transmission of infectious diseases is to recognize the different characteristics of various diseases.[29] Some critical disease characteristics that should be evaluated include virulence, distance traveled by victims, and level of contagiousness. The human strains of Ebola virus, for example, incapacitate their victims extremely quickly and kill them soon after. As a result, the victims of this disease do not have the opportunity to travel very far from the initial infection zone.[30] Also, this virus must spread through skin lesions or permeable membranes such as the eye. Thus, the initial stage of Ebola is not very contagious since its victims experience only internal hemorrhaging. As a result of the above features, the spread of Ebola is very rapid and usually stays within a relatively confined geographical area. In contrast, the Human Immunodeficiency Virus (HIV) kills its victims very slowly by attacking their immune system.[9] As a result, many of its victims transmit the virus to other individuals before even realizing that they are carrying the disease. Also, the relatively low virulence allows its victims to travel long distances, increasing the likelihood of an epidemic.
Another effective way to decrease the transmission rate of infectious diseases is to recognize the effects of small-world networks.[29] In epidemics, there are often extensive interactions within hubs or groups of infected individuals and other interactions within discrete hubs of susceptible individuals. Despite the low interaction between discrete hubs, the disease can jump to and spread in a susceptible hub via a single or few interactions with an infected hub. Thus, infection rates in small-world networks can be reduced somewhat if interactions between individuals within infected hubs are eliminated (Figure 1). However, infection rates can be drastically reduced if the main focus is on the prevention of transmission jumps between hubs. The use of needle exchange programs in areas with a high density of drug users with HIV is an example of the successful implementation of this treatment method. [6] Another example is the use of ring culling or vaccination of potentially susceptible livestock in adjacent farms to prevent the spread of the foot-and-mouth virus in 2001.[31]
A general method to prevent transmission of vector-borne pathogens is pest control.
Infection with most pathogens does not result in death of the host and the offending organism is ultimately cleared after the symptoms of the disease have waned.[8] This process requires immune mechanisms to kill or inactivate the inoculum of the pathogen. Specific acquired immunity against infectious diseases may be mediated by antibodies and/or T lymphocytes. Immunity mediated by these two factors may be manifested by:
The immune system response to a microorganism often causes symptoms such as a high fever and inflammation, and has the potential to be more devastating than direct damage caused by a microbe.[9]
Resistance to infection (immunity) may be acquired following a disease, by asymptomatic carriage of the pathogen, by harboring an organism with a similar structure (crossreacting), or by vaccination. Knowledge of the protective antigens and specific acquired host immune factors is more complete for primary pathogens than for opportunistic pathogens.
There is also the phenomenon of herd immunity which offers a measure of protection to those otherwise vulnerable people when a large enough proportion of the population has acquired immunity from certain infections.
Immune resistance to an infectious disease requires a critical level of either antigen-specific antibodies and/or T cells when the host encounters the pathogen. Some individuals develop natural serum antibodies to the surface polysaccharides of some agents although they have had little or no contact with the agent, these natural antibodies confer specific protection to adults and are passively transmitted to newborns.
The organism that is the target of an infecting action of a specific infectious agent is called the host. The host harbors and agent in a mature, or sexually active stage phase called the definitive host. The intermediate host comes in contact during the larvae stage. A host can be anything living and can attain to asexual and sexual reproduction.[32]
The clearance of the pathogens, either treatment-induced or spontaneous, it can be influenced by the genetic variants carried by the individual patients. For instance, for genotype 1 hepatitis C treated with Pegylated interferon-alpha-2a or Pegylated interferon-alpha-2b (brand names Pegasys or PEG-Intron) combined with ribavirin, it has been shown that genetic polymorphisms near the human IL28B gene, encoding interferon lambda 3, are associated with significant differences in the treatment-induced clearance of the virus. This finding, originally reported in Nature,[33] showed that genotype 1 hepatitis C patients carrying certain genetic variant alleles near the IL28B gene are more possibly to achieve sustained virological response after the treatment than others. Later report from Nature[34] demonstrated that the same genetic variants are also associated with the natural clearance of the genotype 1 hepatitis C virus.
When infection attacks the body, anti-infective drugs can suppress the infection. Several broad types of anti-infective drugs exist, depending on the type of organism targeted; they include antibacterial (antibiotic; including antitubercular), antiviral, antifungal and antiparasitic (including antiprotozoal and antihelminthic) agents. Depending on the severity and the type of infection, the antibiotic may be given by mouth or by injection, or may be applied topically. Severe infections of the brain are usually treated with intravenous antibiotics. Sometimes, multiple antibiotics are used in case there is resistance to one antibiotic. Antibiotics only work for bacteria and do not affect viruses. Antibiotics work by slowing down the multiplication of bacteria or killing the bacteria. The most common classes of antibiotics used in medicine include penicillin, cephalosporins, aminoglycosides, macrolides, quinolones and tetracyclines.[citation needed]
Not all infections require treatment, and for many self-limiting infections the treatment may cause more side-effects than benefits. Antimicrobial stewardship is the concept that healthcare providers should treat an infection with an antimicrobial that specifically works well for the target pathogen for the shortest amount of time and to only treat when there is a known or highly suspected pathogen that will respond to the medication.[35]
In 2010, about 10 million people died of infectious diseases.[37]
The World Health Organization collects information on global deaths by International Classification of Disease (ICD) code categories. The following table lists the top infectious disease by number of deaths in 2002. 1993 data is included for comparison.
The top three single agent/disease killers are HIV/AIDS, TB and malaria. While the number of deaths due to nearly every disease have decreased, deaths due to HIV/AIDS have increased fourfold. Childhood diseases include pertussis, poliomyelitis, diphtheria, measles and tetanus. Children also make up a large percentage of lower respiratory and diarrheal deaths. In 2012, approximately 3.1 million people have died due to lower respiratory infections, making it the number 4 leading cause of death in the world.[43]
A pandemic (or global epidemic) is a disease that affects people over an extensive geographical area.
In most cases, microorganisms live in harmony with their hosts via mutual or commensal interactions. Diseases can emerge when existing parasites become pathogenic or when new pathogenic parasites enter a new host.
Several human activities have led to the emergence of zoonotic human pathogens, including viruses, bacteria, protozoa, and rickettsia,[52] and spread of vector-borne diseases,[51] see also globalization and disease and wildlife disease:
Ideas of contagion became more popular in Europe during the Renaissance, particularly through the writing of the Italian physician Girolamo Fracastoro.[54]
Anton van Leeuwenhoek (1632–1723) advanced the science of microscopy by being the first to observe microorganisms, allowing for easy visualization of bacteria.
In the mid-19th century John Snow and William Budd did important work demonstrating the contagiousness of typhoid and cholera through contaminated water. Both are credited with decreasing epidemics of cholera in their towns by implementing measures to prevent contamination of water.[55]
Louis Pasteur proved beyond doubt that certain diseases are caused by infectious agents, and developed a vaccine for rabies.
Robert Koch, provided the study of infectious diseases with a scientific basis known as Koch's postulates.
Edward Jenner, Jonas Salk and Albert Sabin developed effective vaccines for smallpox and polio, which would later result in the eradication and near-eradication of these diseases, respectively.
Alexander Fleming discovered the world's first antibiotic, Penicillin, which Florey and Chain then developed.
Gerhard Domagk developed sulphonamides, the first broad spectrum synthetic antibacterial drugs.
The medical treatment of infectious diseases falls into the medical field of Infectious Disease and in some cases the study of propagation pertains to the field of Epidemiology. Generally, infections are initially diagnosed by primary care physicians or internal medicine specialists. For example, an "uncomplicated" pneumonia will generally be treated by the internist or the pulmonologist (lung physician). The work of the infectious diseases specialist therefore entails working with both patients and general practitioners, as well as laboratory scientists, immunologists, bacteriologists and other specialists.
An infectious disease team may be alerted when:
A number of studies have reported associations between pathogen load in an area and human behavior. Higher pathogen load is associated with decreased size of ethnic and religious groups in an area. This may be due high pathogen load favoring avoidance of other groups, which may reduce pathogen transmission, or a high pathogen load preventing the creation of large settlements and armies that enforce a common culture. Higher pathogen load is also associated with more restricted sexual behavior, which may reduce pathogen transmission. It also associated with higher preferences for health and attractiveness in mates. Higher fertility rates and shorter or less parental care per child is another association that may be a compensation for the higher mortality rate. There is also an association with polygyny which may be due to higher pathogen load, making selecting males with a high genetic resistance increasingly important. Higher pathogen load is also associated with more collectivism and less individualism, which may limit contacts with outside groups and infections. There are alternative explanations for at least some of the associations although some of these explanations may in turn ultimately be due to pathogen load. Thus, polygny may also be due to a lower male:female ratio in these areas but this may ultimately be due to male infants having increased mortality from infectious diseases. Another example is that poor socioeconomic factors may ultimately in part be due to high pathogen load preventing economic development.[56]
Evidence of infection in fossil remains is a subject of interest for paleopathologists, scientists who study occurrences of injuries and illness in extinct life forms. Signs of infection have been discovered in the bones of carnivorous dinosaurs. When present, however, these infections seem to tend to be confined to only small regions of the body. A skull attributed to the early carnivorous dinosaur Herrerasaurus ischigualastensis exhibits pit-like wounds surrounded by swollen and porous bone. The unusual texture of the bone around the wounds suggests they were afflicted by a short-lived, non-lethal infection. Scientists who studied the skull speculated that the bite marks were received in a fight with another Herrerasaurus. Other carnivorous dinosaurs with documented evidence of infection include Acrocanthosaurus, Allosaurus, Tyrannosaurus and a tyrannosaur from the Kirtland Formation. The infections from both tyrannosaurs were received by being bitten during a fight, like the Herrerasaurus specimen.[57]
A 2006 Space Shuttle experiment found that Salmonella typhimurium, a bacterium that can cause food poisoning, became more virulent when cultivated in space.[58] On April 29, 2013, scientists in Rensselaer Polytechnic Institute, funded by NASA, reported that, during spaceflight on the International Space Station, microbes seem to adapt to the space environment in ways "not observed on Earth" and in ways that "can lead to increases in growth and virulence".[59] More recently, in 2017, bacteria were found to be more resistant to antibiotics and to thrive in the near-weightlessness of space.[60] Microorganisms have been observed to survive the vacuum of outer space.[61][62]



Oncovirus - Wikipedia
An oncovirus is a virus that can cause cancer. This term originated from studies of acutely transforming retroviruses in the 1950–60s,[1] often called oncornaviruses to denote their RNA virus origin.[2]
It now refers to any virus with a DNA or RNA genome causing cancer and is synonymous with "tumor virus" or "cancer virus". The vast majority of human and animal viruses do not cause cancer, probably because of longstanding co-evolution between the virus and its host. Oncoviruses have been important not only in epidemiology, but also in investigations of cell cycle control mechanisms such as the Retinoblastoma protein.
The World Health Organization's International Agency for Research on Cancer estimated that in 2002, infection caused 17.8% of human cancers, with 11.9% caused by one of seven viruses.[3] These cancers might be easily prevented through vaccination (e.g., papillomavirus vaccines), diagnosed with simple blood tests, and treated with less-toxic antiviral compounds.
Generally, tumor viruses cause little or no disease after infection in their hosts, or cause non-neoplastic diseases such as acute hepatitis for hepatitis B virus or mononucleosis for Epstein–Barr virus. A minority of persons (or animals) will go on to develop cancers after infection. This has complicated efforts to determine whether or not a given virus causes cancer. The well-known Koch's postulates, 19th-century constructs developed by Robert Koch to establish the likelihood that Bacillus anthracis will cause anthrax disease, are not applicable to viral diseases. (Firstly, this is because viruses cannot truly be isolated in pure culture—even stringent isolation techniques cannot exclude undetected contaminating viruses with similar density characteristics, and viruses must be grown on cells. Secondly, asymptomatic virus infection and carriage is the norm for most tumor viruses, which violates Koch's third principle. Relman and Fredericks have described the difficulties in applying Koch's postulates to virus-induced cancers.[4] Finally, the host restriction for human viruses makes it unethical to experimentally transmit a suspected cancer virus.) Other measures, such as A. B. Hill's criteria,[5] are more relevant to cancer virology but also have some limitations in determining causality.
Tumor viruses come in a variety of forms: Viruses with a DNA genome, such as adenovirus, and viruses with an RNA genome, like the Hepatitis C virus (HCV), can cause cancers, as can retroviruses having both DNA and RNA genomes (Human T-lymphotropic virus and hepatitis B virus, which normally replicates as a mixed double and single-stranded DNA virus but also has a retroviral replication component). In many cases, tumor viruses do not cause cancer in their native hosts but only in dead-end species. For example, adenoviruses do not cause cancer in humans but are instead responsible for colds, conjunctivitis and other acute illnesses. They only become tumorigenic when infected into certain rodent species, such as Syrian hamsters. Some viruses are tumorigenic when they infect a cell and persist as circular episomes or plasmids, replicating separately from host cell DNA (Epstein–Barr virus and Kaposi's sarcoma-associated herpesvirus). Other viruses are only carcinogenic when they integrate into the host cell genome as part of a biological accident, such as polyomaviruses and papillomaviruses.
A direct oncogenic viral mechanism[6] involves either insertion of additional viral oncogenic genes into the host cell or to enhance already existing oncogenic genes (proto-oncogenes) in the genome. Indirect viral oncogenicity involves chronic nonspecific inflammation occurring over decades of infection, as is the case for HCV-induced liver cancer. These two mechanisms differ in their biology and epidemiology: direct tumor viruses must have at least one virus copy in every tumor cell expressing at least one protein or RNA that is causing the cell to become cancerous. Because foreign virus antigens are expressed in these tumors, persons who are immunosuppressed such as AIDS or transplant patients are at higher risk for these types of cancers. Chronic indirect tumor viruses, on the other hand, can be lost (at least theoretically) from a mature tumor that has accumulated sufficient mutations and growth conditions (hyperplasia) from the chronic inflammation of viral infection. In this latter case, it is controversial but at least theoretically possible that an indirect tumor virus could undergo "hit-and-run" and so the virus would be lost from the clinically diagnosed tumor. In practical terms, this is an uncommon occurrence if it does occur.
The theory that cancer could be caused by a virus began with the experiments of Oluf Bang and Vilhelm Ellerman in 1908 who first show that avian erythroblastosis (a form of chicken leukemia) could be transmitted by cell-free extracts.[28]  This was subsequently confirmed for solid tumors in chickens in 1910-1911 by Peyton Rous.[29][30]
By the early 1950s it was known that viruses could remove and incorporate genes and genetic material in cells. It was suggested that these new genes inserted into cells could make the cell cancerous. Many of these viral oncogenes have been discovered and identified to cause cancer.
The main viruses associated with human cancers are human papillomavirus, hepatitis B and hepatitis C virus, Epstein–Barr virus, human T-lymphotropic virus, Kaposi's sarcoma-associated herpesvirus (KSHV) and Merkel cell polyomavirus. Experimental and epidemiological data imply a causative role for viruses and they appear to be the second most important risk factor for cancer development in humans, exceeded only by tobacco usage.[31] The mode of virally induced tumors can be divided into two, acutely transforming or slowly transforming. In acutely transforming viruses, the viral particles carry a gene that encodes for an overactive oncogene called viral-oncogene (v-onc), and the infected cell is transformed as soon as v-onc is expressed. In contrast, in slowly transforming viruses, the virus genome is inserted, especially as viral genome insertion is an obligatory part of retroviruses, near a proto-oncogene in the host genome. The viral promoter or other transcription regulation elements in turn cause overexpression of that proto-oncogene, which in turn induces uncontrolled cellular proliferation. Because viral genome insertion is not specific to proto-oncogenes and the chance of insertion near that proto-oncogene is low, slowly transforming viruses have very long tumor latency compared to acutely transforming viruses, which already carry the viral oncogene.
Hepatitis viruses, including hepatitis B and hepatitis C, can induce a chronic viral infection that leads to liver cancer in 0.47% of hepatitis B patients per year (especially in Asia, less so in North America), and in 1.4% of hepatitis C carriers per year. Liver cirrhosis, whether from chronic viral hepatitis infection or alcoholism, is associated with the development of liver cancer, and the combination of cirrhosis and viral hepatitis presents the highest risk of liver cancer development. Worldwide, liver cancer is one of the most common, and most deadly, cancers due to a huge burden of viral hepatitis transmission and disease.
Through advances in cancer research, vaccines designed to prevent cancer have been created. The hepatitis B vaccine is the first vaccine that has been established to prevent cancer (hepatocellular carcinoma) by preventing infection with the causative virus.  In 2006, the U.S. Food and Drug Administration approved a human papilloma virus vaccine, called Gardasil. The vaccine protects against four HPV types, which together cause 70% of cervical cancers and 90% of genital warts. In March 2007, the US Centers for Disease Control and Prevention (CDC) Advisory Committee on Immunization Practices (ACIP) officially recommended that females aged 11–12 receive the vaccine, and indicated that females as young as age 9 and as old as age 26 are also candidates for immunization.
DNA oncoviruses typically impair two families of tumor suppressor proteins: tumor proteins p53 and the retinoblastoma proteins (Rb). It is evolutionarily advantageous for viruses to inactivate p53 because p53 can trigger cell cycle arrest or apoptosis in infected cells when the virus attempts to replicate its DNA.[32] Similarly, Rb proteins regulate many essential cell functions, including but not limited to a crucial cell cycle checkpoint, making them a target for viruses attempting to interrupt regular cell function.[33]
While several DNA oncoviruses have been discovered, three have been studied extensively. Adenoviruses can lead to tumors in rodent models but do not cause cancer in humans; however, they have been exploited as delivery vehicles in gene therapy for diseases such as cystic fibrosis and cancer.[34] Simian virus 40 (SV40), a polyomavirus, can cause tumors in rodent models but is not oncogenic in humans.[35] This phenomenon has been one of the major controversies of oncogenesis in the 20th century because an estimated 100 million people were inadvertently exposed to SV40 through polio vaccines.[35] The Human Papillomavirus-16 (HPV-16) has been shown to lead to cervical cancer and other cancers, including head and neck cancer.[36] These three viruses have parallel mechanisms of action, forming an archetype for DNA oncoviruses. All three of these DNA oncoviruses are able to integrate their DNA into the host cell, and use this to transcribe it and transform cells by bypassing the G1/S checkpoint of the cell cycle.
DNA oncoviruses transform infected cells by integrating their DNA into the host cell’s genome.[37] The DNA is believed to be inserted during transcription or replication, when the two annealed strands are separated.[37] This event is relatively rare and generally unpredictable; there seems to be no deterministic predictor of the site of integration.[37] After integration, the host’s cell cycle loses regulation from Rb and p53, and the cell begins cloning to form a tumor.
Rb and p53 regulate the transition between G1 and S phase, arresting the cell cycle before DNA replication until the appropriate checkpoint inputs, such as DNA damage repair, are completed.[38] p53 regulates the p21 gene, which produces a protein which binds to the Cyclin D-Cdk4/6 complex.[39] This prevents Rb phosphorylation and prevents the cell from entering S phase.[39] In mammals, when Rb is active (unphosphorylated), it inhibits the E2F family of transcription factors, which regulate the Cyclin E-Cdk2 complex, which inhibits Rb, forming a positive feedback loop, keeping the cell in G1 until the input crosses a threshold.[38] To drive the cell into S phase prematurely, the viruses must inactivate p53, which plays a central role in the G1/S checkpoint, as well as Rb, which, though downstream of it, is typically kept active by a positive feedback loop.
Viruses employ various methods of inactivating p53. The adenovirus E1B protein (55K) prevents p53 from regulating genes by binding to the site on p53 which binds to the genome.[32] In SV40, the large T antigen (LT) is an analogue; LT also binds to several other cellular proteins, such as p107 and p130, on the same residues.[40] LT binds to p53’s binding domain on the DNA (rather than on the protein), again preventing p53 from appropriately regulating genes.[32] HPV instead degrades p53: the HPV protein E6 binds to a cellular protein called the E6-associated protein (E6-AP, also known as UBE3A), forming a complex which causes the rapid and specific ubiquitination of p53.[41]
Rb is inactivated (thereby allowing the G1/S transition to progress unimpeded) by different but analogous viral oncoproteins. The adenovirus early region 1A (E1A) is an oncoprotein which binds to Rb and can stimulate transcription and transform cells.[32] SV40 uses the same protein for inactivating Rb, LT, to inactivate p53.[39] HPV contains a protein, E7, which can bind to Rb in much the same way.[42] Rb can be inactivated by phosphorylation, or by being bound to a viral oncoprotein, or by mutations—mutations which prevent oncoprotein binding are also associated with cancer.[40]
DNA oncoviruses typically cause cancer by inactivating p53 and Rb, thereby allowing unregulated cell division and creating tumors. There may be many different mechanisms which have evolved separately; in addition to those described above, for example, the Hepatitis B virus (an RNA virus) inactivates p53 by sequestering it in the cytoplasm.[32]
SV40 has been well studied and does not cause cancer in humans, but a recently discovered analogue called Merkel cell polyomavirus has been associated with Merkel cell carcinoma, a form of skin cancer.[43] The Rb binding feature is believed to be the same between the two viruses.[43]
In the 1960s, the replication process of RNA virus was believed to be similar to other single-stranded RNA. Single-stranded RNA replication involves RNA-dependent RNA synthesis which meant that virus-coding enzymes would make partial double-stranded RNA. This belief was proven to be incorrect because there were no double-stranded RNA found in the retrovirus cell. In 1964, Howard Temin proposed a provirus hypothesis, but shortly after reverse transcription in the retrovirus genome was discovered.
All retroviruses have three major coding domains; gag, pol and env. In the gag region of the virus, the synthesis of the internal virion proteins are maintained which make up the matrix, capsid and nucleocapsid proteins. In pol, the information for the reverse transcription and integration enzymes are stored. In env, it is derived from the surface and transmembrane for the viral envelope protein. There is a fourth coding domain which is smaller, but exists in all retroviruses. Pol is the domain that encodes the virion protease.
The retrovirus begins the journey into a host cell by attaching a surface glycoprotein to the cell's plasma membrane receptor. Once inside the cell, the retrovirus goes through reverse transcription in the cytoplasm and generates a double-stranded DNA copy of the RNA genome. Reverse transcription also produces identical structures known as long terminal repeats (LTRs). Long terminal repeats are at the ends of the DNA strands and regulates viral gene expression. The viral DNA is then translocated into the nucleus where one strand of the retroviral genome is put into the chromosomal DNA by the help of the virion intergrase. At this point the retrovirus is referred to as provirus. Once in the chromosomal DNA, the provirus is transcribed by the cellular RNA polymerase II. The transcription leads to the splicing and full-length mRNAs and full-length progeny virion RNA. The virion protein and progeny RNA assemble in the cytoplasm and leave the cell, whereas the other copies send translated viral messages in the cytoplasm.
Not all oncoviruses are DNA viruses.  Some RNA viruses have also been associated such as the hepatitis C virus as well as certain retroviruses, e.g., human T-lymphotropic virus (HTLV-1) and Rous sarcoma virus (RSV).
Estimated percent of new cancers attributable to the virus worldwide in 2002.[3]  NA indicates not available.
The association of other viruses with human cancer is continually under research.



Ionizing radiation - Wikipedia
Ionizing radiation (ionising radiation) is radiation that carries enough energy to liberate electrons from atoms or molecules, thereby ionizing them. Ionizing radiation is made up of energetic subatomic particles, ions or atoms moving at high speeds (usually greater than 1% of the speed of light), and electromagnetic waves on the high-energy end of the electromagnetic spectrum.
Gamma rays, X-rays, and the higher ultraviolet part of the electromagnetic spectrum are ionizing, whereas the lower ultraviolet part of the electromagnetic spectrum and all the spectrum below UV, including visible light (including nearly all types of laser light), infrared, microwaves, and radio waves are considered non-ionizing radiation. The boundary between ionizing and non-ionizing electromagnetic radiation that occurs in the ultraviolet is not sharply defined, since different molecules and atoms ionize at different energies. Conventional definition places the boundary at a photon energy between 10 eV and 33 eV in the ultraviolet (see definition boundary section below).
Typical ionizing subatomic particles from radioactivity include alpha particles, beta particles and neutrons. Almost all products of radioactive decay are ionizing because the energy of radioactive decay is typically far higher than that required to ionize. Other subatomic ionizing particles which occur naturally are muons, mesons, positrons, and other particles that constitute the secondary cosmic rays that are produced after primary cosmic rays interact with Earth's atmosphere.[1][2] Cosmic rays are generated by stars and certain celestial events such as supernova explosions. Cosmic rays may also produce radioisotopes on Earth (for example, carbon-14), which in turn decay and produce ionizing radiation. Cosmic rays and the decay of radioactive isotopes are the primary sources of natural ionizing radiation on Earth referred to as background radiation. Ionizing radiation can also be generated artificially by X-ray tubes, particle accelerators, and any of the various methods that produce radioisotopes artificially.
Ionizing radiation is not detectable by human senses, so radiation detection instruments such as Geiger counters must be used to indicate its presence and measure it. However, high intensities can cause emission of visible light upon interaction with matter, such as in Cherenkov radiation and radioluminescence. Ionizing radiation is used in a wide variety of fields such as medicine, nuclear power, research, manufacturing, construction, and many other areas, but presents a health hazard if proper measures against undesired exposure aren't followed. Exposure to ionizing radiation causes damage to living tissue, and can result in radiation burns, cell damage, radiation sickness, cancer, and death.
Ionizing radiation is categorized by the nature of the particles or electromagnetic waves that create the ionizing effect. These have different ionization mechanisms, and may be grouped as directly or indirectly ionizing.
Any charged massive particle can ionize atoms directly by fundamental interaction through the Coulomb force if it carries sufficient kinetic energy. This includes atomic nuclei, electrons, muons, charged pions, protons, and energetic charged nuclei stripped of their electrons. When moving at relativistic speeds these particles have enough kinetic energy to be ionizing, but relativistic speeds are not required. For example, a typical alpha particle is ionizing, but moves at about 5% c, and an electron with 33 eV (enough to ionize) moves at about 1% c.
The first two ionizing sources to be recognized were given special names used today: Helium nuclei ejected from atomic nuclei are called alpha particles, and electrons ejected usually (but not always) at relativistic speeds, are called beta particles.
Natural cosmic rays are made up primarily of relativistic protons but also include heavier atomic nuclei like helium ions and HZE ions. In the atmosphere such particles are often stopped by air molecules, and this produces short-lived charged pions, which soon decay to muons, a primary type of cosmic ray radiation that reaches the ground (and also penetrates it to some extent). Pions can also be produced in large amounts in particle accelerators.
Alpha particles consist of two protons and two neutrons bound together into a particle identical to a helium nucleus. Alpha particle emissions are generally produced in the process of alpha decay, but may also be produced in other ways. Alpha particles are named after the first letter in the Greek alphabet, α. The symbol for the alpha particle is α or α2+. Because they are identical to helium nuclei, they are also sometimes written as He2+ or 42He2+ indicating a Helium ion with a +2 charge (missing its two electrons). If the ion gains electrons from its environment, the alpha particle can be written as a normal (electrically neutral) helium atom 42He.
Alpha particles are a hugely ionizing form of particle radiation. When they result from radioactive alpha decay they have low penetration depth. In this case they can be absorbed by a few centimeters of air, or by the skin. More powerful, long range alpha particles from ternary fission are three times as energetic, and penetrate proportionately farther in air. The helium nuclei that form 10-12% of cosmic rays, are also usually of much higher energy than those produced by nuclear decay processes, and when encountered in space, are thus able to traverse the human body and dense shielding. However, this type of radiation is significantly attenuated by the Earth's atmosphere, which is a radiation shield equivalent to about 10 meters of water.[3]
Beta particles are high-energy, high-speed electrons or positrons emitted by certain types of radioactive nuclei, such as potassium-40. The production of beta particles is termed beta decay. They are designated by the Greek letter beta (β).
There are two forms of beta decay, β− and β+, which respectively give rise to the electron and the positron.[4]
When something is said to have radioactive contamination, it often means that there are beta particles being emitted from its surface, detectable with a Geiger counter or other radiation detector. When brought into proximity to the beta emitter, the detector will indicate a dramatic increase in radioactivity. When the detector probe is covered with a shield to block the beta rays, the indication will be reduced dramatically.
High-energy beta particles may produce X-rays known as bremsstrahlung ("braking radiation") or secondary electrons (delta ray) as they pass through matter. Both of these can cause an indirect ionization effect.
Bremsstrahlung is of concern when shielding beta emitters, as the interaction of beta particles with the shielding material produces Bremsstrahlung. This effect is greater with material of high atomic numbers, so material with low atomic numbers is used for beta source shielding.
The positron or antielectron is the antiparticle or the antimatter counterpart of the electron. When a low-energy positron collides with a low-energy electron, annihilation occurs, resulting in their conversion into the energy of two or more gamma ray photons (see electron–positron annihilation).
Positrons may be generated by positron emission nuclear decay (through weak interactions), or by pair production from a sufficiently energetic photon. Positrons are common artificial sources of ionizing radiation used in medical positron emission tomography (PET) scans.
As positrons are positively charged particles they can also directly ionize an atom through Coulomb interactions.
Charged nuclei are characteristic of galactic cosmic rays and solar particle events and except for alpha particles (charged helium nuclei) have no natural sources on the earth. In space, however, very high energy protons, helium nuclei, and HZE ions can be initially stopped by relatively thin layers of shielding, clothes, or skin. However, the resulting interaction will generate secondary radiation and cause cascading biological effects. If just one atom of tissue is displaced by an energetic proton, for example, the collision will cause further interactions in the body. This is called "linear energy transfer" (LET), which utilizes elastic scattering.
LET can be visualized as a billiard ball hitting another in the manner of the conservation of momentum, sending both away with the energy of the first ball divided between the two unequally. When a charged nucleus strikes a relatively slow-moving nucleus of an object in space, LET occurs and neutrons, alpha particles, low-energy protons, and other nuclei will be released by the collisions and contribute to the total absorbed dose of tissue.[5]
Indirect ionizing radiation is electrically neutral and therefore does not interact strongly with matter. The bulk of the ionization effects are due to secondary ionizations.
An example of indirectly ionizing radiation is neutron radiation.
Even though photons are electrically neutral, they can ionize atoms directly through the photoelectric effect and the Compton effect. Either of those interactions will cause the ejection of an electron from an atom at relativistic speeds, turning that electron into a beta particle (secondary beta particle) that will ionize many other atoms. Since most of the affected atoms are ionized directly by the secondary beta particles, photons are called indirectly ionizing radiation.[6]
Photon radiation is called gamma rays if produced by a nuclear reaction, subatomic particle decay, or radioactive decay within the nucleus. It is otherwise called x-rays if produced outside the nucleus. The generic term photon is therefore used to describe both.[7][8][9]
X-rays normally have a lower energy than gamma rays, and an older convention was to define the boundary as a wavelength of 10−11 m or a photon energy of 100 keV.[10] That threshold was driven by limitations of older X-ray tubes and low awareness of isomeric transitions. Modern technologies and discoveries have resulted in an overlap between X-ray and gamma energies. In many fields they are functionally identical, differing for terrestrial studies only in origin of the radiation. In astronomy, however, where radiation origin often cannot be reliably determined, the old energy division has been preserved, with X-rays defined as being between about 120 eV and 120 keV, and gamma rays as being of any energy above 100 to 120 keV, regardless of source. Most astronomical "gamma-ray astronomy" are known not to originate in nuclear radioactive processes but, rather, result from processes like those that produce astronomical X-rays, except driven by much more energetic electrons.
Photoelectric absorption is the dominant mechanism in organic materials for photon energies below 100 keV, typical of classical X-ray tube originated X-rays. At energies beyond 100 keV, photons ionize matter increasingly through the Compton effect, and then indirectly through pair production at energies beyond 5 MeV. The accompanying interaction diagram shows two Compton scatterings happening sequentially. In every scattering event, the gamma ray transfers energy to an electron, and it continues on its path in a different direction and with reduced energy.
The lowest ionization energy of any element is 3.89 eV, for caesium. However, US Federal Communications Commission material defines ionizing radiation as that with a photon energy greater than 10 eV (equivalent to a far ultraviolet wavelength of 124 nanometers).[11] Roughly, this corresponds to both the first ionization energy of oxygen, and the ionization energy of hydrogen, both about 14 eV.[12] In some Environmental Protection Agency references, the ionization of a typical water molecule at an energy of 33 eV is referenced[13] as the appropriate biological threshold for ionizing radiation: this value represents the so-called W-value, the colloquial name for the ICRU's mean energy expended in a gas per ion pair formed,[14] which combines ionization energy plus the energy lost to other processes such as excitation.[15] At 38 nanometers wavelength for electromagnetic radiation, 33 eV is close to the energy at the conventional 10 nm wavelength transition between extreme ultraviolet and X-ray radiation, which occurs at about 125 eV. Thus, X-ray radiation is always ionizing, but only extreme-ultraviolet radiation can be considered ionizing under all definitions.
As noted, the biological effect of ionizing radiation on cells somewhat resembles that of a broader spectrum of molecularly damaging radiation, which overlaps ionizing radiation and extends beyond, to somewhat lower energies into all regions of UV and sometimes visible light in some systems (such as photosynthetic systems in leaves). Although DNA is always susceptible to damage by ionizing radiation, the DNA molecule may also be damaged by radiation with enough energy to excite certain molecular bonds to form thymine dimers. This energy may be less than ionizing, but near to it. A good example is ultraviolet spectrum energy which begins at about 3.1 eV (400 nm) at close to the same energy level which can cause sunburn to unprotected skin, as a result of photoreactions in collagen and (in the UV-B range) also damage in DNA (for example, pyrimidine dimers). Thus, the mid and lower ultraviolet electromagnetic spectrum is damaging to biological tissues as a result of electronic excitation in molecules which falls short of ionization, but produces similar non-thermal effects. To some extent, visible light and also ultraviolet A (UVA) which is closest to visible energies, have been proven to result in formation of reactive oxygen species in skin, which cause indirect damage since these are electronically excited molecules which can inflict reactive damage, although they do not cause sunburn (erythema).[16] Like ionization-damage, all these effects in skin are beyond those produced by simple thermal effects.
Neutrons have zero electrical charge and thus often do not directly cause ionization in a single step or interaction with matter. However, fast neutrons will interact with the protons in hydrogen via LET, and this mechanism scatters the nuclei of the materials in the target area, causing direct ionization of the hydrogen atoms. When neutrons strike the hydrogen nuclei, proton radiation (fast protons) results. These protons are themselves ionizing because they are of high energy, are charged, and interact with the electrons in matter.
Neutrons that strike other nuclei besides hydrogen will transfer less energy to the other particle if LET does occur. But, for many nuclei struck by neutrons, inelastic scattering occurs. Whether elastic or inelastic scatter occurs is dependent on the speed of the neutron, whether fast or thermal or somewhere in between. It is also dependent on the nuclei it strikes and its neutron cross section.
In inelastic scattering, neutrons are readily absorbed in a process called neutron capture and attributes to the neutron activation of the nucleus. Neutron interactions with most types of matter in this manner usually produce radioactive nuclei. The abundant oxygen-16 nucleus, for example, undergoes neutron activation, rapidly decays by a proton emission forming nitrogen-16, which decays to oxygen-16. The short-lived nitrogen-16 decay emits a powerful beta ray. This process can be written as:
16O (n,p) 16N (fast neutron capture possible with >11 MeV neutron)
16N → 16O + β− (Decay t1/2 = 7.13 s)

This high-energy β− further interacts rapidly with other nuclei, emitting high-energy γ via BremsstrahlungWhile not a favorable reaction, the 16O (n,p) 16N reaction is a major source of X-rays emitted from the cooling water of a pressurized water reactor and contributes enormously to the radiation generated by a water-cooled nuclear reactor while operating.
For the best shielding of neutrons, hydrocarbons that have an abundance of hydrogen are used.
In fissile materials, secondary neutrons may produce nuclear chain reactions, causing a larger amount of ionization from the daughter products of fission.
Outside the nucleus, free neutrons are unstable and have a mean lifetime of 14 minutes, 42 seconds. Free neutrons decay by emission of an electron and an electron antineutrino to become a proton, a process known as beta decay:[17]
In the adjacent diagram, a neutron collides with a proton of the target material, and then becomes a fast recoil proton that ionizes in turn. At the end of its path, the neutron is captured by a nucleus in an (n,γ)-reaction that leads to the emission of a neutron capture photon. Such photons always have enough energy to qualify as ionizing radiation.
Neutron radiation, alpha radiation, and extremely energetic gamma (> ~20 MeV) can cause nuclear transmutation and induced radioactivity. The relevant mechanisms are neutron activation, alpha absorption, and photodisintegration. A large enough number of transmutations can change macroscopic properties and cause targets to become radioactive themselves, even after the original source is removed.
Ionization of molecules can lead to radiolysis (breaking chemical bonds), and formation of highly reactive free radicals. These free radicals may then react chemically with neighbouring materials even after the original radiation has stopped. (e.g., ozone cracking of polymers by ozone formed by ionization of air). Ionizing radiation can disrupt crystal lattices in metals, causing them to become amorphous, with consequent swelling, material creep, and embrittlement. Ionizing radiation can also accelerate existing chemical reactions such as polymerization and corrosion, by contributing to the activation energy required for the reaction. Optical materials darken under the effect of ionizing radiation.
High-intensity ionizing radiation in air can produce a visible ionized air glow of telltale bluish-purplish color. The glow can be observed, e.g., during criticality accidents, around mushroom clouds shortly after a nuclear explosion, or inside of a damaged nuclear reactor like during the Chernobyl disaster.
Monatomic fluids, e.g. molten sodium, have no chemical bonds to break and no crystal lattice to disturb, so they are immune to the chemical effects of ionizing radiation. Simple diatomic compounds with very negative enthalpy of formation, such as hydrogen fluoride will reform rapidly and spontaneously after ionization.
Ionization of materials temporarily increases their conductivity, potentially permitting damaging current levels. This is a particular hazard in semiconductor microelectronics employed in electronic equipment, with subsequent currents introducing operation errors or even permanently damaging the devices. Devices intended for high radiation environments such as the nuclear industry and extra atmospheric (space) applications may be made radiation hard to resist such effects through design, material selection, and fabrication methods.
Proton radiation found in space can also cause single-event upsets in digital circuits.
The electrical effects of ionizing radiation are exploited in gas-filled radiation detectors, e.g. the Geiger-Muller counter or the ion chamber.
In general, ionizing radiation is harmful and potentially lethal to living beings but some types have medical applications in radiation therapy for the treatment of cancer and thyrotoxicosis.
Most adverse health effects of exposure to ionizing radiation may be grouped in two general categories:
The most common impact is stochastic induction of cancer with a latent period of years or decades after exposure. For example, ionizing radiation is the sole cause of chronic myelogenous leukemia.[19] The mechanism by which this occurs is well understood, but quantitative models predicting the level of risk remain controversial. The most widely accepted model posits that the incidence of cancers due to ionizing radiation increases linearly with effective radiation dose at a rate of 5.5% per sievert.[20] If this linear model is correct, then natural background radiation is the most hazardous source of radiation to general public health, followed by medical imaging as a close second. Other stochastic effects of ionizing radiation are teratogenesis, cognitive decline, and heart disease.
The table below shows radiation and dose quantities in SI and non-SI units. The relationships of the ICRP dose quantities are shown in the accompanying diagram.
Ionizing radiation has many industrial, military, and medical uses. Its usefulness must be balanced with its hazards, a compromise that has shifted over time. For example, at one time, assistants in shoe shops used X-rays to check a child's shoe size, but this practice was halted when the risks of ionizing radiation were better understood.[21]
Neutron radiation is essential to the working of nuclear reactors and nuclear weapons. The penetrating power of x-ray, gamma, beta, and positron radiation is used for medical imaging, nondestructive testing, and a variety of industrial gauges. Radioactive tracers are used in medical and industrial applications, as well as biological and radiation chemistry. Alpha radiation is used in static eliminators and smoke detectors. The sterilizing effects of ionizing radiation are useful for cleaning medical instruments, food irradiation, and the sterile insect technique. Measurements of carbon-14, can be used to date the remains of long-dead organisms (such as wood that is thousands of years old).
Ionizing radiation is generated through nuclear reactions, nuclear decay, by very high temperature, or via acceleration of charged particles in electromagnetic fields. Natural sources include the sun, lightning and supernova explosions. Artificial sources include nuclear reactors, particle accelerators, and x-ray tubes.
The United Nations Scientific Committee on the Effects of Atomic Radiation (UNSCEAR) itemized types of human exposures.
The International Commission on Radiological Protection manages the International System of Radiological Protection, which sets recommended limits for dose uptake.
Background radiation comes from both natural and man-made sources.
The global average exposure of humans to ionizing radiation is about 3 mSv (0.3 rem) per year, 80% of which comes from nature. The remaining 20% results from exposure to man-made radiation sources, primarily from medical imaging. Average man-made exposure is much higher in developed countries, mostly due to CT scans and nuclear medicine.
Natural background radiation comes from five primary sources: cosmic radiation, solar radiation, external terrestrial sources, radiation in the human body, and radon.
The background rate for natural radiation varies considerably with location, being as low as 1.5 mSv/a (1.5 mSv per year) in some areas and over 100 mSv/a in others. The highest level of purely natural radiation recorded on the Earth's surface is 90 µGy/h (0.8 Gy/a) on a Brazilian black beach composed of monazite.[22] The highest background radiation in an inhabited area is found in Ramsar, primarily due to naturally radioactive limestone used as a building material. Some 2000 of the most exposed residents receive an average radiation dose of 10 mGy per year, (1 rad/yr) ten times more than the ICRP recommended limit for exposure to the public from artificial sources.[23] Record levels were found in a house where the effective radiation dose due to external radiation was 135 mSv/a, (13.5 rem/yr) and the committed dose from radon was 640 mSv/a (64.0 rem/yr).[24] This unique case is over 200 times higher than the world average background radiation. Despite the high levels of background radiation that the residents of Ramsar receive there is no compelling evidence that they experience a greater health risks. The ICRP recommendations are conservative limits and may represent an over representation of the actual health risk. Generally radiation safety organization recommend the most conservative limits assuming it is best to err on the side of caution. This level of caution is appropriate but should not be used to create fear about background radiation danger. Radiation danger from background radiation may be a serious threat but is more likely a small overall risk compared to all other factors in the environment.
The Earth, and all living things on it, are constantly bombarded by radiation from outside our solar system. This cosmic radiation consists of relativistic particles: positively charged nuclei (ions) from 1 amu protons (about 85% of it) to 26 amu iron nuclei and even beyond. (The high-atomic number particles are called HZE ions.) The energy of this radiation can far exceed that which humans can create, even in the largest particle accelerators (see ultra-high-energy cosmic ray). This radiation interacts in the atmosphere to create secondary radiation that rains down, including x-rays, muons, protons, antiprotons, alpha particles, pions, electrons, positrons, and neutrons.
The dose from cosmic radiation is largely from muons, neutrons, and electrons, with a dose rate that varies in different parts of the world and based largely on the geomagnetic field, altitude, and solar cycle. The cosmic-radiation dose rate on airplanes is so high that, according to the United Nations UNSCEAR 2000 Report (see links at bottom), airline flight crew workers receive more dose on average than any other worker, including those in nuclear power plants. Airline crews receive more cosmic rays if they routinely work flight routes that take them close to the North or South pole at high altitudes, where this type of radiation is maximal.
Cosmic rays also include high-energy gamma rays, which are far beyond the energies produced by solar or human sources.
Most materials on Earth contain some radioactive atoms, even if in small quantities. Most of the dose received from these sources is from gamma-ray emitters in building materials, or rocks and soil when outside. The major radionuclides of concern for terrestrial radiation are isotopes of potassium, uranium, and thorium. Each of these sources has been decreasing in activity since the formation of the Earth.
All earthly materials that are the building-blocks of life contain a radioactive component. As humans, plants, and animals consume food, air, and water, an inventory of radioisotopes builds up within the organism (see banana equivalent dose). Some radionuclides, like potassium-40, emit a high-energy gamma ray that can be measured by sensitive electronic radiation measurement systems. These internal radiation sources contribute to an individual's total radiation dose from natural background radiation.
An important source of natural radiation is radon gas, which seeps continuously from bedrock but can, because of its high density, accumulate in poorly ventilated houses.
Radon-222 is a gas produced by the decay of radium-226. Both are a part of the natural uranium decay chain. Uranium is found in soil throughout the world in varying concentrations. Among non-smokers, radon is the largest cause of lung cancer and, overall, the second-leading cause.[25]
There are three standard ways to limit exposure:
These can all be applied to natural and man-made sources. For man-made sources the use of Containment is a major tool in reducing dose uptake and is effectively a combination of shielding and isolation from the open environment. Radioactive materials are confined in the smallest possible space and kept out of the environment such as in a hot cell (for radiation) or glove box (for contamination). Radioactive isotopes for medical use, for example, are dispensed in closed handling facilities, usually gloveboxes, while nuclear reactors operate within closed systems with multiple barriers that keep the radioactive materials contained. Work rooms, hot cells and gloveboxes have slightly reduced air pressures to prevent escape of airborne material to the open environment.
In nuclear conflicts or civil nuclear releases civil defense measures can help reduce exposure of populations by reducing ingestion of isotopes and occupational exposure . One is the issue of potassium iodide (KI) tablets, which blocks the uptake of radioactive iodine (one of the major radioisotope products of nuclear fission) into the human thyroid gland.
Occupationally exposed individuals are controlled within the regulatory framework of the country they work in, and in accordance with any local nuclear licence constraints. These are usually based on the recommendations of the ICRP.
The International Commission on Radiological Protection recommends limiting artificial irradiation. For occupational exposure, the limit is 50 mSv in a single year with a maximum of 100 mSv in a consecutive five-year period.[20]
The radiation exposure of these individuals is carefully monitored with the use of dosimeters and other radiological protection instruments which will measure radioactive particulate concentrations, area gamma dose readings and radioactive contamination. A legal record of dose is kept.
Examples of activities where occupational exposure is a concern include:
Some human-made radiation sources affect the body through direct radiation, known as effective dose (radiation) while others take the form of radioactive contamination and irradiate the body from within. The latter is known as committed dose.
Medical procedures, such as diagnostic X-rays, nuclear medicine, and radiation therapy are by far the most significant source of human-made radiation exposure to the general public. Some of the major radionuclides used are I-131, Tc-99m, Co-60, Ir-192, and Cs-137. The public also is exposed to radiation from consumer products, such as tobacco (polonium-210), combustible fuels (gas, coal, etc.), televisions, luminous watches and dials (tritium), airport X-ray systems, smoke detectors (americium), electron tubes, and gas lantern mantles (thorium).
Of lesser magnitude, members of the public are exposed to radiation from the nuclear fuel cycle, which includes the entire sequence from processing uranium to the disposal of the spent fuel. The effects of such exposure have not been reliably measured due to the extremely low doses involved. Opponents use a cancer per dose model to assert that such activities cause several hundred cases of cancer per year, an application of the widely accepted Linear no-threshold model (LNT).
The International Commission on Radiological Protection recommends limiting artificial irradiation to the public to an average of 1 mSv (0.001 Sv) of effective dose per year, not including medical and occupational exposures.[20]
In a nuclear war, gamma rays from both the initial weapon explosion and fallout would be the sources of radiation exposure.
Massive particles are a concern for astronauts outside the earth's magnetic field who would receive solar particles from solar proton events (SPE) and galactic cosmic rays from cosmic sources. These high-energy charged nuclei are blocked by Earth's magnetic field but pose a major health concern for astronauts traveling to the moon and to any distant location beyond the earth orbit. Highly charged HZE ions in particular are known to be extremely damaging, although protons make up the vast majority of galactic cosmic rays. Evidence indicates past SPE radiation levels that would have been lethal for unprotected astronauts.[29]
Air travel exposes people on aircraft to increased radiation from space as compared to sea level, including cosmic rays and from solar flare events.[30][31] Software programs such as Epcard, CARI, SIEVERT, PCAIRE are attempts to simulate exposure by aircrews and passengers.[31] An example of a measured dose (not simulated dose) is 6 μSv per hour from London Heathrow to Tokyo Narita on a high-latitude polar route.[31] However, dosages can vary, such as during periods of high solar activity.[31] The United States FAA requires airlines to provide flight crew with information about cosmic radiation, and an International Commission on Radiological Protection recommendation for the general public is no more than 1 mSv per year.[31] In addition, many airlines do not allow pregnant flightcrew members, to comply with a European Directive.[31] The FAA has a recommended limit of 1 mSv total for a pregnancy, and no more than 0.5 mSv per month.[31] Information originally based on Fundamentals of Aerospace Medicine published in 2008.[31]
Hazardous levels of ionizing radiation are signified by the trefoil sign on a yellow background. These are usually posted at the boundary of a radiation controlled area or in any place where radiation levels are significantly above background due to human intervention.
The red ionizing radiation warning symbol (ISO 21482) was launched in 2007, and is intended for IAEA Category 1, 2 and 3 sources defined as dangerous sources capable of death or serious injury, including food irradiators, teletherapy machines for cancer treatment and industrial radiography units. The symbol is to be placed on the device housing the source, as a warning not to dismantle the device or to get any closer. It will not be visible under normal use, only if someone attempts to disassemble the device. The symbol will not be located on building access doors, transportation packages or containers.[32]
Ionizing radiation hazard symbol
2007 ISO radioactivity danger symbol intended for IAEA Category 1, 2 and 3 sources defined as dangerous sources capable of death or serious injury.[32]



Mutagen - Wikipedia
In genetics, a mutagen is a physical or chemical agent that changes the genetic material, usually DNA, of an organism and thus increases the frequency of mutations above the natural background level. As many mutations can cause cancer, mutagens are therefore also likely to be carcinogens, although not always necessarily so. All mutagens have characteristic mutational signatures with some chemicals becoming mutagenic through cellular processes. Not all mutations are caused by mutagens: so-called "spontaneous mutations" occur due to spontaneous hydrolysis, errors in DNA replication, repair and recombination.
The first mutagens to be identified were carcinogens, substances that were shown to be linked to cancer. Tumors were described more than 2,000 years before the discovery of chromosomes and DNA; in 500 B.C., the Greek physician Hippocrates named tumors resembling a crab karkinos (from which the word "cancer" is derived via Latin), meaning crab.[1]  In 1567, Swiss physician Paracelsus suggested that an unidentified substance in mined ore (identified as radon gas in modern times) caused a wasting disease in miners,[2] and in England, in 1761, John Hill made the first direct link of cancer to chemical substances by noting that excessive use of snuff may cause nasal cancer.[3] In 1775, Sir Percivall Pott wrote a paper on the high incidence of scrotal cancer in chimney sweeps, and suggested chimney soot as the cause of scrotal cancer.[4] In 1915, Yamagawa and Ichikawa showed that repeated application of coal tar to rabbit's ears produced malignant cancer.[5] Subsequently, in the 1930s the carcinogen component in coal tar was identified as a polyaromatic hydrocarbon (PAH), benzo[a]pyrene.[2][6] Polyaromatic hydrocarbons are also present in soot, which was suggested to be a causative agent of cancer over 150 years earlier.
The association of exposure to radiation and cancer had been observed as early as 1902, six years after the discovery of X-ray by Wilhelm Röntgen and radioactivity by Henri Becquerel.[7] Georgii Nadson and German Filippov were the first who created fungi mutants under ionizing radiation in 1925.[8][9] The mutagenic property of mutagens was first demonstrated in 1927, when Hermann Muller discovered that x-rays can cause genetic mutations in fruit flies, producing phenotypic mutants as well as observable changes to the chromosomes,[10][11] visible due to presence of enlarged 'polytene' chromosomes in fruit fly salivary glands.[12] His collaborator Edgar Altenburg also demonstrated the mutational effect of UV radiation in 1928.[13]  Muller went on to use x-rays to create Drosophila mutants that he used in his studies of genetics.[14] He also found that X-rays not only mutate genes in fruit flies,[10] but also have effects on the genetic makeup of humans.[15][better source needed]  Similar work by Lewis Stadler also showed the mutational effect of X-rays on barley in 1928,[16] and ultraviolet (UV) radiation on maize in 1936.[17]  The effect of sunlight had previously been noted in the nineteenth century where rural outdoor workers and sailors were found to be more prone to skin cancer.[18]
Chemical mutagens were not demonstrated to cause mutation until the 1940s, when Charlotte Auerbach and J. M. Robson found that mustard gas can cause mutations in fruit flies.[19] A large number of chemical mutagens have since been identified, especially after the development of the Ames test in the 1970s by Bruce Ames that screens for mutagens and allows for preliminary identification of carcinogens.[20][21]  Early studies by Ames showed around 90% of known carcinogens can be identified in Ames test as mutagenic (later studies however gave lower figures),[22][23][24] and ~80% of the mutagens identified through Ames test may also be carcinogens.[24][25]  Mutagens are not necessarily carcinogens, and vice versa. Sodium azide for example may be mutagenic (and highly toxic), but it has not been shown to be carcinogenic.[26]
Mutagens can cause changes to the DNA and are therefore genotoxic. They can affect the transcription and replication of the DNA, which in severe cases can lead to cell death.  The mutagen produces mutations in the DNA, and deleterious mutation can result in aberrant, impaired or loss of function for a particular gene, and accumulation of mutations may lead to cancer.  Mutagens may therefore be also carcinogens.  However, some mutagens exert their mutagenic effect through their metabolites, and therefore whether such mutagens actually become carcinogenic may be dependent on the metabolic processes of an organism, and a compound shown to be mutagenic in one organism may not necessarily be carcinogenic in another.[27]
Different mutagens act on the DNA differently.  Powerful mutagens may result in chromosomal instability,[28] causing chromosomal breakages and rearrangement of the chromosomes such as translocation, deletion, and inversion.  Such mutagens are called clastogens.
Mutagens may also modify the DNA sequence; the changes in nucleic acid sequences by mutations include substitution of nucleotide base-pairs and insertions and deletions of one or more nucleotides in DNA sequences. Although some of these mutations are lethal or cause serious disease, many have minor effects as they do not result in residue changes that have significant effect on the structure and function of the proteins.  Many mutations are silent mutations, causing no visible effects at all, either because they occur in non-coding or non-functional sequences, or they do not change the amino-acid sequence due to the redundancy of codons.
Some mutagens can cause aneuploidy and change the number of chromosomes in the cell. They are known as aneuploidogens.[29]
In Ames test, where the varying concentrations of the chemical are used in the test, the dose response curve obtained is nearly always linear, suggesting that there may be no threshold for mutagenesis.  Similar results are also obtained in studies with radiations, indicating that there may be no safe threshold for mutagens. However, the no-threshold model is disputed with some arguing for a dose rate dependent threshold for mutagenesis.[30][10]  Some have proposed that low level of some mutagens may stimulate the DNA repair processes and therefore may not necessarily be harmful. More recent approaches with sensitive analytical methods have shown that there may be non-linear or bilinear dose-responses for genotoxic effects, and that the activation of DNA repair pathways can prevent the occurrence of mutation arising from a low dose of mutagen.[31]
Mutagens may be of physical, chemical or biological origin.  They may act directly on the DNA, causing direct damage to the DNA, and most often result in replication error.  Some however may act on the replication mechanism and chromosomal partition.  Many mutagens are not mutagenic by themselves, but can form mutagenic metabolites through cellular processes, for example through the activity of the cytochrome P450 system and other oxygenases such as cyclooxygenase.[32] Such mutagens are called promutagens.
A large number of chemicals may interact directly with DNA.  However, many such as PAHs, aromatic amines, benzene are not necessarily mutagenic by themselves, but through metabolic processes in cells they produce mutagenic compounds.
Many metals, such as arsenic, cadmium, chromium, nickel and their compounds may be mutagenic, but they may act, however, via a number of different mechanisms.[34]  Arsenic, chromium, iron, and nickel may be associated with the production of ROS, and some of these may also alter the fidelity of DNA replication.  Nickel may also be linked to DNA hypermethylation and histone deacetylation, while some metals such as cobalt, arsenic, nickel and cadmium may also affect DNA repair processes such as DNA mismatch repair, and base and nucleotide excision repair.[35]
Antioxidants are an important group of anticarcinogenic compounds that may help remove ROS or potentially harmful chemicals.  These may be found naturally in fruits and vegetables.[38]  Examples of antioxidants are vitamin A and its carotenoid precursors, vitamin C, vitamin E, polyphenols, and various other compounds. β-Carotene is the red-orange colored compounds found in vegetables like carrots and tomatoes.  Vitamin C may prevent some cancers by inhibiting the formation of mutagenic N-nitroso compounds (nitrosamine). Flavonoids, such as EGCG in green tea, have also been shown to be effective antioxidants and may have anti-cancer properties.  Epidemiological studies indicate that a diet rich in fruits and vegetables is associated with lower incidence of some cancers and longer life expectancy,[39] however, the effectiveness of antioxidant supplements in cancer prevention in general is still the subject of some debate.[39][40]
Other chemicals may reduce mutagenesis or prevent cancer via other mechanisms, although for some the precise mechanism for their protective property may not be certain. Selenium, which is present as a micronutrient in vegetables, is a component of important antioxidant enzymes such as gluthathione peroxidase.  Many phytonutrients may counter the effect of mutagens; for example, sulforaphane in vegetables such as broccoli has been shown to be protective against prostate cancer.[41]  Others that may be effective against cancer include indole-3-carbinol from cruciferous vegetables and resveratrol from red wine.[42]
An effective precautionary measure an individual can undertake to protect themselves is by limiting exposure to mutagens such as UV radiations and tobacco smoke.  In Australia, where people with pale skin are often exposed to strong sunlight, melanoma is the most common cancer diagnosed in people aged 15–44 years.[43][44]
In 1981, human epidemiological analysis by Richard Doll and Richard Peto indicated that smoking caused 30% of cancers in the US.[45]  Diet is also thought to cause a significant number of cancer, and it has been estimated that around 32% of cancer deaths may be avoidable by modification to the diet.[46]  Mutagens identified in food include mycotoxins from food contaminated with fungal growths, such as aflatoxins which may be present in contaminated peanuts and corn; heterocyclic amines generated in meat when cooked at high temperature; PAHs in charred meat and smoked fish, as well as in oils, fats, bread, and cereal;[47] and nitrosamines generated from nitrites used as food preservatives in cured meat such as bacon (ascobate, which is added to cured meat, however, reduces nitrosamine formation).[38]  Overly-browned starchy food such as bread, biscuits and potatoes can generate acrylamide, a chemical shown to cause cancer in animal studies.[48][49] Excessive alcohol consumption has also been linked to cancer; the possible mechanisms for its carcinogenicity include formation of the possible mutagen acetaldehyde, and the induction of the cytochrome P450 system which is known to produce mutagenic compounds from promutagens.[50]
For certain mutagens, such as dangerous chemicals and radioactive materials, as well as infectious agents known to cause cancer, government legislations and regulatory bodies are necessary for their control.[51]
Many different systems for detecting mutagen have been developed.[52][53]  Animal systems may more accurately reflect the metabolism of human, however, they are expensive and time-consuming (may take around three years to complete), they are therefore not used as a first screen for mutagenicity or carcinogenicity.
Systems similar to Ames test have been developed in yeast. Saccharomyces cerevisiae is generally used.  These systems can check for forward and reverse mutations, as well as recombinant events.
Sex-Linked Recessive Lethal Test – Males from a strain with yellow bodies are used in this test. The gene for the yellow body lies on the X-chromosome.  The fruit flies are fed on a diet of test chemical, and progenies are separated by sex.  The surviving males are crossed with the females of the same generation, and if no males with yellow bodies are detected in the second generation, it would indicate a lethal mutation on the X-chromosome has occurred.
Plants such as Zea mays, Arabidopsis thaliana and Tradescantia have been used in various test assays for mutagenecity of chemicals.
Mammalian cell lines such as Chinese hamster V79 cells, Chinese hamster ovary (CHO) cells or mouse lymphoma cells may be used to test for mutagenesis.  Such systems include the HPRT assay for resistance to 8-azaguanine or 6-thioguanine, and ouabain-resistance (OUA) assay.
Rat primary hepatocytes may also be used to measure DNA repair following DNA damage.  Mutagens may stimulate unscheduled DNA synthesis that results in more stained nuclear material in cells following exposure to mutagens.
These systems check for large scale changes to the chromosomes and may be used with cell culture or in animal test.  The chromosomes are stained and observed for any changes.  Sister chromatid exchange is a symmetrical exchange of chromosome material between sister chromatids and may be correlated to the mutagenic or carcinogenic potential of a chemical.  In micronucleus Test, cells are examined for micronuclei, which are fragments or chromosomes left behind at anaphase, and is therefore a test for clastogenic agents that cause chromosome breakages.  Other tests may check for various chromosomal aberrations such as chromatid and chromosomal gaps and deletions, translocations, and ploidy.
Rodents are usually used in animal test.  The chemicals under
test are usually administered in the food and in the drinking water, but sometimes by dermal application, by gavage, or by inhalation, and carried out over the major part of the life span for rodents.  In tests that check for carcinogens, maximum tolerated dosage is first determined, then a range of doses are given to around 50 animals throughout the notional lifespan of the animal of two years.  After death the animals are examined for sign of tumours. Differences in metabolism between rat and human however means that human may not respond in exactly the same way to mutagen, and dosages that produce tumours on the animal test may also be unreasonably high for a human, i.e. the equivalent amount required to produce tumours in human may far exceed what a person might encounter in real life.
Mice with recessive mutations for a visible phenotype may also be used to check for mutagens.  Females with recessive mutation crossed with wild-type males would yield the same phenotype as the wild-type, and any observable change to the phenotype would indicate that a mutation induced by the mutagen has occurred.
Mice may also be used for dominant lethal assays where early embryonic deaths are monitored.  Male mice are treated with chemicals under test, mated with females, and the females are then sacrificed before parturition and early fetal deaths are counted in the uterine horns.
Transgenic mouse assay using a mouse strain infected with a viral shuttle vector is another method for testing mutagens.  Animals are first treated with suspected mutagen, the mouse DNA is then isolated and the phage segment recovered and used to infect E. coli.  Using similar method as the blue-white screen, the plaque formed with DNA containing mutation are white, while those without are blue.
Many mutagens are highly toxic to proliferating cells, and they are often used to destroy cancer cells.  Alkylating agents such as cyclophosphamide and cisplatin, as well as intercalating agent such as daunorubicin and doxorubicin may be used in chemotherapy. However, due to their effect on other cells which are also rapidly dividing, they may have side effects such as hair loss and nausea. Research on better targeted therapies may reduce such side-effects.  Ionizing radiations are used in radiation therapy.
In science fiction, mutagens are often represented as substances that are capable of completely changing the form of the recipient or gaining them superpower.  Powerful radiations are the agents of mutation for the superheroes in Marvel Comics's Fantastic Four, Daredevil, and Hulk, while in the Teenage Mutant Ninja Turtles franchise the mutagen is chemical agent also called "ooze", and for Inhumans the mutagen is the Terrigen Mist.  Mutagens are also featured in television series, computer and video games, such as the Cyberia, The Witcher, Metroid Prime: Trilogy, Resistance: Fall of Man, Resident Evil, Infamous, Command & Conquer, Gears of War 3, BioShock, and Fallout.



Ultraviolet - Wikipedia

Ultraviolet  (UV) is electromagnetic radiation with a wavelength from 10 nm  to 400 nm, shorter than that of visible light but longer than X-rays. UV radiation is present in sunlight constituting about 10% of the total light output of the Sun. It is also produced by electric arcs and specialized lights, such as mercury-vapor lamps, tanning lamps, and black lights. Although long-wavelength ultraviolet is not considered an ionizing radiation because its photons lack the energy to ionize atoms, it can cause chemical reactions and causes many substances to glow or fluoresce. Consequently, the chemical and biological effects of UV are greater than simple heating effects, and many practical applications of UV radiation derive from its interactions with organic molecules.
Suntan and sunburn are familiar effects of over-exposure of the skin to UV, along with higher risk of skin cancer. Living things on dry land would be severely damaged by ultraviolet radiation from the Sun if most of it were not filtered out by the Earth's atmosphere.[1] More-energetic, shorter-wavelength "extreme" UV below 121 nm ionizes air so strongly that it is absorbed before it reaches the ground.[2] 
Ultraviolet is also responsible for the formation of bone-strengthening vitamin D in most land vertebrates, including humans (specifically, UVB).[3] The UV spectrum thus has effects both beneficial and harmful to human health.
Ultraviolet rays are invisible to all humans, although insects, birds, and some mammals can see near-UV.
Ultraviolet rays are invisible to most humans. The lens of the human eye blocks most radiation in the wavelength range of 300–400 nm; shorter wavelengths are blocked by the cornea.[4]  Humans lack color receptor adaptations for ultraviolet rays. Nevertheless, the photoreceptors of the retina are sensitive to near-UV, and people lacking a lens (a condition known as aphakia) perceive near-UV as whitish-blue or whitish-violet.[5] Under some conditions, children and young adults can see ultraviolet down to wavelengths of about 310 nm.[6][7] Near-UV radiation is visible to insects, some mammals, and birds. Small birds have a fourth color receptor for ultraviolet rays; this gives birds "true" UV vision.[8][9]
"Ultraviolet" means "beyond violet" (from Latin ultra, "beyond"), violet being the color of the highest frequencies of visible light. Ultraviolet has a higher frequency than violet light.
UV radiation was discovered in 1801 when the German physicist Johann Wilhelm Ritter observed that invisible rays just beyond the violet end of the visible spectrum darkened silver chloride-soaked paper more quickly than violet light itself. He called them "oxidizing rays" to emphasize chemical reactivity and to distinguish them from "heat rays", discovered the previous year at the other end of the visible spectrum. The simpler term "chemical rays" was adopted shortly thereafter, and it remained popular throughout the 19th century, although there were those who held that these were an entirely different sort of radiation from light (notably John William Draper, who named them "tithonic rays"[10][11]). The terms chemical rays and heat rays were eventually dropped in favor of ultraviolet and infrared radiation, respectively.[12][13]
In 1878 the sterilizing effect of short-wavelength light by killing bacteria was discovered. By 1903 it was known the most effective wavelengths were around 250 nm. In 1960, the effect of ultraviolet radiation on DNA was established.[14]
The discovery of the ultraviolet radiation with wavelengths below 200 nm, named "vacuum ultraviolet" because it is strongly absorbed by the air, was made in 1893 by the German physicist Victor Schumann.[15]
The electromagnetic spectrum of ultraviolet radiation (UVR), defined most broadly as 10–400 nanometers, can be subdivided into a number of ranges recommended by the ISO standard ISO-21348:[16]
A variety of solid-state and vacuum devices have been explored for use in different parts of the UV spectrum. Many approaches seek to adapt visible light-sensing devices, but these can suffer from unwanted response to visible light and various instabilities. Ultraviolet can be detected by suitable photodiodes and photocathodes, which can be tailored to be sensitive to different parts of the UV spectrum. Sensitive ultraviolet photomultipliers are available. Spectrometers and radiometers are made for measurement of UV radiation. Silicon detectors are used across the spectrum.[17]
Vacuum UV, or VUV, wavelengths (shorter than 200 nm) are strongly absorbed by molecular oxygen in the air, though the longer wavelengths of about 150–200 nm can propagate through nitrogen. Scientific instruments can therefore utilize this spectral range by operating in an oxygen-free atmosphere (commonly pure nitrogen), without the need for costly vacuum chambers. Significant examples include 193 nm photolithography equipment (for semiconductor manufacturing) and circular dichroism spectrometers.
Technology for VUV instrumentation was largely driven by solar astronomy for many decades. While optics can be used to remove unwanted visible light that contaminates the VUV, in general, detectors can be limited by their response to non-VUV radiation, and the development of "solar-blind" devices has been an important area of research. Wide-gap solid-state devices or vacuum devices with high-cutoff photocathodes can be attractive compared to silicon diodes.
Extreme UV (EUV or sometimes XUV) is characterized by a transition in the physics of interaction with matter. Wavelengths longer than about 30 nm interact mainly with the outer valence electrons of atoms, while wavelengths shorter than that interact mainly with inner-shell electrons and nuclei. The long end of the EUV spectrum is set by a prominent He+ spectral line at 30.4 nm. EUV is strongly absorbed by most known materials, but it is possible to synthesize multilayer optics that reflect up to about 50 percent of EUV radiation at normal incidence. This technology was pioneered by the NIXT and MSSTA sounding rockets in the 1990s, and it has been used to make telescopes for solar imaging. See also the Extreme Ultraviolet Explorer (EUVE) satellite.
Some sources use the distinction of "hard UV" and "soft UV" - in the case of astrophysics the boundary may be at the Lyman limit ie wavelength 91.2 nm, with "hard UV" being more energetic.[18] The same terms may also used in other fields, such as cosmetology, optoelectronic, etc - the numerical value of the boundary between hard/soft even within similar scientific fields do not necessarily coincide; for example one applied physics publication used a boundary of 190 nm between hard and soft UV regions.[19]
Very  hot objects emit UV radiation (see black-body radiation). The Sun emits ultraviolet radiation at all wavelengths, including the extreme ultraviolet where it crosses into X-rays at 10 nm. Extremely hot stars emit proportionally more UV radiation than the Sun. Sunlight in space at the top of Earth's atmosphere (see solar constant) is composed of about 50% infrared light, 40% visible light, and 10% ultraviolet light, for a total intensity of about 1400 W/m2 in vacuum.[20]
However, at ground level sunlight is 44% visible light, 3% ultraviolet (with the Sun at its zenith), and the remainder infrared.[21][22] Thus, the atmosphere blocks about 77% of the Sun's UV, almost entirely in the shorter UV wavelengths, when the Sun is highest in the sky (zenith). Of the ultraviolet radiation that reaches the Earth's surface, more than 95% is the longer wavelengths of UVA, with the small remainder UVB. There is essentially no UVC.[23] The fraction of UVB which remains in UV radiation after passing through the atmosphere is heavily dependent on cloud cover and atmospheric conditions. Thick clouds may block up to 90% of UVB radiation,[24] but in "partly cloudy" days, patches of blue sky showing between clouds are also sources of (scattered) UVA and UVB, which are produced by Rayleigh scattering in the same way as the visible blue light from those parts of the sky. UV-B also plays a major role in plant development as it affects most of the plant hormones.[25]
The shorter bands of UVC, as well as even more-energetic UV radiation produced by the Sun, are absorbed by oxygen and generate the ozone in the ozone layer when single oxygen atoms produced by UV photolysis of dioxygen react with more dioxygen. The ozone layer is especially important in blocking most UVB and the remaining part of UVC not already blocked by ordinary oxygen in air.
Ultraviolet absorbers are molecules used in organic materials (polymers, paints, etc.) to absorb UV radiation to reduce the UV degradation (photo-oxidation) of a material. The absorbers can themselves degrade over time, so monitoring of absorber levels in weathered materials is necessary.
In sunscreen, ingredients that absorb UVA/UVB rays, such as avobenzone, oxybenzone[26] and octyl methoxycinnamate, are organic chemical absorbers or "blockers". They are contrasted with inorganic absorbers/"blockers" of UV radiation such as carbon black, titanium dioxide and zinc oxide.
For clothing, the Ultraviolet Protection Factor (UPF) represents the ratio of sunburn-causing UV without and with the protection of the fabric, similar to SPF (Sun Protection Factor) ratings for sunscreen. Standard summer fabrics have UPF of approximately 6, which means that about 20% of UV will pass through.
Suspended nanoparticles in stained glass prevent UV rays from causing chemical reactions that change image colors. A set of stained glass color reference chips is planned to be used to calibrate the color cameras for the 2019 ESA Mars rover mission, since they will remain unfaded by the high level of UV present at the surface of Mars.[27]
Common soda–lime glass is partially transparent to UVA but is opaque to shorter wavelengths, whereas fused quartz glass, depending on quality, can be transparent even to vacuum UV wavelengths. Ordinary window glass passes about 90% of the light above 350 nm, but blocks over 90% of the light below 300 nm.[28][29][30]
Wood's glass is a nickel-bearing form of glass with a deep blue-purple color that blocks most visible light and passes ultraviolet.
A black light lamp emits long-wave UVA radiation and little visible light. Fluorescent black light lamps work similarly to other fluorescent lamps, but use a phosphor on the inner tube surface which emits UVA radiation instead of visible light. Some lamps use a deep-bluish-purple Wood's glass optical filter that blocks almost all visible light with wavelengths longer than 400 nanometres.[31] Others use plain glass instead of the more expensive Wood's glass, so they appear light-blue to the eye when operating. A black light may also be formed, very inefficiently, by using a layer of Wood's glass in the envelope for an incandescent bulb. Though cheaper than fluorescent UV lamps, only 0.1% of the input power is emitted as usable ultraviolet radiation. Mercury-vapor black lights in ratings up to 1 kW with UV-emitting phosphor and an envelope of Wood's glass are used for theatrical and concert displays. Black lights are used in applications in which extraneous visible light must be minimized; mainly to observe fluorescence, the colored glow that many substances give off when exposed to UV light.   UVA/UVB emitting bulbs are also sold for other special purposes, such as tanning lamps and reptile-keeping.
Shortwave UV lamps are made using a lamp tube with no phosphor coating composed of fused quartz, since ordinary glass absorbs UVC. These lamps emit ultraviolet light with two peaks in the UVC band at 253.7 nm and 185 nm due to the mercury within the lamp, as well as some visible light. From 85% to 90% of the UV produced by these lamps is at 253.7 nm, whereas only 5–10% is at 185 nm.[citation needed] The fused quartz glass tube passes the 253 nm radiation but blocks the 185 nm wavelength. Such tubes have two or three times the UVC power of a regular fluorescent lamp tube. These low-pressure lamps have a typical efficiency of approximately 30–40%, meaning that for every 100 watts of electricity consumed by the lamp, they will produce approximately 30–40 watts of total UV output. These "germicidal" lamps are used extensively for disinfection of surfaces in laboratories and food-processing industries, and for disinfecting water supplies.
Incandescent lamps have been used as ultraviolet sources with a filter coating on the bulb which absorbs most visible light.  Halogen lamps with fused quartz envelopes are used as inexpensive UV light sources in the near UV range, from 400 to 300 nm, in some scientific instruments.  Due to its black-body spectrum a filament light bulb is a very inefficient ultraviolet source, emitting only a fraction of a percent of its energy as UV.
Specialized UV gas-discharge lamps containing different gases produce UV radiation at particular spectral lines for scientific purposes. Argon and deuterium arc lamps are often used as stable sources, either windowless or with various windows such as magnesium fluoride.[32] These are often the emitting sources in UV spectroscopy equipment for chemical analysis.
Other UV sources with more continuous emission spectra include xenon arc lamps (commonly used as sunlight simulators), deuterium arc lamps, mercury-xenon arc lamps, and metal-halide arc lamps.
The excimer lamp, a UV source developed within the last two decades, is seeing increasing use in scientific fields. It has the advantages of high-intensity, high efficiency, and operation at a variety of wavelength bands into the vacuum ultraviolet.
Light-emitting diodes (LEDs) can be manufactured to emit radiation in the ultraviolet range. LED efficiency at 365 nm is about 5–8%, whereas efficiency at 395 nm is closer to 20%, and power outputs at these longer UV wavelengths are also better. Such LED arrays are beginning to be used for UV curing applications, and are already successful in digital print applications and inert UV curing environments. Power densities approaching 3 W/cm2 (30 kW/m2) are now possible, and this, coupled with recent developments by photoinitiator and resin formulators, makes the expansion of LED-cured UV materials likely.
UVC LEDs are beginning to be used in disinfection[33]  and as line sources to replace deuterium lamps in liquid chromatography instruments.[34]
Gas lasers, laser diodes and solid-state lasers can be manufactured to emit ultraviolet rays, and lasers are available which cover the entire UV range. The nitrogen gas laser uses electronic excitation of nitrogen molecules to emit a beam that is mostly UV. The strongest ultraviolet lines are at 337.1 nm and 357.6.6 nm, wavelength. Another type of high power gas laser is the excimer laser. They are widely used lasers emitting in ultraviolet and vacuum ultraviolet wavelength ranges. Presently, UV argon-fluoride (ArF) excimer lasers operating at 193 nm are routinely used in integrated circuit production by photolithography. The current wavelength limit of production of coherent UV is about 126 nm, characteristic of the Ar2* excimer laser.
Direct UV-emitting laser diodes are available at 375 nm.[35] UV diode lasers have been demonstrated using Ce:LiSAF crystals (cerium-doped lithium strontium aluminum fluoride), a process developed in the 1990s at Lawrence Livermore National Laboratory.[36] Wavelengths shorter than 325 nm are commercially generated in diode-pumped solid-state lasers. Ultraviolet lasers can also be made by applying frequency conversion to lower-frequency lasers.
Ultraviolet lasers have applications in industry (laser engraving), medicine (dermatology, and keratectomy), chemistry (MALDI), free air secure communications, computing (optical storage) and manufacture of integrated circuits.
The vacuum ultraviolet (VUV) band (100–200 nm) can be generated by non-linear 4 wave mixing in gases by sum or difference frequency mixing of 2 or more longer wavelength lasers. The generation is generally done in gasses (e.g. krypton, hydrogen which are two-photon resonant near 193 nm)[37] or metal vapors (e.g. magnesium). By making one of the lasers tunable, the VUV can be tuned. If one of the lasers is resonant with a transition in the gas or vapor then the VUV production is intensified. However, resonances also generate wavelength dispersion, and thus the phase matching can limit the tunable range of the 4 wave mixing. Difference frequency mixing (lambda1 + lambda2 − lambda3) has an advantage over sum frequency mixing because the phase matching can provide greater tuning.[37] In particular, difference frequency mixing two photons of an ArF (193 nm) excimer laser with a tunable visible or near IR laser in hydrogen or krypton provides resonantly enhanced tunable VUV covering from 100 nm to 200 nm.[37] Practically, the lack of suitable gas/vapor cell window materials above the lithium fluoride cut-off wavelength limit the tuning range to longer than about 110 nm. Tunable VUV wavelengths down to 75 nm was achieved using window-free configurations.[38]
Lasers have been used to indirectly generate non-coherent extreme UV (EUV) radiation at 13.5 nm for extreme ultraviolet lithography. The EUV is not emitted by the laser, but rather by electron transitions in an extremely hot tin or xenon plasma, which is excited by an excimer laser.[39] This technique does not require a synchrotron, yet can produce UV at the edge of the X-ray spectrum. Synchrotron light sources can also produce all wavelengths of UV, including those at the boundary of the UV and X-ray spectra at 10 nm.
The impact of ultraviolet radiation on human health has implications for the risks and benefits of sun exposure and is also implicated in issues such as fluorescent lamps and health. Getting too much sun exposure can be harmful, but in moderation, sun exposure is beneficial.[40]
There is no doubt that a little sunlight is good for you! But 5 to 15 minutes of casual sun exposure of hands, face and arms two to three times a week during the summer months is sufficient to keep your vitamin D levels high.
UV light causes the body to produce vitamin D (specifically, UVB), which is essential for life. The human body needs some UV radiation in order for one to maintain adequate vitamin D levels; however, excess exposure produces harmful effects that typically outweigh the benefits.[41]
Vitamin D promotes the creation of serotonin. The production of serotonin is in direct proportion to the degree of bright sunlight the body receives.[42]  Serotonin is thought to provide sensations of happiness, well being and serenity to human beings.[43]
UV rays also treat certain skin conditions. Modern phototherapy has been used to successfully treat psoriasis, eczema, jaundice, vitiligo, atopic dermatitis, and localized scleroderma.[44][45] In addition, UV light, in particular UVB radiation, has been shown to induce cell cycle arrest in keratinocytes, the most common type of skin cell.[46] As such, sunlight therapy can be a candidate for treatment of conditions such as psoriasis and exfoliative cheilitis, conditions in which skin cells divide more rapidly than usual or necessary.[47]
In humans, excessive exposure to UV radiation can result in acute and chronic harmful effects on the eye's dioptric system and retina. The risk is elevated at high altitudes and people living in high latitude countries where snow covers the ground right into early summer and sun positions even at zenith are low, are particularly at risk.[48]  Skin, the circadian and immune systems can also be affected.[49]
The differential effects of various wavelengths of light on the human cornea and skin are sometimes called the "erythemal action spectrum.".[50] The action spectrum shows that UVA does not cause immediate reaction, but rather UV begins to cause photokeratitis and skin redness (with Caucasians more sensitive) at wavelengths starting near the beginning of the UVB band at 315 nm, and rapidly increasing to 300 nm. The skin and eyes are most sensitive to damage by UV at 265–275 nm, which is in the lower UVC band. At still shorter wavelengths of UV, damage continues to happen, but the overt effects are not as great with so little penetrating the atmosphere. The WHO-standard ultraviolet index is a widely publicized measurement of total strength of UV wavelengths that cause sunburn on human skin, by weighting UV exposure for action spectrum effects at a given time and location. This standard shows that most sunburn happens due to UV at wavelengths near the boundary of the UVA and UVB bands.
Overexposure to UVB radiation not only can cause sunburn but also some forms of skin cancer. However, the degree of redness and eye irritation (which are largely not caused by UVA) do not predict the long-term effects of UV, although they do mirror the direct damage of DNA by ultraviolet.[51]
All bands of UV radiation damage collagen fibers and accelerate aging of the skin. Both UVA and UVB destroy vitamin A in skin, which may cause further damage.[52]
UVB radiation can cause direct DNA damage.[53] This cancer connection is one reason for concern about ozone depletion and the ozone hole.
The most deadly form of skin cancer, malignant melanoma, is mostly caused by DNA damage independent from UVA radiation. This can be seen from the absence of a direct UV signature mutation in 92% of all melanoma.[54] Occasional overexposure and sunburn are probably greater risk factors for melanoma than long-term moderate exposure.[55] UVC is the highest-energy, most-dangerous type of ultraviolet radiation, and causes adverse effects that can variously be mutagenic or carcinogenic.[56]
In the past, UVA was considered not harmful or less harmful than UVB, but today it is known to contribute to skin cancer via indirect DNA damage (free radicals such as reactive oxygen species). UVA can generate highly reactive chemical intermediates, such as hydroxyl and oxygen radicals, which in turn can damage DNA. The DNA damage caused indirectly to skin by UVA consists mostly of single-strand breaks in DNA, while the damage caused by UVB includes direct formation of thymine dimers or other pyrimidine dimers and double-strand DNA breakage.[57] UVA is immunosuppressive for the entire body (accounting for a large part of the immunosuppressive effects of sunlight exposure), and is mutagenic for basal cell keratinocytes in skin.[58]
UVB photons can cause direct DNA damage. UVB radiation excites DNA molecules in skin cells, causing aberrant covalent bonds to form between adjacent pyrimidine bases, producing a dimer. Most UV-induced pyrimidine dimers in DNA are removed by the process known as nucleotide excision repair that employs about 30 different proteins.[53] Those pyrimidine dimers that escape this repair process can induce a form of programmed cell death (apoptosis) or can cause DNA replication errors leading to mutation.
As a defense against UV radiation, the amount of the brown pigment melanin in the skin increases when exposed to moderate (depending on skin type) levels of radiation; this is commonly known as a sun tan. The purpose of melanin is to absorb UV radiation and dissipate the energy as harmless heat, protecting the skin against both direct and indirect DNA damage from the UV. UVA gives a quick tan that lasts for days by oxidizing melanin that was already present and triggers the release of the melanin from melanocytes. UVB yields a tan that takes roughly 2 days to develop because it stimulates the body to produce more melanin.
Medical organizations recommend that patients protect themselves from UV radiation by using sunscreen. Five sunscreen ingredients have been shown to protect mice against skin tumors. However, some sunscreen chemicals produce potentially harmful substances if they are illuminated while in contact with living cells.[59][60] The amount of sunscreen that penetrates into the lower layers of the skin may be large enough to cause damage.[61]
Sunscreen reduces the direct DNA damage that causes sunburn, by blocking UVB, and the usual SPF rating indicates how effectively this radiation is blocked. SPF is, therefore, also called UVB-PF, for "UVB protection factor".[62] This rating, however, offers no data about important protection against UVA,[63] which does not primarily cause sunburn but is still harmful, since it causes indirect DNA damage and is also considered carcinogenic. Several studies suggest that the absence of UVA filters may be the cause of the higher incidence of melanoma found in sunscreen users compared to non-users.[64][65][66][67][68] Some sunscreen lotions now contain compounds including titanium dioxide, zinc oxide and avobenzone which helps protect against UVA rays.
The photochemical properties of melanin make it an excellent photoprotectant. However, sunscreen chemicals cannot dissipate the energy of the excited state as efficiently as melanin and therefore, if sunscreen ingredients penetrate into the lower layers of the skin, the amount of reactive oxygen species may be increased.[69][59][60][70] The amount of sunscreen that penetrates through the stratum corneum may or may not be large enough to cause damage.
In an experiment by Hanson et al. that was published in 2006, the amount of harmful reactive oxygen species (ROS) was measured in untreated and in sunscreen treated skin. In the first 20 minutes, the film of sunscreen had a protective effect and the number of ROS species was smaller. After 60 minutes, however, the amount of absorbed sunscreen was so high that the amount of ROS was higher in the sunscreen-treated skin than in the untreated skin.[69] The study indicates that sunscreen must be reapplied within 2 hours in order to prevent UV light from penetrating to sunscreen-infused live skin cells.[69]
Ultraviolet radiation can aggravate several skin conditions and diseases, including[71] systemic lupus erythematosus, Sjögren's syndrome, Sinear Usher syndrome, rosacea, dermatomyositis, Darier's disease, and Kindler–Weary syndrome.
The eye is most sensitive to damage by UV in the lower UVC band at 265–275 nm. Radiation of this wavelength is almost absent from sunlight but is found in welder's arc lights and other artificial sources. Exposure to these can cause "welder's flash" or "arc eye" (photokeratitis) and can lead to cataracts, pterygium and pinguecula formation. To a lesser extent, UVB in sunlight from 310–280 nm also causes photokeratitis ("snow blindness"), and the cornea, the lens, and the retina can be damaged.[72]
Protective eyewear is beneficial to those exposed to ultraviolet radiation. Since light can reach the eyes from the sides, full-coverage eye protection is usually warranted if there is an increased risk of exposure, as in high-altitude mountaineering. Mountaineers are exposed to higher-than-ordinary levels of UV radiation, both because there is less atmospheric filtering and because of reflection from snow and ice.[73][74]
Ordinary, untreated eyeglasses give some protection. Most plastic lenses give more protection than glass lenses, because, as noted above, glass is transparent to UVA and the common acrylic plastic used for lenses is less so. Some plastic lens materials, such as polycarbonate, inherently block most UV.[75]
UV degradation is one form of polymer degradation that affects plastics exposed to sunlight. The problem appears as discoloration or fading, cracking, loss of strength or disintegration. The effects of attack increase with exposure time and sunlight intensity. The addition of UV absorbers inhibits the effect.
Sensitive polymers include thermoplastics and speciality fibers like aramids. UV absorption leads to chain degradation and loss of strength at sensitive points in the chain structure. Aramid rope must be shielded with a sheath of thermoplastic if it is to retain its strength.
Many pigments and dyes absorb UV and change colour, so paintings and textiles may need extra protection both from sunlight and fluorescent bulbs, two common sources of UV radiation. Window glass absorbs some harmful UV, but valuable artifacts need extra shielding. Many museums place black curtains over watercolour paintings and ancient textiles, for example. Since watercolours can have very low pigment levels, they need extra protection from UV. Various forms of picture framing glass, including acrylics (plexiglass), laminates, and coatings, offer different degrees of UV (and visible light) protection.
Because of its ability to cause chemical reactions and excite fluorescence in materials, ultraviolet radiation has a number of applications. The following table[76] gives some uses of specific wavelength bands in the UV spectrum
Photographic film responds to ultraviolet radiation but the glass lenses of cameras usually block radiation shorter than 350 nm. Slightly yellow UV-blocking filters are often used for outdoor photography to prevent unwanted bluing and overexposure by UV rays. For photography in the near UV, special filters may be used. Photography with wavelengths shorter than 350 nm requires special quartz lenses which do not absorb the radiation.
Digital cameras sensors may have internal filters that block UV to improve color rendition accuracy. Sometimes these internal filters can be removed, or they may be absent, and an external visible-light filter prepares the camera for near-UV photography. A few cameras are designed for use in the UV.
Photography by reflected ultraviolet radiation is useful for medical, scientific, and forensic investigations, in applications as widespread as detecting bruising of skin, alterations of documents, or restoration work on paintings. Photography of the fluorescence produced by ultraviolet illumination uses visible wavelengths of light.
In ultraviolet astronomy, measurements are used to discern the chemical composition of the interstellar medium, and the temperature and composition of stars. Because the ozone layer blocks many UV frequencies from reaching telescopes on the surface of the Earth, most UV observations are made from space.
Corona discharge on electrical apparatus can be detected by its ultraviolet emissions. Corona causes degradation of electrical insulation and emission of ozone and nitrogen oxide.[78]
EPROMs (Erasable Programmable Read-Only Memory) are erased by exposure to UV radiation. These modules have a transparent (quartz) window on the top of the chip that allows the UV radiation in.
Colorless fluorescent dyes that emit blue light under UV are added as optical brighteners to paper and fabrics. The blue light emitted by these agents counteracts yellow tints that may be present and causes the colors and whites to appear whiter or more brightly colored.
UV fluorescent dyes that glow in the primary colors are used in paints, papers, and textiles either to enhance color under daylight illumination or to provide special effects when lit with UV lamps. Blacklight paints that contain dyes that glow under UV are used in a number of art and aesthetic applications.
To help prevent counterfeiting of currency, or forgery of important documents such as driver's licenses and passports, the paper may include a UV watermark or fluorescent multicolor fibers that are visible under ultraviolet light. Postage stamps are tagged with a phosphor that glows under UV rays to permit automatic detection of the stamp and facing of the letter.
UV fluorescent dyes are used in many applications (for example, biochemistry and forensics). Some brands of pepper spray will leave an invisible chemical (UV dye) that is not easily washed off on a pepper-sprayed attacker, which would help police identify the attacker later.[79]
In some types of nondestructive testing UV stimulates fluorescent dyes to highlight defects in a broad range of materials. These dyes may be carried into surface-breaking defects by capillary action (liquid penetrant inspection) or they may be bound to ferrite particles caught in magnetic leakage fields in ferrous materials (magnetic particle inspection).
UV is an investigative tool at the crime scene helpful in locating and identifying bodily fluids such as semen, blood, and saliva.[80] For example, ejaculated fluids or saliva can be detected by high-power UV sources, irrespective of the structure or colour of the surface the fluid is deposited upon.[81]
UV-Vis microspectroscopy is also used to analyze trace evidence, such as textile fibers and paint chips, as well as questioned documents.
Other applications include the authentication of various collectibles and art, and detecting counterfeit currency. Even materials not specially marked with UV sensitive dyes may have distinctive fluorescence under UV exposure or may fluoresce differently under short-wave versus long-wave ultraviolet.
Using multi-spectral imaging it is possible to read illegible papyrus, such as the burned papyri of the Villa of the Papyri or of Oxyrhynchus, or the Archimedes palimpsest. The technique involves taking pictures of the illegible document using different filters in the infrared or ultraviolet range, finely tuned to capture certain wavelengths of light. Thus, the optimum spectral portion can be found for distinguishing ink from paper on the papyrus surface.
Simple NUV sources can be used to highlight faded iron-based ink on vellum.[82]
Ultraviolet aids in the detection of organic material deposits that remain on surfaces where periodic cleaning and sanitizing may not have been properly accomplished. It is used in the hotel industry, manufacturing, and other industries where levels of cleanliness or contamination are inspected.[83][84][85][86]
Perennial news feature for many television news organizations involves an investigative reporter's using a similar device to reveal unsanitary conditions in hotels, public toilets, hand rails, and such.[87][88]
UV/VIS spectroscopy is widely used as a technique in chemistry to analyze chemical structure, the most notable one being conjugated systems. UV radiation is often used to excite a given sample where the fluorescent emission is measured with a spectrofluorometer. In biological research, UV radiation is used for quantification of nucleic acids or proteins.
Ultraviolet lamps are also used in analyzing minerals and gems.
In pollution control applications, ultraviolet analyzers are used to detect emissions of nitrogen oxides, sulfur compounds, mercury, and ammonia, for example in the flue gas of fossil-fired power plants.[89] Ultraviolet radiation can detect thin sheens of spilled oil on water, either by the high reflectivity of oil films at UV wavelengths, fluorescence of compounds in oil or by absorbing of UV created by Raman scattering in water.[90]
In general, ultraviolet detectors use either a solid-state device, such as one based on silicon carbide or aluminium nitride, or a gas-filled tube as the sensing element. UV detectors that are sensitive to UV in any part of the spectrum respond to irradiation by sunlight and artificial light. A burning hydrogen flame, for instance, radiates strongly in the 185- to 260-nanometer range and only very weakly in the IR region, whereas a coal fire emits very weakly in the UV band yet very strongly at IR wavelengths; thus, a fire detector that operates using both UV and IR detectors is more reliable than one with a UV detector alone. Virtually all fires emit some radiation in the UVC band, whereas the Sun's radiation at this band is absorbed by the Earth's atmosphere. The result is that the UV detector is "solar blind", meaning it will not cause an alarm in response to radiation from the Sun, so it can easily be used both indoors and outdoors.
UV detectors are sensitive to most fires, including hydrocarbons, metals, sulfur, hydrogen, hydrazine, and ammonia. Arc welding, electrical arcs, lightning, X-rays used in nondestructive metal testing equipment (though this is highly unlikely), and radioactive materials can produce levels that will activate a UV detection system. The presence of UV-absorbing gases and vapors will attenuate the UV radiation from a fire, adversely affecting the ability of the detector to detect flames. Likewise, the presence of an oil mist in the air or an oil film on the detector window will have the same effect.
Ultraviolet radiation is used for very fine resolution photolithography, a procedure wherein a chemical called a photoresist is exposed to UV radiation that has passed through a mask. The exposure causes chemical reactions to occur in the photoresist. After removal of unwanted photoresist, a pattern determined by the mask remains on the sample. Steps may then be taken to "etch" away, deposit on or otherwise modify areas of the sample where no photoresist remains.
Photolithography is used in the manufacture of semiconductors, integrated circuit components,[91] and printed circuit boards. Photolithography processes used to fabricate electronic integrated circuits presently use 193 nm UV and are experimentally using 13.5 nm UV for extreme ultraviolet lithography.
Electronic components that require clear transparency for light to exit or enter (photovoltaic panels and sensors) can be potted using acrylic resins that are cured using UV energy. The advantages are low VOC emissions and rapid curing.
Certain inks, coatings, and adhesives are formulated with photoinitiators and resins. When exposed to UV light, polymerization occurs, and so the adhesives harden or cure, usually within a few seconds. Applications include glass and plastic bonding, optical fiber coatings, the coating of flooring, UV coating and paper finishes in offset printing, dental fillings, and decorative fingernail "gels".
UV sources for UV curing applications include UV lamps, UV LEDs, and Excimer flash lamps. Fast processes such as flexo or offset printing require high-intensity light focused via reflectors onto a moving substrate and medium so high-pressure Hg (mercury) or Fe (iron, doped)-based bulbs are used, energized with electric arcs or microwaves. Lower-power fluorescent lamps and LEDs can be used for static applications. Small high-pressure lamps can have light focused and transmitted to the work area via liquid-filled or fiber-optic light guides.
The impact of UV on polymers is used for modification of the (roughness and hydrophobicity) of polymer surfaces. For example, a poly(methyl methacrylate) surface can be smoothed by vacuum ultraviolet.[92]
UV radiation is useful in preparing low-surface-energy polymers for adhesives. Polymers exposed to UV will oxidize, thus raising the surface energy of the polymer. Once the surface energy of the polymer has been raised, the bond between the adhesive and the polymer is stronger.
Using a catalytic chemical reaction from titanium dioxide and UVC exposure, oxidation of organic matter converts pathogens, pollens, and mold spores into harmless inert byproducts. However, the reaction of titanium dioxide and UVC is not a straight path.  Several hundreds of reactions occur prior to the inert byproducts stage and can hinder the resulting reaction creating formadehyde, aldehyde, and other VOC's en route to a final stage.  Thus, the use of Titanium Dioxide and UVC requires very specific parameters for a successful outcome. The cleansing mechanism of UV is a photochemical process. Contaminants in the indoor environment are almost entirely organic carbon-based compounds, which break down when exposed to high-intensity UV at 240 to 280 nm. Short-wave ultraviolet radiation can destroy DNA in living microorganisms.[93] UVC's effectiveness is directly related to intensity and exposure time.
UV has also been shown to reduce gaseous contaminants such as carbon monoxide and VOCs.[94][95][96] UV lamps radiating at 184 and 254 nm can remove low concentrations of hydrocarbons and carbon monoxide if the air is recycled between the room and the lamp chamber. This arrangement prevents the introduction of ozone into the treated air. Likewise, air may be treated by passing by a single UV source operating at 184 nm and passed over iron pentaoxide to remove the ozone produced by the UV lamp.
Ultraviolet lamps are used to sterilize workspaces and tools used in biology laboratories and medical facilities. Commercially available low-pressure mercury-vapor lamps emit about 86% of their radiation at 254 nanometers (nm), with 265 nm being the peak germicidal effectiveness curve. UV at these germicidal wavelengths damage a microorganism's DNA so that it cannot reproduce, making it harmless, (even though the organism may not be killed). Since microorganisms can be shielded from ultraviolet rays in small cracks and other shaded areas, these lamps are used only as a supplement to other sterilization techniques.
UV-C LEDs are relatively new to the commercial market and are gaining in popularity.[97] Due to their monochromatic nature (± 5 nm) these LEDs can target a specific wavelength needed for disinfection. This is especially important knowing that pathogens vary in their sensitivity to specific UV wavelengths. LEDs are mercury free, instant on/off, and have unlimited cycling throughout the day.[98]
Disinfection using UV radiation is commonly used in wastewater treatment applications and is finding an increased usage in municipal drinking water treatment. Many bottlers of spring water use UV disinfection equipment to sterilize their water. Solar water disinfection[99] has been researched for cheaply treating contaminated water using natural sunlight. The UV-A irradiation and increased water temperature kill organisms in the water.
Ultraviolet radiation is used in several food processes to kill unwanted microorganisms. UV can be used to pasteurize fruit juices by flowing the juice over a high-intensity ultraviolet source.[100] The effectiveness of such a process depends on the UV absorbance of the juice.
Pulsed light (PL) is a technique of killing microorganisms on surfaces using pulses of an intense broad spectrum, rich in UV-C between 200 and 280 nm. Pulsed light works with xenon flash lamps that can produce flashes several times per second. Disinfection robots use pulsed UV[101]
Some animals, including birds, reptiles, and insects such as bees, can see near-ultraviolet wavelengths. Many fruits, flowers, and seeds stand out more strongly from the background in ultraviolet wavelengths as compared to human color vision. Scorpions glow or take on a yellow to green color under UV illumination, thus assisting in the control of these arachnids. Many birds have patterns in their plumage that are invisible at usual wavelengths but observable in ultraviolet, and the urine and other secretions of some animals, including dogs, cats, and human beings, are much easier to spot with ultraviolet. Urine trails of rodents can be detected by pest control technicians for proper treatment of infested dwellings.
Butterflies use ultraviolet as a communication system for sex recognition and mating behavior. For example, in the Colias eurytheme butterfly, males rely on visual cues to locate and identify females. Instead of using chemical stimuli to find mates, males are attracted to the ultraviolet-reflecting color of female hind wings.[102]  In Pieris napi butterflies it was shown that females in northern Finland with less UV-radiation present in the environment possessed stronger UV signals to attract their males than those occurring further south. This suggested that it was evolutionarily more difficult to increase the UV-sensitivity of the eyes of the males than to increase the UV-signals emitted by the females.[103]
Many insects use the ultraviolet wavelength emissions from celestial objects as references for flight navigation. A local ultraviolet emitter will normally disrupt the navigation process and will eventually attract the flying insect.
The green fluorescent protein (GFP) is often used in genetics as a marker. Many substances, such as proteins, have significant light absorption bands in the ultraviolet that are of interest in biochemistry and related fields. UV-capable spectrophotometers are common in such laboratories.
Ultraviolet traps called bug zappers are used to eliminate various small flying insects. They are attracted to the UV and are killed using an electric shock, or trapped once they come into contact with the device. Different designs of ultraviolet radiation traps are also used by entomologists for collecting nocturnal insects during faunistic survey studies.
Ultraviolet radiation is helpful in the treatment of skin conditions such as psoriasis and vitiligo. Exposure to UVA, while the skin is hyper-photosensitive, by taking psoralens is an effective treatment for psoriasis. Due to the potential of psoralens to cause damage to the liver, PUVA therapy may be used only a limited number of times over a patient's lifetime.
UVB phototherapy does not require additional medications or topical preparations for the therapeutic benefit; only the exposure is needed. However, phototherapy can be effective when used in conjunction with certain topical treatments such as anthralin, coal tar, and vitamin A and D derivatives, or systemic treatments such as methotrexate and Soriatane.[104]
Reptiles need UVB for biosynthesis of vitamin D, and other metabolic processes.  Specifically cholecalciferol (vitamin D3),  which is needed to for basic cellular / neural functioning as well as the utilization calcium for bone and egg production. The UVA wavelength is also visible to many reptiles and might play a signifiant role in their ability survive in the wild as well as visual communication between individuals.  Therefore, in a typical reptile enclosure, a fluorescent UV a/b source (at the proper strength / spectrum for the species),  must be available for many captive species to survive.  Simple supplementation with cholecalciferol  (Vitamin D3) will not be enough as there's a complete biosynthetic pathway that is "leapfrogged"  (risks of possible overdoses), the intermediate molecules and metabolites also place important functions in the animals health.  Natural sunlight in the right levels is always going to be superior to artificial sources, but this might be possible for keepers in different parts of the world.
It is a known problem that high levels of output of the UVa part of the spectrum can both cause cellular and DNA damage to sensitive parts of their bodies - especially the eyes were blindness is the result from an improper UVa/b source use and placement photokeratitis.   For many keepers there must also be a provision for an adequate heat source this has resulted in the marketing of heat and light "combination" products.   Keepers should be careful of these "combination' light/ heat and UVa/b generators, they typically emit high levels of UVa with lower levels of UVb that are set and difficult to control so that animals can have their needs met.   A better strategy is to use individual sources of these elements and so they can be placed and controlled by the keepers for the max benefit of the animals.[105]
The evolution of early reproductive proteins and enzymes is attributed in modern models of evolutionary theory to ultraviolet radiation. UVB causes thymine base pairs next to each other in genetic sequences to bond together into thymine dimers, a disruption in the strand that reproductive enzymes cannot copy. This leads to frameshifting during genetic replication and protein synthesis, usually killing the cell. Before formation of the UV-blocking ozone layer, when early prokaryotes approached the surface of the ocean, they almost invariably died out. The few that survived had developed enzymes that monitored the genetic material and removed thymine dimers by nucleotide excision repair enzymes. Many enzymes and proteins involved in modern mitosis and meiosis are similar to repair enzymes, and are believed to be evolved modifications of the enzymes originally used to overcome DNA damages caused by UV.[106]



Radio frequency - Wikipedia
Radio frequency (RF) refers to an oscillation rate of an alternating electric current or voltage or of a magnetic, electric or electromagnetic field or mechanical system in the frequency range from around twenty thousand times per second (7004200000000000000♠20 kHz) to around three hundred billion times per second (7011300000000000000♠300 GHz). This is roughly between the upper limit of audio frequencies and the lower limit of infrared frequencies.[1][2] Different sources specify different upper and lower bounds for the frequency range. While RF usually refers to electrical rather than mechanical oscillations, mechanical RF systems are not uncommon (see mechanical filter and RF MEMS).
Electric currents that oscillate at radio frequencies have special properties not shared by direct current or alternating current of lower frequencies.
The radio spectrum of frequencies is divided into bands with conventional names designated by the International Telecommunications Union (ITU):
Frequencies of 1 GHz and above are conventionally called microwave,[7] while frequencies of 30 GHz and above are designated millimeter wave.
More detailed band designations are given by the standard IEEE letter- band frequency designations[5] and the 	EU/NATO frequency designations.[8]
Radio frequencies are generated and processed within very many functional units such as transmitters, receivers, computers, and televisions to name a few. Radio frequencies are also applied in carrier current systems including telephony and control circuits.
Radio frequency (RF) energy, in the form of radiating waves or electrical currents, has been used in medical treatments for over 75 years,[9] generally for minimally invasive surgeries using radiofrequency ablation  including the treatment of sleep apnea.[10] Magnetic resonance imaging (MRI) uses radio frequency waves to generate images of the human body.[11]
Radio frequencies at non-ablation energy levels are commonly used as a part of aesthetic treatments that can tighten skin, reduce fat by lipolysis and also apoptosis,[12] or promote healing.[13]
RF diathermy is a medical treatment that uses RF induced heat as a form of physical or occupational therapy and in surgical procedures. It is commonly used for muscle relaxation. It is also a method of heating tissue electromagnetically for therapeutic purposes in medicine. Diathermy is used in physical therapy and occupational therapy to deliver moderate heat directly to pathologic lesions in the deeper tissues of the body.  Surgically, the extreme heat that can be produced by diathermy may be used to destroy neoplasms, warts, and infected tissues, and to cauterize blood vessels to prevent excessive bleeding. The technique is particularly valuable in neurosurgery and surgery of the eye.  Diathermy equipment typically operates in the short-wave radio frequency (range 1–100 MHz) or microwave energy (range 434–915 MHz).
Pulsed electromagnetic field therapy (PEMF) is a medical treatment that purportedly helps to heal bone tissue reported in a recent NASA study. This method usually employs electromagnetic radiation of different frequencies - ranging from static magnetic fields, through extremely low frequencies (ELF) to higher radio frequencies (RF) administered in pulses.
Radio frequency current through tissue will generate heat in the tissue and can cause burns.
Test apparatus for radio frequencies can include standard instruments at the lower end of the range, but at higher frequencies the test equipment becomes more specialized.
ELF    3 Hz/100 Mm    30 Hz/10 Mm
SLF   30 Hz/10 Mm    300 Hz/1 Mm
ULF  300 Hz/1 Mm       3 kHz/100 km
VLF    3 kHz/100 km   30 kHz/10 km
LF   30 kHz/10 km   300 kHz/1 km
MF  300 kHz/1 km      3 MHz/100 m
HF    3 MHz/100 m    30 MHz/10 m
VHF   30 MHz/10 m    300 MHz/1 m
UHF  300 MHz/1 m       3 GHz/100 mm
SHF    3 GHz/100 mm   30 GHz/10 mm
EHF   30 GHz/10 mm   300 GHz/1 mm
THF  300 GHz/1 mm      3 THz/0.1 mm



Cancer syndrome - Wikipedia
A cancer syndrome or family cancer syndrome is a genetic disorder in which inherited genetic mutations in one or more genes predispose the affected individuals to the development of cancers and may also cause the early onset of these cancers. Cancer syndromes often show not only a high lifetime risk of developing cancer, but also the development of multiple independent primary tumors.[1] Many of these syndromes are caused by mutations in tumor suppressor genes, genes that are involved in protecting the cell from turning cancerous. Other genes that may be affected are DNA repair genes, oncogenes and genes involved in the production of blood vessels (angiogenesis).[2] Common examples of inherited cancer syndromes are hereditary breast-ovarian cancer syndrome and hereditary non-polyposis colon cancer (Lynch syndrome).[3][4]
Hereditary cancer syndromes underlie 5 to 10% of all cancers and there are over 50 identifiable hereditary forms of cancer.[5] Scientific understanding of cancer susceptibility syndromes is actively expanding: additional syndromes are being found,[6] the underlying biology is becoming clearer, and commercialization of diagnostic genetics methodology is improving clinical access.[citation needed] Given the prevalence of breast and colon cancer, the most widely recognized syndromes include hereditary breast-ovarian cancer syndrome (HBOC) and hereditary non-polyposis colon cancer (HNPCC, Lynch syndrome).[6]
Some rare cancers are strongly associated with hereditary cancer predisposition syndromes. Genetic testing should be considered with adrenocortical carcinoma; carcinoid tumors; diffuse gastric cancer; fallopian tube/primary peritoneal cancer; leiomyosarcoma; medullary thyroid cancer; paraganglioma/pheochromocytoma; renal cell carcinoma of chromophobe, hybrid oncocytic, or oncocytoma histology; sebaceous carcinoma; and sex cord tumors with annular tubules.[6] Primary care physicians can identify people who are at risk of heridatary cancer syndrome.[7]
Two copies of every gene are present in all cells of the body and each one is called an allele. Most cancer syndromes are transmitted in a mendelian autosomal dominant manner. In these cases, only one faulty allele has to be present for an individual to have a predisposition to cancer. Individuals with one normal allele and one faulty allele are known as heterozygous. A heterozygous individual and a person with two normal alleles (homozygous) will have a 50% chance of producing an affected child.[8] The mutation in the inherited gene is known as a germline mutation and a further mutation in the normal allele results in the development of cancer. This is known as Knudson's two hit hypothesis, where the first hit of the gene is the inherited mutation and the second hit occurs later in life.[2] As only one allele needs to be mutated (as compared to both in so called "sporadic cancers"), the individual has a higher chance of developing the cancer than the general population.[citation needed]
Less often, syndromes may be transmitted as an autosomal recessive trait. Both alleles of a gene must be mutated in autosomal recessive disorders for an individual to have a predisposition to cancer. A person with two recessive alleles is known as homozygous recessive. Both parents must have at least one faulty allele in order for a child to be homozygous recessive. If both parents have one mutant allele and one normal allele (heterozygous) then they have a 25% chance of producing a homozygous recessive child (has predisposition), 50% chance of producing a heterozygous child (carrier of the faulty gene) and 25% chance of produced a child with two normal alleles.[8]
Examples of autosomal dominant cancer syndromes are autoimmune lymphoproliferative syndrome (Canale-Smith syndrome), Beckwith–Wiedemann syndrome (although 85% of cases are sporadic),[citation needed] Birt–Hogg–Dubé syndrome, Carney syndrome, familial chordoma, Cowden syndrome, dysplastic nevus syndrome with familial melanoma, familial adenomatous polyposis, hereditary breast-ovarian cancer syndrome, hereditary diffuse gastric cancer (HDGC), hereditary non-polyposis colon cancer (Lynch syndrome), Howel–Evans syndrome of eosophageal cancer with tylosis, juvenile polyposis syndrome, Li-Fraumeni syndrome, multiple endocrine neoplasia type 1/2, multiple osteochondromatosis, neurofibromatosis type 1/2, nevoid basal cell carcinoma syndrome (Gorlin syndrome),  Peutz-Jeghers syndrome, familial prostate cancer, hereditary leiomyomatosis renal cell cancer (LRCC), hereditary papillary renal cell cancer (HPRCC), hereditary  paraganglioma-pheochromocytoma syndrome, retinoblastoma, tuberous sclerosis, von Hippel-Lindau disease and Wilm's tumor.[9]
Examples of autosomal recessive cancer syndromes are ataxia telangiectasia, Bloom syndrome, Fanconi anemia, MUTYH-associated polyposis, Rothmund-Thomson syndrome, Werner's syndrome and Xeroderma pigmentosum.[9]
Although cancer syndromes exhibit an increased risk of cancer, the risk varies. For some of these diseases, cancer is not their primary feature. The discussion here focuses on their association with an increased risk of cancer.  This list is far from exhaustive.
Fanconi anemia (FA) is a disorder with a wide clinical spectrum, including: early onset and increased risk of cancer; bone marrow failure; and congenital abnormalities. The most prominent manifestations of this disorder are those related to hematopoeisis (production of blood by the bone marrow); these include aplastic anemia, myelodysplastic syndrome and acute myeloid leukemia. Hepatic tumors and squamous cell carcinomas of the esophagus, oropharynx and uvula are solid tumors commonly linked to FA. Congenital abnormalities include: skeletal anomalies (especially those affecting the hands), cafe au lait spots and hypopigmentation. To date, the genes known to cause FA are: FANCA, FANCB, FANCC, FANCD2, FANCE, FANCF, FANCG, FANCI, FANCJ, FANCL, FANCM, FANCN, FANCO, FANCP and BRCA2 (previously known as FANCD1). Inheritance of this syndrome is primarily autosomal recessive, but FANCB can be inherited from the maternal or paternal x-chromosome (x-linked recessive inheritance). The FA pathway is involved in DNA repair when the two strands of DNA are incorrectly joined together (interstrand crosslinks). Many pathways are coordinated by the FA pathway for this including nucleotide excision repair, translesion synthesis and homologous recombination.[10][11][12][13][14]
Familial adenomatous polyposis (FAP) is an autosomal dominant syndrome that greatly increases the risk of colorectal cancer. Around 1 in 8000 people will have this disease and it has approximately 100% penetrance. An individual with this disease will have hundreds to thousands of benign adenomas throughout their colon, which will in most cases progress to cancer. Other tumors increased in frequency include; osteomas, adrenal adenomas and carcinomas, thyroid tumors and desmoid tumors. The cause of this disorder is a mutated APC gene, which is involved in β-catenin regulation. Faulty APC causes β-catenin to accumulate in cells and activate transcription factors involved in cell proliferation, migration, differentiation and apoptosis (programmed cell death).[15][16][17]
Hereditary breast-ovarian cancer syndrome (HBOC) is an autosomal dominant genetic disorder caused by genetic mutations of the BRCA1 and BRCA2 genes. In women this disorder primarily increases the risk of breast and ovarian cancer, but also increases the risk of fallopian tube carcinoma and papillary serous carcinoma of the peritoneum. In men the risk of prostate cancer is increased. Other cancers that are inconsistently linked to this syndrome are pancreatic cancer, male breast cancer, colorectal cancer and cancers of the uterus and cervix. Genetic mutations account for approximately 7% and 14% of breast and ovarian cancer, respectively, and BRCA1 and BRCA2 account for 80% of these cases. BRCA1 and BRCA2 are both tumor suppressor genes implicated in maintaining and repairing DNA, which in turn leads to genome instability. Mutations in these genes allow further damage to DNA, which can lead to cancer.[18][19]
Hereditary non-polyposis colon cancer (HNPCC), also known as Lynch syndrome, is an autosomal dominant cancer syndrome that increases the risk of colorectal cancer. It is caused by genetic mutations in DNA mismatch repair (MMR) genes, notably MLH1, MSH2, MSH6 and PMS2. In addition to colorectal cancer many other cancers are increased in frequency. These include; endometrial cancer, stomach cancer, ovarian cancer, cancers of the small bowel and pancreatic cancer. HNPCC is also associated with an early onset of colorectal cancer. MMR genes are involved in repairing DNA when the bases on each strand of DNA do not match. Defective MMR genes allow continuous insertion and  deletion mutations in regions of DNA known as microsatellites. These short repetitive sequences of DNA become unstable, leading to a state of microsatellite instability (MSI). Mutated microsatellites are often found in genes involved in tumor initiation and progression, and MSI can enhance the survival of cells, leading to cancer.[4][20][21][22]
Most cases of familial paraganglioma are caused by mutations in the succinate dehydrogenase (SDH; succinate:ubiquinone oxidoreductase) subunit genes (SDHD, SDHAF2, SDHC, SDHB).
PGL-1 is associated with SDHD mutation, and most PGL-1 individuals with paraganglioma have affected fathers rather than affected mothers. PGL1 and PGL2 are autosomal dominant with imprinting. PGL-4 is associated with SDHB mutation, and is associated with a higher risk of pheochromocytoma, as well as renal cell cancer and non-medullary thyroid cancer.[23]
Li-Fraumeni syndrome is an autosomal dominant syndrome primarily caused by mutations in the TP53 gene, which greatly increases the risk of many cancers and is also highly associated with early onset of these cancers. Cancers linked to this disorder include; soft tissue sarcomas (often found in childhood), osteosarcoma, breast cancer, brain cancer, leukaemia and adrenocortical carcinoma. Individuals with Li-Fraumeni syndrome often have multiple independent primary cancers. The reason for the large clinical spectrum of this disorder may be due to other gene mutations that modify the disease. The protein produced by the TP53 gene, p53, is involved in cell cycle arrest, DNA repair and apoptosis. Defective p53 may not be able to properly perform these processes, which may be the reason for tumor formation. Because only 60-80% of individuals with the disorder have detectable mutations in TP53, other mutations in the p53 pathway may be involved in Li-Fraumeni syndrome.[24][25][26][27]
MUTYH-associated polyposis shares most of its clinical features with FAP; the difference is that it's an autosomal recessive disorder caused by mutations in the MUTYH DNA repair gene. Tumors with increased risk in this disorder are colorectal cancer, gastric adenomas and duodenal adenomas.[15][28]
Nevoid basal cell carcinoma syndrome (NBCCS), also known as Gorlin syndrome, is an autosomal dominant cancer syndrome in which the risk of basal cell carcinoma is very high. The disease is characterized by basal cell nevi, jaw keratocysts and skeletal abnormalities. Estimates of NBCCS prevalence varies, but is approximately 1 in 60000. The presence of basal cell carcinoma is much higher in white than black individuals; 80% and 38%, respectively. Odontogenic keratocysts are found in approximately 75% of individuals with the disease and often occur early in life. The most common skeletal abnormalities occur in the head and face, but other areas are often affected such as the rib cage. The causative genetic mutation of this disease occurs in the PTCH gene, and the product of PTCH is a tumor suppressor involved in cell signaling. Although the exact role of this protein in NBCCS is not known, it is involved in the hedgehog signaling pathway, known to control cell growth and development.[29][30]
Von Hippel–Lindau (VHL) disease is a rare, autosomal dominant genetic condition that predisposes individuals to benign and malignant tumors. The most common tumors in VHL are central nervous system and retinal hemangioblastomas, clear cell renal carcinomas, pheochromocytomas, pancreatic neuroendocrine tumours, pancreatic cysts, endolymphatic sac tumors and epididymal papillary cystadenomas.[31][32] VHL results from a mutation in the von Hippel–Lindau tumor suppressor gene on chromosome 3p25.3.[33]
Xeroderma pigmentosum (XP) is an autosomal recessive disorder characterized by sensitivity to ultra-violet (UV) light, massively increased risk of sunburn and increased risk of skin cancers. The risk of skin cancer is more than 10000 times that of normal individuals and includes many types of skin cancer, including melanoma and non-melanoma skin cancers. Also, sun exposed areas of the tongue, lips and eyes have an increased risk of becoming cancerous. XP may be associated with other internal cancers and benign tumors.[citation needed] In addition to cancer, some genetic mutations that cause XP are associated with neurodegeneration. XP may be caused by genetic mutations in 8 genes, which produce the following enzymes: XPA, XPB, XPC, XPD, XPE, XPF, XPG and Pol η. XPA-XPF are  nucleotide excision repair enzymes that repair UV light-damaged DNA and faulty proteins will allow the buildup of mutations caused by UV light. Pol η is a polymerase, which is an enzyme involved in DNA replication. There are many polymerases, but pol η is the enzyme that replicates UV light-damaged DNA. Mutations in this gene will produce a faulty pol η enzyme that cannot replicate DNA with UV light damage. Individuals with mutations of this gene have a subset of XP; XP-variant disease.[34][35]
Many cancer syndromes are due to an inherited impairment in DNA repair capability.[36]  When an inherited mutation is present in a DNA repair gene, the repair gene will either not be expressed or expressed in an altered form.  Then the repair function will likely be deficient, and, as a consequence, DNA damages will tend to accumulate.  Such DNA damages can cause errors during DNA synthesis leading to mutations, some of which may give rise to cancer.  Germ-line DNA repair mutations that increase the risk of cancer are listed in the Table.
sebaceous adenomas)  [46]
Genetic testing can be used to identify mutated genes or chromosomes that are passed through generations. People who test positive for having a genetic mutation aren’t necessarily condemned to develop the cancer linked with the mutation, however they possess an increased risk of developing cancer in comparison to the general population. It’s advised that people get a genetic test if their family medical history includes: Multiple family members with cancer, someone in their family that got cancer at a particularly young age or by being part of a certain ethnic group.[65]
The process of genetic screening is a simple, non-invasive procedure. However, before genes are tested for mutations the patient usually must go to a health care provider and go through a one-on-one consultation, where they discuss both the personal and family history of cancer. The medical professional can then assess the likelihood of the patient having the mutation and can guide them through the process that is genetic screening.[66] It’s important that this consultation takes place because it ensures that the person gives informed consent to engage in genetic testing, is aware and understands the steps, benefits and limitations of the procedure and is more knowledgeable of the consequences of hearing test results.[67]  The test can be done by using body fluids or cells of the patient, this includes; blood (which is the most common), saliva, amniotic fluid and even cells from the interior of the mouth gotten from a buccal swab. This material is then sent to a specialized genetics lab where technicians will examine it, the test results are sent back to the health provider who requested the analysis and results are discussed with the patient.[65]
Direct to consumer testing can be obtained without a medical professional but isn’t recommended as the consumer loses the opportunity to discuss their decision with an educated professional.[68] According to the National Library of Medicine in the U.S. genetic testing in America costs in the price range of $100-$2000 depending on the type and intricacy of test.[69]
Genetic testing is important as if a test comes out positive they are more aware of their own personal health and the health of immediate family members.[70] With the help and advice from a medical professional they can take steps to reduce their elevated risk of cancer development through:
There are other forms of preventative actions, an example for Hereditary Breast and Ovarian Cancer would be to go through surgery: A hysterectomy is the removal of all or some of the uterus, whereas a mastectomy is removing a breast (double mastectomy meaning that both breasts are removed), this can often add years onto their life expectancy.[72] Another preventative measure is regular cancer screening and check-ups. If a person has Lynch’s syndrome then they should have a regular colonoscopy to examine if there is any change in the cells lining the intestinal wall, regular check-ups have been proven to add an average of 7 years onto the life expectancy of a person suffering from Lynch’s syndrome as early detection means the correct preventative actions and surgery can be taken quicker.[73] Regular breast screening is also recommended for women diagnosed with BRCA mutations, as well as that, recent studies show that men with increased risks of developing prostate cancer due to BRCA mutations can decrease their risk by taking aspirin.[74] Aspirin is hugely beneficial in lowering cancer prevalence; however, it must be taken regularly over at least a five-year period to have any effect.[75]
Often genetic mutations are more common in certain ethnic groups, this is because a race can track their ancestors back to one geographic location, the mutated genes are then passed from ancestors down through generations which is why some ethnicities are more susceptible to mutations, thus increasing their chances of developing cancer [61]. As mentioned above, this can be useful as it can help health professionals assess a patient’s risk of having a mutation before they undergo testing.[66] Werner's Syndrome has a prevalence of 1 in 200,000 live births in the U.S., but it affects individuals in Japan in 1 in 20,000-40,000 cases.[76]
1 in 40 Ashkenazi Jews have a BRCA mutation, this is a huge contrast from the general population in the United States where 1 in 400 people are affected. Ashkenazi Jews are at high risk of developing hereditary breast and ovarian cancer and it's recommend that they undergo both genetic testing to see if they have a mutation and regular screening for cancer.[77]



Risk ratio - Wikipedia
In epidemiology, risk ratio (RR) or relative risk is the ratio of the probability of an outcome in an exposed group to the probability of an outcome in an unexposed group. It is computed as 




I

e



/


I

u




{\displaystyle I_{e}/I_{u}}

, where 




I

e




{\displaystyle I_{e}}

 is the incidence in the exposed group, and 




I

u




{\displaystyle I_{u}}

 is the incidence in the unexposed group.[1] Together with risk difference and odds ratio, risk ratio measures the association between the exposure and the outcome.[2]
Risk ratio is used in the statistical analysis of the data of experimental, cohort and cross-sectional studies, to estimate the strength of the association between treatments or risk factors, and outcome.[2][3] For example, it is used to compare the risk of an adverse outcome when receiving a medical treatment versus no treatment (or placebo), or when exposed to an environmental risk factor versus not exposed.
Assuming the causal effect between the exposure and the outcome, values of RR can be interpreted as follows:
Risk ratio is commonly used to present the results of randomized controlled trials.[4] This can be problematic, if risk ratio is presented without the absolute measures, such as absolute risk, or risk difference.[5] In the case when the base rate of the outcome is low, large or small values of risk ratio may not translate to significant effect, and the importance of the effect to the public health can be overestimated. Equivalently, in the case when the base rate of the outcome is high, values of the risk ratio close to 1 may still result in a significant effect and can be underestimated. Thus, presentation of both absolute and relative measures is recommended.[6]
Risk ratio can be estimated from a 2x2 contingency table:
The point estimate of the risk ratio is
The sampling distribution of the 



log
⁡
(
R
R
)


{\displaystyle \log(RR)}

is approximately normal,[7] with standard error
The 



1
−
α


{\displaystyle 1-\alpha }

 confidence interval for the 



log
⁡
(
R
R
)


{\displaystyle \log(RR)}

 is then
where 




z

α




{\displaystyle z_{\alpha }}

 is the standard score for the chosen level of significance[8][9]. To find the confidence interval around the RR itself, the two bounds of the above confidence interval can be exponentiated.[8]
In regression models, the exposure is typically included as a indicator variable along with other factors that may affect risk. The risk ratio is usually reported as calculated for the mean of the sample values of the explanatory variables.
Risk ratio is different from the odds ratio, although it asymptotically approaches it for small probabilities of outcomes. If EE is substantially smaller than EN, then EE/(EE + EN) 




≈



{\displaystyle \scriptstyle \approx }

 EE/EN. Similarly, if CE is much smaller than CN, then CE/(CN + CE) 




≈



{\displaystyle \scriptstyle \approx }

 CE/CN. Thus, under the rare disease assumption
In epidemiological research, the odds ratio is commonly used for case-control studies, as the risk ratio cannot be estimated.[10]
In fact, the odds ratio has much broader use in statistics, since logistic regression, often associated with clinical trials, works with the log of the odds ratio, not risk ratio. Because the record of the odds ratio is estimated as a linear function of the explanatory variables, the estimated odds ratio for 70-year-olds and 60-year-olds associated with the type of treatment would be the same in logistic regression models where the outcome is associated with drug and age, although the risk ratio might be significantly different. In cases like this, statistical models of the odds ratio often reflect the underlying mechanisms more efficiently.
Since risk ratio is a more intuitive measure of effectiveness, the distinction is important especially in cases of medium to high probabilities.  If action A carries a risk of 99.9% and action B a risk of 99.0% then the risk ratio is just over 1, while the odds associated with action A are more than 10 times higher than the odds with B.
In statistical modelling, approaches like poisson regression (for counts of events per unit exposure) have risk ratio interpretations: the estimated effect of an explanatory variable is multiplicative on the rate and thus leads to a risk ratio. Logistic regression (for binary outcomes, or counts of successes out of a number of trials) must be interpreted in odds-ratio terms: the effect of an explanatory variable is multiplicative on the odds and thus leads to an odds ratio.
We could assume a disease noted by 



D


{\displaystyle D}

, and no disease noted by 



¬
D


{\displaystyle \neg D}

, exposure noted by 



E


{\displaystyle E}

, and no exposure noted by 



¬
E


{\displaystyle \neg E}

. Risk ratio can be written as
This way the risk ratio can be interpreted in Bayesian terms as the posterior ratio of the exposure (i.e. after seeing the disease) normalized by the prior ratio of exposure.[11]  If the posterior ratio of exposure is similar to that of the prior, the effect is approximately 1, indicating no association with the disease, since it didn't change beliefs of the exposure. If on the other hand, the posterior ratio of exposure is smaller or higher than that of the prior ratio, then the disease has changed the view of the exposure danger, and the magnitude of this change is the risk ratio.



Inflammation - Wikipedia

Inflammation (from Latin: inflammatio) is part of the complex biological response of body tissues to harmful stimuli, such as pathogens, damaged cells, or irritants,[1] and is a protective response involving immune cells, blood vessels, and molecular mediators. The function of inflammation is to eliminate the initial cause of cell injury, clear out necrotic cells and tissues damaged from the original insult and the inflammatory process, and initiate tissue repair.
The five classical signs of inflammation are heat, pain, redness, swelling, and loss of function (Latin calor, dolor, rubor, tumor, and functio laesa).[1] Inflammation is a generic response, and therefore it is considered as a mechanism of innate immunity, as compared to adaptive immunity, which is specific for each pathogen.[2] Too little inflammation could lead to progressive tissue destruction by the harmful stimulus (e.g. bacteria) and compromise the survival of the organism. In contrast, chronic inflammation may lead to a host of diseases, such as hay fever, periodontitis, atherosclerosis, rheumatoid arthritis, and even cancer (e.g., gallbladder carcinoma). Inflammation is therefore normally closely regulated by the body.
Inflammation can be classified as either acute or chronic. Acute inflammation is the initial response of the body to harmful stimuli and is achieved by the increased movement of plasma and leukocytes (especially granulocytes) from the blood into the injured tissues. A series of biochemical events propagates and matures the inflammatory response, involving the local vascular system, the immune system, and various cells within the injured tissue. Prolonged inflammation, known as chronic inflammation, leads to a progressive shift in the type of cells present at the site of inflammation, such as mononuclear cells, and is characterized by simultaneous destruction and healing of the tissue from the inflammatory process.
Inflammation is not a synonym for infection. Infection describes the interaction between the action of microbial invasion and the reaction of the body's inflammatory response — the two components are considered together when discussing an infection, and the word is used to imply a microbial invasive cause for the observed inflammatory reaction. Inflammation on the other hand describes purely the body's immunovascular response, whatever the cause may be. But because of how often the two are correlated, words ending in the suffix -itis (which refers to inflammation) are sometimes informally described as referring to infection. For example, the word urethritis strictly means only "urethral inflammation", but clinical health care providers usually discuss urethritis as a urethral infection because urethral microbial invasion is the most common cause of urethritis.
It is useful to differentiate inflammation and infection as there are many pathological situations where inflammation is not driven by microbial invasion – for example, atherosclerosis, type III hypersensitivity, trauma, ischaemia. There are also pathological situations where microbial invasion does not result in classic inflammatory response—for example, parasitosis, eosinophilia.
Biological:
Chemical:[3]
Psychological:

These are the original, or "cardinal signs" of inflammation.[6]*
Functio laesa is an antiquated notion, as it is not unique to inflammation and is a characteristic of many disease states.[7]**
Acute inflammation is a short-term process, usually appearing within a few minutes or hours and begins to cease upon the removal of the injurious stimulus.[8] It involves a coordinated and systemic mobilization response locally of various immune, endocrine and neurological mediators of acute inflammation. In a normal healthy response, it becomes activated, clears the pathogen and begins a repair process and then ceases.[9] It is characterized by five cardinal signs:[10]
An acronym that may be used to remember the key symptoms is "PRISH", for pain, redness, immobility (loss of function), swelling and heat.
The traditional names for signs of inflammation come from Latin:
The first four (classical signs) were described by Celsus (ca. 30 BC–38 AD),[12] while loss of function was probably added later by Galen.[13] However, the addition of this fifth sign has also been ascribed to Thomas Sydenham[14] and Virchow.[8][10]
Redness and heat are due to increased blood flow at body core temperature to the inflamed site; swelling is caused by accumulation of fluid; pain is due to the release of chemicals such as bradykinin and histamine that stimulate nerve endings. Loss of function has multiple causes.[10]
Acute inflammation of the lung (usually caused in response to pneumonia) does not cause pain unless the inflammation involves the parietal pleura, which does have pain-sensitive nerve endings.[10]
The process of acute inflammation is initiated by resident immune cells already present in the involved tissue, mainly resident macrophages, dendritic cells, histiocytes, Kupffer cells and mast cells. These cells possess surface receptors known as pattern recognition receptors (PRRs), which recognize (i.e., bind) two subclasses of molecules: pathogen-associated molecular patterns (PAMPs) and damage-associated molecular patterns (DAMPs). PAMPs are compounds that are associated with various pathogens, but which are distinguishable from host molecules.  DAMPs are compounds that are associated with host-related injury and cell damage.
At the onset of an infection, burn, or other injuries, these cells undergo activation (one of the PRRs recognize a PAMP or DAMP) and release inflammatory mediators responsible for the clinical signs of inflammation. Vasodilation and its resulting increased blood flow causes the redness (rubor) and increased heat (calor). Increased permeability of the blood vessels results in an exudation (leakage) of plasma proteins and fluid into the tissue (edema), which manifests itself as swelling (tumor). Some of the released mediators such as bradykinin increase the sensitivity to pain (hyperalgesia, dolor).  The mediator molecules also alter the blood vessels to permit the migration of leukocytes, mainly neutrophils and macrophages, outside of the blood vessels (extravasation) into the tissue. The neutrophils migrate along a chemotactic gradient created by the local cells to reach the site of injury.[8] The loss of function (functio laesa) is probably the result of a neurological reflex in response to pain.
In addition to cell-derived mediators, several acellular biochemical cascade systems consisting of preformed plasma proteins act in parallel to initiate and propagate the inflammatory response. These include the complement system activated by bacteria and the coagulation and fibrinolysis systems activated by necrosis, e.g. a burn or a trauma.[8]
Acute inflammation may be regarded as the first line of defense against injury. Acute  inflammatory response requires constant stimulation to be sustained. Inflammatory mediators are short-lived and are quickly degraded in the tissue. Hence, acute inflammation begins to cease once the stimulus has been removed.[8]
As defined, acute inflammation is an immunovascular response to an inflammatory stimulus. This means acute inflammation can be broadly divided into a vascular phase that occurs first, followed by a cellular phase involving immune cells (more specifically myeloid granulocytes in the acute setting). The vascular component of acute inflammation involves the movement of plasma fluid, containing important proteins such as fibrin and immunoglobulins (antibodies), into inflamed tissue.
Upon contact with PAMPs, tissue macrophages and mastocytes release vasoactive amines such as histamine and serotonin, as well as eicosanoids such as prostaglandin E2 and leukotriene B4 to remodel the local vasculature. Macrophages and endothelial cells release nitric oxide. These mediators vasodilate and permeabilize the blood vessels, which results in the net distribution of blood plasma from the vessel into the tissue space. The increased collection of fluid into the tissue causes it to swell (edema). This exuded tissue fluid contain various antimicrobial mediators from the plasma such as complement, lysozyme, antibodies, which can immediately deal damage to microbes, and opsonise the microbes in preparation for the cellular phase. If the inflammatory stimulus is a lacerating wound, exuded platelets, coagulants, plasmin and kinins can clot the wounded area and provide haemostasis in the first instance. These clotting mediators also provide a structural staging framework at the inflammatory tissue site in the form of a fibrin lattice – as would construction scaffolding at a construction site – for the purpose of aiding phagocytic debridement and wound repair later on. Some of the exuded tissue fluid is also funnelled by lymphatics to the regional lymph nodes, flushing bacteria along to start the recognition and attack phase of the adaptive immune system.
Acute inflammation is characterized by marked vascular changes, including vasodilation, increased permeability and increased blood flow, which are induced by the actions of various inflammatory mediators. Vasodilation occurs first at the arteriole level, progressing to the capillary level, and brings about a net increase in the amount of blood present, causing the redness and heat of inflammation. Increased permeability of the vessels results in the movement of plasma into the tissues, with resultant stasis due to the increase in the concentration of the cells within blood – a condition characterized by enlarged vessels packed with cells. Stasis allows leukocytes to marginate (move) along the endothelium, a process critical to their recruitment into the tissues. Normal flowing blood prevents this, as the shearing force along the periphery of the vessels moves cells in the blood into the middle of the vessel.
* non-exhaustive list
The cellular component involves leukocytes, which normally reside in blood and must move into the inflamed tissue via extravasation to aid in inflammation. Some act as phagocytes, ingesting bacteria, viruses, and cellular debris. Others release enzymatic granules that damage pathogenic invaders. Leukocytes also release inflammatory mediators that develop and maintain the inflammatory response. In general, acute inflammation is mediated by granulocytes, whereas chronic inflammation is mediated by mononuclear cells such as monocytes and lymphocytes.
Various leukocytes, particularly neutrophils, are critically involved in the initiation and maintenance of inflammation. These cells must be able to move to the site of injury from their usual location in the blood, therefore mechanisms exist to recruit and direct leukocytes to the appropriate place. The process of leukocyte movement from the blood to the tissues through the blood vessels is known as extravasation, and can be broadly divided up into a number of steps:
Extravasated neutrophils in the cellular phase come into contact with microbes at the inflamed tissue. Phagocytes express cell-surface endocytic pattern recognition receptors (PRRs) that have affinity and efficacy against non-specific microbe-associated molecular patterns (PAMPs). Most PAMPs that bind to endocytic PRRs and initiate phagocytosis are cell wall components, including complex carbohydrates such as mannans and β-glucans, lipopolysaccharides (LPS), peptidoglycans, and surface proteins. Endocytic PRRs on phagocytes reflect these molecular patterns, with C-type lectin receptors binding to mannans and β-glucans, and scavenger receptors binding to LPS.
Upon endocytic PRR binding, actin-myosin cytoskeletal rearrangement adjacent to the plasma membrane occurs in a way that endocytoses the plasma membrane containing the PRR-PAMP complex, and the microbe. Phosphatidylinositol and Vps34-Vps15-Beclin1 signalling pathways have been implicated to traffic the endocytosed phagosome to intracellular lysosomes, where fusion of the phagosome and the lysosome produces a phagolysosome. The reactive oxygen species, superoxides and hypochlorite bleach within the phagolysosomes then kill microbes inside the phagocyte.
Phagocytic efficacy can be enhanced by opsonization. Plasma derived complement C3b  and antibodies that exude into the inflamed tissue during the vascular phase bind to and coat the microbial antigens. As well as endocytic PRRs, phagocytes also express opsonin receptors Fc receptor and complement receptor 1 (CR1), which bind to antibodies and C3b, respectively. The co-stimulation of endocytic PRR and opsonin receptor increases the efficacy of the phagocytic process, enhancing the lysosomal elimination of the infective agent.
* non-exhaustive list
Specific patterns of acute and chronic inflammation are seen during particular situations that arise in the body, such as when inflammation occurs on an epithelial surface, or pyogenic bacteria are involved.
Inflammatory abnormalities are a large group of disorders that underlie a vast variety of human diseases. The immune system is often involved with inflammatory disorders, demonstrated in both allergic reactions and some myopathies, with many immune system disorders resulting in abnormal inflammation. Non-immune diseases with causal origins in inflammatory processes include cancer, atherosclerosis, and ischemic heart disease.[8]
Examples of disorders associated with inflammation include:
Atherosclerosis, formerly considered a bland lipid storage disease, actually involves an ongoing inflammatory response. Recent advances in basic science have established a fundamental role for inflammation in mediating all stages of this disease from initiation through progression and, ultimately, the thrombotic complications of atherosclerosis. These new findings provide important links between risk factors and the mechanisms of atherogenesis. Clinical studies have shown that this emerging biology of inflammation in atherosclerosis applies directly to human patients. Elevation in markers of inflammation predicts outcomes of patients with acute coronary syndromes, independently of myocardial damage. In addition, low-grade chronic inflammation, as indicated by levels of the inflammatory marker C-reactive protein, prospectively defines risk of atherosclerotic complications, thus adding to prognostic information provided by traditional risk factors. Moreover, certain treatments that reduce coronary risk also limit inflammation. In the case of lipid lowering with statins, this anti-inflammatory effect does not appear to correlate with reduction in low-density lipoprotein levels. These new insights into inflammation in atherosclerosis not only increase our understanding of this disease but also have practical clinical applications in risk stratification and targeting of therapy for this scourge of growing worldwide importance.[19]
An allergic reaction, formally known as type 1 hypersensitivity, is the result of an inappropriate immune response triggering inflammation, vasodilation, and nerve irritation. A common example is hay fever, which is caused by a hypersensitive response by mast cells to allergens. Pre-sensitised mast cells respond by degranulating, releasing vasoactive chemicals such as histamine. These chemicals propagate an excessive inflammatory response characterised by blood vessel dilation, production of pro-inflammatory molecules, cytokine release, and recruitment of leukocytes.[8] Severe inflammatory response may mature into a systemic response known as anaphylaxis.
Inflammatory myopathies are caused by the immune system inappropriately attacking components of muscle, leading to signs of muscle inflammation. They may occur in conjunction with other immune disorders, such as systemic sclerosis, and include dermatomyositis, polymyositis, and inclusion body myositis.[8]
Due to the central role of leukocytes in the development and propagation of inflammation, defects in leukocyte functionality often result in a decreased capacity for inflammatory defense with subsequent vulnerability to infection.[8] Dysfunctional leukocytes may be unable to correctly bind to blood vessels due to surface receptor mutations, digest bacteria (Chédiak–Higashi syndrome), or produce microbicides (chronic granulomatous disease). In addition, diseases affecting the bone marrow may result in abnormal or few leukocytes.
Certain drugs or exogenous chemical compounds are known to affect inflammation. Vitamin A deficiency causes an increase in inflammatory responses,[20] and anti-inflammatory drugs work specifically by inhibiting the enzymes that produce inflammatory eicosanoids. Certain illicit drugs such as cocaine and ecstasy may exert some of their detrimental effects by activating transcription factors intimately involved with inflammation (e.g. NF-κB).[21][22]
Inflammation orchestrates the microenvironment around tumours, contributing to proliferation, survival and migration.[23] Cancer cells use selectins, chemokines and their receptors for invasion, migration and metastasis.[24] On the other hand, many cells of the immune system contribute to cancer immunology, suppressing cancer.[25]
Molecular intersection between receptors of steroid hormones, which have important effects on cellular development, and transcription factors that play key roles in inflammation, such as NF-κB, may mediate some of the most critical effects of inflammatory stimuli on cancer cells.[26] This capacity of a mediator of inflammation to influence the effects of steroid hormones in cells, is very likely to affect carcinogenesis on the one hand; on the other hand, due to the modular nature of many steroid hormone receptors, this interaction may offer ways to interfere with cancer progression, through targeting of a specific protein domain in a specific cell type. Such an approach may limit side effects that are unrelated to the tumor of interest, and may help preserve vital homeostatic functions and developmental processes in the organism.
According to a review of 2009, recent data suggests that cancer-related inflammation (CRI) may lead to accumulation of random genetic alterations in cancer cells.[27]
In 1863, Rudolf  Virchow hypothesized that the origin of cancer was at sites of chronic inflammation.[28][29]  At present, chronic inflammation is estimated to contribute to approximately 15% to 25% of human cancers.[30][29][31][32]
An inflammatory mediator is a messenger that acts on blood vessels and/or cells to promote an inflammatory response.[33]  Inflammatory mediators that contribute to neoplasia include prostaglandins, inflammatory cytokines such as IL-1β, TNF-α, IL-6 and IL-15 and chemokines such as IL-8 and GRO-alpha.[34][29]  These inflammatory mediators, and others, orchestrate an environment that fosters proliferation and survival.[28][34]
Inflammation also causes DNA damages due to the induction of reactive oxygen species (ROS) by various intracellular inflammatory mediators.[28][34][29]  In addition, leukocytes and other phagocytic cells attracted to the site of inflammation induce DNA damages in proliferating cells through their generation of ROS and reactive nitrogen species (RNS).   ROS and RNS are normally produced by these cells to fight infection.[28]  ROS, alone, cause more than 20 types of DNA damage.[35]  Oxidative DNA damages cause both mutations[36] and epigenetic alterations.[37][29][31]  RNS also cause mutagenic DNA damages.[38]
A normal cell may undergo carcinogenesis to become a cancer cell if it is frequently subjected to DNA damage during long periods of chronic inflammation.  DNA damages may cause genetic mutations due to inaccurate repair.  In addition, mistakes in the DNA repair process may cause epigenetic alterations.[29][34][31]  Mutations and epigenetic alterations that are replicated and provide a selective advantage during somatic cell proliferation may be carcinogenic.
Genome-wide analyses of human cancer tissues reveal that a single typical cancer cell may possess roughly 100 mutations in coding regions, 10-20 of which are “driver mutations” that contribute to cancer development.[29]  However, chronic inflammation also causes epigenetic changes such as DNA methylations, that are often more common than mutations.  Typically, several hundreds to thousands of genes are methylated in a cancer cell (see DNA methylation in cancer).  Sites of oxidative damage in chromatin can recruit complexes that contain DNA methyltransferases (DNMTs), a histone deacetylase (SIRT1), and a histone methyltransferase (EZH2), and thus induce DNA methylation.[29][39][40]  DNA methylation of a CpG island in a promoter region may cause silencing of its downstream gene (see CpG site and regulation of transcription in cancer).   DNA repair genes, in particular, are frequently inactivated by methylation in various cancers (see hypermethylation of DNA repair genes in cancer).  A 2018 report[41] evaluated the relative importance of mutations and epigenetic alterations in progression to two different types of cancer.  This report showed that epigenetic alterations were much more important than mutations in generating gastric cancers (associated with inflammation).[42]  However, mutations and epigenetic alterations were of roughly equal importance in generating esophageal squamous cell cancers (associated with tobacco chemicals and acetaldehyde, a product of alcohol metabolism).
It has long been recognized that infection with HIV is characterized not only by development of profound immunodeficiency but also by sustained inflammation and immune activation.[43][44][45] A substantial body of evidence implicates chronic inflammation as a critical driver of immune dysfunction, premature appearance of aging-related diseases, and immune deficiency.[43][46] Many now regard HIV infection not only as an evolving virus-induced immunodeficiency but also as chronic inflammatory disease.[47] Even after the introduction of effective antiretroviral therapy (ART) and effective suppression of viremia in HIV-infected individuals, chronic inflammation persists. Animal studies also support the relationship between immune activation and progressive cellular immune deficiency: SIVsm infection of its natural nonhuman primate hosts, the sooty mangabey, causes high-level viral replication but limited evidence of disease.[48][49] This lack of pathogenicity is accompanied by a lack of inflammation, immune activation and cellular proliferation. In sharp contrast, experimental SIVsm infection of rhesus macaque produces immune activation and AIDS-like disease with many parallels to human HIV infection.[50]
Delineating how CD4 T cells are depleted and how chronic inflammation and immune activation are induced lies at the heart of understanding HIV pathogenesis––one of the top priorities for HIV research by the Office of AIDS Research, National Institutes of Health. Recent studies demonstrated that caspase-1-mediated pyroptosis, a highly inflammatory form of programmed cell death, drives CD4 T-cell depletion and inflammation by HIV.[51][52][53] These are the two signature events that propel HIV disease progression to AIDS. Pyroptosis appears to create a pathogenic vicious cycle in which dying CD4 T cells and other immune cells (including macrophages and neutrophils) release inflammatory signals that recruit more cells into the infected lymphoid tissues to die. The feed-forward nature of this inflammatory response produces chronic inflammation and tissue injury.[54] Identifying pyroptosis as the predominant mechanism that causes CD4 T-cell depletion and chronic inflammation, provides novel therapeutic opportunities, namely caspase-1 which controls the pyroptotic pathway. In this regard, pyroptosis of CD4 T cells and secretion of pro-inflmammatory cytokines such as IL-1β and IL-18 can be blocked in HIV-infected human lymphoid tissues by addition of the caspase-1 inhibitor VX-765,[51] which has already proven to be safe and well tolerated in phase II human clinical trials.[55] These findings could propel development of an entirely new class of “anti-AIDS” therapies that act by targeting the host rather than the virus. Such agents would almost certainly be used in combination with ART. By promoting “tolerance” of the virus instead of suppressing its replication, VX-765 or related drugs may mimic the evolutionary solutions occurring in multiple monkey hosts (e.g. the sooty mangabey) infected with species-specific lentiviruses that have led to a lack of disease, no decline in CD4 T-cell counts, and no chronic inflammation.
The inflammatory response must be actively terminated when no longer needed to prevent unnecessary "bystander" damage to tissues.[8]  Failure to do so results in chronic inflammation, and cellular destruction. Resolution of inflammation occurs by different mechanisms in different tissues.
Mechanisms that serve to terminate inflammation include:[8][56]
There is evidence for a link between inflammation and depression.[68] Inflammatory processes can be triggered by negative cognitions or their consequences, such as stress, violence, or deprivation. Thus, negative cognitions can cause inflammation that can, in turn, lead to depression.[69][70][dubious  – discuss]
In addition there is increasing evidence that inflammation can cause depression because of the increase of cytokines, setting the brain into a "sickness mode".[71] Classical symptoms of being physically sick like lethargy show a large overlap in behaviors that characterize depression. Levels of cytokines tend to increase sharply during the depressive episodes of people with bipolar disorder and drop off during remission.[72] Furthermore, it has been shown in clinical trials that anti-inflammatory medicines taken in addition to antidepressants not only significantly improves symptoms but also increases the proportion of subjects positively responding to treatment.[73]
Inflammations that lead to serious depression could be caused by common infections such as those caused by a virus, bacteria or even parasites.[74]
An infectious organism can escape the confines of the immediate tissue via the circulatory system or lymphatic system, where it may spread to other parts of the body. If an organism is not contained by the actions of acute inflammation it may gain access to the lymphatic system via nearby lymph vessels. An infection of the lymph vessels is known as lymphangitis, and infection of a lymph node is known as lymphadenitis. When lymph nodes cannot destroy all pathogens, the infection spreads further. A pathogen can gain access to the bloodstream through lymphatic drainage into the circulatory system.
When inflammation overwhelms the host, systemic inflammatory response syndrome is diagnosed. When it is due to infection, the term sepsis is applied, with the terms bacteremia being applied specifically for bacterial sepsis and viremia specifically to viral sepsis. Vasodilation and organ dysfunction are serious problems associated with widespread infection that may lead to septic shock and death.
Inflammation also induces high systemic levels of acute-phase proteins. In acute inflammation, these proteins prove beneficial; however, in chronic inflammation they can contribute to amyloidosis.[8] These proteins include C-reactive protein, serum amyloid A, and serum amyloid P, which cause a range of systemic effects including:[8]
Inflammation often affects the numbers of leukocytes present in the body:
With the discovery of interleukins (IL), the concept of systemic inflammation developed. Although the processes involved are identical to tissue inflammation, systemic inflammation is not confined to a particular tissue but involves the endothelium and other organ systems.
Chronic inflammation is widely observed in obesity.[75][76] The obese commonly have many elevated markers of inflammation, including:[77][78]
Low-grade chronic inflammation is characterized by a two- to threefold increase in the systemic concentrations of cytokines such as TNF-α, IL-6, and CRP.[81]  Waist circumference correlates significantly with systemic inflammatory response.[82] A predominant factor in this correlation is due to the autoimmune response triggered by adiposity, whereby immune cells may mistake fatty deposits for intruders. The body attacks fat like it does bacteria and fungi. When expanded fat cells leak or break open, macrophages mobilize to clean up and embed into the adipose tissue. Then macrophages release inflammatory chemicals, including TNF-α and IL-6. TNF's primary role is to regulate the immune cells and induce inflammation. White blood cells then assist by releasing more cytokines. This link between adiposity and inflammation has been shown to produce 10–35% of IL-6 in a resting individual, and this production increases with increasing adiposity.[83]
Loss of white adipose tissue reduces levels of inflammation markers.[75] The association of systemic inflammation with insulin resistance and atherosclerosis is the subject of intense research.[84]
In the obese mouse models, inflammation and macrophage-specific genes are upregulated in white adipose tissue (WAT). There were also signs of dramatic increase in circulating insulin level, adipocyte lipolysis and formation of multinucleate giant cells.[85] The fat-derived protein called angiopoietin-like protein 2 (Angptl2) elevates in fat tissues. Higher than normal Angptl2 level in fat tissues develop inflammation as well as insulin and leptin resistance. Stored fat secretes leptin to signal satiety. Leptin resistance plays a role in the process where appetite overrules the message of satiety. Angptl2 then starts an inflammatory cascade causing blood vessels to remodel and attract macrophages. Angptl2 is an adipocyte-derived inflammatory mediator linking obesity to systemic insulin resistance.[86] It is possible that, as an inflammatory marker, leptin responds specifically to adipose-derived inflammatory cytokines.
C-reactive protein (CRP) is generated at a higher level in obese people. It raises when there is inflammation throughout the body. Mild elevation in CRP increases risk of heart attacks, strokes, high blood pressure, muscle weakness and fragility.[87][citation needed]
Hyperglycemia induces IL-6 production from endothelial cells and macrophages.[88] Meals high in saturated fat, as well as meals high in calories have been associated with increases in inflammatory markers.[89][90]  In addition, interstitial abdominal adiposity (also referred to as accumulated intra-abdominal fat) may be a factor in increasing systemic risk for multiple inflammatory diseases. Although the exact mechanisms are still being investigated, a review published in 2010 suggested that significant growth of adipose tissue in response to overeating can evoke a chronic inflammatory response.[91]
The outcome in a particular circumstance will be determined by the tissue in which the injury has occurred and the injurious agent that is causing it. Here are the possible outcomes to inflammation:[8]
Inflammation is usually indicated by adding the suffix "itis", as shown below. However, some conditions such as asthma and pneumonia do not follow this convention. More examples are available at list of types of inflammation.
Acute appendicitis
Acute dermatitis
Acute infective meningitis
Acute tonsillitis
The Dietary Inflammatory Index (DII) is a score (number) that describes the potential of diet to modulate systemic inflammation within the body.  As stated chronic inflammation is linked to most chronic diseases including arthritis, many types of cancer, cardiovascular diseases, inflammatory bowel diseases, and diabetes.
Acute inflammation of the muscle cells, as understood in exercise physiology,[92] can result after induced eccentric and concentric muscle training.  Participation in eccentric training and conditioning, including resistance training and activities that emphasize eccentric lengthening of the muscle including downhill running on a moderate to high incline can result in considerable soreness within 24 to 48 hours, even though blood lactate levels, previously thought to cause muscle soreness, were much higher with level running.  This delayed onset muscle soreness (DOMS) results from structural damage to the contractile filaments and z-disks, which has been noted especially in marathon runners whose muscle fibers revealed remarkable damage to the muscle fibers after both training and marathon competition [93][citation needed].  The onset and timing of this gradient damage to the muscle parallels the degree of muscle soreness experienced by the runners.
Z-disks are the point of contact for the contractile proteins.  They provide structural support for transmission of force when muscle fibers are activated to shorten.  However, in marathon runners and those who subscribe to the overload principle to enhance their muscles, show moderate Z-disk streaming and major disruption of thick and thin filaments in parallel groups of sarcomeres as a result of the force of eccentric actions or stretching of tightened muscle fibers.
This disruption of muscle fibers triggers white blood cells to increase following induced muscle soreness, leading to the inflammatory response observation from induced muscle soreness. Elevations in plasma enzymes, myoglobinemia, and abnormal muscle histology and ultrastructure are concluded to be associated with inflammatory response. High tension in the contractile-elastic system of muscle results in structural damage to the muscle fiber and plasmalemma and its epimysium, perimysium, and/or endomysium.  The mysium damage disrupts calcium homeostasis in injured fibers and fiber bundles, resulting in necrosis that peaks about 48 hours after exercise.  The products of macrophage activity and intracellular contents (such as histamines, kinins, and K+) accumulate outside cells. These substances then stimulate free nerve endings in the muscle; a process that appears accentuated by eccentric exercise, in which large forces are distributed over a relatively small cross-sectional area of the muscle[93][citation needed].
There is a known relationship between inflammation and muscle growth.[94]  For instance, high doses of anti-inflammatory medicines (e.g., NSAIDs) are able to blunt muscle growth.[95][96][original research?] Cold therapy has been shown to negatively affect muscle growth as well. Reducing inflammation results in decreased macrophage activity and lower levels of IGF-1[97] Acute effects of cold therapy on training adaptations show reduced satellite cell proliferation.[98] Long term effects include less muscular hypertrophy and an altered cell structure of muscle fibers.[99]
It has been further theorized that the acute localized inflammatory responses to muscular contraction during exercise, as described above, are a necessary precursor to muscle growth.[100] As a response to muscular contractions, the acute inflammatory response initiates the breakdown and removal of damaged muscle tissue.[101] Muscles can synthesize cytokines in response to contractions,[102][103][104] such that the cytokines interleukin-1 beta (IL-1β), TNF-α, and IL-6 are expressed in skeletal muscle up to 5 days after exercise.[101]
In particular, the increase in levels of IL-6 (interleukin 6), a myokine, can reach up to one hundred times that of resting levels.[104] Depending on volume, intensity, and other training factors, the IL-6 increase associated with
training initiates about 4 hours after resistance training and remains elevated for up to 24 hours.[105][106][107]
These acute increases in cytokines, as a response to muscle contractions, help initiate the process of muscle repair and growth by activating satellite cells within the inflamed muscle. Satellite cells are crucial for skeletal muscle adaptation to exercise.[108] They contribute to hypertrophy by providing new myonuclei and repair damaged segments of mature myofibers for successful regeneration following injury- or exercise-induced muscle damage;[109][110][111] high-level powerlifters can have up to 100% more satellite cells than untrained controls.[112][113]
A rapid and transient localization of the IL-6 receptor and increased IL-6 expression occurs in satellite cells following contractions.[105] IL-6 has been shown to mediate
hypertrophic muscle growth both in vitro and in vivo.[108] Unaccustomed exercise can increase IL-6 by up to sixfold at 5 hours post-exercise and threefold 8 days after exercise.[114] Also telling is the fact that NSAIDs can decrease satellite cell response to exercise,[95] thereby reducing exercise-induced protein synthesis.[96]
The increase in cytokines (myokines) after resistance exercise coincides with the decrease in levels of myostatin, a protein that inhibits muscle differentiation and growth.[107] The cytokine response to resistance exercise and moderate-intensity running occur differently, with the latter causing a more prolonged response, especially at the 12–24 hour mark.[107]
Developing research has demonstrated that many of the benefits of exercise are mediated through the role of skeletal muscle as an endocrine organ. That is, contracting muscles release multiple substances known as myokines, including but not limited to those cited in the above description, which promote the growth of new tissue, tissue repair, and various anti-inflammatory functions, which in turn reduce the risk of developing various inflammatory diseases. The new view that muscle is an endocrine organ is transforming our understanding of exercise physiology and with it, of the role of inflammation in adaptation to stress.[115]
Both chronic and extreme inflammation are associated with disruptions of anabolic signals initiating muscle growth. Chronic inflammation has been implicated as part of the cause of the muscle loss that occurs with aging.[94][116] Increased protein levels of myostatin have been described in patients with diseases characterized by chronic low-grade inflammation.[117] Increased levels of TNF-α can suppress the AKT/mTOR pathway, a crucial pathway for regulating skeletal muscle hypertrophy,[118] thereby increasing muscle catabolism.[119][120][121] Cytokines may antagonize the anabolic effects of insulin-like growth factor 1 (IGF-1).[122][123] In the case of sepsis, an extreme whole body inflammatory state, the synthesis of both myofibrillar and sarcoplasmic proteins are inhibited, with the inhibition taking place preferentially in fast-twitch muscle fibers.[122][124] Sepsis is also able to prevent leucine from stimulating muscle protein synthesis.[102] In animal models, when inflammation is created, mTOR loses its ability to be stimulated by muscle growth.[125]
Regular physical activity is reported to decrease markers of inflammation,[126][127][128] although the correlation is imperfect and seems to reveal differing results contingent upon training intensity. For instance, while baseline measurements of circulating inflammatory markers do not seem to differ greatly between healthy trained and untrained adults,[129][130] long-term training may help reduce chronic low-grade inflammation.[131] On the other hand, levels of the anti-inflammatory myokine IL-6 (interleukin 6) remained elevated longer into the recovery period following an acute bout of exercise in patients with inflammatory diseases, relative to the recovery of healthy controls.[131] It may well be that low-intensity training can reduce resting pro-inflammatory markers (CRP, IL-6), while moderate-intensity training has milder and less-established anti-inflammatory benefits.[129][132][133][134] There is a strong relationship between exhaustive exercise and chronic low-grade inflammation.[135] Marathon running may enhance IL-6 levels as much as 100 times over normal and increases total leuckocyte count and neturophil mobilization.[135]
Regarding the above, IL-6 had previously been classified as a proinflammatory cytokine. Therefore, it was first thought that the exercise-induced IL-6 response was related to muscle damage.[136] However, it has become evident that eccentric exercise is not associated with a larger increase in plasma IL-6 than exercise involving concentric “nondamaging” muscle contractions. This finding clearly demonstrates that muscle damage is not required to provoke an increase in plasma IL-6 during exercise. As a matter of fact, eccentric exercise may result in a delayed peak and a much slower decrease of plasma IL-6 during recovery.[137]
Recent work has shown that both upstream and downstream signalling pathways for IL-6 differ markedly between myocytes and macrophages. It appears that unlike IL-6 signalling in macrophages, which is dependent upon activation of the NFκB signalling pathway, intramuscular IL-6 expression is regulated by a network of signalling cascades, including the Ca2+/NFAT and glycogen/p38 MAPK pathways. Thus, when IL-6 is signalling in monocytes or macrophages, it creates a pro-inflammatory response, whereas IL-6 activation and signalling in muscle is totally independent of a preceding TNF-response or NFκB activation, and is anti-inflammatory.[138]
Several studies show that markers of inflammation are reduced following longer-term behavioural changes involving both reduced energy intake and a regular program of increased physical activity, and that, in particular, IL-6 was miscast as an inflammatory marker. For example, the anti-inflammatory effects of IL-6 have been demonstrated by IL-6 stimulating the production of the classical anti-inflammatory cytokines IL-1ra and IL-10.[138] As such, individuals pursuing exercise as a means to treat the causal factors underlying chronic inflammation are pursuing a course of action strongly supported by current research, as an inactive lifestyle is strongly associated with the development and progression of multiple inflammatory diseases. Note that cautions regarding over-exertion may apply in certain cases, as discussed above, though this concern rarely applies to the general population.
Given that localized acute inflammation is a necessary component for muscle growth, and that chronic low-grade inflammation is associated with a disruption of anabolic signals initiating muscle growth, it has been theorized that a signal-to-noise model may best describe the relationship between inflammation and muscle growth.[139] By keeping the "noise" of chronic inflammation to a minimum, the localized acute inflammatory response signals a stronger anabolic response than could be achieved with higher levels of chronic inflammation.



Hormone - Wikipedia
A hormone (from the Greek participle “ὁρμῶ”, "to set in motion, urge on") is any member of a class of signaling molecules produced by  glands in multicellular organisms that are transported by the circulatory system to target distant organs to regulate physiology and behaviour.  Hormones have diverse chemical structures, mainly of 3 classes: eicosanoids, steroids, and amino acid/protein derivatives (amines, peptides, and proteins). The glands that secrete hormones comprise the endocrine signaling system. The term hormone is sometimes extended to include chemicals produced by cells that affect the same cell (autocrine or intracrine signalling) or nearby cells (paracrine signalling).
Hormones are used to communicate between organs and tissues for physiological regulation and behavioral activities, such as digestion, metabolism, respiration, tissue function, sensory perception, sleep, excretion, lactation, stress, growth and development, movement, reproduction, and mood.[1][2]  Hormones affect distant cells by binding to specific receptor proteins in the target cell resulting in a change in cell function. When a hormone binds to the receptor, it results in the activation of a signal transduction pathway that typically activates gene transcription resulting in increased expression of target proteins; non-genomic effects are more rapid, and can be synergistic with genomic effects.[3] Amino acid–based hormones (amines and peptide or protein hormones) are water-soluble and act on the surface of target cells via second messengers; steroid hormones, being lipid-soluble, move through the plasma membranes of target cells (both cytoplasmic and nuclear) to act within their nuclei.
Hormone secretion may occur in many tissues. Endocrine glands are the cardinal example, but specialized cells in various other organs also secrete hormones. Hormone secretion occurs in response to specific biochemical signals from a wide range of regulatory systems. For instance, serum calcium concentration affects parathyroid hormone synthesis; blood sugar (serum glucose concentration) affects insulin synthesis; and because the outputs of the stomach and exocrine pancreas (the amounts of gastric juice and pancreatic juice) become the input of the small intestine, the small intestine secretes hormones to stimulate or inhibit the stomach and pancreas based on how busy it is. Regulation of hormone synthesis of gonadal hormones, adrenocortical hormones, and thyroid hormones is often dependent on complex sets of direct influence and feedback interactions involving the hypothalamic-pituitary-adrenal (HPA), -gonadal (HPG), and -thyroid (HPT) axes.
Upon secretion, certain hormones, including protein hormones and catecholamines, are water-soluble and are thus readily transported through the circulatory system. Other hormones, including steroid and thyroid hormones, are lipid-soluble; to allow for their widespread distribution, these hormones must bond to carrier plasma glycoproteins (e.g., thyroxine-binding globulin (TBG)) to form ligand-protein complexes. Some hormones are completely active when released into the bloodstream (as is the case for insulin and growth hormones), while others are prohormones that must be activated in specific cells through a series of activation steps that are commonly highly regulated. The endocrine system secretes hormones directly into the bloodstream, typically via fenestrated capillaries, whereas the exocrine system secretes its hormones indirectly using ducts. Hormones with paracrine function diffuse through the interstitial spaces to nearby target tissue.
Hormonal signaling involves the following steps:[4]
Hormone producing cells are typically of a specialized cell type, residing within a particular endocrine gland, such as the thyroid gland, ovaries, and testes. Hormones exit their cell of origin via exocytosis or another means of membrane transport. The hierarchical model is an oversimplification of the hormonal signaling process. Cellular recipients of a particular hormonal signal may be one of several cell types that reside within a number of different tissues, as is the case for insulin, which triggers a diverse range of systemic physiological effects. Different tissue types may also respond differently to the same hormonal signal.
The rate of hormone biosynthesis and secretion is often regulated by a homeostatic negative feedback control mechanism. Such a mechanism depends on factors that influence the metabolism and excretion of hormones. Thus, higher hormone concentration alone cannot trigger the negative feedback mechanism. Negative feedback must be triggered by overproduction of an "effect" of the hormone.
Hormone secretion can be stimulated and inhibited by:
One special group of hormones is the tropic hormones that stimulate the hormone production of other endocrine glands.  For example, thyroid-stimulating hormone (TSH) causes growth and increased activity of another endocrine gland, the thyroid, which increases output of thyroid hormones.
To release active hormones quickly into the circulation, hormone biosynthetic cells may produce and store biologically inactive hormones in the form of pre- or prohormones. These can then be quickly converted into their active hormone form in response to a particular stimulus.
Eicosanoids are considered to act as local hormones. They are considered to be "local" because they possess specific effects on target cells close to their site of formation. They also have a rapid degradation cycle, making sure they do not reach distant sites within the body.[5]
 Most hormones initiate a cellular response by initially binding to either cell membrane associated or intracellular receptors. A cell may have several different receptor types that recognize the same hormone but activate different signal transduction pathways, or a cell may have several different receptors that recognize different hormones and activate the same biochemical pathway.
Receptors for most peptide as well as many eicosanoid hormones are embedded in the plasma membrane at the surface of the cell and the majority of these receptors belong to the G protein-coupled receptor (GPCR) class of seven alpha helix transmembrane proteins.  The interaction of hormone and receptor typically triggers a cascade of secondary effects within the cytoplasm of the cell, often involving phosphorylation or dephosphorylation of various other cytoplasmic proteins, changes in ion channel permeability, or increased concentrations of intracellular molecules that may act as secondary messengers (e.g., cyclic AMP). Some protein hormones also interact with intracellular receptors located in the cytoplasm or nucleus by an intracrine mechanism.
For steroid or thyroid hormones, their receptors are located inside the cell within the cytoplasm of the target cell. These receptors belong to the  nuclear receptor family of ligand-activated transcription factors. To bind their receptors, these hormones must first cross the cell membrane. They can do so because they are lipid-soluble. The combined hormone-receptor complex then moves across the nuclear membrane into the nucleus of the cell, where it binds to specific DNA sequences, regulating the expression of certain genes, and thereby increasing the levels of the proteins encoded by these genes.[6] However, it has been shown that not all steroid receptors are located inside the cell. Some are associated with the plasma membrane.[7]
Hormones have the following effects on the body:
A hormone may also regulate the production and release of other hormones. Hormone signals control the internal environment of the body through homeostasis.
As hormones are defined functionally, not structurally, they may have diverse chemical structures. Hormones occur in multicellular organisms (plants, animals, fungi, brown algae and red algae). These compounds occur also in unicellular organisms, and may act as signaling molecules,[8][9] but there is no consensus if, in this case, they can be called hormones.
Vertebrate hormones fall into three main chemical classes:
Compared with vertebrates, insects and crustaceans possess a number of structurally unusual hormones such as the juvenile hormone, a sesquiterpenoid.[12]
Plant hormones include abscisic acid, auxin, cytokinin, ethylene, and gibberellin.
Many hormones and their structural and functional analogs are used as medication. The most commonly prescribed hormones are estrogens and progestogens (as methods of hormonal contraception and as HRT),[13] thyroxine (as levothyroxine, for hypothyroidism) and steroids (for autoimmune diseases and several respiratory disorders). Insulin is used by many diabetics. Local preparations for use in otolaryngology often contain pharmacologic equivalents of adrenaline, while steroid and vitamin D creams are used extensively in dermatological practice.
A "pharmacologic dose" or "supraphysiological dose" of a hormone is a medical usage referring to an amount of a hormone far greater than naturally occurs in a healthy body. The effects of pharmacologic doses of hormones may be different from responses to naturally occurring amounts and may be therapeutically useful, though not without potentially adverse side effects. An example is the ability of pharmacologic doses of glucocorticoids to suppress inflammation.
At the neurological level, behavior can be inferred based on: hormone concentrations; hormone-release patterns; the numbers and locations of hormone receptors; and the efficiency of hormone receptors for those involved in gene transcription. Not only do hormones influence behavior, but also behavior and the environment influence hormones. Thus, a feedback loop is formed. For example, behavior can affect hormones, which in turn can affect behavior, which in turn can affect hormones, and so on.
Three broad stages of reasoning may be used when determining hormone-behavior interactions:
There are various clear distinctions between hormones and neurotransmitters:
Hormone transport and the involvement of binding proteins is an essential aspect when considering the function of hormones. There are several benefits with the formation of a complex with a binding protein: the effective half-life of the bound hormone is increased; a reservoir of bound hormones is created, which evens the variations in concentration of unbound hormones (bound hormones will replace the unbound hormones when these are eliminated).[14]
The discovery of hormones and endocrine signaling occurred during studies of how the digestive system regulates its activities, as explained at Secretin § Discovery.



Endometrium - Wikipedia
The endometrium is the inner epithelial layer, along with its mucous membrane, of the mammalian uterus.  It has a basal layer and a functional layer; the functional layer thickens and then is shed during menstruation in humans, as well as some other mammals including apes, Old World monkeys, some species of bat, and the elephant shrew.[1] In most other mammals the endometrium is reabsorbed in the estrous cycle.  During pregnancy, the glands and blood vessels in the endometrium further increase in size and number. Vascular spaces fuse and become interconnected, forming the placenta, which supplies oxygen and nutrition to the embryo and fetus.[2][3] The speculated presence of an endometrial microbiota[4]
has been argued against.[5][6]
The endometrium consists of a single layer of columnar epithelium plus the stroma on which it rests. The stroma is a layer of connective tissue that varies in thickness according to hormonal influences. In the uterus, simple tubular glands reach from the endometrial surface through to the base of the stroma, which also carries a rich blood supply provided by the spiral arteries. In a woman of reproductive age, two layers of endometrium can be distinguished. These two layers occur only in the endometrium lining the cavity of the uterus, and not in the lining of the Fallopian tubes.[2][3]
In the absence of progesterone, the arteries supplying blood to the functional layer constrict, so that cells in that layer become ischaemic and die, leading to menstruation.
It is possible to identify the phase of the menstrual cycle by reference to either the ovarian cycle or the uterine cycle by observing microscopic differences at each phase—for example in the ovarian cycle:
About 20,000 protein coding genes are expressed in human cells and some 70% of these genes are expressed in the normal endometrium.[7][8] Just over 100 of these genes are more specifically expressed in the endometrium with only a handful genes being highly endometrium specific. The corresponding specific proteins are expressed in the glandular and stromal cells of the endometrial mucosa. The expression of many of these proteins vary depending on the menstrual cycle, for example the progestorone receptor and thyrotropin-releasing hormone both expressed in the proliferative phase, and PAEP expressed in the secretory phase. Other proteins such as the HOX11 protein that is required for female fertility, is expressed in endometrial stroma cells throughout the menstrual cycle. Certain specific proteins such as the estrogen receptor are also expressed in other types of female tissue types, such as the cervix, fallopian tubes, ovaries and breast.[9]
The uterus and endometrium was for a long time thought to be sterile. The cervical plug of mucosa was seen to prevent the entry of any microorganisms ascending from the vagina. In the 1980s this view was challenged when it was shown that uterine infections could arise from weaknesses in the barrier of the cervical plug. Organisms from the vaginal microbiota could enter the uterus during uterine contractions in the menstrual cycle. Further studies sought to identify microbiota specific to the uterus which would be of help in identifying cases of unsuccessful IVF and miscarriages. Their findings were seen to be unreliable due to the possibility of cross-contamination in the sampling procedures used. The well-documented presence of Lactobacillus species, for example, was easily explained by an increase in the vaginal population being able to seep into the cervical mucous.[5] Another study highlighted the flaws of the earlier studies including cross-contamination. It was also argued that the evidence from studies using germ-free offspring of axenic animals (germ-free) clearly showed the sterility of the uterus. The authors concluded that in light of these findings there was no existence of a microbiome.[6]
The normal dominance of Lactobacilli in the vagina is seen as a marker for vaginal health. However, in the uterus this much lower population is seen as invasive in a closed environment that is highly regulated by female sex hormones, and that could have unwanted consequences. In studies of endometriosis Lactobacillus is not the dominant type and there are higher levels of Streptococcus and Staphylococcus species. Half of the cases of bacterial vaginitis showed a polymicrobial biofilm attached to the endometrium.[5]
The endometrium is the innermost layer of ovary (female reproductive organ) and functions as a lining for the uterus, preventing adhesions between the opposed walls of the myometrium, thereby maintaining the patency of the uterine cavity. During the menstrual cycle or estrous cycle, the endometrium grows to a thick, blood vessel-rich, glandular tissue layer. This represents an optimal environment for the implantation of a blastocyst upon its arrival in the uterus. The endometrium is central, echogenic (detectable using ultrasound scanners), and has an average thickness of 6.7 mm.
During pregnancy, the glands and blood vessels in the endometrium further increase in size and number. Vascular spaces fuse and become interconnected, forming the placenta, which supplies oxygen and nutrition to the embryo and fetus.
The endometrial lining undergoes cyclic regeneration. Humans, apes, and some other species display the menstrual cycle, whereas most other mammals are subject to an estrous cycle.[1] In both cases, the endometrium initially proliferates under the influence of estrogen. However, once ovulation occurs, the ovary (specifically the corpus luteum) will produce much larger amounts of progesterone.  This changes the proliferative pattern of the endometrium to a secretory lining. Eventually, the secretory lining provides a hospitable environment for one or more blastocysts.
Upon fertilization, the egg may implant into the uterine wall and provide feedback to the body with human chorionic gonadotropin (HCG). HCG provides continued feedback throughout pregnancy by maintaining the corpus luteum, which will continue its role of releasing progesterone and estrogen. The endometrial lining is either reabsorbed (estrous cycle) or shed (menstrual cycle). In the latter case, the process of shedding involves the breaking down of the lining, the tearing of small connective blood vessels, and the loss of the tissue and blood that had constituted it through the vagina. The entire process occurs over a period of several days. Menstruation may be accompanied by a series of uterine contractions; these help expel the menstrual endometrium.
In case of implantation, however, the endometrial lining is neither absorbed nor shed. Instead, it remains as decidua. The decidua becomes part of the placenta; it provides support and protection for the gestation.
If there is inadequate stimulation of the lining, due to lack of hormones, the endometrium remains thin and inactive. In humans, this will result in amenorrhea, or the absence of a menstrual period. After menopause, the lining is often described as being atrophic.  In contrast, endometrium that is chronically exposed to estrogens, but not to progesterone, may become hyperplastic. Long-term use of oral contraceptives with highly potent progestins can also induce endometrial atrophy.[10][11]
In humans, the cycle of building and shedding the endometrial lining lasts an average of 28 days. The endometrium develops at different rates in different mammals. Various factors including the seasons, climate, and stress can affect its development. The endometrium itself produces certain hormones at different stages of the cycle and this affects other parts of the reproductive system.
Chorionic tissue can result in marked endometrial changes, known as an Arias-Stella reaction, that have an appearance similar to cancer.[12]  Historically, this change was diagnosed as endometrial cancer and it is important only in so far as it should not be misdiagnosed as cancer.
Thin endometrium may be defined as an endometrial thickness of less than 8 mm. It usually occurs after menopause. Treatments that can improve endometrial thickness include Vitamin E, L-arginine and sildenafil citrate.[13]
Gene expression profiling using cDNA microarray can be used for the diagnosis of endometrial disorders.[14]
The European Menopause and Andropause Society (EMAS) released Guidelines with detailed information to assess the endometrium.
[15]

An endometrial thickness of less than 7 mm decreases the pregnancy rate in in vitro fertilization by an odds ratio of approximately 0.4 compared to an EMT of over 7 mm. However, such low thickness rarely occurs, and any routine use of this parameter is regarded as not justified.[16]
Observation of the endometrium by transvaginal ultrasonography is used when administering fertility medication, such as in in vitro fertilization. At the time of embryo transfer, it is favorable to have an endometrium of a thickness of between 7 and 14 mm with a triple-line configuration,[17] which means that the endometrium contains a hyperechoic (usually displayed as light) line in the middle surrounded by two more hypoechoic (darker) lines. A triple-line endometrium reflects the separation of the stratum basalis and functionalis layers, and is also observed in the periovulatory period secondary to rising estradiol levels, and disappears after ovulation.[18]
Endometrioid adenocarcinoma from biopsy. H&E stain.
Micrograph of decidualized endometrium due to exogenous progesterone. H&E stain.
Micrograph of decidualized endometrium due to exogenous progesterone. H&E stain.
Micrograph showing endometrial stromal condensation, a finding seen in menses.



Coeliac disease - Wikipedia

Coeliac disease, spelled celiac disease, is a long-term autoimmune disorder that primarily affects the small intestine.[10] Classic symptoms include gastrointestinal problems such as chronic diarrhoea, abdominal distention, malabsorption, loss of appetite and among children failure to grow normally.[1] This often begins between six months and two years of age.[1] Non-classic symptoms are more common, especially in people older than two years.[8][15][16][17] There may be mild or absent gastrointestinal symptoms, a wide number of symptoms involving any part of the body or no obvious symptoms.[1] Coeliac disease was first described in childhood;[6][8] however, it may develop at any age.[1][8] It is associated with other autoimmune diseases, such as diabetes mellitus type 1 and thyroiditis, among others.[6]
Coeliac disease is caused by a reaction to gluten, which are various proteins found in wheat and in other grains such as barley and rye.[9][18][19] Moderate quantities of oats, free of contamination with other gluten-containing grains, are usually tolerated.[18][20] The occurrence of problems may depend on the variety of oat.[18][21] It occurs in people who are genetically predisposed.[10] Upon exposure to gluten, an abnormal immune response may lead to the production of several different autoantibodies that can affect a number of different organs.[4][22] In the small bowel, this causes an inflammatory reaction and may produce shortening of the villi lining the small intestine (villous atrophy).[10][11] This affects the absorption of nutrients, frequently leading to anaemia.[10][19]
Diagnosis is typically made by a combination of blood antibody tests and intestinal biopsies, helped by specific genetic testing.[10] Making the diagnosis is not always straightforward.[23] Frequently, the autoantibodies in the blood are negative,[24][25] and many people have only minor intestinal changes with normal villi.[16][26] People may have severe symptoms and be investigated for years before a diagnosis is achieved.[27] Increasingly, the diagnosis is being made in people without symptoms, as a result of screening.[28] Evidence regarding the effects of screening, however, is not sufficient to determine its usefulness.[29] While the disease is caused by a permanent intolerance to wheat proteins, it is not a form of wheat allergy.[10]
The only known effective treatment is a strict lifelong gluten-free diet, which leads to recovery of the intestinal mucosa, improves symptoms and reduces risk of developing complications in most people.[13] If untreated, it may result in cancers such as intestinal lymphoma and a slightly increased risk of early death.[3] Rates vary between different regions of the world, from as few as 1 in 300 to as many as 1 in 40, with an average of between 1 in 100 and 1 in 170 people.[14] In developed countries, it is estimated that 80% of cases remain undiagnosed, usually because of minimal or absent gastrointestinal complaints and poor awareness of the condition.[5][30] Coeliac disease is slightly more common in women than in men.[31] The term "coeliac" is from the Greek κοιλιακός (koiliakós, "abdominal") and was introduced in the 19th century in a translation of what is generally regarded as an Ancient Greek description of the disease by Aretaeus of Cappadocia.[32][33]
The classic symptoms of untreated coeliac disease include pale, loose, and greasy stool (steatorrhoea), and weight loss or failure to gain weight. Other common symptoms may be subtle or primarily occur in organs other than the bowel itself.[34] It is also possible to have coeliac disease without any of the classic symptoms at all.[19] This has been shown to comprise at least 43% of presentations in children.[35] Further, many adults with subtle disease may only present with fatigue or anaemia.[28] Many undiagnosed individuals who consider themselves asymptomatic are in fact not, but rather have become accustomed to living in a state of chronically compromised health. Indeed, after starting a gluten-free diet and subsequent improvement becomes evident, such individuals are often able to retrospectively recall and recognise prior symptoms of their untreated disease which they had mistakenly ignored.[5][27][30]
The diarrhoea that is characteristic of coeliac disease is chronic, pale, of large volume, and abnormally bad smelling. Abdominal pain and cramping, bloatedness with abdominal distension (thought to be due to fermentative production of bowel gas), and mouth ulcers[36] may be present. As the bowel becomes more damaged, a degree of lactose intolerance may develop.[19] Frequently, the symptoms are ascribed to irritable bowel syndrome (IBS), only later to be recognised as coeliac disease; a small proportion of people with symptoms of IBS have underlying coeliac disease, and screening for coeliac disease is recommended for those with IBS symptoms.[37]
Coeliac disease leads to an increased risk of both adenocarcinoma and lymphoma of the small bowel (enteropathy-associated T-cell lymphoma (EATL) or other non-Hodgkin's lymphomas).[38] This risk is also higher in first-degree relatives such as siblings, parents and children. Whether or not a gluten-free diet brings this risk back to baseline is not clear.[39] Long-standing and untreated disease may lead to other complications, such as ulcerative jejunitis (ulcer formation of the small bowel) and stricturing (narrowing as a result of scarring with obstruction of the bowel).[40]
The changes in the bowel make it less able to absorb nutrients, minerals, and the fat-soluble vitamins A, D, E, and K.[19][41]
Coeliac disease has been linked with a number of conditions. In many cases, it is unclear whether the gluten-induced bowel disease is a causative factor or whether these conditions share a common predisposition.
Coeliac disease is associated with a number of other medical conditions, many of which are autoimmune disorders: diabetes mellitus type 1, hypothyroidism, primary biliary cholangitis, microscopic colitis, gluten ataxia, psoriasis, vitiligo, autoimmune hepatitis, dermatitis herpetiformis, primary sclerosing cholangitis, and more.[4]
Coeliac disease is caused by a reaction to gliadins and glutenins (gluten proteins)[48] found in wheat, and similar proteins found in the crops of the tribe Triticeae (which includes other common grains such as barley and rye)[19] and the tribe Aveneae (oats).[49] Wheat subspecies (such as spelt, durum and Kamut) and wheat hybrids (such as triticale) also induce symptoms of coeliac disease.[49][50]
A small number of people with coeliac react to oats.[19] Oats toxicity in coeliac people depends on the oat cultivar consumed because of prolamin genes, protein amino acid sequences, and the immunoreactivities of toxic prolamins, which are different among oat varieties.[21][51] Also, oats are frequently cross-contaminated with other grains containing gluten.[21][51][52] "Pure oat" refers to oats uncontaminated with other gluten-containing cereals.[21] The long-term effects of pure oats consumption are still unclear[53] and further studies identifying the cultivars used are needed before making final recommendations on their inclusion in the gluten-free diet.[52] Coeliac people who choose to consume oats need a more rigorous lifelong follow-up, possibly including periodic performance of intestinal biopsies.[53]
Other cereals such as corn, millet, sorghum, teff, rice, and wild rice are safe for people with coeliac to consume, as well as noncereals such as amaranth, quinoa, and buckwheat.[50][54] Noncereal carbohydrate-rich foods such as potatoes and bananas do not contain gluten and do not trigger symptoms.[50]
There are various theories as to what determines whether a genetically susceptible individual will go on to develop coeliac disease. Major theories include surgery, pregnancy, infection and emotional stress.[55]
The eating of gluten early in a baby's life does not appear to increase the risk of CD but later introduction after 6 months may increase it.[56][57] There is uncertainty whether breastfeeding reduces risk. Prolonging breastfeeding until the introduction of gluten-containing grains into the diet appears to be associated with a 50% reduced risk of developing coeliac disease in infancy; whether this persists into adulthood is not clear.[58] These factors may just influence the timing of onset.[59]
Coeliac disease appears to be multifactorial, both in that more than one genetic factor can cause the disease and in that more than one factor is necessary for the disease to manifest in a person.
Almost all people (95%) with coeliac disease have either the variant HLA-DQ2 allele or (less commonly) the HLA-DQ8 allele.[28][60] However, about 20–30% of people without coeliac disease have also inherited either of these alleles.[61] This suggests additional factors are needed for coeliac disease to develop; that is, the predisposing HLA risk allele is necessary but not sufficient to develop coeliac disease. Furthermore, around 5% of those people who do develop coeliac disease do not have typical HLA-DQ2 or HLA-DQ8 alleles (see below).[28]
The vast majority of people with coeliac have one of two types of the HLA-DQ protein.[61] HLA-DQ is part of the MHC class II antigen-presenting receptor (also called the human leukocyte antigen) system and distinguishes cells between self and non-self for the purposes of the immune system. The two subunits of the HLA-DQ protein are encoded by the HLA-DQA1 and HLA-DQB1 genes, located on the short arm of the sixth chromosome.
There are seven HLA-DQ variants (DQ2 and DQ4–DQ9). Over 95% of people with coeliac have the isoform of DQ2 or DQ8, which is inherited in families. The reason these genes produce an increase in risk of coeliac disease is that the receptors formed by these genes bind to gliadin peptides more tightly than other forms of the antigen-presenting receptor. Therefore, these forms of the receptor are more likely to activate T lymphocytes and initiate the autoimmune process.[28]
Most people with coeliac bear a two-gene HLA-DQ2 haplotype referred to as DQ2.5 haplotype. This haplotype is composed of two adjacent gene alleles, DQA1*0501 and DQB1*0201, which encode the two subunits, DQ α5 and DQ β2. In most individuals, this DQ2.5 isoform is encoded by one of two chromosomes 6 inherited from parents (DQ2.5cis). Most coeliacs inherit only one copy of this DQ2.5 haplotype, while some inherit it from both parents; the latter are especially at risk for coeliac disease as well as being more susceptible to severe complications.[63]
Some individuals inherit DQ2.5 from one parent and an additional portion of the haplotype (either DQB1*02 or DQA1*05) from the other parent, increasing risk. Less commonly, some individuals inherit the DQA1*05 allele from one parent and the DQB1*02 from the other parent (DQ2.5trans) (called a trans-haplotype association), and these individuals are at similar risk for coeliac disease as those with a single DQ2.5-bearing chromosome 6, but in this instance disease tends not to be familial. Among the 6% of European coeliacs that do not have DQ2.5 (cis or trans) or DQ8 (encoded by the haplotype DQA1*03:DQB1*0302), 4% have the DQ2.2 isoform, and the remaining 2% lack DQ2 or DQ8.[64]
The frequency of these genes varies geographically. DQ2.5 has high frequency in peoples of North and Western Europe (Basque Country and Ireland[65] with highest frequencies) and portions of Africa and is associated with disease in India,[66] but it is not found along portions of the West Pacific rim. DQ8 has a wider global distribution than DQ2.5 and is particularly common in South and Central America; up to 90% of individuals in certain Amerindian populations carry DQ8 and thus may display the coeliac phenotype.[67]
Other genetic factors have been repeatedly reported in coeliac disease; however, involvement in disease has variable geographic recognition. Only the HLA-DQ loci show a consistent involvement over the global population.[68] Many of the loci detected have been found in association with other autoimmune diseases. One locus, the LPP or lipoma-preferred partner gene, is involved in the adhesion of extracellular matrix to the cell surface, and a minor variant (SNP = rs1464510) increases the risk of disease by approximately 30%. This gene strongly associates with coeliac disease (p < 10−39) in samples taken from a broad area of Europe and the US.[68]
The prevalence of coeliac disease genotypes in the modern population is not completely understood. Given the characteristics of the disease and its apparent strong heritability, it would normally be expected that the genotypes would undergo negative selection and to be absent in societies where agriculture has been practised the longest (compare with a similar condition, Lactose intolerance, which has been negatively selected so strongly that its prevalence went from ~100% in ancestral populations to less than 5% in some European countries). This expectation was first proposed by Simoons (1981).[69] By now, however, it is apparent that this is not the case; on the contrary, there is evidence of positive selection in coeliac disease genotypes. It is suspected that some of them may have been beneficial by providing protection against bacterial infections.[70][71]
The majority of the proteins in food responsible for the immune reaction in coeliac disease are the prolamins. These are storage proteins rich in proline (prol-) and glutamine (-amin) that dissolve in alcohols and are resistant to proteases and peptidases of the gut.[28][72] Prolamins are found in cereal grains with different grains having different but related prolamins: wheat (gliadin), barley (hordein), rye (secalin) and oats (avenin).[49] One region of α-gliadin stimulates membrane cells, enterocytes, of the intestine to allow larger molecules around the sealant between cells. Disruption of tight junctions allow peptides larger than three amino acids to enter the intestinal lining.[73]
Membrane leaking permits peptides of gliadin that stimulate two levels of immune response, the innate response and the adaptive (T-helper cell mediated) response. One protease-resistant peptide from α-gliadin contains a region that stimulates lymphocytes and results in the release of interleukin-15. This innate response to gliadin results in immune-system signalling that attracts inflammatory cells and increases the release of inflammatory chemicals.[28] The strongest and most common adaptive response to gliadin is directed toward an α2-gliadin fragment of 33 amino acids in length.[28]
The response to the 33mer occurs in most coeliacs who have a DQ2 isoform. This peptide, when altered by intestinal transglutaminase, has a high density of overlapping T-cell epitopes. This increases the likelihood that the DQ2 isoform will bind and stay bound to peptide when recognised by T-cells.[74] Gliadin in wheat is the best-understood member of this family, but other prolamins exist, and hordein (from barley), secalin (from rye), and avenin (from oats) may contribute to coeliac disease.[28][49][75] Avenins toxicity in people with coeliac disease depends on the oat cultivar consumed because of prolamin genes, protein amino acid sequences, and the immunoreactivities of toxic prolamins, which vary among oat varieties.[21]
Anti-transglutaminase antibodies to the enzyme tissue transglutaminase (tTG) are found in the blood of the majority of people with classic symptoms and complete villous atrophy, but only in 70% of the cases with partial villous atrophy and 30% of the cases with minor mucosal lesions.[24] Tissue transglutaminase modifies gluten peptides into a form that may stimulate the immune system more effectively.[28] These peptides are modified by tTG in two ways, deamidation or transamidation.[76]
Deamidation is the reaction by which a glutamate residue is formed by cleavage of the epsilon-amino group of a glutamine side chain. Transamidation, which occurs three times more often than deamidation, is the cross-linking of a glutamine residue from the gliadin peptide to a lysine residue of tTg in a reaction that is catalysed by the transglutaminase. Crosslinking may occur either within or outside the active site of the enzyme. The latter case yields a permanently covalently linked complex between the gliadin and the tTg.[77] This results in the formation of new epitopes believed to trigger the primary immune response by which the autoantibodies against tTg develop.[78][79][80]
Stored biopsies from people with suspected coeliac disease have revealed that autoantibody deposits in the subclinical coeliacs are detected prior to clinical disease. These deposits are also found in people who present with other autoimmune diseases, anaemia, or malabsorption phenomena at a much increased rate over the normal population.[81] Endomysial components of antibodies (EMA) to tTG are believed to be directed toward cell-surface transglutaminase, and these antibodies are still used in confirming a coeliac disease diagnosis. However, a 2006 study showed that EMA-negative people with coeliac tend to be older males with more severe abdominal symptoms and a lower frequency of "atypical" symptoms, including autoimmune disease.[82] In this study, the anti-tTG antibody deposits did not correlate with the severity of villous destruction. These findings, coupled with recent work showing that gliadin has an innate response component,[83] suggest that gliadin may be more responsible for the primary manifestations of coeliac disease, whereas tTG is a bigger factor in secondary effects such as allergic responses and secondary autoimmune diseases. In a large percentage of people with coeliac, the anti-tTG antibodies also recognise a rotavirus protein called VP7. These antibodies stimulate monocyte proliferation, and rotavirus infection might explain some early steps in the cascade of immune cell proliferation.[84]
Indeed, earlier studies of rotavirus damage in the gut showed this causes a villous atrophy.[85] This suggests that viral proteins may take part in the initial flattening and stimulate self-crossreactive anti-VP7 production. Antibodies to VP7 may also slow healing until the gliadin-mediated tTG presentation provides a second source of crossreactive antibodies.
Other intestinal disorders may have biopsy that look like coeliac disease including lesions caused by Candida.[86]
The inflammatory process, mediated by T cells, leads to disruption of the structure and function of the small bowel's mucosal lining and causes malabsorption as it impairs the body's ability to absorb nutrients, minerals, and fat-soluble vitamins A, D, E, and K from food. Lactose intolerance may be present due to the decreased bowel surface and reduced production of lactase but typically resolves once the condition is treated.
Alternative causes of this tissue damage have been proposed and involve release of interleukin 15 and activation of the innate immune system by a shorter gluten peptide (p31–43/49). This would trigger killing of enterocytes by lymphocytes in the epithelium.[28] The villous atrophy seen on biopsy may also be due to unrelated causes, such as tropical sprue, giardiasis and radiation enteritis. While positive serology and typical biopsy are highly suggestive of coeliac disease, lack of response to diet may require these alternative diagnoses to be considered.[40]
Diagnosis is often very difficult so that most cases are diagnosed with great delay.[23] There are several tests that can be used. The level of symptoms may determine the order of the tests, but all tests lose their usefulness if the person is already eating a gluten-free diet. Intestinal damage begins to heal within weeks of gluten being removed from the diet, and antibody levels decline over months. For those who have already started on a gluten-free diet, it may be necessary to perform a rechallenge with some gluten-containing food in one meal a day over 6 weeks before repeating the investigations.[22]
Serological blood tests are the first-line investigation required to make a diagnosis of coeliac disease. Its sensitivity correlates with the degree of histological lesions. People who present minor damage of the small intestine may have seronegative findings so many patients with coeliac disease often are missed. In patients with villous atrophy, anti-endomysial (EMA) antibodies of the immunoglobulin A (IgA) type can detect coeliac disease with a sensitivity and specificity of 90% and 99%, respectively.[87] Serology for anti-transglutaminase antibodies (anti-tTG) was initially reported to have a higher sensitivity (99%) and specificity (>90%). However, it is now thought to have similar characteristics to anti-endomysial antibody.[87] Both anti-transglutaminase and anti-endomysial antibodies have high sensitivity to diagnose people with classic symptoms and complete villous atrophy, but they are only found in 30–89% of the cases with partial villous atrophy and in less than 50% of the people who have minor mucosal lesions (duodenal lymphocytosis) with normal villi.[24][25]
Tissue transglutaminase modifies gluten peptides into a form that may stimulate the immune system more effectively.[28] These peptides are modified by tTG in two ways, deamidation or transamidation.[76] Modern anti-tTG assays rely on a human recombinant protein as an antigen.[88] tTG testing should be done first as it is an easier test to perform. An equivocal result on tTG testing should be followed by anti-endomysial antibodies.[22]
Guidelines recommend that a total serum IgA level is checked in parallel, as people with coeliac with IgA deficiency may be unable to produce the antibodies on which these tests depend ("false negative"). In those people, IgG antibodies against transglutaminase (IgG-tTG) may be diagnostic.[22][89]
If all these antibodies are negative, then it should be determined anti-DGP antibodies (antibodies against deamidated gliadin peptides). IgG class anti-DGP antibodies may be useful in people with IgA deficiency. In children younger than two years, anti-DGP antibodies perform better than anti-endomysial and anti-transglutaminase antibodies tests.[8]
Because of the major implications of a diagnosis of coeliac disease, professional guidelines recommend that a positive blood test is still followed by an endoscopy/gastroscopy and biopsy. A negative serology test may still be followed by a recommendation for endoscopy and duodenal biopsy if clinical suspicion remains high.[22][40][90]
Historically three other antibodies were measured: anti-reticulin (ARA), anti-gliadin (AGA) and anti-endomysial (EMA) antibodies.[91] ARA testing, however, is not accurate enough for routine diagnostic use.[92] Serology may be unreliable in young children, with anti-gliadin performing somewhat better than other tests in children under five.[91] Serology tests are based on indirect immunofluorescence (reticulin, gliadin and endomysium) or ELISA (gliadin or tissue transglutaminase, tTG).[93]
Other antibodies such as anti–Saccharomyces cerevisiae antibodies occur in some people with coeliac disease but also occur in other autoimmune disorders and about 5% of those who donate blood.[94]
Antibody testing may be combined with HLA testing if the diagnosis is unclear. TGA and EMA testing are the most sensitive serum antibody tests, but as a negative HLA-DQ type excludes the diagnosis of coeliac disease, testing also for HLA-DQ2 or DQ8 maximises sensitivity and negative predictive values.[61] However, widespread use of HLA typing to rule out coeliac disease is not currently recommended.[22]
An upper endoscopy with biopsy of the duodenum (beyond the duodenal bulb) or jejunum is performed to obtain multiple samples (four to eight) from the duodenum. Not all areas may be equally affected; if biopsies are taken from healthy bowel tissue, the result would be a false negative.[40] Even in the same bioptic fragment, different degrees of
damage may be present.[17]
Most people with coeliac disease have a small intestine that appears to be normal on endoscopy before the biopsies are examined. However, five findings have been associated with a high specificity for coeliac disease: scalloping of the small bowel folds (pictured), paucity in the folds, a mosaic pattern to the mucosa (described as a "cracked-mud" appearance), prominence of the submucosa blood vessels, and a nodular pattern to the mucosa.[95]
European guidelines suggest that in children and adolescents with symptoms compatible with coeliac disease, the diagnosis can be made without the need for intestinal biopsy if anti-tTG antibodies titres are very high (10 times the upper limit of normal).[8]
Until the 1970s, biopsies were obtained using metal capsules attached to a suction device. The capsule was swallowed and allowed to pass into the small intestine. After x-ray verification of its position, suction was applied to collect part of the intestinal wall inside the capsule. Often-utilised capsule systems were the Watson capsule and the Crosby–Kugler capsule. This method has now been largely replaced by fibre-optic endoscopy, which carries a higher sensitivity and a lower frequency of errors.[96]
Capsule endoscopy (CE) allows identification of typical mucosal changes observed in coeliac disease but has a lower sensitivity compared to regular endoscopy and histology. CE is therefore not the primary diagnostic tool for coeliac disease. However, CE can be used for diagnosing T-cell lymphoma, ulcerative jejunoileitis and adenocarcinoma in refractory or complicated coeliac disease.[97]
The classic pathology changes of coeliac disease in the small bowel are categorised by the "Marsh classification":[98]
Marsh's classification, introduced in 1992, was subsequently modified in 1999 to six stages, where the previous stage 3 was split in three substages.[100] Further studies demonstrated that this system was not always reliable and that the changes observed in coeliac disease could be described in one of three stages:[19][101]
The changes classically improve or reverse after gluten is removed from the diet. However, most guidelines do not recommend a repeat biopsy unless there is no improvement in the symptoms on diet.[40][90] In some cases, a deliberate gluten challenge, followed by biopsy, may be conducted to confirm or refute the diagnosis. A normal biopsy and normal serology after challenge indicates the diagnosis may have been incorrect.[40]
In untreated coeliac disease, villous atrophy is more common in children younger than three years, but in older children and adults, it is common to find minor intestinal lesions (duodenal lymphocytosis) with normal intestinal villi.[16][11][26]
At the time of diagnosis, further investigations may be performed to identify complications, such as iron deficiency (by full blood count and iron studies), folic acid and vitamin B12 deficiency and hypocalcaemia (low calcium levels, often due to decreased vitamin D levels). Thyroid function tests may be requested during blood tests to identify hypothyroidism, which is more common in people with coeliac disease.[41]
Osteopenia and osteoporosis, mildly and severely reduced bone mineral density, are often present in people with coeliac disease, and investigations to measure bone density may be performed at diagnosis, such as dual-energy X-ray absorptiometry (DXA) scanning, to identify risk of fracture and need for bone protection medication.[40][41]
Although blood antibody tests, biopsies, and genetic tests usually provide a clear diagnosis,[25][87] occasionally the response to gluten withdrawal on a gluten-free diet is needed to support the diagnosis. Currently, gluten challenge is no longer required to confirm the diagnosis in patients with intestinal lesions compatible with coeliac disease and a positive response to a gluten-free diet.[25] Nevertheless, in some cases, a gluten challenge with a subsequent biopsy may be useful to support the diagnosis, for example in people with a high suspicion for coeliac disease, without a biopsy confirmation, who have negative blood antibodies and are already on a gluten-free diet.[25] Gluten challenge is discouraged before the age of 5 years and during pubertal growth.[102] The alternative diagnosis of non-coeliac gluten sensitivity may be made where there is only symptomatic evidence of gluten sensitivity.[103] Gastrointestinal and extraintestinal symptoms of people with non-coeliac gluten sensitivity can be similar to those of coeliac disease,[17] and improve when gluten is removed from the diet,[104][105] after coeliac disease and wheat allergy are reasonably excluded.[106]
Up to 30% of people often continue having or redeveloping symptoms after starting a gluten-free diet.[13] A careful interpretation of the symptomatic response is needed, as a lack of response in a person with coeliac disease may be due to continued ingestion of small amounts of gluten, either voluntary or inadvertent,[11] or be due to other commonly associated conditions such as small intestinal bacterial overgrowth (SIBO), lactose intolerance, fructose,[107] sucrose,[108] and sorbitol[109] malabsorption, exocrine pancreatic insufficiency,[110][111] and microscopic colitis,[111] among others. In untreated coeliac disease, these are often transient conditions derived from the intestinal damage.[108][109][112][113][114] They normally revert or improve several months after starting a gluten-free diet, but may need temporary interventions such as supplementation with pancreatic enzymes,[113][114] dietary restrictions of lactose, fructose, sucrose or sorbitol containing foods,[108][112] or treatment with oral antibiotics in the case of associated bacterial overgrowth.[114] In addition to gluten withdrawal, some people need to follow a low-FODMAPs diet or avoid consumption of commercial gluten-free products, which are usually rich in preservatives and additives (such as sulfites, glutamates, nitrates and benzoates) and might have a role in triggering functional gastrointestinal symptoms.[115]
There is debate as to the benefits of screening. As of 2017, the United States Preventive Services Task Force found insufficient evidence to make a recommendation among those without symptoms.[29] In the United Kingdom, the National Institute for Health and Clinical Excellence (NICE) recommend testing for coeliac disease in first-degree relatives of those with the disease already confirmed, in people with persistent fatigue, abdominal or gastrointestinal symptoms, faltering growth, unexplained weight loss or iron, vitamin B12 or folate deficiency, severe mouth ulcers, and with diagnoses of type 1 diabetes, autoimmune thyroid disease,[22] and with newly diagnosed chronic fatigue syndrome[116] and irritable bowel syndrome.[37] Dermatitis herpetiformis is included in other recommendations.[117] The NICE also recommend offering serological testing for coeliac disease in people with metabolic bone disease (reduced bone mineral density or osteomalacia), unexplained neurological disorders (such as peripheral neuropathy and ataxia), fertility problems or recurrent miscarriage, persistently raised liver enzymes with unknown cause, dental enamel defects and with diagnose of Down syndrome or Turner syndrome.[22]
Some evidence has found that early detection may decrease the risk of developing health complications, such as osteoporosis, anaemia, and certain types of cancer, neurological disorders, cardiovascular diseases, and reproductive problems.[7][28][46][118][119] They thus recommend screening in people with certain health problems.[119]
Serology has been proposed as a screening measure, because the presence of antibodies would detect some previously undiagnosed cases of coeliac disease and prevent its complications in those people. However, serologic tests have high sensitivity only in people with total villous atrophy and have very low ability to detect cases with partial villous atrophy or minor intestinal lesions.[25] Testing for coeliac disease may be offered to those with commonly associated conditions.[19][22]
At present, the only effective treatment is a lifelong gluten-free diet.[50] No medication exists that prevents damage or prevents the body from attacking the gut when gluten is present. Strict adherence to the diet helps the intestines heal, leading to resolution of all symptoms in most cases and, depending on how soon the diet is begun, can also eliminate the heightened risk of osteoporosis and intestinal cancer and in some cases sterility.[120] The diet can be cumbersome; failure to comply with the diet may cause relapse.
Dietitian input is generally requested to ensure the person is aware which foods contain gluten, which foods are safe, and how to have a balanced diet despite the limitations. In many countries, gluten-free products are available on prescription and may be reimbursed by health insurance plans. Gluten-free products are usually more expensive and harder to find than common gluten-containing foods.[121] Since ready-made products often contain traces of gluten, some coeliacs may find it necessary to cook from scratch.[122]
The term "gluten-free" is generally used to indicate a supposed harmless level of gluten rather than a complete absence.[123] The exact level at which gluten is harmless is uncertain and controversial. A recent systematic review tentatively concluded that consumption of less than 10 mg of gluten per day is unlikely to cause histological abnormalities, although it noted that few reliable studies had been done.[123] Regulation of the label "gluten-free" varies. In the European Union, the European Commission issued regulations in 2009 limiting the use of "gluten-free" labels for food products to those with less than 20 mg/kg of gluten, and "very low gluten" labels for those with less than 100 mg/kg.[124] In the United States, the FDA issued regulations in 2013 limiting the use of "gluten-free" labels for food products to those with less than 20 ppm of gluten.[125][126][127] The current international Codex Alimentarius standard allows for 20 ppm of gluten in so-called "gluten-free" foods.[128] Several organisations, such as the Gluten-Free Certification Organization (GFCO), the Celiac Sprue Association (CSA), and the National Foundation for Celiac Awareness (NFCA), also certify products and companies as gluten-free.[129]
Gluten-free diet improves healthcare-related quality of life, and strict adherence to the diet gives more benefit than incomplete adherence. Nevertheless, gluten-free diet doesn't completely normalise the quality of life.[130]
Between 0.3% and 10% of people have refractory disease, which means that they have persistent villous atrophy on a gluten-free diet despite the lack of gluten exposure for more than 12 months.[111] Nevertheless, inadvertent exposure to gluten is the main cause of persistent villous atrophy, and must be ruled out before a diagnosis of refractory disease is made.[111] People with poor basic education and understanding of gluten-free diet often believe that they are strictly following the diet, but are making regular errors.[13][111][131] Also, a lack of symptoms is not a reliable indicator of intestinal recuperation.[111]
If alternative causes of villous atrophy have been eliminated, steroids or immunosuppressants (such as azathioprine) may be considered in this scenario.[40]
Refractory coeliac disease should not be confused with the persistence of symptoms despite gluten withdrawal[111] caused by transient conditions derived from the intestinal damage,[108][109][112] which generally revert or improve several months after starting a gluten-free diet,[113][114] such as small intestinal bacterial overgrowth, lactose intolerance, fructose,[107] sucrose,[108] and sorbitol[109] malabsorption, exocrine pancreatic insufficiency,[110][111] and microscopic colitis,[111] among others.
Globally coeliac diseases affects between 1 in 100 and 1 in 170 people.[14][132] Rates, however, vary between different regions of the world from as few as 1 in 300 to as many as 1 in 40.[14] In the United States it is thought to affect between 1 in 1750 (defined as clinical disease including dermatitis herpetiformis with limited digestive tract symptoms) to 1 in 105 (defined by presence of IgA TG in blood donors).[133] Due to variable signs and symptoms it is believed that about 85% of people affected are undiagnosed.[134] The percentage of people with clinically diagnosed disease (symptoms prompting diagnostic testing) is 0.05–0.27% in various studies. However, population studies from parts of Europe, India, South America, Australasia and the USA (using serology and biopsy) indicate that the percentage of people with the disease may be between 0.33 and 1.06% in children (but 5.66% in one study of children of the predisposed Sahrawi people[135]) and 0.18–1.2% in adults.[28] Among those in primary care populations who report gastrointestinal symptoms, the rate of coeliac disease is about 3%.[87] The rate amongst adult blood donors in Iran, Israel, Syria and Turkey is 0.60%, 0.64%, 1.61% and 1.15%, respectively.[39]
People of African, Japanese and Chinese descent are rarely diagnosed;[136] this reflects a much lower prevalence of the genetic risk factors, such as HLA-B8.[137] People of Indian ancestry seem to have a similar risk to those of Western Caucasian ancestry.[39] Population studies also indicate that a large proportion of coeliacs remain undiagnosed; this is due, in part, to many clinicians being unfamiliar with the condition and also due to the fact it can be asymptomatic.[138] Coeliac disease is slightly more common in women than in men.[31] A large multicentre study in the U.S. found a prevalence of 0.75% in not-at-risk groups, rising to 1.8% in symptomatic people, 2.6% in second-degree relatives (like grandparents, aunt or uncle, grandchildren, etc.) of a person with coeliac disease and 4.5% in first-degree relatives (siblings, parents or children).[39] This profile is similar to the prevalence in Europe.[39] Other populations at increased risk for coeliac disease, with prevalence rates ranging from 5% to 10%, include individuals with Down and Turner syndromes, type 1 diabetes, and autoimmune thyroid disease, including both hyperthyroidism (overactive thyroid) and hypothyroidism (underactive thyroid).[139]
Historically, coeliac disease was thought to be rare, with a prevalence of about 0.02%.[139] The reason for the recent increases in the number of reported cases is unclear.[132] It may be at least in part due to changes in diagnostic practice.[140] There also appears to be an approximately 4.5 fold true increase that may be due to less exposure to bacteria and other pathogens in Western environments.[132]
Humans first started to cultivate grains in the Neolithic period (beginning about 9500 BCE) in the Fertile Crescent in Western Asia, and it is likely that coeliac disease did not occur before this time. Aretaeus of Cappadocia, living in the second century in the same area, recorded a malabsorptive syndrome with chronic diarrhoea, causing a debilitation of the whole body.[32] His "Cœliac Affection" (coeliac from Greek κοιλιακός koiliakos, "abdominal") gained the attention of Western medicine when Francis Adams presented a translation of Aretaeus's work at the Sydenham Society in 1856. The patient described in Aretaeus' work had stomach pain and was atrophied, pale, feeble and incapable of work. The diarrhoea manifested as loose stools that were white, malodorous and flatulent, and the disease was intractable and liable to periodic return. The problem, Aretaeus believed, was a lack of heat in the stomach necessary to digest the food and a reduced ability to distribute the digestive products throughout the body, this incomplete digestion resulting in the diarrhoea. He regarded this as an affliction of the old and more commonly affecting women, explicitly excluding children. The cause, according to Aretaeus, was sometimes either another chronic disease or even consuming "a copious draught of cold water."[32][33]
The paediatrician Samuel Gee gave the first modern-day description of the condition in children in a lecture at Hospital for Sick Children, Great Ormond Street, London, in 1887. Gee acknowledged earlier descriptions and terms for the disease and adopted the same term as Aretaeus (coeliac disease). He perceptively stated: "If the patient can be cured at all, it must be by means of diet." Gee recognised that milk intolerance is a problem with coeliac children and that highly starched foods should be avoided. However, he forbade rice, sago, fruit and vegetables, which all would have been safe to eat, and he recommended raw meat as well as thin slices of toasted bread. Gee highlighted particular success with a child "who was fed upon a quart of the best Dutch mussels daily." However, the child could not bear this diet for more than one season.[33][141]
Christian Archibald Herter, an American physician, wrote a book in 1908 on children with coeliac disease, which he called "intestinal infantilism." He noted their growth was retarded and that fat was better tolerated than carbohydrate. The eponym Gee-Herter disease was sometimes used to acknowledge both contributions.[142][143] Sidney V. Haas, an American paediatrician, reported positive effects of a diet of bananas in 1924.[144] This diet remained in vogue until the actual cause of coeliac disease was determined.[33]
While a role for carbohydrates had been suspected, the link with wheat was not made until the 1940s by the Dutch paediatrician Dr Willem Karel Dicke.[145] It is likely that clinical improvement of his patients during the Dutch famine of 1944 (during which flour was scarce) may have contributed to his discovery.[146] Dicke noticed that the shortage of bread led to a significant drop in the death rate among children affected by coeliac disease from greater than 35% to essentially zero. He also reported that once wheat was again available after the conflict, the mortality rate soared to previous levels.[147] The link with the gluten component of wheat was made in 1952 by a team from Birmingham, England.[148] Villous atrophy was described by British physician John W. Paulley in 1954 on samples taken at surgery.[149] This paved the way for biopsy samples taken by endoscopy.[33]
Throughout the 1960s, other features of coeliac disease were elucidated. Its hereditary character was recognised in 1965.[150] In 1966, dermatitis herpetiformis was linked to gluten sensitivity.[33][44]
May has been designated as "Coeliac Awareness Month" by several coeliac organisations.[151][152]
Speaking generally, the various denominations of Christians celebrate a Eucharist in which a wafer or small piece of sacramental bread from wheat bread is blessed and then eaten. A typical wafer weighs about half a gram.[153] Wheat flour contains around 10 to 13% gluten, so a single communion wafer may have more than 50 mg of gluten, an amount that harms many people with coeliac, especially if consumed every day (see Diet above).
Many Christian churches offer their communicants gluten-free alternatives, usually in the form of a rice-based cracker or gluten-free bread. These include the United Methodist, Christian Reformed, Episcopal, the Anglican Church (Church of England, UK) and Lutheran. Catholics may receive from the Chalice alone, or ask for gluten-reduced hosts; gluten-free ones however are not considered to still be wheat bread, and hence invalid matter.[154]
Roman Catholic doctrine states that for a valid Eucharist, the bread to be used at Mass must be made from wheat. Low-gluten hosts meet all of the Catholic Church's requirements, but they are not entirely gluten free. Requests to use rice wafers have been denied.[155]
The issue is more complex for priests. As a celebrant, a priest is, for the fullness of the sacrifice of the Mass, absolutely required to receive under both species. On 24 July 2003, the Congregation for the Doctrine of the Faith stated, "Given the centrality of the celebration of the Eucharist in the life of a priest, one must proceed with great caution before admitting to Holy Orders those candidates unable to ingest gluten or alcohol without serious harm."[156]
By January 2004, extremely low-gluten Church-approved hosts had become available in the United States, Italy and Australia.[157] As of July 2017, the Vatican still outlawed the use of gluten-free bread for Holy Communion.[158]
The Jewish festival of Pesach (Passover) may present problems with its obligation to eat matzo, which is unleavened bread made in a strictly controlled manner from wheat, barley, spelt, oats, or rye. This rules out many other grains that are normally used as substitutes for people with gluten sensitivity, especially for Ashkenazi Jews, who also avoid rice. Many kosher-for-Passover products avoid grains altogether and are therefore gluten-free. Potato starch is the primary starch used to replace the grains.
The search for environmental factors that could be responsible for genetically susceptible people becoming intolerant to gluten has resulted in increasing research activity looking at gastrointestinal infections.[159] Research published in April 2017 suggests an often symptomless infection with a common strain of reovirus can increase sensitivity to foods such as gluten.[160]
Various other approaches are being studied that would reduce the need of dieting. All are still under development, and are not expected to be available to the general public for a while.[28][161][162]
Three main approaches have been proposed as new therapeutic modalities for coeliac disease: gluten detoxification, modulation of the intestinal permeability, and modulation of the immune response.[163]
Using genetically engineered wheat species, or wheat species that have been selectively bred to be minimally immunogenic, may allow the consumption of wheat. This, however, could interfere with the effects that gliadin has on the quality of dough. Alternatively, gluten exposure can be minimised by the ingestion of a combination of enzymes (prolyl endopeptidase and a barley glutamine-specific cysteine endopeptidase (EP-B2)) that degrade the putative 33-mer peptide in the duodenum.[28]
Alternative treatments under investigation include the inhibition of zonulin, an endogenous signalling protein linked to increased permeability of the bowel wall and hence increased presentation of gliadin to the immune system. One inhibitor of this pathway is larazotide acetate, which is currently scheduled for phase 3 clinical trials.[164] Other modifiers of other well-understood steps in the pathogenesis of coeliac disease, such as the action of HLA-DQ2 or tissue transglutaminase and the MICA/NKG2D interaction that may be involved in the killing of enterocytes.[28]
Attempts to modulate the immune response with regard to coeliac disease are mostly still in phase I of clinical testing; one agent (CCX282-B) has been evaluated in a phase II clinical trial on the basis of small-intestinal biopsies taken from people with coeliac disease before and after gluten exposure.[163]
Although popularly used as an alternative treatment for people with autism, there is no good evidence that a gluten-free diet is of benefit.[165][166][167] In the subset of people who have gluten sensitivity there is limited evidence that suggests that a gluten free diet may improve some autistic behaviors.[165][168][169]




Malignant transformation - Wikipedia
Malignant transformation is the process by which cells acquire the properties of cancer. This may occur as a primary process in normal tissue, or secondarily as malignant degeneration of a previously existing benign tumor.
There are many causes of primary malignant transformation, or tumorigenesis.  Most human cancers in the United States are caused by external factors, and these factors are largely avoidable.[1][2][3]  These factors were summarized by Doll and Peto in 1981,[1] and were still considered to be valid in 2015.[2]  These factors are listed in the table.
a Reproductive and sexual behaviors include:  number of partners; age at first menstruation; zero versus one or more live births
Colon cancer provides one example of the mechanisms by which diet, the top factor listed in the table, is an external factor in cancer.  The Western diet of African Americans in the United States is associated with a yearly colon cancer rate of 65 per 100,000 individuals, while the high fiber/low fat diet of rural Native Africans in South Africa is associated with a yearly colon cancer rate of <5 per 100,000.[4]  Feeding the Western diet for two weeks to Native Africans increased their secondary bile acids, including carcinogenic deoxycholic acid,[5] by 400%, and also changed the colonic microbiota.[4]  Evidence reviewed by Sun and Kato[6] indicates that differences in human colonic microbiota play an important role in the progression of colon cancer.
A second example, relating a dietary component to a cancer, is illustrated by lung cancer.  Two large population-based studies were performed, one in Italy and one in the United States.[7]  In Italy, a study population of 1721 individuals diagnosed with lung cancer and no severe disease and 1918 control individuals with absence of lung cancer history or any advanced diseases.  All individuals filled out a food frequency questionnaire including consumption of walnuts, hazelnuts, almonds, and peanuts, and indicating smoking status.  In the United States, 495,785 members of AARP were questioned on consumption of peanuts, walnuts, seeds, or other nuts in addition to other foods and smoking status.  In this U.S. study 18,533 incident lung cancer cases were identified during up to 16 years of follow-up.  Overall, individuals in the highest quintile of frequency of nut consumption had a 26% lower risk of lung cancer in the Italian study and a 14% lower risk of lung cancer in the U.S. study.  Similar results were obtained among individuals who were smokers.
The most important chemical compounds in smoked tobacco that are carcinogenic are those that produce DNA damage since such damage appears to be the primary underlying cause of cancer.[8][9]  Cunningham et al.[10] combined the microgram weight of the compound in the smoke of one cigarette with the known genotoxic effect per microgram to identify the most carcinogenic compounds in cigarette smoke.  These compounds and their genotoxic effects are listed in the article Cigarette.  The top three compounds are acrolein, formaldehyde and acrylonitrile, all known carcinogens.
In 2002 the World Health Organizations International Agency for Research on Cancer[11] estimated that 11.9% of human cancers are caused by one of seven viruses (see Oncovirus overview table).  These are Epstein-Barr virus (EBV or HHV4); Kaposi's sarcoma-associated herpesvirus (KSHV or HHV8); Hepatitis B and Hepatitis C viruses (HBV and HCV); Human T-lymphotrophic virus 1 (HTLV-1); Merkel cell polyomavirus (MCPyV); and a group of alpha Human papillomaviruses (HPVs).[12]
In 1995 epidemiologic evidence indicated that Helicobacter pylori infection increases the risk for gastric carcinoma.[13]  More recently, experimental evidence showed that infection with Helicobacter pylori cagA+ve bacterial strains results in severe degrees of inflammation and oxidative DNA damage, leading to progression to gastric cancer.[14]
Perera et al.[15] referred to a number of articles pointing to roles of bacteria in other cancers.  They pointed to single studies on the role of Chlamydia trachomatis in cervical cancer, Salmonella typhi in gallbladder cancer, and both Bacteroides fragilis and Fusobacterium nucleatum in colon cancer.  Meurman has recently summarized evidence connecting oral microbiota with carcinogenesis.[16]  However, these studies are suggestive but still need further confirmation.
One underlying commonality in cancers is genetic mutation, acquired either by inheritance, or, more commonly, by mutations in one's somatic DNA over time.  The mutations considered important in cancers are those that alter protein coding genes (the exome).  As Vogelstein et al. point out, a typical tumor contains two to eight exome "driver gene" mutations, and a larger number of exome mutations that are "passengers" that confer no selective growth advantage.[17]
Cancers also generally have genome instability, that includes a high frequency of mutations in the noncoding DNA that makes up about 98% of the human genome.  The average number of DNA sequence mutations in the entire genome of breast cancer tissue is about 20,000.[18]  In an average melanom (where melanomas have a higher exome mutation frequency[17]) the total number of DNA sequence mutations is about 80,000.[19]
A second underlying commonality in cancers is altered epigenetic regulation of transcription.  In cancers, loss of gene expression occurs about 10 times more frequently by epigenetic transcription silencing (caused, for example, by promoter hypermethylation of CpG islands) than by mutations.  As Vogelstein et al.[17] point out, in a colorectal cancer there are usually about 3 to 6 driver mutations and 33 to 66 hitchhiker or passenger mutations.[17]  In contrast, the frequency of epigenetic alterations is much higher.  In colon tumors compared to adjacent normal-appearing colonic mucosa, there are about 600 to 800 heavily methylated CpG islands in promoters of genes in the tumors while the corresponding CpG islands are not methylated in the adjacent mucosa.[20][21][22]  Such methylation turns off expression of a gene as completely as a mutation would.  Around 60–70% of human genes have a CpG island in their promoter region.[23][24]  In colon cancers, in addition to hypermethylated genes, several hundred other genes have hypomethylated (under-methylated) promoters, thereby causing these genes to be turned on when they ordinarily would be turned off.[22]
Epigenetic alterations are also carried out by another major regulatory element, the microRNAs.  In mammals, microRNAs (miRNAs) regulate about 60% of the transcriptional activity of protein-encoding genes.[25]  Epigenetic silencing or epigenetic over-expression of miRNA genes, caused by aberrant DNA methylation of the promoter regions controlling their expression, is a frequent event in cancer cells.  Almost one third of miRNA promoters active in normal mammary cells were found to be hypermethylated in breast cancer cells, and that is a several fold greater proportion of promoters with altered methylation than is usually observed for protein coding genes.[26]  Other microRNA promoters are hypomethylated in breast cancers, and, as a result, these microRNAs are over-expressed.  Several of these over-expressed microRNAs have a major influence in progression to breast cancer.  BRCA1 is normally expressed in the cells of breast and other tissue, where it helps repair damaged DNA, or destroy cells if DNA cannot be repaired.[27]  BRCA1 is involved in the repair of chromosomal damage with an important role in the error-free repair of DNA double-strand breaks.[28] BRCA1 expression is reduced or undetectable in the majority of high grade, ductal breast cancers.[29]  Only about 3%-8% of all women with breast cancer carry a mutation in BRCA1 or BRCA2.[30] BRCA1 promoter hypermethylation was present in only 13% of unselected primary breast carcinomas.[31]   However, breast cancers were found to have an average of about 100-fold increase in miR-182, compared to normal breast tissue.[32]  In breast cancer cell lines, there is an inverse correlation of BRCA1 protein levels with miR-182 expression.[33] Thus it appears that much of the reduction or absence of BRCA1 in high grade ductal breast cancers may be due to over-expressed miR-182. (For review see ref.[34])  In addition to miR-182, a pair of almost identical microRNAs, miR-146a and miR-146b-5p, also repress BRCA1 expression.  These two microRNAs are over-expressed in triple-negative tumors and their over-expression results in BRCA1 inactivation.[35]  Thus, miR-146a and/or miR-146b-5p may also contribute to reduced expression of BRCA1 in these triple-negative breast cancers.
Post-transcriptional regulation by microRNA occurs either through translational silencing of the target mRNA or through degradation of the target mRNA, via complementary binding, mostly to specific sequences in the three prime untranslated region of the target gene's mRNA.[36]   The mechanism of translational silencing or degradation of target mRNA is implemented through the RNA-induced silencing complex (RISC).
Silencing of a DNA repair gene by hypermethylation or other epigenetic alteration appears to be a frequent step in progression to cancer.  As summarized in a review,[34] promoter hypermethylation of  DNA repair gene MGMT occurs in 93% of bladder cancers, 88% of stomach cancers, 74% of thyroid cancers, 40%-90% of colorectal cancers and 50% of brain cancers.  In addition, promoter hypermethylation of DNA repair genes LIG4, NEIL1, ATM, MLH1 or FANCB occurs at frequencies between 33% to 82% in one or more of head and neck cancers, non-small-cell lung cancers or non-small-cell lung cancer squamous cell carcinomas.   Further, the article Werner syndrome ATP-dependent helicase indicates the DNA repair gene WRN has a promoter that is often hypermethylated in a variety of cancers, with WRN hypermethylation occurring in 11% to 38% of colorectal, head and neck, stomach, prostate, breast, thyroid, non-Hodgkin lymphoma, chondrosarcoma and osteosarcoma cancers.
Such silencing likely acts similarly to a germ-line mutation in a DNA repair gene, and predisposes the cell and its descendants to progression to cancer.[37]  Another review[38] points out that when a gene necessary for DNA repair is epigenetically silenced, DNA repair would tend to be deficient and DNA damages can accumulate.  Increased DNA damage can cause increased errors during DNA synthesis, leading to mutations that give rise to cancer.
The heavy metals cadmium, arsenic and nickel are all carcinogenic when present above certain levels.[39][40][41][42]
Cadmium is known to be carcinogenic, possibly due to reduction of DNA repair.  Lei et al.[43] evaluated five DNA repair genes in rats after exposure of the rats to low levels of cadmium.  They found that cadmium caused repression of three of the DNA repair genes: XRCC1 needed for base excision repair, OGG1 needed for base excision repair, and ERCC1 needed for nucleotide excision repair.  Repression of these genes was not due to methylation of their promoters.
Arsenic carcinogenicity was reviewed by Bhattacharjee et al.[41]  They summarized the role of arsenic and its metabolites in generating oxidative stress, resulting in DNA damage.  In addition to causing DNA damage, arsenic also causes repression of several DNA repair enzymes in both the base excision repair pathway and the nucleotide excision repair pathway.  Bhattacharjee et al. further reviewed the role of arsenic in causing telomere dysfunction, mitotic arrest, defective apoptosis, as well as altered promoter methylation and miRNA expression.  Each of these alterations could contribute to arsenic-induced carcinogenesis.
Nickel compounds are carcinogenic and occupational exposure to nickel is associated with an increased risk of lung and nasal cancers.[44]  Nickel compounds exhibit weak mutagenic activity, but they considerably alter the transcriptional landscape of the DNA of exposed individuals.[44]  Arita et al.[44] examined the peripheral blood mononuclear cells of eight nickel-refinery workers and ten non-exposed workers.  They found  2756 differentially expressed genes with 770 up-regulated genes and 1986 down-regulated genes.  DNA repair genes were significantly over-represented among the differentially expressed genes, with 29 DNA repair genes repressed in the nickel-refinery workers and two over-expressed.  The alterations in gene expression appear to be due to epigenetic alterations of histones, methylations of gene promoters, and hypermethylation of at least microRNA miR-152.[42][45]
Malignant transformation of cells in a benign tumor may be detected by pathologic examination of tissues. Often the clinical signs and symptoms are suggestive of a malignant tumor.  The physician, during the medical history examination, can find that there have been changes in size or patient sensation and, upon direct examination, that there has been a change in the lesion itself.
Risk assessments can be done and are known for certain types of benign tumor which are known to undergo malignant transformation. One of the better-known examples of this phenomenon is the progression of a nevus to melanoma.



Oncogene - Wikipedia
An oncogene is a gene that has the potential to cause cancer.[1] In tumor cells, they are often mutated and/or expressed at high levels.[2]
Most normal cells will undergo a programmed form of rapid cell death (apoptosis) when critical functions are altered and malfunctioning. Activated oncogenes can cause those cells designated for apoptosis to survive and proliferate instead.[3] Most oncogenes began as proto-oncogenes, normal genes involved in cell growth and proliferation or inhibition of apoptosis. If normal genes promoting cellular growth, through mutation, are up-regulated, (gain of function mutation) they will predispose the cell to cancer and are thus termed oncogenes. Usually multiple oncogenes, along with mutated apoptotic and/or tumor suppressor genes will all act in concert to cause cancer.  Since the 1970s, dozens of oncogenes have been identified in human cancer. Many cancer drugs target the proteins encoded by oncogenes.[2][4][5][6]
The theory of oncogenes was foreshadowed by the German biologist Theodor Boveri in his 1914 book Zur Frage der Entstehung Maligner Tumoren ('The Origin of Malignant Tumours'), Gustav Fisher, Jena, 1914.  Oncogenes (Teilungsfoerdernde Chromosomen) that become amplified (im permanenten Übergewicht) during tumour development.
Later on the term "oncogene" was rediscovered in 1969 by National Cancer Institute scientists George Todaro and Robert Heubner.[7]
The first confirmed oncogene was discovered in 1970 and was termed src (pronounced sarc as in sarcoma).  Src was in fact first discovered as an oncogene in a chicken retrovirus. Experiments performed by Dr. G. Steve Martin of the University of California, Berkeley demonstrated that the Src was indeed the oncogene of the virus.[8]  The first nucleotide sequence of v-src was sequenced in 1980 by A.P. Czernilofsky et al.[9]
In 1976 Drs. Dominique Stehelin, J. Michael Bishop and Harold E. Varmus of the University of California, San Francisco demonstrated that oncogenes were activated proto-oncogenes, found in many organisms including humans. Bishop and Varmus were awarded the Nobel Prize in Physiology or Medicine in 1989 for their discovery of the cellular origin of retroviral oncogenes.[10]
The resultant protein encoded by an oncogene is termed oncoprotein.[11] Oncogenes play an important role in the regulation or synthesis of proteins linked to tumorigenic cell growth. Some oncoproteins are accepted and used as tumor markers. The Spanish biochemist Mariano Barbacid isolated the first oncogene. His discovery was published in the prestigious journal Nature in 1982 in an article titled "A point mutation is responsible for the acquisition of transforming properties by the T24 human bladder-carcinoma oncogene".[12] He spent the following months extending his research, eventually discovering that such oncogene was the mutation of an allele of the Ras subfamily, as well as its activation mechanism.
A proto-oncogene is a normal gene that could become an oncogene due to mutations or increased expression. Proto-oncogenes code for proteins that help to regulate the cell growth and differentiation. Proto-oncogenes are often involved in signal transduction and execution of mitogenic signals, usually through their protein products. Upon acquiring an activating mutation, a proto-oncogene becomes a tumor-inducing agent, an oncogene.[13] Examples of proto-oncogenes include RAS, WNT, MYC, ERK, and TRK. The MYC gene is implicated in Burkitt's lymphoma, which starts when a chromosomal translocation moves an enhancer sequence within the vicinity of the MYC gene. The MYC gene codes for widely used transcription factors. When the enhancer sequence is wrongly placed, these transcription factors are produced at much higher rates. Another example of an oncogene is the Bcr-Abl gene found on the Philadelphia chromosome, a piece of genetic material seen in Chronic Myelogenous Leukemia caused by the translocation of pieces from chromosomes 9 and 22. Bcr-Abl codes for a tyrosine kinase, which is constitutively active, leading to uncontrolled cell proliferation. (More information about the Philadelphia Chromosome below)
The proto-oncogene can become an oncogene by a relatively small modification of its original function. There are three basic methods of activation:
The expression of oncogenes can be regulated by microRNAs (miRNAs), small RNAs 21-25 nucleotides in length that control gene expression by downregulating them.[15] Mutations in such microRNAs (known as oncomirs) can lead to activation of oncogenes.[16] Antisense messenger RNAs could theoretically be used to block the effects of oncogenes.
There are several systems for classifying oncogenes,[17] but there is not yet a widely accepted standard. They are sometimes grouped both spatially (moving from outside the cell inwards) and chronologically (parallelling the "normal" process of signal transduction). There are several categories that are commonly used:
Additional oncogenetic regulator properties include:



Chromosome - Wikipedia

A chromosome (/ˈkroʊməˌsoʊm, -ˌzoʊm/; from Ancient Greek: χρωμόσωμα, chromosoma, chroma means colour, soma means body) is a DNA molecule with part or all of the genetic material (genome) of an organism. Most eukaryotic chromosomes include packaging proteins which, aided by chaperone proteins, bind to and condense the DNA molecule to prevent it from becoming an unmanageable tangle.[1][2]
Chromosomes are normally visible under a light microscope only when the cell is undergoing the metaphase of cell division (where all chromosomes are aligned in the center of the cell in their condensed form).[3] Before this happens, every chromosome is copied once (S phase), and the copy is joined to the original by a centromere, resulting either in an X-shaped structure (pictured to the right) if the centromere is located in the middle of the chromosome or a two-arm structure if the centromere is located near one of the ends. The original chromosome and the copy are now called sister chromatids. During metaphase the X-shape structure is called a metaphase chromosome. In this highly condensed form chromosomes are easiest to distinguish and study.[4] In animal cells, chromosomes reach their highest compaction level in anaphase during chromosome segregation.[5]
Chromosomal recombination during meiosis and subsequent sexual reproduction play a significant role in genetic diversity. If these structures are manipulated incorrectly, through processes known as chromosomal instability and translocation, the cell may undergo mitotic catastrophe and die. Mutations in the cell can allow it to inappropriately evade apoptosis and lead to the progression of cancer.
Some use the term chromosome in a wider sense, to refer to the individualized portions of chromatin in cells, either visible or not under light microscopy. Others use the concept in a narrower sense, to refer to the individualized portions of chromatin during cell division, visible under light microscopy due to high condensation.
The word chromosome (/ˈkroʊməˌsoʊm, -ˌzoʊm/[6][7]) comes from the Greek χρῶμα (chroma, "colour") and σῶμα (soma, "body"), describing their strong staining by particular dyes.[8] The term was coined by von Waldeyer-Hartz,[9] referring to the term chromatin, which was introduced by Walther Flemming.
Emilio Battaglia (1917-2011)[10][11] points out that over time many of the most familiar caryological terms have become
inadequate or illogical or, in some cases, etymologically incorrect so that they should be replaced
by more adequate alternatives suggested by the present scientific progress. The author has been
particularly disappointed by the illogicality of the present chromosomal (chromatin-chromosome)
terminology based on, or inferred by, two terms, Chromatin (Flemming 1880) and Chromosom (Waldeyer
1888), both inappropriately ascribed to a basically non coloured state.[12]
The German scientists Schleiden,[4] Virchow and Bütschli were among the first scientists who recognized the structures now familiar as chromosomes.[13]
In a series of experiments beginning in the mid-1880s, Theodor Boveri gave the definitive demonstration that chromosomes are the vectors of heredity. His two principles were the continuity of chromosomes and the individuality of chromosomes.[citation needed][further explanation needed] It is the second of these principles that was so original.[citation needed] Wilhelm Roux suggested that each chromosome carries a different genetic load. Boveri was able to test and confirm this hypothesis. Aided by the rediscovery at the start of the 1900s of Gregor Mendel's earlier work, Boveri was able to point out the connection between the rules of inheritance and the behaviour of the chromosomes. Boveri influenced two generations of American cytologists: Edmund Beecher Wilson, Nettie Stevens, Walter Sutton and Theophilus Painter were all influenced by Boveri (Wilson, Stevens, and Painter actually worked with him).[14]
In his famous textbook The Cell in Development and Heredity, Wilson linked together the independent work of Boveri and Sutton (both around 1902) by naming the chromosome theory of inheritance the Boveri–Sutton chromosome theory (the names are sometimes reversed).[15] Ernst Mayr remarks that the theory was hotly contested by some famous geneticists: William Bateson, Wilhelm Johannsen, Richard Goldschmidt and T.H. Morgan, all of a rather dogmatic turn of mind. Eventually, complete proof came from chromosome maps in Morgan's own lab.[16]
The number of human chromosomes was published in 1923 by Theophilus Painter. By inspection through the microscope, he counted 24 pairs, which would mean 48 chromosomes. His error was copied by others and it was not until 1956 that the true number, 46, was determined by Indonesia-born cytogeneticist Joe Hin Tjio.[17]
The prokaryotes – bacteria and archaea – typically have a single circular chromosome, but many variations exist.[18] The chromosomes of most bacteria, which some authors prefer to call genophores, can range in size from only 130,000 base pairs in the endosymbiotic bacteria Candidatus Hodgkinia cicadicola[19] and Candidatus Tremblaya princeps,[20] to more than 14,000,000 base pairs in the soil-dwelling bacterium Sorangium cellulosum.[21] Spirochaetes of the genus Borrelia are a notable exception to this arrangement, with bacteria such as Borrelia burgdorferi, the cause of Lyme disease, containing a single linear chromosome.[22]
Prokaryotic chromosomes have less sequence-based structure than eukaryotes. Bacteria typically have a one-point (the origin of replication) from which replication starts, whereas some archaea contain multiple replication origins.[23] The genes in prokaryotes are often organized in operons, and do not usually contain introns, unlike eukaryotes.
Prokaryotes do not possess nuclei. Instead, their DNA is organized into a structure called the nucleoid.[24][25] The nucleoid is a distinct structure and occupies a defined region of the bacterial cell. This structure is, however, dynamic and is maintained and remodeled by the actions of a range of histone-like proteins, which associate with the bacterial chromosome.[26] In archaea, the DNA in chromosomes is even more organized, with the DNA packaged within structures similar to eukaryotic nucleosomes.[27][28]
Certain bacteria also contain plasmids or other extrachromosomal DNA. These are circular structures in the cytoplasm that contain cellular DNA and play a role in horizontal gene transfer.[4] In prokaryotes (see nucleoids) and viruses,[29] the DNA is often densely packed and organized; in the case of archaea, by homology to eukaryotic histones, and in the case of bacteria, by histone-like proteins.
Bacterial chromosomes tend to be tethered to the plasma membrane of the bacteria. In molecular biology application, this allows for its isolation from plasmid DNA by centrifugation of lysed bacteria and pelleting of the membranes (and the attached DNA).
Prokaryotic chromosomes and plasmids are, like eukaryotic DNA, generally supercoiled. The DNA must first be released into its relaxed state for access for transcription, regulation, and replication.
Chromosomes in eukaryotes are composed of chromatin fiber. Chromatin fiber is made of nucleosomes (histone octamers with part of a DNA strand attached to and wrapped around it). Chromatin fibers are packaged by proteins into a condensed structure called chromatin. Chromatin contains the vast majority of DNA and a small amount inherited maternally, can be found in the mitochondria. Chromatin is present in most cells, with a few exceptions, for example, red blood cells.
Chromatin allows the very long DNA molecules to fit into the cell nucleus. During cell division chromatin condenses further to form microscopically visible chromosomes. The structure of chromosomes varies through the cell cycle. During cellular division chromosomes are replicated, divided, and passed successfully to their daughter cells so as to ensure the genetic diversity and survival of their progeny. Chromosomes may exist as either duplicated or unduplicated. Unduplicated chromosomes are single double helixes, whereas duplicated chromosomes contain two identical copies (called chromatids or sister chromatids) joined by a centromere.
Eukaryotes (cells with nuclei such as those found in plants, fungi, and animals) possess multiple large linear chromosomes contained in the cell's nucleus. Each chromosome has one centromere, with one or two arms projecting from the centromere, although, under most circumstances, these arms are not visible as such. In addition, most eukaryotes have a small circular mitochondrial genome, and some eukaryotes may have additional small circular or linear cytoplasmic chromosomes.
In the nuclear chromosomes of eukaryotes, the uncondensed DNA exists in a semi-ordered structure, where it is wrapped around histones (structural proteins), forming a composite material called chromatin.
During interphase (the period of the cell cycle where the cell is not dividing), two types of chromatin can be distinguished:
Structure of Eukaryotic chromosome
In the early stages of mitosis or meiosis (cell division), the chromatin double helix become more and more condensed. They cease to function as accessible genetic material (transcription stops) and become a compact transportable form. This compact form makes the individual chromosomes visible, and they form the classic four arm structure, a pair of sister chromatids attached to each other at the centromere. The shorter arms are called p arms (from the French petit, small) and the longer arms are called q arms (q follows p in the Latin alphabet; q-g "grande"; alternatively it is sometimes said q is short for queue meaning tail in French[30]). This is the only natural context in which individual chromosomes are visible with an optical microscope.
Mitotic metaphase chromosomes are best described by a linearly organized longitudinally compressed array of consecutive chromatin loops.[31]
During mitosis, microtubules grow from centrosomes located at opposite ends of the cell and also attach to the centromere at specialized structures called kinetochores, one of which is present on each sister chromatid. A special DNA base sequence in the region of the kinetochores provides, along with special proteins, longer-lasting attachment in this region. The microtubules then pull the chromatids apart toward the centrosomes, so that each daughter cell inherits one set of chromatids. Once the cells have divided, the chromatids are uncoiled and DNA can again be transcribed. In spite of their appearance, chromosomes are structurally highly condensed, which enables these giant DNA structures to be contained within a cell nucleus.
Chromosomes in humans can be divided into two types: autosomes (body chromosome(s)) and allosome (sex chromosome(s)). Certain genetic traits are linked to a person's sex and are passed on through the sex chromosomes. The autosomes contain the rest of the genetic hereditary information. All act in the same way during cell division. Human cells have 23 pairs of chromosomes (22 pairs of autosomes and one pair of sex chromosomes), giving a total of 46 per cell. In addition to these, human cells have many hundreds of copies of the mitochondrial genome. Sequencing of the human genome has provided a great deal of information about each of the chromosomes. Below is a table compiling statistics for the chromosomes, based on the Sanger Institute's human genome information in the Vertebrate Genome Annotation (VEGA) database.[32] Number of genes is an estimate, as it is in part based on gene predictions. Total chromosome length is an estimate as well, based on the estimated size of unsequenced heterochromatin regions.
These tables give the total number of chromosomes (including sex chromosomes) in a cell nucleus. For example, most eukaryotes are diploid, like humans who have 22 different types of autosomes, each present as two homologous pairs, and two sex chromosomes. This gives 46 chromosomes in total. Other organisms have more than two copies of their chromosome types, such as bread wheat, which is hexaploid and has six copies of seven different chromosome types – 42 chromosomes in total.
Normal members of a particular eukaryotic species all have the same number of nuclear chromosomes (see the table). Other eukaryotic chromosomes, i.e., mitochondrial and plasmid-like small chromosomes, are much more variable in number, and there may be thousands of copies per cell.
Asexually reproducing species have one set of chromosomes that are the same in all body cells. However, asexual species can be either haploid or diploid.
Sexually reproducing species have somatic cells (body cells), which are diploid [2n] having two sets of chromosomes (23 pairs in humans with one set of 23 chromosomes from each parent), one set from the mother and one from the father. Gametes, reproductive cells, are haploid [n]: They have one set of chromosomes. Gametes are produced by meiosis of a diploid germ line cell. During meiosis, the matching chromosomes of father and mother can exchange small parts of themselves (crossover), and thus create new chromosomes that are not inherited solely from either parent. When a male and a female gamete merge (fertilization), a new diploid organism is formed.
Some animal and plant species are polyploid [Xn]: They have more than two sets of homologous chromosomes. Plants important in agriculture such as tobacco or wheat are often polyploid, compared to their ancestral species. Wheat has a haploid number of seven chromosomes, still seen in some cultivars as well as the wild progenitors. The more-common pasta and bread wheat types are polyploid, having 28 (tetraploid) and 42 (hexaploid) chromosomes, compared to the 14 (diploid) chromosomes in the wild wheat.[60]
Prokaryote species generally have one copy of each major chromosome, but most cells can easily survive with multiple copies.[61] For example, Buchnera, a symbiont of aphids has multiple copies of its chromosome, ranging from 10–400 copies per cell.[62] However, in some large bacteria, such as Epulopiscium fishelsoni up to 100,000 copies of the chromosome can be present.[63] Plasmids and plasmid-like small chromosomes are, as in eukaryotes, highly variable in copy number. The number of plasmids in the cell is almost entirely determined by the rate of division of the plasmid – fast division causes high copy number.
In general, the karyotype is the characteristic chromosome complement of a eukaryote species.[64] The preparation and study of karyotypes is part of cytogenetics.
Although the replication and transcription of DNA is highly standardized in eukaryotes, the same cannot be said for their karyotypes, which are often highly variable. There may be variation between species in chromosome number and in detailed organization.
In some cases, there is significant variation within species. Often there is:
Also, variation in karyotype may occur during development from the fertilized egg.
The technique of determining the karyotype is usually called karyotyping. Cells can be locked part-way through division (in metaphase) in vitro (in a reaction vial) with colchicine. These cells are then stained, photographed, and arranged into a karyogram, with the set of chromosomes arranged, autosomes in order of length, and sex chromosomes (here X/Y) at the end.
Like many sexually reproducing species, humans have special gonosomes (sex chromosomes, in contrast to autosomes). These are XX in females and XY in males. 
Investigation into the human karyotype took many years to settle the most basic question: How many chromosomes does a normal diploid human cell contain? In 1912, Hans von Winiwarter reported 47 chromosomes in spermatogonia and 48 in oogonia, concluding an XX/XO sex determination mechanism.[65] Painter in 1922 was not certain whether the diploid number of man is 46 or 48, at first favouring 46.[66] He revised his opinion later from 46 to 48, and he correctly insisted on humans having an XX/XY system.[67]
New techniques were needed to definitively solve the problem:
It took until 1954 before the human diploid number was confirmed as 46.[68][69] Considering the techniques of Winiwarter and Painter, their results were quite remarkable.[70] Chimpanzees, the closest living relatives to modern humans, have 48 chromosomes as do the other great apes: in humans two chromosomes fused to form chromosome 2.
Chromosomal aberrations are disruptions in the normal chromosomal content of a cell and are a major cause of genetic conditions in humans, such as Down syndrome, although most aberrations have little to no effect. Some chromosome abnormalities do not cause disease in carriers, such as translocations, or chromosomal inversions, although they may lead to a higher chance of bearing a child with a chromosome disorder. Abnormal numbers of chromosomes or chromosome sets, called aneuploidy, may be lethal or may give rise to genetic disorders.[71] Genetic counseling is offered for families that may carry a chromosome rearrangement.
The gain or loss of DNA from chromosomes can lead to a variety of genetic disorders. Human examples include:
Exposure of males to certain lifestyle, environmental and/or occupational hazards may increase the risk of aneuploid spermatozoa.[75]  In particular, risk of aneuploidy is increased by tobacco smoking,[76][77] and occupational exposure to benzene,[78] insecticides,[79][80]  and perfluorinated compounds.[81]  Increased aneuploidy is often associated with increased DNA damage in spermatozoa.



Gene duplication - Wikipedia
Gene duplication (or chromosomal duplication or      gene amplification) is a major mechanism through which new genetic material is generated during molecular evolution. It can be defined as any duplication of a region of DNA that contains a gene.  Gene duplications can arise as products of several types of errors in DNA replication and repair machinery as well as through fortuitous capture by selfish genetic elements.  Common sources of gene duplications include ectopic recombination,  retrotransposition event, aneuploidy, polyploidy, and replication slippage.[1]
Duplications arise from an event termed unequal crossing-over that occurs during meiosis between misaligned homologous chromosomes.The chance of this happening is a function of the degree of sharing of repetitive elements between two chromosomes. The products of this recombination are a duplication at the site of the exchange and a reciprocal deletion. Ectopic recombination is typically mediated by sequence similarity at the duplicate breakpoints, which form direct repeats.  Repetitive genetic elements such as transposable elements offer one source of repetitive DNA that can facilitate recombination, and they are often found at duplication breakpoints in plants and mammals.[2]
Replication slippage is an error in DNA replication that can produce duplications of short genetic sequences.  During replication DNA polymerase begins to copy the DNA.  At some point during the replication process, the polymerase dissociates from the DNA and replication stalls.  When the polymerase reattaches to the DNA strand, it aligns the replicating strand to an incorrect position and incidentally copies the same section more than once.  Replication slippage is also often facilitated by repetitive sequences, but requires only a few bases of similarity.
During the cellular invasion by a replicating retroelement or retrovirus, viral proteins copy their genome by reverse transcribing RNA to DNA. If viral proteins aberrantly attach to cellular mRNA, they can reverse transcribe copies of genes to create retrogenes.  Retrogenes usually lack intronic sequences and often contain poly A sequences that are also integrated into the genome.  Many retrogenes display changes in gene regulation in comparison to their parental gene sequences, which sometimes results in novel functions.
Aneuploidy occurs when nondisjunction at a single chromosome results in an abnormal number of chromosomes. Aneuploidy is often harmful and in mammals regularly leads to spontaneous abortions (miscarriages).  Some aneuploid individuals are viable, for example trisomy 21 in humans, which leads to Down syndrome.  Aneuploidy often alters gene dosage in ways that are detrimental to the organism; therefore, it is unlikely to spread through populations.
Whole genome duplication, or  polyploidy, is a product of nondisjunction during meiosis which results in additional copies of the entire genome.  Polyploidy is common in plants, but historically has also occurred in animals, with two rounds of whole genome duplication in the vertebrate lineage leading to humans.[3] After whole genome duplications many sets of additional genes are eventually lost, returning to singleton state.  However, retention of many genes, most notably Hox genes, has led to adaptive innovation.
Polyploidy is also a well known source of speciation, as offspring, which have different numbers of chromosomes compared to parent species, are often unable to interbreed with non-polyploid organisms.  Whole genome duplications are thought to be less detrimental than aneuploidy as the relative dosage of individual genes should be the same.
Gene duplications are an essential source of genetic novelty that can lead to evolutionary innovation. Duplication creates genetic redundancy, where the second copy of the gene is often free from selective pressure—that is, mutations of it have no deleterious effects to its host organism.  If one copy of a gene experiences a mutation that affects its original function, the second copy can serve as a 'spare part' and continue to function correctly.  Thus, duplicate genes accumulate mutations faster than a functional single-copy gene, over generations of organisms, and it is possible for one of the two copies to develop a new and different function. Some examples of such neofunctionalization is the apparent mutation of a duplicated digestive gene in a family of ice fish into an antifreeze gene and duplication leading to a novel snake venom gene[4] and the synthesis of 1 beta-hydroxytestosterone.[5]
Gene duplication is believed to play a major role in evolution; this stance has been held by members of the scientific community for over 100 years.[6] Susumu Ohno was one of the most famous developers of this theory in his classic book Evolution by gene duplication (1970).[7]  Ohno argued that gene duplication is the most important evolutionary force since the emergence of the universal common ancestor.[8]
Major genome duplication events can be quite common. It is believed that the entire yeast genome underwent duplication about 100 million years ago.[9] Plants are the most prolific genome duplicators.  For example, wheat is hexaploid (a kind of polyploid), meaning that it has six copies of its genome.
Another possible fate for duplicate genes is that both copies are equally free to accumulate degenerative mutations, so long as any defects are complemented by the other copy. This leads to a neutral "subfunctionalization" or DDC (duplication-degeneration-complementation) model,[10][11] in which the functionality of the original gene is distributed among the two copies.  Neither gene can be lost, as both now perform important non-redundant functions, but ultimately neither is able to achieve novel functionality.
Subfunctionalization can occur through neutral processes in which mutations accumulate with no detrimental or beneficial effects.  However, in some cases subfunctionalization can occur with clear adaptive benefits.  If an ancestral gene is pleiotropic and performs two functions, often neither one of these two functions can be changed without affecting the other function.  In this way, partitioning the ancestral functions into two separate genes can allow for adaptive specialization of subfunctions, thereby providing an adaptive benefit.[12]
Often the resulting genomic variation leads to gene dosage dependent neurological disorders such as Rett-like syndrome and Pelizaeus–Merzbacher disease.[13]  Such detrimental mutations are likely to be lost from the population and will not be preserved or develop novel functions. However, many duplications are, in fact, not detrimental or beneficial, and these neutral sequences may be lost or may spread through the population through random fluctuations via genetic drift.
The two genes that exist after a gene duplication event are called paralogs and usually code for proteins with a similar function and/or structure.  By contrast, orthologous genes present in different species which are each originally derived from the same ancestral sequence.  (See Homology of sequences in genetics).
It is important (but often difficult) to differentiate between paralogs and orthologs in biological research. Experiments on human gene function can often be carried out on other species if a homolog to a human gene can be found in the genome of that species, but only if the homolog is orthologous. If they are paralogs and resulted from a gene duplication event, their functions are likely to be too different. One or more copies of duplicated genes that constitute a gene family may be affected by insertion of transposable elements that causes significant variation between them in their sequence and finally may become responsible for divergent evolution. This may also render the chances and the rate of gene conversion between the homologs of gene duplicates due to less or no similarity in their sequences.
Paralogs can be identified in single genomes through a sequence comparison of all annotated gene models to one another.  Such a comparison can be performed on translated amino acid sequences (e.g. BLASTp, tBLASTx) to identify ancient duplications or on DNA nucleotide sequences (e.g. BLASTn, megablast) to identify more recent duplications.  Most studies to identify gene duplications require reciprocal-best-hits or fuzzy reciprocal-best-hits, where each paralog must be the other's single best match in a sequence comparison.[14]
Most gene duplications exist as low copy repeats (LCRs), rather highly repetitive sequences like transposable elements. They are mostly found in pericentronomic, subtelomeric and interstitial regions of a chromosome. Many LCRs, due to their size (>1Kb), similarity, and orientation, are highly susceptible to duplications and deletions.
Technologies such as genomic microarrays, also called array comparative genomic hybridization (array CGH), are used to detect chromosomal abnormalities, such as microduplications, in a high throughput fashion from genomic DNA samples. In particular, DNA microarray technology can simultaneously monitor the expression levels of thousands of genes across many  treatments or experimental conditions, greatly facilitating the evolutionary studies of gene regulation after gene duplication or speciation.[15][16]
Gene duplications can also be identified through the use of next-generation sequencing platforms.  The simplest means to identify duplications in genomic resequencing data is through the use of paired-end sequencing reads.  Tandem duplications are indicated by sequencing read pairs which map in abnormal orientations.  Through a combination of increased sequence coverage and abnormal mapping orientation, it is possible to identify duplications in genomic sequencing data.
Gene duplication does not necessarily constitute a lasting change in a species' genome.  In fact, such changes often don't last past the initial host organism.  From the perspective of molecular genetics, amplification is one of many ways in which a gene can be overexpressed.  Genetic amplification can occur artificially, as with the use of the polymerase chain reaction technique to amplify short strands of DNA in vitro using enzymes, or it can occur naturally, as described above.  If it's a natural duplication, it can still take place in a somatic cell, rather than a germline cell (which would be necessary for a lasting evolutionary change).
Duplications of oncogenes are a common cause of many types of cancer. In such cases the genetic duplication occurs in a somatic cell and affects only the genome of the cancer cells themselves, not the entire organism, much less any subsequent offspring.



Promoter (genetics) - Wikipedia
In genetics, a promoter is a region of DNA that initiates transcription of a particular gene. Promoters are located near the transcription start sites of genes, on the same strand and upstream on the DNA (towards the 5' region of the sense strand).
Promoters can be about 100–1000 base pairs long.[1]
For transcription to take place, the enzyme that synthesizes RNA, known as RNA polymerase, must attach to the DNA near a gene. Promoters contain specific DNA sequences such as response elements that provide a secure initial binding site for RNA polymerase and for proteins called transcription factors that recruit RNA polymerase. These transcription factors have specific activator or repressor sequences of corresponding nucleotides that attach to specific promoters and regulate gene expression.
Promoters represent critical elements that can work in concert with other regulatory regions (enhancers, silencers, boundary elements/insulators) to direct the level of transcription of a given gene.
A promoter is induced in response to changes in abundance or conformation of regulatory proteins in a cell, which enable activating transcription factors to recruit RNA polymerase.[2][3]
As promoters are typically immediately adjacent to the gene in question, positions in the promoter are designated relative to the transcriptional start site, where transcription of DNA begins for a particular gene (i.e., positions upstream are negative numbers counting back from -1, for example -100 is a position 100 base pairs upstream).
In the cell nucleus, it seems that promoters are distributed preferentially at the edge of the chromosomal territories, likely for the co-expression of genes on different chromosomes.[4] Furthermore, in humans, promoters show certain structural features characteristic for each chromosome.[4]

In bacteria, the promoter contains two short sequence elements approximately 10 (Pribnow Box) and 35 nucleotides upstream from the transcription start site.
The above promoter sequences are recognized only by RNA polymerase holoenzyme containing sigma-70. RNA polymerase holoenzymes containing other sigma factors recognize different core promoter sequences.
Eukaryotic promoters are diverse and can be difficult to characterize, however, recent studies show that they are divided in more than 10 classes.[8]
Gene promoters are typically located upstream of the gene and can have regulatory elements several kilobases away from the transcriptional start site (enhancers). In eukaryotes, the transcriptional complex can cause the DNA to bend back on itself, which allows for placement of regulatory sequences far from the actual site of transcription. Eukaryotic RNA-polymerase-II-dependent promoters can contain a TATA element (consensus sequence TATAAA), which is recognized by the general transcription factor TATA-binding protein (TBP); and a B recognition element (BRE), which is recognized by the general transcription factor TFIIB.[5][9][10] The TATA element and BRE typically are located close to the transcriptional start site (typically within 30 to 40 base pairs).
Eukaryotic promoter regulatory sequences typically bind proteins called transcription factors that are involved in the formation of the transcriptional complex. An example is the E-box (sequence CACGTG), which binds transcription factors in the basic helix-loop-helix (bHLH) family (e.g. BMAL1-Clock, cMyc).[11] Some promoters that are targeted by multiple transcription factors might achieve a hyperactive state, leading to increased transcriptional activity.[12]
Bidirectional promoters are short (<1 kbp) intergenic regions of DNA between the 5' ends of the genes in a bidirectional gene pair.[13] A “bidirectional gene pair” refers to two adjacent genes coded on opposite strands, with their 5' ends oriented toward one another.[14] The two genes are often functionally related, and modification of their shared promoter region allows them to be co-regulated and thus co-expressed.[15] Bidirectional promoters are a common feature of mammalian genomes.[16] About 11% of human genes are bidirectionally paired.[13]
Bidirectionally paired genes in the Gene Ontology database shared at least one database-assigned functional category with their partners 47% of the time.[17] Microarray analysis has shown bidirectionally paired genes to be co-expressed to a higher degree than random genes or neighboring unidirectional genes.[13] Although co-expression does not necessarily indicate co-regulation, methylation of bidirectional promoter regions has been shown to downregulate both genes, and demethylation to upregulate both genes.[18] There are exceptions to this, however. In some cases (about 11%), only one gene of a bidirectional pair is expressed.[13] In these cases, the promoter is implicated in suppression of the non-expressed gene. The mechanism behind this could be competition for the same polymerases, or chromatin modification. Divergent transcription could shift nucleosomes to upregulate transcription of one gene, or remove bound transcription factors to downregulate transcription of one gene.[19]
Some functional classes of genes are more likely to be bidirectionally paired than others. Genes implicated in DNA repair are five times more likely to be regulated by bidirectional promoters than by unidirectional promoters. Chaperone proteins are three times more likely, and mitochondrial genes are more than twice as likely. Many basic housekeeping and cellular metabolic genes are regulated by bidirectional promoters.[13]
The overrepresentation of bidirectionally paired DNA repair genes associates these promoters with cancer. Forty-five percent of human somatic oncogenes seem to be regulated by bidirectional promoters – significantly more than non-cancer causing genes. Hypermethylation of the promoters between gene pairs WNT9A/CD558500, CTDSPL/BC040563, and KCNK15/BF195580 has been associated with tumors.[18]
Certain sequence characteristics have been observed in bidirectional promoters, including a lack of TATA boxes, an abundance of CpG islands, and a symmetry around the midpoint of dominant Cs and As on one side and Gs and Ts on the other. CCAAT boxes are common, as they are in many promoters that lack TATA boxes. In addition, the motifs NRF-1, GABPA, YY1, and ACTACAnnTCCC are represented in bidirectional promoters at significantly higher rates than in unidirectional promoters. The absence of TATA boxes in bidirectional promotors suggests that TATA boxes play a role in determining the directionality of promoters, but counterexamples of bidirectional promoters do possess TATA boxes and unidirectional promoters without them indicates that they cannot be the only factor.[20]
Although the term "bidirectional promoter" refers specifically to promoter regions of mRNA-encoding genes, luciferase assays have shown that over half of human genes do not have a strong directional bias. Research suggests that non-coding RNAs are frequently associated with the promoter regions of mRNA-encoding genes. It has been hypothesized that the recruitment and initiation of RNA polymerase II usually begins bidirectionally, but divergent transcription is halted at a checkpoint later during elongation. Possible mechanisms behind this regulation include sequences in the promoter region, chromatin modification, and the spatial orientation of the DNA.[19]
A subgenomic promoter is a promoter added to a virus for a specific heterologous gene, resulting in the formation of mRNA for that gene alone. Many positive-sense RNA viruses produce these subgenomic mRNAs (sgRNA) as one of the common infection techniques used by these viruses and generally transcribe late viral genes. Subgenomic promoters range from 24 nucleotide (Sindbis virus) to over 100 nucleotides (Beet necrotic yellow vein virus) and are usually found upstream of the transcription start. [21]
A wide variety of algorithms have been developed to facilitate detection of promoters in genomic sequence, and promoter prediction is a common element of many gene prediction methods. A promoter region is located before the -35 and -10 Consensus sequences. The closer the promoter region is to the consensus sequences the more often transcription of that gene will take place. There is not a set pattern for promoter regions as there are for consensus sequences.
Changes in promoter sequences are critical in evolution as indicated by the relatively stable number of genes in many lineages. For instance, most vertebrates have roughly the same number of protein-coding genes (about 20,000) which are often highly conserved in sequence, hence much of evolutionary change must come from changes in gene expression.[4][8]
Given the short sequences of most promoter elements, promoters can rapidly evolve from random sequences. For instance, in E. coli, ~60% of random sequences can evolve expression levels comparable to the wild-type lac promoter with only one mutation, and that ~10% of random sequences can serve as active promoters even without evolution.[23]
Other recent studies suggest that promoters of genes may be the primary cause of diabetes.[24] Promoters of genes associated with diabetes by Genome-wide association studies (GWAS) show specific DNA patterns for each phenotype.[24] This observation indicates that the promoters of these genes use specific transcription factors for each diabetes phenotype.[24]
The initiation of the transcription is a multistep sequential process that involves several mechanisms: promoter location, initial reversible binding of RNA polymerase, conformational changes in RNA polymerase, conformational changes in DNA, binding of nucleoside triphosphate  (NTP) to the functional RNA polymerase-promoter complex,  and nonproductive and productive initiation of RNA synthesis.[25]
The promoter binding process is crucial in the understanding of the process of gene expression.
Although RNA polymerase holoenzyme shows high affinity to non-specific sites of the DNA, this characteristic does not allow us to clarify the process of promoter location.[26] This process of promoter location has been attributed to the structure of the holoenzyme to DNA and sigma 4 to DNA complexes.[27]
Most diseases are heterogeneous in cause, meaning that one "disease" is often many different diseases at the molecular level, though symptoms exhibited and response to treatment may be identical.  How diseases of different molecular origin respond to treatments is partially addressed in the discipline of pharmacogenomics.
Not listed here are the many kinds of cancers involving aberrant transcriptional regulation owing to creation of chimeric genes through pathological chromosomal translocation. Importantly, intervention in the number or structure of promoter-bound proteins is one key to treating a disease without affecting expression of unrelated genes sharing elements with the target gene.[28] Some genes whose change is not desirable are capable of influencing the potential of a cell to become cancerous.[29]
In humans, about 70% of promoters located near the transcription start site of a gene (proximal promoters) contain a CpG island.[30][31]   CpG islands are generally 200 to 2000 base pairs long, have a C:G base pair content >50%, and have regions of DNA where a cytosine nucleotide is followed by a guanine nucleotide and this occurs frequently in the linear sequence of bases along its 5' → 3' direction.
Distal promoters also frequently contain CpG islands, such as the promoter of the DNA repair gene ERCC1, where the CpG island-containing promoter is located about 5,400 nucleotides upstream of the coding region of the ERCC1 gene.[32]  CpG islands also occur frequently in promoters for functional noncoding RNAs such as microRNAs.
In humans, DNA methylation occurs at the 5' position of the pyrimidine ring of the cytosine residues within CpG sites to form 5-methylcytosines.  The presence of multiple methylated CpG sites in CpG islands of promoters causes stable silencing of genes.[33]  Silencing of a gene may be initiated by other mechanisms, but this is often followed by methylation of CpG sites in the promoter CpG island to cause the stable silencing of the gene.[33]
Generally, in progression to cancer, hundreds of genes are silenced or activated.  Although silencing of some genes in cancers occurs by mutation, a large proportion of carcinogenic gene silencing is a result of altered DNA methylation (see DNA methylation in cancer).  DNA methylation causing silencing in cancer typically occurs at multiple CpG sites in the CpG islands that are present in the promoters of protein coding genes.
Altered expressions of microRNAs also silence or activate many genes in progression to cancer (see microRNAs in cancer).  Altered microRNA expression occurs through hyper/hypo-methylation of CpG sites in CpG islands in promoters controlling transcription of the microRNAs.
Silencing of DNA repair genes through methylation of CpG islands in their promoters appears to be especially important in progression to cancer (see methylation of DNA repair genes in cancer).
The usage of the term canonical sequence to refer to a promoter is often problematic, and can lead to misunderstandings about promoter sequences. Canonical implies perfect, in some sense.
In the case of a transcription factor binding site, there may be a single sequence that binds the protein most strongly under specified cellular conditions. This might be called canonical.
However, natural selection may favor less energetic binding as a way of regulating transcriptional output. In this case, we may call the most common sequence in a population the wild-type sequence. It may not even be the most advantageous sequence to have under prevailing conditions.
Recent evidence also indicates that several genes (including the proto-oncogene c-myc) have G-quadruplex motifs as potential regulatory signals.
Some cases of many genetic diseases are associated with variations in promoters or transcription factors.
Examples include:
Some promoters are called constitutive as they are active in all circumstances in the cell, while others are regulated, becoming active in the cell only in response to specific stimuli.
When referring to a promoter some authors actually mean promoter + operator; i.e., the lac promoter is IPTG inducible, meaning that besides the lac promoter, the lac operator is also present. If the lac operator were not present the IPTG would not have an inducible effect.[38]
Another example is the Tac-Promoter system (Ptac). Notice how tac is written as a tac promoter, while in fact tac is actually both a promoter and an operator.[39]



Probability - Wikipedia

Related concepts and fundamentals:
Probability is the measure of the likelihood that an event will occur.[1] See glossary of probability and statistics. Probability is quantified as a number between 0 and 1, where, loosely speaking,[2] 0 indicates impossibility and 1 indicates certainty.[3][4] The higher the probability of an event, the more likely it is that the event will occur.     A simple example is the tossing of a fair (unbiased) coin. Since the coin is fair, the two outcomes ("heads" and "tails") are both equally probable; the probability of "heads" equals the probability of "tails"; and since no other outcomes are possible, the probability  of either "heads" or "tails" is 1/2 (which could also be written as 0.5 or 50%).
These concepts have been given an axiomatic mathematical formalization in probability theory, which is used widely in such areas of study as mathematics, statistics, finance, gambling, science (in particular physics), artificial intelligence/machine learning, computer science, game theory, and philosophy to, for example, draw inferences about the expected frequency of events. Probability theory is also used to describe the underlying mechanics and regularities of complex systems.[5]
When dealing with experiments that are random and well-defined in a purely theoretical setting (like tossing a fair coin), probabilities can be numerically described by the number of desired outcomes divided by the total number of all outcomes. For example, tossing a fair coin twice will yield "head-head", "head-tail", "tail-head", and "tail-tail" outcomes. The probability of getting an outcome of "head-head" is 1 out of 4 outcomes, or, in numerical terms, 1/4, 0.25 or 25%. However, when it comes to practical application, there are two major competing categories of probability interpretations, whose adherents possess different views about the fundamental nature of probability:
The word probability derives from the Latin probabilitas, which can also mean "probity", a measure of the authority of a witness in a legal case in Europe, and often correlated with the witness's nobility. In a sense, this differs much from the modern meaning of probability, which, in contrast, is a measure of the weight of empirical evidence, and is arrived at from inductive reasoning and statistical inference.[11]
The scientific study of probability is a modern development of mathematics. Gambling shows that there has been an interest in quantifying the ideas of probability for millennia, but exact mathematical descriptions arose much later. There are reasons for the slow development of the mathematics of probability. Whereas games of chance provided the impetus for the mathematical study of probability, fundamental issues[clarification needed] are still obscured by the superstitions of gamblers.[12]
According to Richard Jeffrey, "Before the middle of the seventeenth century, the term 'probable' (Latin probabilis) meant approvable, and was applied in that sense, unequivocally, to opinion and to action. A probable action or opinion was one such as sensible people would undertake or hold, in the circumstances."[13] However, in legal contexts especially, 'probable' could also apply to propositions for which there was good evidence.[14]
The sixteenth century Italian polymath Gerolamo Cardano demonstrated the efficacy of defining odds as the ratio of favourable to unfavourable outcomes (which implies that the probability of an event is given by the ratio of favourable outcomes to the total number of possible outcomes[15]).
Aside from the elementary work by Cardano, the doctrine of probabilities dates to the correspondence of Pierre de Fermat and Blaise Pascal (1654). Christiaan Huygens (1657) gave the earliest known scientific treatment of the subject.[16] Jakob Bernoulli's Ars Conjectandi (posthumous, 1713) and Abraham de Moivre's Doctrine of Chances (1718) treated the subject as a branch of mathematics.[17] See Ian Hacking's The Emergence of Probability[11] and James Franklin's The Science of Conjecture[18] for histories of the early development of the very concept of mathematical probability.
The theory of errors may be traced back to Roger Cotes's Opera Miscellanea (posthumous, 1722), but a memoir prepared by Thomas Simpson in 1755 (printed 1756) first applied the theory to the discussion of errors of observation.[citation needed] The reprint (1757) of this memoir lays down the axioms that positive and negative errors are equally probable, and that certain assignable limits define the range of all errors. Simpson also discusses continuous errors and describes a probability curve.
The first two laws of error that were proposed both originated with Pierre-Simon Laplace. The first law was published in 1774 and stated that the frequency of an error could be expressed as an exponential function of the numerical magnitude of the error, disregarding sign. The second law of error was proposed in 1778 by Laplace and stated that the frequency of the error is an exponential function of the square of the error.[19] The second law of error is called the normal distribution or the Gauss law. "It is difficult historically to attribute that law to Gauss, who in spite of his well-known precocity had probably not made this discovery before he was two years old."[19]
Daniel Bernoulli (1778) introduced the principle of the maximum product of the probabilities of a system of concurrent errors.
Adrien-Marie Legendre (1805) developed the method of least squares, and introduced it in his Nouvelles méthodes pour la détermination des orbites des comètes (New Methods for Determining the Orbits of Comets).[20] In ignorance of Legendre's contribution, an Irish-American writer, Robert Adrain, editor of "The Analyst" (1808), first deduced the law of facility of error,
where 



h


{\displaystyle h}

 is a constant depending on precision of observation, and 



c


{\displaystyle c}

 is a scale factor ensuring that the area under the curve equals 1. He gave two proofs, the second being essentially the same as John Herschel's (1850).[citation needed] Gauss gave the first proof that seems to have been known in Europe (the third after Adrain's) in 1809. Further proofs were given by Laplace (1810, 1812), Gauss (1823), James Ivory (1825, 1826), Hagen (1837), Friedrich Bessel (1838), W. F. Donkin (1844, 1856), and Morgan Crofton (1870). Other contributors were Ellis (1844), De Morgan (1864), Glaisher (1872), and Giovanni Schiaparelli (1875). Peters's (1856) formula[clarification needed] for r, the probable error of a single observation, is well known. 
In the nineteenth century authors on the general theory included Laplace, Sylvestre Lacroix (1816), Littrow (1833), Adolphe Quetelet (1853), Richard Dedekind (1860), Helmert (1872), Hermann Laurent (1873), Liagre, Didion, and Karl Pearson. Augustus De Morgan and George Boole improved the exposition of the theory.
Andrey Markov introduced[21] the notion of Markov chains (1906), which played an important role in stochastic processes theory and its applications. The modern theory of probability based on the measure theory was developed by Andrey Kolmogorov (1931).[22]
On the geometric side (see integral geometry) contributors to The Educational Times were influential (Miller, Crofton, McColl, Wolstenholme, Watson, and Artemas Martin).[citation needed]
Like other theories, the theory of probability is a representation of its concepts in formal terms—that is, in terms that can be considered separately from their meaning. These formal terms are manipulated by the rules of mathematics and logic, and any results are interpreted or translated back into the problem domain.
There have been at least two successful attempts to formalize probability, namely the Kolmogorov formulation and the Cox formulation. In Kolmogorov's formulation (see probability space), sets are interpreted as events and probability itself as a measure on a class of sets. In Cox's theorem, probability is taken as a primitive (that is, not further analyzed) and the emphasis is on constructing a consistent assignment of probability values to propositions. In both cases, the laws of probability are the same, except for technical details.
There are other methods for quantifying uncertainty, such as the Dempster–Shafer theory or possibility theory, but those are essentially different and not compatible with the laws of probability as usually understood.
Probability theory is applied in everyday life in risk assessment and modeling. The insurance industry and markets use actuarial science to determine pricing and make trading decisions. Governments apply probabilistic methods in environmental regulation, entitlement analysis (Reliability theory of aging and longevity), and financial regulation.
A good example of the use of probability theory in equity trading is the effect of the perceived probability of any widespread Middle East conflict on oil prices, which have ripple effects in the economy as a whole. An assessment by a commodity trader that a war is more likely can send that commodity's prices up or down, and signals other traders of that opinion. Accordingly, the probabilities are neither assessed independently nor necessarily very rationally. The theory of behavioral finance emerged to describe the effect of such groupthink on pricing, on policy, and on peace and conflict.[23]
In addition to financial assessment, probability can be used to analyze trends in biology (e.g. disease spread) as well as ecology (e.g. biological Punnett squares). As with finance, risk assessment can be used as a statistical tool to calculate the likelihood of undesirable events occurring and can assist with implementing protocols to avoid encountering such circumstances. Probability is used to design games of chance so that casinos can make a guaranteed profit, yet provide payouts to players that are frequent enough to encourage continued play.[24]
The discovery of rigorous methods to assess and combine probability assessments has changed society.[25][citation needed] It is important for most citizens to understand how probability assessments are made, and how they contribute to decisions.[25][citation needed]
Another significant application of probability theory in everyday life is reliability. Many consumer products, such as automobiles and consumer electronics, use reliability theory in product design to reduce the probability of failure. Failure probability may influence a manufacturer's decisions on a product's warranty.[26]
The cache language model and other statistical language models that are used in natural language processing are also examples of applications of probability theory.
Consider an experiment that can produce a number of results. The collection of all possible results is called the sample space of the experiment. The power set of the sample space is formed by considering all different collections of possible results. For example, rolling a dice can produce six possible results. One collection of possible results gives an odd number on the dice. Thus, the subset {1,3,5} is an element of the power set of the sample space of dice rolls. These collections are called "events". In this case, {1,3,5} is the event that the dice falls on some odd number. If the results that actually occur fall in a given event, the event is said to have occurred.
A probability is a way of assigning every event a value between zero and one, with the requirement that the event made up of all possible results (in our example, the event {1,2,3,4,5,6}) is assigned a value of one. To qualify as a probability, the assignment of values must satisfy the requirement that if you look at a collection of mutually exclusive events (events with no common results, e.g., the events {1,6}, {3}, and {2,4} are all mutually exclusive), the probability that at least one of the events will occur is given by the sum of the probabilities of all the individual events.[27]
The probability of an event A is written as 



P
(
A
)


{\displaystyle P(A)}

, 



p
(
A
)


{\displaystyle p(A)}

, or 




Pr

(
A
)


{\displaystyle {\text{Pr}}(A)}

.[28] This mathematical definition of probability can extend to infinite sample spaces, and even uncountable sample spaces, using the concept of a measure.
The opposite or complement of an event A is the event [not A] (that is, the event of A not occurring), often denoted as 





A
¯


,

A

∁


,
¬
A


{\displaystyle {\overline {A}},A^{\complement },\neg A}

, or 




∼

A


{\displaystyle {\sim }A}

; its probability is given by P(not A) = 1 − P(A).[29] As an example, the chance of not rolling a six on a six-sided die is 1 – (chance of rolling a six) 



=
1
−



1
6



=



5
6





{\displaystyle =1-{\tfrac {1}{6}}={\tfrac {5}{6}}}

. See Complementary event for a more complete treatment.
If two events A and B occur on a single performance of an experiment, this is called the intersection or joint probability of A and B, denoted as 



P
(
A
∩
B
)


{\displaystyle P(A\cap B)}

.
If two events, A and B are independent then the joint probability is
for example, if two coins are flipped the chance of both being heads is 






1
2



×



1
2



=



1
4





{\displaystyle {\tfrac {1}{2}}\times {\tfrac {1}{2}}={\tfrac {1}{4}}}

.[30]
If either event A or event B but never both occurs on a single performance of an experiment, then they are called mutually exclusive events.
If two events are mutually exclusive then the probability of both occurring is denoted as 



P
(
A
∩
B
)


{\displaystyle P(A\cap B)}

.
If two events are mutually exclusive then the probability of either occurring is denoted as 



P
(
A
∪
B
)


{\displaystyle P(A\cup B)}

.
For example, the chance of rolling a 1 or 2 on a six-sided die is 



P
(
1


 or 


2
)
=
P
(
1
)
+
P
(
2
)
=



1
6



+



1
6



=



1
3



.


{\displaystyle P(1{\mbox{ or }}2)=P(1)+P(2)={\tfrac {1}{6}}+{\tfrac {1}{6}}={\tfrac {1}{3}}.}


If the events are not mutually exclusive then
For example, when drawing a single card at random from a regular deck of cards, the chance of getting a heart or a face card (J,Q,K) (or one that is both) is 






13
52



+



12
52



−



3
52



=



11
26





{\displaystyle {\tfrac {13}{52}}+{\tfrac {12}{52}}-{\tfrac {3}{52}}={\tfrac {11}{26}}}

, because of the 52 cards of a deck 13 are hearts, 12 are face cards, and 3 are both: here the possibilities included in the "3 that are both" are included in each of the "13 hearts" and the "12 face cards" but should only be counted once.
Conditional probability is the probability of some event A, given the occurrence of some other event B.
Conditional probability is written 



P
(
A
∣
B
)


{\displaystyle P(A\mid B)}

, and is read "the probability of A, given B". It is defined by[31]
If 



P
(
B
)
=
0


{\displaystyle P(B)=0}

 then 



P
(
A
∣
B
)


{\displaystyle P(A\mid B)}

 is formally undefined by this expression. However, it is possible to define a conditional probability for some zero-probability events using a σ-algebra of such events (such as those arising from a continuous random variable).[citation needed]
For example, in a bag of 2 red balls and 2 blue balls (4 balls in total), the probability of taking a red ball is 



1

/

2


{\displaystyle 1/2}

; however, when taking a second ball, the probability of it being either a red ball or a blue ball depends on the ball previously taken, such as, if a red ball was taken, the probability of picking a red ball again would be 



1

/

3


{\displaystyle 1/3}

 since only 1 red and 2 blue balls would have been remaining.
In probability theory and applications, Bayes' rule relates the odds of event 




A

1




{\displaystyle A_{1}}

 to event 




A

2




{\displaystyle A_{2}}

, before (prior to) and after (posterior to) conditioning on another event 



B


{\displaystyle B}

. The odds on 




A

1




{\displaystyle A_{1}}

 to event 




A

2




{\displaystyle A_{2}}

 is simply the ratio of the probabilities of the two events. When arbitrarily many events 



A


{\displaystyle A}

 are of interest, not just two, the rule can be rephrased as posterior is proportional to prior times likelihood, 



P
(
A

|

B
)
∝
P
(
A
)
P
(
B

|

A
)


{\displaystyle P(A|B)\propto P(A)P(B|A)}

 where the proportionality symbol means that the left hand side is proportional to  (i.e., equals a constant times) the right hand side as 



A


{\displaystyle A}

 varies, for fixed or given 



B


{\displaystyle B}

 (Lee, 2012; Bertsch McGrayne, 2012). In this form it goes back to Laplace (1774) and to Cournot (1843); see Fienberg (2005). See Inverse probability and Bayes' rule.
In a deterministic universe, based on Newtonian concepts, there would be no probability if all conditions were known (Laplace's demon), (but there are situations in which sensitivity to initial conditions exceeds our ability to measure them, i.e. know them).  In the case of a roulette wheel, if the force of the hand and the period of that force are known, the number on which the ball will stop would be a certainty (though as a practical matter, this would likely be true only of a roulette wheel that had not been exactly levelled – as Thomas A. Bass' Newtonian Casino revealed).  This also assumes knowledge of inertia and friction of the wheel, weight, smoothness and roundness of the ball, variations in hand speed during the turning and so forth. A probabilistic description can thus be more useful than Newtonian mechanics for analyzing the pattern of outcomes of repeated rolls of a roulette wheel. Physicists face the same situation in kinetic theory of gases, where the system, while deterministic in principle, is so complex (with the number of molecules typically the order of magnitude of the Avogadro constant 7023602000000000000♠6.02×1023) that only a statistical description of its properties is feasible.
Probability theory is required to describe quantum phenomena.[32] A revolutionary discovery of early 20th century physics was the random character of all physical processes that occur at sub-atomic scales and are governed by the laws of quantum mechanics. The objective wave function evolves deterministically but, according to the Copenhagen interpretation, it deals with probabilities of observing, the outcome being explained by a wave function collapse when an observation is made. However, the loss of determinism for the sake of instrumentalism did not meet with universal approval. Albert Einstein famously remarked in a letter to Max Born: "I am convinced that God does not play dice".[33] Like Einstein, Erwin Schrödinger, who discovered the wave function, believed quantum mechanics is a statistical approximation of an underlying deterministic reality.[34] In some modern interpretations of the statistical mechanics of measurement, quantum decoherence is invoked to account for the appearance of subjectively probabilistic experimental outcomes.



Carcinogen - Wikipedia
A carcinogen is any substance, radionuclide, or radiation that promotes carcinogenesis, the formation of cancer. This may be due to the ability to damage the genome or to the disruption of cellular metabolic processes. Several radioactive substances are considered carcinogens, but their carcinogenic activity is attributed to the radiation, for example gamma rays and alpha particles, which they emit. Common examples of non-radioactive carcinogens are inhaled asbestos, certain dioxins, and tobacco smoke.  Although the public generally associates carcinogenicity with synthetic chemicals, it is equally likely to arise in both natural and synthetic substances.[1] Carcinogens are not necessarily immediately toxic; thus, their effect can be insidious.
Cancer is any disease in which normal cells are damaged and do not undergo programmed cell death as fast as they divide via mitosis. Carcinogens may increase the risk of cancer by altering cellular metabolism or damaging DNA directly in cells, which interferes with biological processes, and induces the uncontrolled, malignant division, ultimately leading to the formation of tumors. Usually, severe DNA damage leads to programmed cell death, but if the programmed cell death pathway is damaged, then the cell cannot prevent itself from becoming a cancer cell.
There are many natural carcinogens. Aflatoxin B1, which is produced by the fungus Aspergillus flavus growing on stored grains, nuts and peanut butter, is an example of a potent, naturally occurring microbial carcinogen. Certain viruses such as hepatitis B and human papilloma virus have been found to cause cancer in humans. The first one shown to cause cancer in animals is Rous sarcoma virus, discovered in 1910 by Peyton Rous.  Other infectious organisms which cause cancer in humans include some bacteria (e.g. Helicobacter pylori [2][3]) and helminths (e.g. Opisthorchis viverrini [4] and Clonorchis sinensis [5].
Dioxins and dioxin-like compounds, benzene, kepone, EDB, and asbestos have all been classified as carcinogenic.[6] As far back as the 1930s, Industrial smoke and tobacco smoke were identified as sources of dozens of carcinogens, including benzo[a]pyrene, tobacco-specific nitrosamines such as nitrosonornicotine, and reactive aldehydes such as formaldehyde, which is also a hazard in embalming and making plastics. Vinyl chloride, from which PVC is manufactured, is a carcinogen and thus a hazard in PVC production.
Co-carcinogens are chemicals that do not necessarily cause cancer on their own, but promote the activity of other carcinogens in causing cancer.
After the carcinogen enters the body, the body makes an attempt to eliminate it through a process called biotransformation. The purpose of these reactions is to make the carcinogen more water-soluble so that it can be removed from the body. However, in some cases, these reactions can also convert a less toxic carcinogen into a more toxic carcinogen.
DNA is nucleophilic; therefore, soluble carbon electrophiles are carcinogenic, because DNA attacks them. For example, some alkenes are toxicated by human enzymes to produce an electrophilic epoxide. DNA attacks the epoxide, and is bound permanently to it. This is the mechanism behind the carcinogenicity of benzo[a]pyrene in tobacco smoke, other aromatics, aflatoxin and mustard gas.
CERCLA identifies all radionuclides as carcinogens, although the nature of the emitted radiation (alpha, beta, gamma, or neutron and the radioactive strength), its consequent capacity to cause ionization in tissues, and the magnitude of radiation exposure, determine the potential hazard. Carcinogenicity of radiation depends on the type of radiation, type of exposure, and penetration. For example, alpha radiation has low penetration and is not a hazard outside the body, but emitters are carcinogenic when inhaled or ingested.  For example, Thorotrast, a (incidentally radioactive) suspension previously used as a contrast medium in x-ray diagnostics, is a potent human carcinogen known because of its retention within various organs and persistent emission of alpha particles.  Low-level ionizing radiation may induce irreparable DNA damage (leading to replicational and transcriptional errors needed for neoplasia or may trigger viral interactions) leading to pre-mature aging and cancer.[8][9][10]
Not all types of electromagnetic radiation are carcinogenic. Low-energy waves on the electromagnetic spectrum including radio waves, microwaves, infrared radiation and visible light are thought not to be, because they have insufficient energy to break chemical bonds. Evidence for carcinogenic effects of non-ionizing radiation is generally inconclusive, though there are some documented cases of radar technicians with prolonged high exposure experiencing significantly higher cancer incidence.[11]
Higher-energy radiation, including ultraviolet radiation (present in sunlight), x-rays, and gamma radiation, generally is carcinogenic, if received in sufficient doses.   For most people, ultraviolet radiations from sunlight is the most common cause of skin cancer.  In Australia, where people with pale skin are often exposed to strong sunlight, melanoma is the most common cancer diagnosed in people aged 15–44 years.[12][13]
Substances or foods irradiated with electrons or electromagnetic radiation (such as microwave, X-ray or gamma) are not carcinogenic.[citation needed] In contrast, non-electromagnetic neutron radiation produced inside nuclear reactors can produce secondary radiation through nuclear transmutation.
Chemicals used in processed and cured meat such as some brands of bacon, sausages and ham may or may not produce carcinogens.[14] For example, nitrites used as food preservatives in cured meat such as bacon have also been noted as being carcinogenic with demographic links, but not causation, to colon cancer.[15] Cooking food at high temperatures, for example grilling or barbecuing meats, can, or can not, also lead to the formation of minute quantities of many potent carcinogens that are comparable to those found in cigarette smoke (i.e., benzo[a]pyrene).[16] Charring of food looks like coking and tobacco pyrolysis, and produces carcinogens. There are several carcinogenic pyrolysis products, such as polynuclear aromatic hydrocarbons, which are converted by human enzymes into epoxides, which attach permanently to DNA. Pre-cooking meats in a microwave oven for 2–3 minutes before grilling shortens the time on the hot pan, and removes heterocyclic amine (HCA) precursors, which can help minimize the formation of these carcinogens.[17]
Reports from the Food Standards Agency have found that the known animal carcinogen acrylamide is generated in fried or overheated carbohydrate foods (such as french fries and potato chips).[18] Studies are underway at the FDA and European regulatory agencies to assess its potential risk to humans.
There is a strong association of smoking with lung cancer; the lifetime risk of developing lung cancer increases significantly in smokers.[19] A large number of known carcinogens are found in cigarette smoke. Potent carcinogens found in cigarette smoke include polycyclic aromatic hydrocarbons (PAH, such as benzo[a]pyrene), Benzene, and Nitrosamine.[20] The tar from cigarette smoke is similar to that of marijuana smoke and  contains similar carcinogens.[21]
Carcinogens can be classified as genotoxic or nongenotoxic. Genotoxins cause irreversible genetic damage or mutations by binding to DNA. Genotoxins include chemical agents like N-nitroso-N-methylurea (NMU) or non-chemical agents such as ultraviolet light and ionizing radiation. Certain viruses can also act as carcinogens by interacting with DNA.
Nongenotoxins do not directly affect DNA but act in other ways to promote growth. These include hormones and some organic compounds.[22]
The International Agency for Research on Cancer (IARC) is an intergovernmental agency established in 1965, which forms part of the World Health Organization of the United Nations. It is based in Lyon, France. Since 1971 it has published a series of Monographs on the Evaluation                          of Carcinogenic Risks to Humans[23] that have been highly influential in the classification of possible carcinogens.
The Globally Harmonized System of Classification and Labelling of Chemicals (GHS) is a United Nations initiative to attempt to harmonize the different systems of assessing chemical risk which currently exist (as of March 2009) around the world. It classifies carcinogens into two categories, of which the first may be divided again into subcategories if so desired by the competent regulatory authority:
The National Toxicology Program of the U.S. Department of Health and Human Services is mandated to produce a biennial Report on Carcinogens.[24] As of June 2011, the latest edition was the 12th report (2011).[6] It classifies carcinogens into two groups:
The American Conference of Governmental Industrial Hygienists (ACGIH) is a private organization best known for its publication of threshold limit values (TLVs) for occupational exposure and monographs on workplace chemical hazards. It assesses carcinogenicity as part of a wider assessment of the occupational hazards of chemicals.
The European Union classification of carcinogens is contained in the Dangerous Substances Directive and the Dangerous Preparations Directive. It consists of three categories:
This assessment scheme is being phased out in favor of the GHS scheme (see above), to which it is very close in category definitions.
Under a previous name, the NOHSC, in 1999 Safe Work Australia published the Approved Criteria for Classifying Hazardous Substances [NOHSC:1008(1999)].[25]
Section 4.76 of this document outlines the criteria for classifying carcinogens as approved by the Australian government. This classification consists of three categories:
A procarcinogen is a precursor to a carcinogen. One example is nitrites when taken in by the diet. They are not carcinogenic themselves, but turn into nitrosamines in the body, which can be carcinogenic.[26]
Occupational carcinogens are agents that pose a risk of cancer in several specific work-locations:
Not in widespread use, but found in:
circadian disruption[29]
In this section, the carcinogens implicated as the main causative agents of the four most common cancers worldwide are briefly described. These four cancers are lung, breast, colon, and stomach cancers.  Together they account for about 41% of worldwide cancer incidence and 42% of cancer deaths (for more detailed information on the carcinogens implicated in these and other cancers, see references[33][34]).
Lung cancer (pulmonary carcinoma) is the most common cancer in the world, both in terms of cases (1.6 million cases; 12.7% of total cancer cases) and deaths (1.4 million deaths; 18.2% of total cancer deaths).[35]  Lung cancer is largely caused by tobacco smoke. Risk estimates for lung cancer in the United States indicate that tobacco smoke is responsible for 90% of lung cancers. Other factors are implicated in lung cancer, and these factors can interact synergistically with smoking so that total attributable risk adds up to more than 100%. These factors include occupational exposure to carcinogens (about 9-15%), radon (10%) and outdoor air pollution (1-2%).[36] Tobacco smoke is a complex mixture of more than 5,300 identified chemicals. The most important carcinogens in tobacco smoke have been determined by a “Margin of Exposure” approach.[37] Using this approach, the most important tumorigenic compounds in tobacco smoke were, in order of importance, acrolein, formaldehyde, acrylonitrile, 1,3-butadiene, cadmium, acetaldehyde, ethylene oxide, and isoprene. Most of these compounds cause DNA damage by forming DNA adducts or by inducing other alterations in DNA.[34] DNA damages are subject to error-prone DNA repair or can cause replication errors. Such errors in repair or replication can result in mutations in tumor suppressor genes or oncogenes leading to cancer.
Breast cancer is the second most common cancer [(1.4 million cases, 10.9%), but ranks 5th as cause of death (458,000, 6.1%)].[35]  Increased risk of breast cancer is associated with persistently elevated blood levels of estrogen.[38]  Estrogen appears to contribute to breast carcinogenesis by three processes; (1) the metabolism of estrogen to genotoxic, mutagenic carcinogens, (2) the stimulation of tissue growth, and (3) the repression of phase II detoxification enzymes that metabolize ROS leading to increased oxidative DNA damage.[39][40][41]  The major estrogen in humans, estradiol, can be metabolized to quinone derivatives that form adducts with DNA.[42]  These derivatives can cause dupurination, the removal of bases from the phosphodiester backbone of DNA, followed by inaccurate repair or replication of the apurinic site leading to mutation and eventually cancer.  This genotoxic mechanism may interact in synergy with estrogen receptor-mediated, persistent cell proliferation to ultimately cause breast cancer.[42] Genetic background, dietary practices and environmental factors also likely contribute to the incidence of DNA damage and breast cancer risk.
Colorectal cancer is the third most common cancer [1.2 million cases (9.4%), 608,000 deaths (8.0%)].[35] Tobacco smoke may be responsible for up to 20% of colorectal cancers in the United States.[43]  In addition, substantial evidence implicates bile acids as an important factor in colon cancer.  Twelve studies (summarized in Bernstein et al.[44]) indicate that the bile acids deoxycholic acid (DCA) and/or lithocholic acid (LCA) induce production of DNA-damaging reactive oxygen species and/or reactive nitrogen species in human or animal colon cells.  Furthermore, 14 studies showed that DCA and LCA induce DNA damage in colon cells.  Also 27 studies reported that bile acids cause programmed cell death (apoptosis).  Increased apoptosis can result in selective survival of cells that are resistant to induction of apoptosis.[44]  Colon cells with reduced ability to undergo apoptosis in response to DNA damage would tend to accumulate mutations, and such cells may give rise to colon cancer.[44]  Epidemiologic studies have found that fecal bile acid concentrations are increased in populations with a high incidence of colon cancer.  Dietary increases in total fat or saturated fat result in elevated DCA and LCA in feces and elevated exposure of the colon epithelium to these bile acids.  When the bile acid DCA was added to the standard diet of wild-type mice invasive colon cancer was induced in 56% of the mice after 8 to 10 months.[45]  Overall, the available evidence indicates that DCA and LCA are centrally important DNA-damaging carcinogens in colon cancer.
Stomach cancer is the fourth most common cancer [990,000 cases (7.8%), 738,000 deaths (9.7%)].[35] Helicobacter pylori infection is the main causative factor in stomach cancer.  Chronic gastritis (inflammation) caused by H. pylori is often long-standing if not treated.  Infection of gastric epithelial cells with H. pylori results in increased production of reactive oxygen species (ROS).[46][47]  ROS cause oxidative DNA damage including the major base alteration 8-hydroxydeoxyguanosine (8-OHdG).  8-OHdG resulting from ROS is increased in chronic gastritis.  The altered DNA base can cause errors during DNA replication that have mutagenic and carcinogenic potential.  Thus H. pylori-induced ROS appear to be the major carcinogens in stomach cancer because they cause oxidative DNA damage leading to carcinogenic mutations.  Diet is thought to be a contributing factor in stomach cancer - in Japan where very salty pickled foods are popular, the incidence of stomach cancer is high.  Preserved meat such as bacon, sausages, and ham increases the risk while a diet high in fresh fruit and vegetables may reduce the risk. The risk also increases with age.[48]



Chain reaction - Wikipedia
A chain reaction is a sequence of reactions where a reactive product or by-product causes additional reactions to take place. In a chain reaction, positive feedback leads to a self-amplifying chain of events.
Chain reactions are one way that systems which are not in thermodynamic equilibrium can release energy or increase entropy in order to reach a state of higher entropy. For example, a system may not be able to reach a lower energy state by releasing energy into the environment, because it is hindered or prevented in some way from taking the path that will result in the energy release. If a reaction results in a small energy release making way for more energy releases in an expanding chain, then the system will typically collapse explosively until much or all of the stored energy has been released. 
A macroscopic metaphor for chain reactions is thus a snowball causing a larger snowball until finally an avalanche results ("snowball effect"). This is a result of stored gravitational potential energy seeking a path of release over friction. Chemically, the equivalent to a snow avalanche is a spark causing a forest fire. In nuclear physics, a single stray neutron can result in a prompt critical event, which may finally be energetic enough for a nuclear reactor meltdown or (in a bomb) a nuclear explosion.
Numerous chain reactions can be represented by a mathematical model based on Markov chains.
In 1913, the German chemist Max Bodenstein first put forth the idea of chemical chain reactions. If two molecules react, not only molecules of the final reaction products are formed, but also some unstable molecules which can further react with the parent molecules with a far larger probability than the initial reactants (In the new reaction, further unstable molecules are formed besides the stable products, and so on ..).
In 1918, Walther Nernst proposed that the photochemical reaction between hydrogen and chlorine is a chain reaction in order to explain what's known as the quantum yield phenomena. This means that one photon of light is responsible for the formation of as many as 106 molecules of the product HCl. Nernst suggested that the photon dissociates a Cl2 molecule into two Cl atoms which each initiate a long chain of reaction steps forming HCl.[1]
In 1923, Danish and Dutch scientists Christian Christiansen and Hendrik Anthony Kramers, in an analysis of the formation of polymers, pointed out that such a chain reaction need not start with a molecule excited by light, but could also start with two molecules colliding violently due to thermal energy as previously proposed for initiation of chemical reactions by van' t Hoff. [2]
Christiansen and Kramers also noted that if, in one link of the reaction chain, two or more unstable molecules are produced, the reaction chain would branch and grow. The result is in fact an exponential growth, thus giving rise to explosive increases in reaction rates, and indeed to chemical explosions themselves. This was the first proposal for the mechanism of chemical explosions.
A quantitative chain chemical reaction theory was created later on by Soviet physicist Nikolay Semyonov in 1934. [3] Semyonov shared the Nobel Prize in 1956 with Sir Cyril Norman Hinshelwood, who independently developed many of the same quantitative concepts. [2]
The main types of steps in chain reaction are of the following types.[1]
The chain length is defined as the average number of times the propagation cycle is repeated, and equals the overall reaction rate divided by the initiation rate.[1]
Some chain reactions have complex rate equations with fractional order or mixed order kinetics.
The reaction H2 + Br2 → 2 HBr proceeds by the following mechanism:[4][5]
As can be explained using the steady-state approximation, the thermal  reaction has an initial rate of fractional order (3/2), and a complete rate equation with a two-term denominator (mixed-order kinetics).[4][5]
A nuclear chain reaction was proposed by Leo Szilard in 1933, shortly after the neutron was discovered, yet more than five years before nuclear fission was first discovered. Szilárd knew of chemical chain reactions, and he had been reading about an energy-producing nuclear reaction involving high-energy protons bombarding lithium, demonstrated by John Cockcroft and Ernest Walton, in 1932. Now, Szilárd proposed to use neutrons theoretically produced from certain nuclear reactions in lighter isotopes, to induce further reactions in light isotopes that produced more neutrons. This would in theory produce a chain reaction at the level of the nucleus. He did not envision fission as one of these neutron-producing reactions, since this reaction was not known at the time. Experiments he proposed using beryllium and indium failed.
Later, after fission was discovered in 1938, Szilárd immediately realized the possibility of using neutron-induced fission as the particular nuclear reaction necessary to create a chain-reaction, so long as fission also produced neutrons. In 1939, with Enrico Fermi, Szilárd proved this neutron-multiplying reaction in uranium. In this reaction, a neutron plus a fissionable atom causes a fission resulting in a larger number of neutrons than the single one that was consumed in the initial reaction. Thus was born the practical nuclear chain reaction by the mechanism of neutron-induced nuclear fission.
Specifically, if one or more of the produced neutrons themselves interact with other fissionable nuclei, and these also undergo fission, then there is a possibility that the macroscopic overall fission reaction will not stop, but continue throughout the reaction material. This is then a self-propagating and thus self-sustaining chain reaction. This is the principle for nuclear reactors and atomic bombs.
Demonstration of a self-sustaining nuclear chain reaction was accomplished by Enrico Fermi and others, in the successful operation of Chicago Pile-1, the first artificial nuclear reactor, in late 1942.
An electron avalanche happens between two unconnected electrodes in a gas when an electric field exceeds a certain threshold. Random thermal collisions of gas atoms may result in a few free electrons and positively charged gas ions, in a process called impact ionization. Acceleration of these free electrons in a strong electric field causes them to gain energy, and when they impact other atoms, the energy causes release of new free electrons and ions (ionization), which fuels the same process. If this process happens faster than it is naturally quenched by ions recombining, the new ions multiply in successive cycles until the gas breaks down into a plasma and current flows freely in a discharge.
Electron avalanches are essential to the dielectric breakdown process within gases. The process can culminate in corona discharges, streamers, leaders, or in a spark or continuous electric arc that completely bridges the gap. The process may extend huge sparks — streamers in lightning discharges propagate by formation of electron avalanches created in the high potential gradient ahead of the streamers' advancing tips. Once begun, avalanches are often intensified by the creation of photoelectrons as a result of ultraviolet radiation emitted by the excited medium's atoms in the aft-tip region. The extremely high temperature of the resulting plasma cracks the surrounding gas molecules and the free ions recombine to create new chemical compounds.[7]
The process can also be used to detect radiation that initiates the process, as the passage of a single particles can amplified to large discharges. This is the mechanism of a Geiger counter and also the visualization possible with a spark chamber and other wire chambers.
An avalanche breakdown process can happen in semiconductors, which in some ways conduct electricity analogously to a mildly ionized gas. Semiconductors rely on free electrons knocked out of the crystal by thermal vibration for conduction. Thus, unlike metals, semiconductors become better conductors the higher the temperature. This sets up conditions for the same type of positive feedback—heat from current flow causes temperature to rise, which increases charge carriers, lowering resistance, and causing more current to flow. This can continue to the point of complete breakdown of normal resistance at a semiconductor junction, and failure of the device (this may be temporary or permanent depending on whether there is physical damage to the crystal). Certain devices, such as avalanche diodes, deliberately make use of the effect.



Epigenetics - Wikipedia

Epigenetics is the study of heritable phenotype changes that do not involve alterations in the DNA sequence.[1] The Greek prefix epi- (ἐπι- "over, outside of, around") in epigenetics implies features that are "on top of" or "in addition to" the traditional genetic basis for inheritance.[2] Epigenetics most often denotes changes that affect gene activity and expression, but can also be used to describe any heritable phenotypic change. Such effects on cellular and physiological phenotypic traits may result from external or environmental factors, or be part of normal developmental program. The standard definition of epigenetics requires these alterations to be heritable,[3][4] either in the progeny of cells or of organisms.
The term also refers to the changes themselves: functionally relevant changes to the genome that do not involve a change in the nucleotide sequence. Examples of mechanisms that produce such changes are DNA methylation and histone modification, each of which alters how genes are expressed without altering the underlying DNA sequence. Gene expression can be controlled through the action of repressor proteins that attach to silencer regions of the DNA. These epigenetic changes may last through cell divisions for the duration of the cell's life, and may also last for multiple generations even though they do not involve changes in the underlying DNA sequence of the organism;[5] instead, non-genetic factors cause the organism's genes to behave (or "express themselves") differently.[6]
One example of an epigenetic change in eukaryotic biology is the process of cellular differentiation. During morphogenesis, totipotent stem cells become the various pluripotent cell lines of the embryo, which in turn become fully differentiated cells. In other words, as a single fertilized egg cell – the zygote – continues to divide, the resulting daughter cells change into all the different cell types in an organism, including neurons, muscle cells, epithelium, endothelium of blood vessels, etc., by activating some genes while inhibiting the expression of others.[7]
Historically, some phenomena not necessarily heritable have also been described as epigenetic. For example, the term epigenetic has been used to describe any modification of chromosomal regions, especially histone modifications, whether or not these changes are heritable or associated with a phenotype. The consensus definition now requires a trait to be heritable for it to be considered epigenetic.[4]
The term epigenetics in its contemporary usage emerged in the 1990s, but for some years has been used in somewhat variable meanings.[8] A consensus definition of the concept of epigenetic trait as "stably heritable phenotype resulting from changes in a chromosome without alterations in the DNA sequence" was formulated at a Cold Spring Harbor meeting in 2008,[4] although alternate definitions that include non-heritable traits are still being used.[9]
The term epigenesis has a generic meaning "extra growth". It has been used in English since the 17th century.[10]
From the generic meaning, and the associated adjective epigenetic, C. H. Waddington coined the term epigenetics in 1942 as pertaining to epigenesis, in parallel to Valentin Haecker's 'phenogenetics' (Phänogenetik).[11] Epigenesis in the context of the biology of that period referred to the differentiation of cells from their initial totipotent state in embryonic development.[12]
When Waddington coined the term, the physical nature of genes and their role in heredity was not known; he used it as a conceptual model of how genes might interact with their surroundings to produce a phenotype; he used the phrase "epigenetic landscape" as a metaphor for biological development. Waddington held that cell fates were established in development (canalisation) much as a marble rolls down to the point of lowest local elevation.[13]
Waddington suggested visualising increasing irreversibility of cell type differentiation as ridges rising between the valleys where the marbles (cells) are travelling.[14] In recent times Waddington's notion of the epigenetic landscape has been rigorously formalized in the context of the systems dynamics state approach to the study of cell-fate.[15][16] Cell-fate determination is predicted to exhibit certain dynamics, such as attractor-convergence (the attractor can be an equilibrium point, limit cycle or strange attractor) or oscillatory.[16]
The term "epigenetic" has also been used in developmental psychology to describe psychological development as the result of an ongoing, bi-directional interchange between heredity and the environment.[17] Interactivist ideas of development have been discussed in various forms and under various names throughout the 19th and 20th centuries. An early version was proposed, among the founding statements in embryology, by Karl Ernst von Baer and popularized by Ernst Haeckel. A radical epigenetic view (physiological epigenesis) was developed by Paul Wintrebert. Another variation, probabilistic epigenesis, was presented by Gilbert Gottlieb in 2003.[18] This view encompasses all of the possible developing factors on an organism and how they not only influence the organism and each other, but how the organism also influences its own development.
The developmental psychologist Erik Erikson wrote of an epigenetic principle in his book Identity: Youth and Crisis (1968), encompassing the notion that we develop through an unfolding of our personality in predetermined stages, and that our environment and surrounding culture influence how we progress through these stages. This biological unfolding in relation to our socio-cultural settings is done in stages of psychosocial development, where "progress through each stage is in part determined by our success, or lack of success, in all the previous stages."[19][20][21]
Robin Holliday defined epigenetics as "the study of the mechanisms of temporal and spatial control of gene activity during the development of complex organisms."[22] Thus epigenetic can be used to describe anything other than DNA sequence that influences the development of an organism.
The more recent usage of the word in science has a stricter definition. It is, as defined by Arthur Riggs and colleagues, "the study of mitotically and/or meiotically heritable changes in gene function that cannot be explained by changes in DNA sequence."[23]
The term "epigenetics", however, has been used to describe processes which have not been demonstrated to be heritable such as some forms of histone modification; there are therefore attempts to redefine it in broader terms that would avoid the constraints of requiring heritability. For example, Adrian Bird defined epigenetics as "the structural adaptation of chromosomal regions so as to register, signal or perpetuate altered activity states."[5] This definition would be inclusive of transient modifications associated with DNA repair or cell-cycle phases as well as stable changes maintained across multiple cell generations, but exclude others such as templating of membrane architecture and prions unless they impinge on chromosome function. Such redefinitions however are not universally accepted and are still subject to dispute.[3] The NIH "Roadmap Epigenomics Project", ongoing as of 2016, uses the following definition: "For purposes of this program, epigenetics refers to both heritable changes in gene activity and expression (in the progeny of cells or of individuals) and also stable, long-term alterations in the transcriptional potential of a cell that are not necessarily heritable."[9]
In 2008, a consensus definition of the epigenetic trait, "stably heritable phenotype resulting from changes in a chromosome without alterations in the DNA sequence", was made at a Cold Spring Harbor meeting.[4]
The similarity of the word to "genetics" has generated many parallel usages. The "epigenome" is a parallel to the word "genome", referring to the overall epigenetic state of a cell, and epigenomics refers to more global analyses of epigenetic changes across the entire genome.[9] The phrase "genetic code" has also been adapted – the "epigenetic code" has been used to describe the set of epigenetic features that create different phenotypes in different cells. Taken to its extreme, the "epigenetic code" could represent the total state of the cell, with the position of each molecule accounted for in an epigenomic map, a diagrammatic representation of the gene expression, DNA methylation and histone modification status of a particular genomic region. More typically, the term is used in reference to systematic efforts to measure specific, relevant forms of epigenetic information such as the histone code or DNA methylation patterns.
Epigenetic changes modify the activation of certain genes, but not the genetic code sequence of DNA. The microstructure (not code) of DNA itself or the associated chromatin proteins may be modified, causing activation or silencing. This mechanism enables differentiated cells in a multicellular organism to express only the genes that are necessary for their own activity. Epigenetic changes are preserved when cells divide. Most epigenetic changes only occur within the course of one individual organism's lifetime; however, these epigenetic changes can be transmitted to the organism's offspring through a process called transgenerational epigenetic inheritance.  Moreover, if gene inactivation occurs in a sperm or egg cell that results in fertilization, this epigenetic modification may also be transferred to the next generation.[24]
Specific epigenetic processes include paramutation, bookmarking, imprinting, gene silencing, X chromosome inactivation, position effect, DNA methylation reprogramming, transvection, maternal effects, the progress of carcinogenesis, many effects of teratogens, regulation of histone modifications and heterochromatin, and technical limitations affecting parthenogenesis and cloning.
DNA damage can also cause epigenetic changes.[25][26][27] DNA damage is very frequent, occurring on average about 60,000 times a day per cell of the human body (see DNA damage (naturally occurring)). These damages are largely repaired, but at the site of a DNA repair, epigenetic changes can remain.[28] In particular, a double strand break in DNA can initiate unprogrammed epigenetic gene silencing both by causing DNA methylation as well as by promoting silencing types of histone modifications (chromatin remodeling - see next section).[29] In addition, the enzyme Parp1 (poly(ADP)-ribose polymerase) and its product poly(ADP)-ribose (PAR) accumulate at sites of DNA damage as part of a repair process.[30] This accumulation, in turn, directs recruitment and activation of the chromatin remodeling protein ALC1 that can cause nucleosome remodeling.[31] Nucleosome remodeling has been found to cause, for instance, epigenetic silencing of DNA repair gene MLH1.[23][32] DNA damaging chemicals, such as benzene, hydroquinone, styrene, carbon tetrachloride and trichloroethylene, cause considerable hypomethylation of DNA, some through the activation of oxidative stress pathways.[33]
Foods are known to alter the epigenetics of rats on different diets.[34] Some food components epigenetically increase the levels of DNA repair enzymes such as MGMT and MLH1[35] and p53.[36][37] Other food components can reduce DNA damage, such as soy isoflavones. In one study, markers for oxidative stress, such as modified nucleotides that can result from DNA damage, were decreased by a 3-week diet supplemented with soy.[38] A decrease in oxidative DNA damage was also observed 2 h after consumption of anthocyanin-rich bilberry (Vaccinium myrtillius L.) pomace extract.[39]
Epigenetic research uses a wide range of molecular biological techniques to further understanding of epigenetic phenomena, including chromatin immunoprecipitation (together with its large-scale variants ChIP-on-chip and ChIP-Seq), fluorescent in situ hybridization, methylation-sensitive restriction enzymes, DNA adenine methyltransferase identification (DamID) and bisulfite sequencing.[40] Furthermore, the use of bioinformatics methods has a role in (computational epigenetics).[40]
Several types of epigenetic inheritance systems may play a role in what has become known as cell memory,[41] note however that not all of these are universally accepted to be examples of epigenetics.
Covalent modifications of either DNA (e.g. cytosine methylation and hydroxymethylation) or of histone proteins (e.g. lysine acetylation, lysine and arginine methylation, serine and threonine phosphorylation, and lysine ubiquitination and sumoylation) play central roles in many types of epigenetic inheritance. Therefore, the word "epigenetics" is sometimes used as a synonym for these processes. However, this can be misleading. Chromatin remodeling is not always inherited, and not all epigenetic inheritance involves chromatin remodeling.[42]
Because the phenotype of a cell or individual is affected by which of its genes are transcribed, heritable transcription states can give rise to epigenetic effects. There are several layers of regulation of gene expression. One way that genes are regulated is through the remodeling of chromatin. Chromatin is the complex of DNA and the histone proteins with which it associates. If the way that DNA is wrapped around the histones changes, gene expression can change as well. Chromatin remodeling is accomplished through two main mechanisms:
Mechanisms of heritability of histone state are not well understood; however, much is known about the mechanism of heritability of DNA methylation state during cell division and differentiation. Heritability of methylation state depends on certain enzymes (such as DNMT1) that have a higher affinity for 5-methylcytosine than for cytosine. If this enzyme reaches a "hemimethylated" portion of DNA (where 5-methylcytosine is in only one of the two DNA strands) the enzyme will methylate the other half.
Although histone modifications occur throughout the entire sequence, the unstructured N-termini of histones (called histone tails) are particularly highly modified. These modifications include acetylation, methylation, ubiquitylation, phosphorylation, sumoylation, ribosylation and citrullination. Acetylation is the most highly studied of these modifications. For example, acetylation of the K14 and K9 lysines of the tail of histone H3 by histone acetyltransferase enzymes (HATs) is generally related to transcriptional competence.[citation needed]
One mode of thinking is that this tendency of acetylation to be associated with "active" transcription is biophysical in nature. Because it normally has a positively charged nitrogen at its end, lysine can bind the negatively charged phosphates of the DNA backbone. The acetylation event converts the positively charged amine group on the side chain into a neutral amide linkage. This removes the positive charge, thus loosening the DNA from the histone. When this occurs, complexes like SWI/SNF and other transcriptional factors can bind to the DNA and allow transcription to occur. This is the "cis" model of epigenetic function. In other words, changes to the histone tails have a direct effect on the DNA itself.[citation needed]
Another model of epigenetic function is the "trans" model. In this model, changes to the histone tails act indirectly on the DNA. For example, lysine acetylation may create a binding site for chromatin-modifying enzymes (or transcription machinery as well). This chromatin remodeler can then cause changes to the state of the chromatin. Indeed, a bromodomain – a protein domain that specifically binds acetyl-lysine – is found in many enzymes that help activate transcription, including the SWI/SNF complex. It may be that acetylation acts in this and the previous way to aid in transcriptional activation.
The idea that modifications act as docking modules for related factors is borne out by histone methylation as well. Methylation of lysine 9 of histone H3 has long been associated with constitutively transcriptionally silent chromatin (constitutive heterochromatin). It has been determined that a chromodomain (a domain that specifically binds methyl-lysine) in the transcriptionally repressive protein HP1 recruits HP1 to K9 methylated regions. One example that seems to refute this biophysical model for methylation is that tri-methylation of histone H3 at lysine 4 is strongly associated with (and required for full) transcriptional activation. Tri-methylation in this case would introduce a fixed positive charge on the tail.
It has been shown that the histone lysine methyltransferase (KMT) is responsible for this methylation activity in the pattern of histones H3 & H4. This enzyme utilizes a catalytically active site called the SET domain (Suppressor of variegation, Enhancer of zeste, Trithorax). The SET domain is a 130-amino acid sequence involved in modulating gene activities. This domain has been demonstrated to bind to the histone tail and causes the methylation of the histone.[43]
Differing histone modifications are likely to function in differing ways; acetylation at one position is likely to function differently from acetylation at another position. Also, multiple modifications may occur at the same time, and these modifications may work together to change the behavior of the nucleosome. The idea that multiple dynamic modifications regulate gene transcription in a systematic and reproducible way is called the histone code, although the idea that histone state can be read linearly as a digital information carrier has been largely debunked. One of the best-understood systems that orchestrates chromatin-based silencing is the SIR protein based silencing of the yeast hidden mating type loci HML and HMR.
DNA methylation frequently occurs in repeated sequences, and helps to suppress the expression and mobility of 'transposable elements':[44] Because 5-methylcytosine can be spontaneously deaminated (replacing nitrogen by oxygen) to thymidine, CpG sites are frequently mutated and become rare in the genome, except at CpG islands where they remain unmethylated. Epigenetic changes of this type thus have the potential to direct increased frequencies of permanent genetic mutation. DNA methylation patterns are known to be established and modified in response to environmental factors by a complex interplay of at least three independent DNA methyltransferases, DNMT1, DNMT3A, and DNMT3B, the loss of any of which is lethal in mice.[45] DNMT1 is the most abundant methyltransferase in somatic cells,[46] localizes to replication foci,[47] has a 10–40-fold preference for hemimethylated DNA and interacts with the proliferating cell nuclear antigen (PCNA).[48]
By preferentially modifying hemimethylated DNA, DNMT1 transfers patterns of methylation to a newly synthesized strand after DNA replication, and therefore is often referred to as the ‘maintenance' methyltransferase.[49] DNMT1 is essential for proper embryonic development, imprinting and X-inactivation.[45][50] To emphasize the difference of this molecular mechanism of inheritance from the canonical Watson-Crick base-pairing mechanism of transmission of genetic information, the term 'Epigenetic templating' was introduced.[51] Furthermore, in addition to the maintenance and transmission of methylated DNA states, the same principle could work in the maintenance and transmission of histone modifications and even cytoplasmic (structural) heritable states.[52]
Histones H3 and H4 can also be manipulated through demethylation using histone lysine demethylase (KDM). This recently identified enzyme has a catalytically active site called the Jumonji domain (JmjC). The demethylation occurs when JmjC utilizes multiple cofactors to hydroxylate the methyl group, thereby removing it. JmjC is capable of demethylating mono-, di-, and tri-methylated substrates.[53]
Chromosomal regions can adopt stable and heritable alternative states resulting in bistable gene expression without changes to the DNA sequence. Epigenetic control is often associated with alternative covalent modifications of histones.[54] The stability and heritability of states of larger chromosomal regions are suggested to involve positive feedback where modified nucleosomes recruit enzymes that similarly modify nearby nucleosomes.[55] A simplified stochastic model for this type of epigenetics is found here.[56][57]
It has been suggested that chromatin-based transcriptional regulation could be mediated by the effect of small RNAs. Small interfering RNAs can modulate transcriptional gene expression via epigenetic modulation of targeted promoters.[58]
Sometimes a gene, after being turned on, transcribes a product that (directly or indirectly) maintains the activity of that gene. For example, Hnf4 and MyoD enhance the transcription of many liver- and muscle-specific genes, respectively, including their own, through the transcription factor activity of the proteins they encode. RNA signalling includes differential recruitment of a hierarchy of generic chromatin modifying complexes and DNA methyltransferases to specific loci by RNAs during differentiation and development.[59] Other epigenetic changes are mediated by the production of different splice forms of RNA, or by formation of double-stranded RNA (RNAi). Descendants of the cell in which the gene was turned on will inherit this activity, even if the original stimulus for gene-activation is no longer present. These genes are often turned on or off by signal transduction, although in some systems where syncytia or gap junctions are important, RNA may spread directly to other cells or nuclei by diffusion. A large amount of RNA and protein is contributed to the zygote by the mother during oogenesis or via nurse cells, resulting in maternal effect phenotypes. A smaller quantity of sperm RNA is transmitted from the father, but there is recent evidence that this epigenetic information can lead to visible changes in several generations of offspring.[60]
MicroRNAs (miRNAs) are members of non-coding RNAs that range in size from 17 to 25 nucleotides. miRNAs regulate a large variety of biological functions in plants and animals.[61] So far, in 2013, about 2000 miRNAs have been discovered in humans and these can be found online in a miRNA database.[62] Each miRNA expressed in a cell may target about 100 to 200 messenger RNAs that it downregulates.[63] Most of the downregulation of mRNAs occurs by causing the decay of the targeted mRNA, while some downregulation occurs at the level of translation into protein.[64]
It appears that about 60% of human protein coding genes are regulated by miRNAs.[65] Many miRNAs are epigenetically regulated. About 50% of miRNA genes are associated with CpG islands,[61] that may be repressed by epigenetic methylation. Transcription from methylated CpG islands is strongly and heritably repressed.[66] Other miRNAs are epigenetically regulated by either histone modifications or by combined DNA methylation and histone modification.[61]
In 2011, it was demonstrated that the methylation of mRNA plays a critical role in human energy homeostasis. The obesity-associated FTO gene is shown to be able to demethylate N6-methyladenosine in RNA.[67][68]
sRNAs are small (50–250 nucleotides), highly structured, non-coding RNA fragments found in bacteria. They control gene expression including virulence genes in pathogens and are viewed as new targets in the fight against drug-resistant bacteria.[69] They play an important role in many biological processes, binding to mRNA and protein targets in prokaryotes. Their phylogenetic analyses, for example through sRNA–mRNA target interactions or protein binding properties, are used to build comprehensive databases.[70] sRNA-gene maps based on their targets in microbial genomes are also constructed.[71]
Prions are infectious forms of proteins. In general, proteins fold into discrete units that perform distinct cellular functions, but some proteins are also capable of forming an infectious conformational state known as a prion. Although often viewed in the context of infectious disease, prions are more loosely defined by their ability to catalytically convert other native state versions of the same protein to an infectious conformational state. It is in this latter sense that they can be viewed as epigenetic agents capable of inducing a phenotypic change without a modification of the genome.[72]
Fungal prions are considered by some to be epigenetic because the infectious phenotype caused by the prion can be inherited without modification of the genome. PSI+ and URE3, discovered in yeast in 1965 and 1971, are the two best studied of this type of prion.[73][74] Prions can have a phenotypic effect through the sequestration of protein in aggregates, thereby reducing that protein's activity. In PSI+ cells, the loss of the Sup35 protein (which is involved in termination of translation) causes ribosomes to have a higher rate of read-through of stop codons, an effect that results in suppression of nonsense mutations in other genes.[75] The ability of Sup35 to form prions may be a conserved trait. It could confer an adaptive advantage by giving cells the ability to switch into a PSI+ state and express dormant genetic features normally terminated by stop codon mutations.[76][77][78][79]
In ciliates such as Tetrahymena and Paramecium, genetically identical cells show heritable differences in the patterns of ciliary rows on their cell surface. Experimentally altered patterns can be transmitted to daughter cells. It seems existing structures act as templates for new structures. The mechanisms of such inheritance are unclear, but reasons exist to assume that multicellular organisms also use existing cell structures to assemble new ones.[80][81][82]
Eukaryotic genomes have numerous nucleosomes. Nucleosome position is not random, and determine the accessibility of DNA to regulatory proteins. This determines differences in gene expression and cell differentiation. It has been shown that at least some nucleosomes are retained in sperm cells (where most but not all histones are replaced by protamines). Thus nucleosome positioning is to some degree inheritable. Recent studies have uncovered connections between nucleosome positioning and other epigenetic factors, such as DNA methylation and hydroxymethylation.[83]
Developmental epigenetics can be divided into predetermined and probabilistic epigenesis. Predetermined epigenesis is a unidirectional movement from structural development in DNA to the functional maturation of the protein. "Predetermined" here means that development is scripted and predictable. Probabilistic epigenesis on the other hand is a bidirectional structure-function development with experiences and external molding development.[84]
Somatic epigenetic inheritance, particularly through DNA and histone covalent modifications and nucleosome repositioning, is very important in the development of multicellular eukaryotic organisms.[83] The genome sequence is static (with some notable exceptions), but cells differentiate into many different types, which perform different functions, and respond differently to the environment and intercellular signalling. Thus, as individuals develop, morphogens activate or silence genes in an epigenetically heritable fashion, giving cells a memory. In mammals, most cells terminally differentiate, with only stem cells retaining the ability to differentiate into several cell types ("totipotency" and "multipotency"). In mammals, some stem cells continue producing new differentiated cells throughout life, such as in neurogenesis, but mammals are not able to respond to loss of some tissues, for example, the inability to regenerate limbs, which some other animals are capable of. Epigenetic modifications regulate the transition from neural stem cells to glial progenitor cells (for example, differentiation into oligodendrocytes is regulated by the deacetylation and methylation of histones.[85] Unlike animals, plant cells do not terminally differentiate, remaining totipotent with the ability to give rise to a new individual plant. While plants do utilise many of the same epigenetic mechanisms as animals, such as chromatin remodeling, it has been hypothesised that some kinds of plant cells do not use or require "cellular memories", resetting their gene expression patterns using positional information from the environment and surrounding cells to determine their fate.[86]
Epigenetic changes can occur in response to environmental exposure – for example, mice given some dietary supplements have epigenetic changes affecting expression of the agouti gene, which affects their fur color, weight, and propensity to develop cancer.[87][88]
Controversial results from one study suggested that traumatic experiences might produce an epigenetic signal that is capable of being passed to future generations. Mice were trained, using foot shocks, to fear a cherry blossom odor. The investigators reported that the mouse offspring had an increased aversion to this specific odor.[89][90] They suggested epigenetic changes that increase gene expression, rather than in DNA itself, in a gene, M71, that governs the functioning of an odor receptor in the nose that responds specifically to this cherry blossom smell. There were physical changes that correlated with olfactory (smell) function in the brains of the trained mice and their descendants. Several criticisms were reported, including the study's low statistical power as evidence of some irregularity such as bias in reporting results.[91] Due to limits of sample size, there is a probability that an effect will not be demonstrated to within statistical significance even if it exists. The criticism suggested that the probability that all the experiments reported would show positive results if an identical protocol was followed, assuming the claimed effects exist, is merely 0.4%. The authors also did not indicate which mice were siblings, and treated all of the mice as statistically independent.[92] The original researchers pointed out negative results in the paper's appendix that the criticism omitted in its calculations, and undertook to track which mice were siblings in the future.[93]
Epigenetics can affect evolution when epigenetic changes are heritable.[8] A sequestered germ line or Weismann barrier is specific to animals, and epigenetic inheritance is more common in plants and microbes. Eva Jablonka, Marion J. Lamb and Étienne Danchin have argued that these effects may require enhancements to the standard conceptual framework of the modern synthesis and have called for an extended evolutionary synthesis.[94][95][96] Other evolutionary biologists have incorporated epigenetic inheritance into population genetics models and are openly skeptical, stating that epigenetic mechanisms such as DNA methylation and histone modification are genetically inherited under the control of natural selection.[97][98][99]
Two important ways in which epigenetic inheritance can be different from traditional genetic inheritance, with important consequences for evolution, are that rates of epimutation can be much faster than rates of mutation[100] and the epimutations are more easily reversible.[101] In plants, heritable DNA methylation mutations are 100.000 times more likely to occur compared to DNA mutations.[102] An epigenetically inherited element such as the PSI+ system can act as a "stop-gap", good enough for short-term adaptation that allows the lineage to survive for long enough for mutation and/or recombination to genetically assimilate the adaptive phenotypic change.[103] The existence of this possibility increases the evolvability of a species.
More than 100 cases of transgenerational epigenetic inheritance phenomena have been reported in a wide range of organisms, including prokaryotes, plants, and animals.[104] For instance, mourning cloak butterflies will change color through hormone changes in response to experimentation of varying temperatures.[105]
The filamentous fungus Neurospora crassa is a prominent model system for understanding the control and function of cytosine methylation. In this organisms, DNA methylation is associated with relics of a genome defense system called RIP (repeat-induced point mutation) and silences gene expression by inhibiting transcription elongation.[106]
The yeast prion PSI is generated by a conformational change of a translation termination factor, which is then inherited by daughter cells. This can provide a survival advantage under adverse conditions. This is an example of epigenetic regulation enabling unicellular organisms to respond rapidly to environmental stress. Prions can be viewed as epigenetic agents capable of inducing a phenotypic change without modification of the genome.[107]
Direct detection of epigenetic marks in microorganisms is possible with single molecule real time sequencing, in which polymerase sensitivity allows for measuring methylation and other modifications as a DNA molecule is being sequenced.[108] Several projects have demonstrated the ability to collect genome-wide epigenetic data in bacteria.[109][110][111][112]
While epigenetics is of fundamental importance in eukaryotes, especially metazoans, it plays a different role in bacteria. Most importantly, eukaryotes use epigenetic mechanisms primarily to regulate gene expression which bacteria rarely do. However, bacteria make widespread use of postreplicative DNA methylation for the epigenetic control of DNA-protein interactions. Bacteria also use DNA adenine methylation (rather than DNA cytosine methylation) as an epigenetic signal. DNA adenine methylation is important in bacteria virulence in organisms such as Escherichia coli, Salmonella, Vibrio, Yersinia, Haemophilus, and Brucella. In Alphaproteobacteria, methylation of adenine regulates the cell cycle and couples gene transcription to DNA replication. In Gammaproteobacteria, adenine methylation provides signals for DNA replication, chromosome segregation, mismatch repair, packaging of bacteriophage, transposase activity and regulation of gene expression.[107][113] There exists a genetic switch controlling Streptococcus pneumoniae (the pneumococcus) that allows the bacterium to randomly change its characteristics into six alternative states that could pave the way to improved vaccines. Each form is randomly generated by a phase variable methylation system. The ability of the pneumococcus to cause deadly infections is different in each of these six states. Similar systems exist in other bacterial genera.[114]
Epigenetics has many and varied potential medical applications.[115] In 2008, the National Institutes of Health announced that $190 million had been earmarked for epigenetics research over the next five years. In announcing the funding, government officials noted that epigenetics has the potential to explain mechanisms of aging, human development, and the origins of cancer, heart disease, mental illness, as well as several other conditions. Some investigators, like Randy Jirtle, PhD, of Duke University Medical Center, think epigenetics may ultimately turn out to have a greater role in disease than genetics.[116]
Direct comparisons of identical twins constitute an optimal model for interrogating environmental epigenetics. In the case of humans with different environmental exposures, monozygotic (identical) twins were epigenetically indistinguishable during their early years, while older twins had remarkable differences in the overall content and genomic distribution of 5-methylcytosine DNA and histone acetylation.[8] The twin pairs who had spent less of their lifetime together and/or had greater differences in their medical histories were those who showed the largest differences in their levels of 5-methylcytosine DNA and acetylation of histones H3 and H4.[117]
Dizygotic (fraternal) and monozygotic (identical) twins show evidence of epigenetic influence in humans.[117][118][119] DNA sequence differences that would be abundant in a singleton-based study do not interfere with the analysis. Environmental differences can produce long-term epigenetic effects, and different developmental monozygotic twin subtypes may be different with respect to their susceptibility to be discordant from an epigenetic point of view.[120]
A high-throughput study, which denotes technology that looks at extensive genetic markers, focused on epigenetic differences between monozygotic twins to compare global and locus-specific changes in DNA methylation and histone modifications in a sample of 40 monozygotic twin pairs.[117] In this case, only healthy twin pairs were studied, but a wide range of ages was represented, between 3 and 74 years. One of the major conclusions from this study was that there is an age-dependent accumulation of epigenetic differences between the two siblings of twin pairs. This accumulation suggests the existence of epigenetic “drift”. Epigenetic drift is the term given to epigenetic modifications as they occur as a direct function with age. While age is a known risk factor for many diseases, age-related methylation
has been found to occur differentially at specific sites along the genome. Over time, this  can result in measurable differences between biological and chronological age. Epigenetic changes have been found to be reflective of lifestyle and may act as functional biomarkers of disease before clinical threshold is reached.[121]
A more recent study, where 114 monozygotic twins and 80 dizygotic twins were analyzed for the DNA methylation status of around 6000 unique genomic regions, concluded that epigenetic similarity at the time of blastocyst splitting may also contribute to phenotypic similarities in monozygotic co-twins. This supports the notion that microenvironment at early stages of embryonic development can be quite important for the establishment of epigenetic marks.[122]
Congenital genetic disease is well understood and it is clear that epigenetics can play a role, for example, in the case of Angelman syndrome and Prader-Willi syndrome. These are normal genetic diseases caused by gene deletions or inactivation of the genes, but are unusually common because individuals are essentially hemizygous because of genomic imprinting, and therefore a single gene knock out is sufficient to cause the disease, where most cases would require both copies to be knocked out.[123]
Some human disorders are associated with genomic imprinting, a phenomenon in mammals where the father and mother contribute different epigenetic patterns for specific genomic loci in their germ cells.[124] The best-known case of imprinting in human disorders is that of Angelman syndrome and Prader-Willi syndrome – both can be produced by the same genetic mutation, chromosome 15q partial deletion, and the particular syndrome that will develop depends on whether the mutation is inherited from the child's mother or from their father.[125] This is due to the presence of genomic imprinting in the region. Beckwith-Wiedemann syndrome is also associated with genomic imprinting, often caused by abnormalities in maternal genomic imprinting of a region on chromosome 11.
Rett syndrome is underlain by mutations in the MECP2 gene despite no large-scale changes in expression of MeCP2 being found in microarray analyses. BDNF is downregulated in the MECP2 mutant resulting in Rett syndrome.
In the Överkalix study, paternal (but not maternal) grandsons[126] of Swedish men who were exposed during preadolescence to famine in the 19th century were less likely to die of cardiovascular disease. If food was plentiful, then diabetes mortality in the grandchildren increased, suggesting that this was a transgenerational epigenetic inheritance.[127] The opposite effect was observed for females – the paternal (but not maternal) granddaughters of women who experienced famine while in the womb (and therefore while their eggs were being formed) lived shorter lives on average.[128]
A variety of epigenetic mechanisms can be perturbed in different types of cancer. Epigenetic alterations of DNA repair genes or cell cycle control genes are very frequent in sporadic (non-germ line) cancers, being significantly more common than germ line (familial) mutations in these sporadic cancers.[129][130] Epigenetic alterations are important in cellular transformation to cancer, and their manipulation holds great promise for cancer prevention, detection, and therapy.[131][132] Several medications which have epigenetic impact are used in several of these diseases. These aspects of epigenetics are addressed in cancer epigenetics.
Epigenetic modifications have given insight into the understanding of the pathophysiology of different disease conditions. Though, they are strongly associated with cancer, their role in other pathological conditions are of equal importance. It appears that, the hyperglycaemic environment could imprint such changes at the genomic level, that macrophages are primed towards a pro-inflammatory
state and could fail to exhibit any phenotypic alteration towards the pro-healing type. This phenomenon of altered Macrophage Polarization is mostly associated with all the diabetic complications in a clinical set-up. At present, several reports reveal the relevance of different epigenetic modifications with respect to diabetic complications. Sooner or later, with the advancements in biomedical
tools, the detection of such biomarkers as prognostic and diagnostic tools in patients could possibly emerge out as alternative approaches. It is noteworthy to mention here that the use of epigenetic modifications as therapeutic targets warrant extensive preclinical as well as clinical evaluation prior to use.[133]
In a groundbreaking 2003 report, Caspi and colleagues demonstrated that in a robust cohort of over one-thousand subjects assessed multiple times from preschool to adulthood, subjects who carried one or two copies of the short allele of the serotonin transporter promoter polymorphism exhibited higher rates of adult depression and suicidality when exposed to childhood maltreatment when compared to long allele homozygotes with equal ELS exposure.[134]
Parental nutrition, in utero exposure to stress, male-induced maternal effects such as attraction of differential mate quality, and maternal as well as paternal age, and offspring gender could all possibly influence whether a germline epimutation is ultimately expressed in offspring and the degree to which intergenerational inheritance remains stable throughout posterity.[135]
Addiction is a disorder of the brain's reward system which arises through transcriptional and neuroepigenetic mechanisms and occurs over time from chronically high levels of exposure to an addictive stimulus (e.g., morphine, cocaine, sexual intercourse, gambling, etc.).[136][137][138][139] Transgenerational epigenetic inheritance of addictive phenotypes has been noted to occur in preclinical studies.[140][141]
Transgenerational epigenetic inheritance of anxiety-related phenotypes has been reported in a preclinical study using mice.[142] In this investigation, transmission of paternal stress-induced traits across generations involved small non-coding RNA signals transmitted via the male germline.
Epigenetic inheritance of depression-related phenotypes has also been reported in a preclinical study.[142] Inheritance of paternal stress-induced traits across generations involved small non-coding RNA signals transmitted via the paternal germline.
Studies on mice have shown that certain conditional fears can be inherited from either parent. In one example, mice were conditioned to fear a strong scent, acetophenone, by accompanying the smell with an electric shock. Consequently, the mice learned to fear the scent of acetophenone alone. It was discovered that this fear could be passed down to the mice offspring. Despite the offspring never experiencing the electric shock themselves the mice still display a fear of the acetophenone scent, because they inherited the fear epigenetically by site-specific DNA methylation. These epigenetic changes lasted up to two generations without reintroducing the shock.[143]
The two forms of heritable information, namely genetic and epigenetic, are collectively denoted as dual inheritance. Members of the APOBEC/AID family of cytosine deaminases may concurrently influence genetic and epigenetic inheritance using similar molecular mechanisms, and may be a point of crosstalk between these conceptually compartmentalized processes.[144]
Fluoroquinolone antibiotics induce epigenetic changes in mammalian cells through iron chelation. This leads to epigenetic effects through inhibition of α-ketoglutarate-dependent dioxygenases that require iron as a co-factor.[145]
Various pharmacological agents are applied for the production of induced pluripotent stem cells (iPSC) or maintain the embryonic stem cell (ESC) phenotypic via epigenetic approach. Adult stem cells like bone marrow stem cells have also shown a potential to differentiate into cardiac competent cells when treated with G9a histone methyltransferase inhibitor BIX01294.[146][147]
Due to epigenetics being in the early stages of development as a science and the sensationalism surrounding it in the public media, David Gorski and geneticist Adam Rutherford advised caution against proliferation of false and pseudoscientific conclusions by new age authors who make unfounded suggestions that a person's genes and health can be manipulated by mind control. Misuse of the scientific term by quack authors has produced misinformation to the general public.[148][149]



Epigenetics - Wikipedia

Epigenetics is the study of heritable phenotype changes that do not involve alterations in the DNA sequence.[1] The Greek prefix epi- (ἐπι- "over, outside of, around") in epigenetics implies features that are "on top of" or "in addition to" the traditional genetic basis for inheritance.[2] Epigenetics most often denotes changes that affect gene activity and expression, but can also be used to describe any heritable phenotypic change. Such effects on cellular and physiological phenotypic traits may result from external or environmental factors, or be part of normal developmental program. The standard definition of epigenetics requires these alterations to be heritable,[3][4] either in the progeny of cells or of organisms.
The term also refers to the changes themselves: functionally relevant changes to the genome that do not involve a change in the nucleotide sequence. Examples of mechanisms that produce such changes are DNA methylation and histone modification, each of which alters how genes are expressed without altering the underlying DNA sequence. Gene expression can be controlled through the action of repressor proteins that attach to silencer regions of the DNA. These epigenetic changes may last through cell divisions for the duration of the cell's life, and may also last for multiple generations even though they do not involve changes in the underlying DNA sequence of the organism;[5] instead, non-genetic factors cause the organism's genes to behave (or "express themselves") differently.[6]
One example of an epigenetic change in eukaryotic biology is the process of cellular differentiation. During morphogenesis, totipotent stem cells become the various pluripotent cell lines of the embryo, which in turn become fully differentiated cells. In other words, as a single fertilized egg cell – the zygote – continues to divide, the resulting daughter cells change into all the different cell types in an organism, including neurons, muscle cells, epithelium, endothelium of blood vessels, etc., by activating some genes while inhibiting the expression of others.[7]
Historically, some phenomena not necessarily heritable have also been described as epigenetic. For example, the term epigenetic has been used to describe any modification of chromosomal regions, especially histone modifications, whether or not these changes are heritable or associated with a phenotype. The consensus definition now requires a trait to be heritable for it to be considered epigenetic.[4]
The term epigenetics in its contemporary usage emerged in the 1990s, but for some years has been used in somewhat variable meanings.[8] A consensus definition of the concept of epigenetic trait as "stably heritable phenotype resulting from changes in a chromosome without alterations in the DNA sequence" was formulated at a Cold Spring Harbor meeting in 2008,[4] although alternate definitions that include non-heritable traits are still being used.[9]
The term epigenesis has a generic meaning "extra growth". It has been used in English since the 17th century.[10]
From the generic meaning, and the associated adjective epigenetic, C. H. Waddington coined the term epigenetics in 1942 as pertaining to epigenesis, in parallel to Valentin Haecker's 'phenogenetics' (Phänogenetik).[11] Epigenesis in the context of the biology of that period referred to the differentiation of cells from their initial totipotent state in embryonic development.[12]
When Waddington coined the term, the physical nature of genes and their role in heredity was not known; he used it as a conceptual model of how genes might interact with their surroundings to produce a phenotype; he used the phrase "epigenetic landscape" as a metaphor for biological development. Waddington held that cell fates were established in development (canalisation) much as a marble rolls down to the point of lowest local elevation.[13]
Waddington suggested visualising increasing irreversibility of cell type differentiation as ridges rising between the valleys where the marbles (cells) are travelling.[14] In recent times Waddington's notion of the epigenetic landscape has been rigorously formalized in the context of the systems dynamics state approach to the study of cell-fate.[15][16] Cell-fate determination is predicted to exhibit certain dynamics, such as attractor-convergence (the attractor can be an equilibrium point, limit cycle or strange attractor) or oscillatory.[16]
The term "epigenetic" has also been used in developmental psychology to describe psychological development as the result of an ongoing, bi-directional interchange between heredity and the environment.[17] Interactivist ideas of development have been discussed in various forms and under various names throughout the 19th and 20th centuries. An early version was proposed, among the founding statements in embryology, by Karl Ernst von Baer and popularized by Ernst Haeckel. A radical epigenetic view (physiological epigenesis) was developed by Paul Wintrebert. Another variation, probabilistic epigenesis, was presented by Gilbert Gottlieb in 2003.[18] This view encompasses all of the possible developing factors on an organism and how they not only influence the organism and each other, but how the organism also influences its own development.
The developmental psychologist Erik Erikson wrote of an epigenetic principle in his book Identity: Youth and Crisis (1968), encompassing the notion that we develop through an unfolding of our personality in predetermined stages, and that our environment and surrounding culture influence how we progress through these stages. This biological unfolding in relation to our socio-cultural settings is done in stages of psychosocial development, where "progress through each stage is in part determined by our success, or lack of success, in all the previous stages."[19][20][21]
Robin Holliday defined epigenetics as "the study of the mechanisms of temporal and spatial control of gene activity during the development of complex organisms."[22] Thus epigenetic can be used to describe anything other than DNA sequence that influences the development of an organism.
The more recent usage of the word in science has a stricter definition. It is, as defined by Arthur Riggs and colleagues, "the study of mitotically and/or meiotically heritable changes in gene function that cannot be explained by changes in DNA sequence."[23]
The term "epigenetics", however, has been used to describe processes which have not been demonstrated to be heritable such as some forms of histone modification; there are therefore attempts to redefine it in broader terms that would avoid the constraints of requiring heritability. For example, Adrian Bird defined epigenetics as "the structural adaptation of chromosomal regions so as to register, signal or perpetuate altered activity states."[5] This definition would be inclusive of transient modifications associated with DNA repair or cell-cycle phases as well as stable changes maintained across multiple cell generations, but exclude others such as templating of membrane architecture and prions unless they impinge on chromosome function. Such redefinitions however are not universally accepted and are still subject to dispute.[3] The NIH "Roadmap Epigenomics Project", ongoing as of 2016, uses the following definition: "For purposes of this program, epigenetics refers to both heritable changes in gene activity and expression (in the progeny of cells or of individuals) and also stable, long-term alterations in the transcriptional potential of a cell that are not necessarily heritable."[9]
In 2008, a consensus definition of the epigenetic trait, "stably heritable phenotype resulting from changes in a chromosome without alterations in the DNA sequence", was made at a Cold Spring Harbor meeting.[4]
The similarity of the word to "genetics" has generated many parallel usages. The "epigenome" is a parallel to the word "genome", referring to the overall epigenetic state of a cell, and epigenomics refers to more global analyses of epigenetic changes across the entire genome.[9] The phrase "genetic code" has also been adapted – the "epigenetic code" has been used to describe the set of epigenetic features that create different phenotypes in different cells. Taken to its extreme, the "epigenetic code" could represent the total state of the cell, with the position of each molecule accounted for in an epigenomic map, a diagrammatic representation of the gene expression, DNA methylation and histone modification status of a particular genomic region. More typically, the term is used in reference to systematic efforts to measure specific, relevant forms of epigenetic information such as the histone code or DNA methylation patterns.
Epigenetic changes modify the activation of certain genes, but not the genetic code sequence of DNA. The microstructure (not code) of DNA itself or the associated chromatin proteins may be modified, causing activation or silencing. This mechanism enables differentiated cells in a multicellular organism to express only the genes that are necessary for their own activity. Epigenetic changes are preserved when cells divide. Most epigenetic changes only occur within the course of one individual organism's lifetime; however, these epigenetic changes can be transmitted to the organism's offspring through a process called transgenerational epigenetic inheritance.  Moreover, if gene inactivation occurs in a sperm or egg cell that results in fertilization, this epigenetic modification may also be transferred to the next generation.[24]
Specific epigenetic processes include paramutation, bookmarking, imprinting, gene silencing, X chromosome inactivation, position effect, DNA methylation reprogramming, transvection, maternal effects, the progress of carcinogenesis, many effects of teratogens, regulation of histone modifications and heterochromatin, and technical limitations affecting parthenogenesis and cloning.
DNA damage can also cause epigenetic changes.[25][26][27] DNA damage is very frequent, occurring on average about 60,000 times a day per cell of the human body (see DNA damage (naturally occurring)). These damages are largely repaired, but at the site of a DNA repair, epigenetic changes can remain.[28] In particular, a double strand break in DNA can initiate unprogrammed epigenetic gene silencing both by causing DNA methylation as well as by promoting silencing types of histone modifications (chromatin remodeling - see next section).[29] In addition, the enzyme Parp1 (poly(ADP)-ribose polymerase) and its product poly(ADP)-ribose (PAR) accumulate at sites of DNA damage as part of a repair process.[30] This accumulation, in turn, directs recruitment and activation of the chromatin remodeling protein ALC1 that can cause nucleosome remodeling.[31] Nucleosome remodeling has been found to cause, for instance, epigenetic silencing of DNA repair gene MLH1.[23][32] DNA damaging chemicals, such as benzene, hydroquinone, styrene, carbon tetrachloride and trichloroethylene, cause considerable hypomethylation of DNA, some through the activation of oxidative stress pathways.[33]
Foods are known to alter the epigenetics of rats on different diets.[34] Some food components epigenetically increase the levels of DNA repair enzymes such as MGMT and MLH1[35] and p53.[36][37] Other food components can reduce DNA damage, such as soy isoflavones. In one study, markers for oxidative stress, such as modified nucleotides that can result from DNA damage, were decreased by a 3-week diet supplemented with soy.[38] A decrease in oxidative DNA damage was also observed 2 h after consumption of anthocyanin-rich bilberry (Vaccinium myrtillius L.) pomace extract.[39]
Epigenetic research uses a wide range of molecular biological techniques to further understanding of epigenetic phenomena, including chromatin immunoprecipitation (together with its large-scale variants ChIP-on-chip and ChIP-Seq), fluorescent in situ hybridization, methylation-sensitive restriction enzymes, DNA adenine methyltransferase identification (DamID) and bisulfite sequencing.[40] Furthermore, the use of bioinformatics methods has a role in (computational epigenetics).[40]
Several types of epigenetic inheritance systems may play a role in what has become known as cell memory,[41] note however that not all of these are universally accepted to be examples of epigenetics.
Covalent modifications of either DNA (e.g. cytosine methylation and hydroxymethylation) or of histone proteins (e.g. lysine acetylation, lysine and arginine methylation, serine and threonine phosphorylation, and lysine ubiquitination and sumoylation) play central roles in many types of epigenetic inheritance. Therefore, the word "epigenetics" is sometimes used as a synonym for these processes. However, this can be misleading. Chromatin remodeling is not always inherited, and not all epigenetic inheritance involves chromatin remodeling.[42]
Because the phenotype of a cell or individual is affected by which of its genes are transcribed, heritable transcription states can give rise to epigenetic effects. There are several layers of regulation of gene expression. One way that genes are regulated is through the remodeling of chromatin. Chromatin is the complex of DNA and the histone proteins with which it associates. If the way that DNA is wrapped around the histones changes, gene expression can change as well. Chromatin remodeling is accomplished through two main mechanisms:
Mechanisms of heritability of histone state are not well understood; however, much is known about the mechanism of heritability of DNA methylation state during cell division and differentiation. Heritability of methylation state depends on certain enzymes (such as DNMT1) that have a higher affinity for 5-methylcytosine than for cytosine. If this enzyme reaches a "hemimethylated" portion of DNA (where 5-methylcytosine is in only one of the two DNA strands) the enzyme will methylate the other half.
Although histone modifications occur throughout the entire sequence, the unstructured N-termini of histones (called histone tails) are particularly highly modified. These modifications include acetylation, methylation, ubiquitylation, phosphorylation, sumoylation, ribosylation and citrullination. Acetylation is the most highly studied of these modifications. For example, acetylation of the K14 and K9 lysines of the tail of histone H3 by histone acetyltransferase enzymes (HATs) is generally related to transcriptional competence.[citation needed]
One mode of thinking is that this tendency of acetylation to be associated with "active" transcription is biophysical in nature. Because it normally has a positively charged nitrogen at its end, lysine can bind the negatively charged phosphates of the DNA backbone. The acetylation event converts the positively charged amine group on the side chain into a neutral amide linkage. This removes the positive charge, thus loosening the DNA from the histone. When this occurs, complexes like SWI/SNF and other transcriptional factors can bind to the DNA and allow transcription to occur. This is the "cis" model of epigenetic function. In other words, changes to the histone tails have a direct effect on the DNA itself.[citation needed]
Another model of epigenetic function is the "trans" model. In this model, changes to the histone tails act indirectly on the DNA. For example, lysine acetylation may create a binding site for chromatin-modifying enzymes (or transcription machinery as well). This chromatin remodeler can then cause changes to the state of the chromatin. Indeed, a bromodomain – a protein domain that specifically binds acetyl-lysine – is found in many enzymes that help activate transcription, including the SWI/SNF complex. It may be that acetylation acts in this and the previous way to aid in transcriptional activation.
The idea that modifications act as docking modules for related factors is borne out by histone methylation as well. Methylation of lysine 9 of histone H3 has long been associated with constitutively transcriptionally silent chromatin (constitutive heterochromatin). It has been determined that a chromodomain (a domain that specifically binds methyl-lysine) in the transcriptionally repressive protein HP1 recruits HP1 to K9 methylated regions. One example that seems to refute this biophysical model for methylation is that tri-methylation of histone H3 at lysine 4 is strongly associated with (and required for full) transcriptional activation. Tri-methylation in this case would introduce a fixed positive charge on the tail.
It has been shown that the histone lysine methyltransferase (KMT) is responsible for this methylation activity in the pattern of histones H3 & H4. This enzyme utilizes a catalytically active site called the SET domain (Suppressor of variegation, Enhancer of zeste, Trithorax). The SET domain is a 130-amino acid sequence involved in modulating gene activities. This domain has been demonstrated to bind to the histone tail and causes the methylation of the histone.[43]
Differing histone modifications are likely to function in differing ways; acetylation at one position is likely to function differently from acetylation at another position. Also, multiple modifications may occur at the same time, and these modifications may work together to change the behavior of the nucleosome. The idea that multiple dynamic modifications regulate gene transcription in a systematic and reproducible way is called the histone code, although the idea that histone state can be read linearly as a digital information carrier has been largely debunked. One of the best-understood systems that orchestrates chromatin-based silencing is the SIR protein based silencing of the yeast hidden mating type loci HML and HMR.
DNA methylation frequently occurs in repeated sequences, and helps to suppress the expression and mobility of 'transposable elements':[44] Because 5-methylcytosine can be spontaneously deaminated (replacing nitrogen by oxygen) to thymidine, CpG sites are frequently mutated and become rare in the genome, except at CpG islands where they remain unmethylated. Epigenetic changes of this type thus have the potential to direct increased frequencies of permanent genetic mutation. DNA methylation patterns are known to be established and modified in response to environmental factors by a complex interplay of at least three independent DNA methyltransferases, DNMT1, DNMT3A, and DNMT3B, the loss of any of which is lethal in mice.[45] DNMT1 is the most abundant methyltransferase in somatic cells,[46] localizes to replication foci,[47] has a 10–40-fold preference for hemimethylated DNA and interacts with the proliferating cell nuclear antigen (PCNA).[48]
By preferentially modifying hemimethylated DNA, DNMT1 transfers patterns of methylation to a newly synthesized strand after DNA replication, and therefore is often referred to as the ‘maintenance' methyltransferase.[49] DNMT1 is essential for proper embryonic development, imprinting and X-inactivation.[45][50] To emphasize the difference of this molecular mechanism of inheritance from the canonical Watson-Crick base-pairing mechanism of transmission of genetic information, the term 'Epigenetic templating' was introduced.[51] Furthermore, in addition to the maintenance and transmission of methylated DNA states, the same principle could work in the maintenance and transmission of histone modifications and even cytoplasmic (structural) heritable states.[52]
Histones H3 and H4 can also be manipulated through demethylation using histone lysine demethylase (KDM). This recently identified enzyme has a catalytically active site called the Jumonji domain (JmjC). The demethylation occurs when JmjC utilizes multiple cofactors to hydroxylate the methyl group, thereby removing it. JmjC is capable of demethylating mono-, di-, and tri-methylated substrates.[53]
Chromosomal regions can adopt stable and heritable alternative states resulting in bistable gene expression without changes to the DNA sequence. Epigenetic control is often associated with alternative covalent modifications of histones.[54] The stability and heritability of states of larger chromosomal regions are suggested to involve positive feedback where modified nucleosomes recruit enzymes that similarly modify nearby nucleosomes.[55] A simplified stochastic model for this type of epigenetics is found here.[56][57]
It has been suggested that chromatin-based transcriptional regulation could be mediated by the effect of small RNAs. Small interfering RNAs can modulate transcriptional gene expression via epigenetic modulation of targeted promoters.[58]
Sometimes a gene, after being turned on, transcribes a product that (directly or indirectly) maintains the activity of that gene. For example, Hnf4 and MyoD enhance the transcription of many liver- and muscle-specific genes, respectively, including their own, through the transcription factor activity of the proteins they encode. RNA signalling includes differential recruitment of a hierarchy of generic chromatin modifying complexes and DNA methyltransferases to specific loci by RNAs during differentiation and development.[59] Other epigenetic changes are mediated by the production of different splice forms of RNA, or by formation of double-stranded RNA (RNAi). Descendants of the cell in which the gene was turned on will inherit this activity, even if the original stimulus for gene-activation is no longer present. These genes are often turned on or off by signal transduction, although in some systems where syncytia or gap junctions are important, RNA may spread directly to other cells or nuclei by diffusion. A large amount of RNA and protein is contributed to the zygote by the mother during oogenesis or via nurse cells, resulting in maternal effect phenotypes. A smaller quantity of sperm RNA is transmitted from the father, but there is recent evidence that this epigenetic information can lead to visible changes in several generations of offspring.[60]
MicroRNAs (miRNAs) are members of non-coding RNAs that range in size from 17 to 25 nucleotides. miRNAs regulate a large variety of biological functions in plants and animals.[61] So far, in 2013, about 2000 miRNAs have been discovered in humans and these can be found online in a miRNA database.[62] Each miRNA expressed in a cell may target about 100 to 200 messenger RNAs that it downregulates.[63] Most of the downregulation of mRNAs occurs by causing the decay of the targeted mRNA, while some downregulation occurs at the level of translation into protein.[64]
It appears that about 60% of human protein coding genes are regulated by miRNAs.[65] Many miRNAs are epigenetically regulated. About 50% of miRNA genes are associated with CpG islands,[61] that may be repressed by epigenetic methylation. Transcription from methylated CpG islands is strongly and heritably repressed.[66] Other miRNAs are epigenetically regulated by either histone modifications or by combined DNA methylation and histone modification.[61]
In 2011, it was demonstrated that the methylation of mRNA plays a critical role in human energy homeostasis. The obesity-associated FTO gene is shown to be able to demethylate N6-methyladenosine in RNA.[67][68]
sRNAs are small (50–250 nucleotides), highly structured, non-coding RNA fragments found in bacteria. They control gene expression including virulence genes in pathogens and are viewed as new targets in the fight against drug-resistant bacteria.[69] They play an important role in many biological processes, binding to mRNA and protein targets in prokaryotes. Their phylogenetic analyses, for example through sRNA–mRNA target interactions or protein binding properties, are used to build comprehensive databases.[70] sRNA-gene maps based on their targets in microbial genomes are also constructed.[71]
Prions are infectious forms of proteins. In general, proteins fold into discrete units that perform distinct cellular functions, but some proteins are also capable of forming an infectious conformational state known as a prion. Although often viewed in the context of infectious disease, prions are more loosely defined by their ability to catalytically convert other native state versions of the same protein to an infectious conformational state. It is in this latter sense that they can be viewed as epigenetic agents capable of inducing a phenotypic change without a modification of the genome.[72]
Fungal prions are considered by some to be epigenetic because the infectious phenotype caused by the prion can be inherited without modification of the genome. PSI+ and URE3, discovered in yeast in 1965 and 1971, are the two best studied of this type of prion.[73][74] Prions can have a phenotypic effect through the sequestration of protein in aggregates, thereby reducing that protein's activity. In PSI+ cells, the loss of the Sup35 protein (which is involved in termination of translation) causes ribosomes to have a higher rate of read-through of stop codons, an effect that results in suppression of nonsense mutations in other genes.[75] The ability of Sup35 to form prions may be a conserved trait. It could confer an adaptive advantage by giving cells the ability to switch into a PSI+ state and express dormant genetic features normally terminated by stop codon mutations.[76][77][78][79]
In ciliates such as Tetrahymena and Paramecium, genetically identical cells show heritable differences in the patterns of ciliary rows on their cell surface. Experimentally altered patterns can be transmitted to daughter cells. It seems existing structures act as templates for new structures. The mechanisms of such inheritance are unclear, but reasons exist to assume that multicellular organisms also use existing cell structures to assemble new ones.[80][81][82]
Eukaryotic genomes have numerous nucleosomes. Nucleosome position is not random, and determine the accessibility of DNA to regulatory proteins. This determines differences in gene expression and cell differentiation. It has been shown that at least some nucleosomes are retained in sperm cells (where most but not all histones are replaced by protamines). Thus nucleosome positioning is to some degree inheritable. Recent studies have uncovered connections between nucleosome positioning and other epigenetic factors, such as DNA methylation and hydroxymethylation.[83]
Developmental epigenetics can be divided into predetermined and probabilistic epigenesis. Predetermined epigenesis is a unidirectional movement from structural development in DNA to the functional maturation of the protein. "Predetermined" here means that development is scripted and predictable. Probabilistic epigenesis on the other hand is a bidirectional structure-function development with experiences and external molding development.[84]
Somatic epigenetic inheritance, particularly through DNA and histone covalent modifications and nucleosome repositioning, is very important in the development of multicellular eukaryotic organisms.[83] The genome sequence is static (with some notable exceptions), but cells differentiate into many different types, which perform different functions, and respond differently to the environment and intercellular signalling. Thus, as individuals develop, morphogens activate or silence genes in an epigenetically heritable fashion, giving cells a memory. In mammals, most cells terminally differentiate, with only stem cells retaining the ability to differentiate into several cell types ("totipotency" and "multipotency"). In mammals, some stem cells continue producing new differentiated cells throughout life, such as in neurogenesis, but mammals are not able to respond to loss of some tissues, for example, the inability to regenerate limbs, which some other animals are capable of. Epigenetic modifications regulate the transition from neural stem cells to glial progenitor cells (for example, differentiation into oligodendrocytes is regulated by the deacetylation and methylation of histones.[85] Unlike animals, plant cells do not terminally differentiate, remaining totipotent with the ability to give rise to a new individual plant. While plants do utilise many of the same epigenetic mechanisms as animals, such as chromatin remodeling, it has been hypothesised that some kinds of plant cells do not use or require "cellular memories", resetting their gene expression patterns using positional information from the environment and surrounding cells to determine their fate.[86]
Epigenetic changes can occur in response to environmental exposure – for example, mice given some dietary supplements have epigenetic changes affecting expression of the agouti gene, which affects their fur color, weight, and propensity to develop cancer.[87][88]
Controversial results from one study suggested that traumatic experiences might produce an epigenetic signal that is capable of being passed to future generations. Mice were trained, using foot shocks, to fear a cherry blossom odor. The investigators reported that the mouse offspring had an increased aversion to this specific odor.[89][90] They suggested epigenetic changes that increase gene expression, rather than in DNA itself, in a gene, M71, that governs the functioning of an odor receptor in the nose that responds specifically to this cherry blossom smell. There were physical changes that correlated with olfactory (smell) function in the brains of the trained mice and their descendants. Several criticisms were reported, including the study's low statistical power as evidence of some irregularity such as bias in reporting results.[91] Due to limits of sample size, there is a probability that an effect will not be demonstrated to within statistical significance even if it exists. The criticism suggested that the probability that all the experiments reported would show positive results if an identical protocol was followed, assuming the claimed effects exist, is merely 0.4%. The authors also did not indicate which mice were siblings, and treated all of the mice as statistically independent.[92] The original researchers pointed out negative results in the paper's appendix that the criticism omitted in its calculations, and undertook to track which mice were siblings in the future.[93]
Epigenetics can affect evolution when epigenetic changes are heritable.[8] A sequestered germ line or Weismann barrier is specific to animals, and epigenetic inheritance is more common in plants and microbes. Eva Jablonka, Marion J. Lamb and Étienne Danchin have argued that these effects may require enhancements to the standard conceptual framework of the modern synthesis and have called for an extended evolutionary synthesis.[94][95][96] Other evolutionary biologists have incorporated epigenetic inheritance into population genetics models and are openly skeptical, stating that epigenetic mechanisms such as DNA methylation and histone modification are genetically inherited under the control of natural selection.[97][98][99]
Two important ways in which epigenetic inheritance can be different from traditional genetic inheritance, with important consequences for evolution, are that rates of epimutation can be much faster than rates of mutation[100] and the epimutations are more easily reversible.[101] In plants, heritable DNA methylation mutations are 100.000 times more likely to occur compared to DNA mutations.[102] An epigenetically inherited element such as the PSI+ system can act as a "stop-gap", good enough for short-term adaptation that allows the lineage to survive for long enough for mutation and/or recombination to genetically assimilate the adaptive phenotypic change.[103] The existence of this possibility increases the evolvability of a species.
More than 100 cases of transgenerational epigenetic inheritance phenomena have been reported in a wide range of organisms, including prokaryotes, plants, and animals.[104] For instance, mourning cloak butterflies will change color through hormone changes in response to experimentation of varying temperatures.[105]
The filamentous fungus Neurospora crassa is a prominent model system for understanding the control and function of cytosine methylation. In this organisms, DNA methylation is associated with relics of a genome defense system called RIP (repeat-induced point mutation) and silences gene expression by inhibiting transcription elongation.[106]
The yeast prion PSI is generated by a conformational change of a translation termination factor, which is then inherited by daughter cells. This can provide a survival advantage under adverse conditions. This is an example of epigenetic regulation enabling unicellular organisms to respond rapidly to environmental stress. Prions can be viewed as epigenetic agents capable of inducing a phenotypic change without modification of the genome.[107]
Direct detection of epigenetic marks in microorganisms is possible with single molecule real time sequencing, in which polymerase sensitivity allows for measuring methylation and other modifications as a DNA molecule is being sequenced.[108] Several projects have demonstrated the ability to collect genome-wide epigenetic data in bacteria.[109][110][111][112]
While epigenetics is of fundamental importance in eukaryotes, especially metazoans, it plays a different role in bacteria. Most importantly, eukaryotes use epigenetic mechanisms primarily to regulate gene expression which bacteria rarely do. However, bacteria make widespread use of postreplicative DNA methylation for the epigenetic control of DNA-protein interactions. Bacteria also use DNA adenine methylation (rather than DNA cytosine methylation) as an epigenetic signal. DNA adenine methylation is important in bacteria virulence in organisms such as Escherichia coli, Salmonella, Vibrio, Yersinia, Haemophilus, and Brucella. In Alphaproteobacteria, methylation of adenine regulates the cell cycle and couples gene transcription to DNA replication. In Gammaproteobacteria, adenine methylation provides signals for DNA replication, chromosome segregation, mismatch repair, packaging of bacteriophage, transposase activity and regulation of gene expression.[107][113] There exists a genetic switch controlling Streptococcus pneumoniae (the pneumococcus) that allows the bacterium to randomly change its characteristics into six alternative states that could pave the way to improved vaccines. Each form is randomly generated by a phase variable methylation system. The ability of the pneumococcus to cause deadly infections is different in each of these six states. Similar systems exist in other bacterial genera.[114]
Epigenetics has many and varied potential medical applications.[115] In 2008, the National Institutes of Health announced that $190 million had been earmarked for epigenetics research over the next five years. In announcing the funding, government officials noted that epigenetics has the potential to explain mechanisms of aging, human development, and the origins of cancer, heart disease, mental illness, as well as several other conditions. Some investigators, like Randy Jirtle, PhD, of Duke University Medical Center, think epigenetics may ultimately turn out to have a greater role in disease than genetics.[116]
Direct comparisons of identical twins constitute an optimal model for interrogating environmental epigenetics. In the case of humans with different environmental exposures, monozygotic (identical) twins were epigenetically indistinguishable during their early years, while older twins had remarkable differences in the overall content and genomic distribution of 5-methylcytosine DNA and histone acetylation.[8] The twin pairs who had spent less of their lifetime together and/or had greater differences in their medical histories were those who showed the largest differences in their levels of 5-methylcytosine DNA and acetylation of histones H3 and H4.[117]
Dizygotic (fraternal) and monozygotic (identical) twins show evidence of epigenetic influence in humans.[117][118][119] DNA sequence differences that would be abundant in a singleton-based study do not interfere with the analysis. Environmental differences can produce long-term epigenetic effects, and different developmental monozygotic twin subtypes may be different with respect to their susceptibility to be discordant from an epigenetic point of view.[120]
A high-throughput study, which denotes technology that looks at extensive genetic markers, focused on epigenetic differences between monozygotic twins to compare global and locus-specific changes in DNA methylation and histone modifications in a sample of 40 monozygotic twin pairs.[117] In this case, only healthy twin pairs were studied, but a wide range of ages was represented, between 3 and 74 years. One of the major conclusions from this study was that there is an age-dependent accumulation of epigenetic differences between the two siblings of twin pairs. This accumulation suggests the existence of epigenetic “drift”. Epigenetic drift is the term given to epigenetic modifications as they occur as a direct function with age. While age is a known risk factor for many diseases, age-related methylation
has been found to occur differentially at specific sites along the genome. Over time, this  can result in measurable differences between biological and chronological age. Epigenetic changes have been found to be reflective of lifestyle and may act as functional biomarkers of disease before clinical threshold is reached.[121]
A more recent study, where 114 monozygotic twins and 80 dizygotic twins were analyzed for the DNA methylation status of around 6000 unique genomic regions, concluded that epigenetic similarity at the time of blastocyst splitting may also contribute to phenotypic similarities in monozygotic co-twins. This supports the notion that microenvironment at early stages of embryonic development can be quite important for the establishment of epigenetic marks.[122]
Congenital genetic disease is well understood and it is clear that epigenetics can play a role, for example, in the case of Angelman syndrome and Prader-Willi syndrome. These are normal genetic diseases caused by gene deletions or inactivation of the genes, but are unusually common because individuals are essentially hemizygous because of genomic imprinting, and therefore a single gene knock out is sufficient to cause the disease, where most cases would require both copies to be knocked out.[123]
Some human disorders are associated with genomic imprinting, a phenomenon in mammals where the father and mother contribute different epigenetic patterns for specific genomic loci in their germ cells.[124] The best-known case of imprinting in human disorders is that of Angelman syndrome and Prader-Willi syndrome – both can be produced by the same genetic mutation, chromosome 15q partial deletion, and the particular syndrome that will develop depends on whether the mutation is inherited from the child's mother or from their father.[125] This is due to the presence of genomic imprinting in the region. Beckwith-Wiedemann syndrome is also associated with genomic imprinting, often caused by abnormalities in maternal genomic imprinting of a region on chromosome 11.
Rett syndrome is underlain by mutations in the MECP2 gene despite no large-scale changes in expression of MeCP2 being found in microarray analyses. BDNF is downregulated in the MECP2 mutant resulting in Rett syndrome.
In the Överkalix study, paternal (but not maternal) grandsons[126] of Swedish men who were exposed during preadolescence to famine in the 19th century were less likely to die of cardiovascular disease. If food was plentiful, then diabetes mortality in the grandchildren increased, suggesting that this was a transgenerational epigenetic inheritance.[127] The opposite effect was observed for females – the paternal (but not maternal) granddaughters of women who experienced famine while in the womb (and therefore while their eggs were being formed) lived shorter lives on average.[128]
A variety of epigenetic mechanisms can be perturbed in different types of cancer. Epigenetic alterations of DNA repair genes or cell cycle control genes are very frequent in sporadic (non-germ line) cancers, being significantly more common than germ line (familial) mutations in these sporadic cancers.[129][130] Epigenetic alterations are important in cellular transformation to cancer, and their manipulation holds great promise for cancer prevention, detection, and therapy.[131][132] Several medications which have epigenetic impact are used in several of these diseases. These aspects of epigenetics are addressed in cancer epigenetics.
Epigenetic modifications have given insight into the understanding of the pathophysiology of different disease conditions. Though, they are strongly associated with cancer, their role in other pathological conditions are of equal importance. It appears that, the hyperglycaemic environment could imprint such changes at the genomic level, that macrophages are primed towards a pro-inflammatory
state and could fail to exhibit any phenotypic alteration towards the pro-healing type. This phenomenon of altered Macrophage Polarization is mostly associated with all the diabetic complications in a clinical set-up. At present, several reports reveal the relevance of different epigenetic modifications with respect to diabetic complications. Sooner or later, with the advancements in biomedical
tools, the detection of such biomarkers as prognostic and diagnostic tools in patients could possibly emerge out as alternative approaches. It is noteworthy to mention here that the use of epigenetic modifications as therapeutic targets warrant extensive preclinical as well as clinical evaluation prior to use.[133]
In a groundbreaking 2003 report, Caspi and colleagues demonstrated that in a robust cohort of over one-thousand subjects assessed multiple times from preschool to adulthood, subjects who carried one or two copies of the short allele of the serotonin transporter promoter polymorphism exhibited higher rates of adult depression and suicidality when exposed to childhood maltreatment when compared to long allele homozygotes with equal ELS exposure.[134]
Parental nutrition, in utero exposure to stress, male-induced maternal effects such as attraction of differential mate quality, and maternal as well as paternal age, and offspring gender could all possibly influence whether a germline epimutation is ultimately expressed in offspring and the degree to which intergenerational inheritance remains stable throughout posterity.[135]
Addiction is a disorder of the brain's reward system which arises through transcriptional and neuroepigenetic mechanisms and occurs over time from chronically high levels of exposure to an addictive stimulus (e.g., morphine, cocaine, sexual intercourse, gambling, etc.).[136][137][138][139] Transgenerational epigenetic inheritance of addictive phenotypes has been noted to occur in preclinical studies.[140][141]
Transgenerational epigenetic inheritance of anxiety-related phenotypes has been reported in a preclinical study using mice.[142] In this investigation, transmission of paternal stress-induced traits across generations involved small non-coding RNA signals transmitted via the male germline.
Epigenetic inheritance of depression-related phenotypes has also been reported in a preclinical study.[142] Inheritance of paternal stress-induced traits across generations involved small non-coding RNA signals transmitted via the paternal germline.
Studies on mice have shown that certain conditional fears can be inherited from either parent. In one example, mice were conditioned to fear a strong scent, acetophenone, by accompanying the smell with an electric shock. Consequently, the mice learned to fear the scent of acetophenone alone. It was discovered that this fear could be passed down to the mice offspring. Despite the offspring never experiencing the electric shock themselves the mice still display a fear of the acetophenone scent, because they inherited the fear epigenetically by site-specific DNA methylation. These epigenetic changes lasted up to two generations without reintroducing the shock.[143]
The two forms of heritable information, namely genetic and epigenetic, are collectively denoted as dual inheritance. Members of the APOBEC/AID family of cytosine deaminases may concurrently influence genetic and epigenetic inheritance using similar molecular mechanisms, and may be a point of crosstalk between these conceptually compartmentalized processes.[144]
Fluoroquinolone antibiotics induce epigenetic changes in mammalian cells through iron chelation. This leads to epigenetic effects through inhibition of α-ketoglutarate-dependent dioxygenases that require iron as a co-factor.[145]
Various pharmacological agents are applied for the production of induced pluripotent stem cells (iPSC) or maintain the embryonic stem cell (ESC) phenotypic via epigenetic approach. Adult stem cells like bone marrow stem cells have also shown a potential to differentiate into cardiac competent cells when treated with G9a histone methyltransferase inhibitor BIX01294.[146][147]
Due to epigenetics being in the early stages of development as a science and the sensationalism surrounding it in the public media, David Gorski and geneticist Adam Rutherford advised caution against proliferation of false and pseudoscientific conclusions by new age authors who make unfounded suggestions that a person's genes and health can be manipulated by mind control. Misuse of the scientific term by quack authors has produced misinformation to the general public.[148][149]



Genome instability - Wikipedia
Genome instability (also genetic instability or genomic instability) refers to a high frequency of mutations within the genome of a cellular lineage.  These mutations can include changes in nucleic acid sequences, chromosomal rearrangements or aneuploidy. Genome instability does occur in bacteria.[1] In multicellular organisms genome instability is central to carcinogenesis,[2] and in humans it is also a factor in some neurodegenerative diseases such as amyotrophic lateral sclerosis or the neuromuscular disease myotonic dystrophy.
The sources of genome instability have only recently begun to be elucidated. A high frequency of externally caused DNA damage[3] can be one source of genome instability since DNA damages can cause inaccurate translesion synthesis past the damages or errors in repair, leading to mutation.  Another source of genome instability may be epigenetic or mutational reductions in expression of DNA repair genes.  Because endogenous (metabolically-caused) DNA damage is very frequent, occurring on average more than 60,000 times a day in the genomes of human cells, any reduced DNA repair is likely an important source of genome instability.
Usually, all cells in an individual in a given species (plant or animal) show a constant number of chromosomes, which constitute what is known as the karyotype defining this species (see also List of number of chromosomes of various organisms), although some species present a very high karyotypic variability.  In humans, mutations that would change an amino acid within the protein coding region of the genome occur at an average of only 0.35 per generation (less than one mutated protein per generation).[4]
Sometimes, in a species with a stable karyotype, random variations that modify the normal number of chromosomes may be observed. In other cases, there are structural alterations (chromosomal translocations, deletions ...) that modify the standard chromosomal complement. In these cases, it is indicated that the affected organism presents genome instability (also genetic instability, or even chromosomic instability). The process of genome instability often leads to a situation of aneuploidy, in which the cells present a chromosomic number that is either higher or lower than the normal complement for the species.
In the cell cycle, DNA is usually most vulnerable during replication. The replisome must be able to navigate obstacles such as tightly wound chromatin with bound proteins, single and double stranded breaks which can lead to the stalling of the replication fork. Each protein or enzyme in the replisome must perform its function well to result in a perfect copy of DNA. Mutations of proteins such as DNA polymerase, ligase, can lead to impairment of replication and lead to spontaneous chromosomal exchanges.[5] Proteins such as Tel1, Mec1 (ATR, ATM in humans) can detect single and double-stranded breaks and recruit factors such as Rmr3 helicase to stabilize the replication fork in order to prevent its collapse. Mutations in Tel1, Mec1, and Rmr3 helicase result in a significant increase of chromosomal recombination. ATR responds specifically to stalled replication forks and single-stranded breaks resulting from UV damage while ATM responds directly to double-stranded breaks. These proteins also prevent progression into mitosis by inhibiting the firing of late replication origins until the DNA breaks are fixed by phosphorylating CHK1, CHK2 which results in a signaling cascade arresting the cell in S-phase.[6] For single stranded breaks, replication occurs until the location of the break, then the other strand is nicked to form a double stranded break, which can then be repaired by Break Induced Replication or homologous recombination using the sister chromatid as an error-free template.[7] In addition to S-phase checkpoints, G1 and G2 checkpoints exist to check for transient DNA damage which could be caused by mutagens such as UV damage. An example is the Saccharomyces pombe gene rad9 which arrests the cells in late S/G2 phase in the presence of DNA damage caused by radiation. The yeast cells with defective rad9 failed to arrest following radiation, continued cell division and died rapidly while the cells with wild-type rad9 successfully arrested in late S/G2 phase and remained viable. The cells that arrested were able to survive due to the increased time in S/G2 phase allowing for DNA repair enzymes to function fully.[8]
There are hotspots in the genome where DNA sequences are prone to gaps and breaks after inhibition of DNA synthesis such as in the aforementioned checkpoint arrest. These sites are called fragile sites, and can occur commonly as naturally present in most mammalian genomes or occur rarely as a result of mutations, such as DNA-repeat expansion. Rare fragile sites can lead to genetic disease such as fragile X mental retardation syndrome, myotonic dystrophy, Friedrich’s ataxia, and Huntington’s disease, most of which are caused by expansion of repeats at the DNA, RNA, or protein level.[9] Although, seemingly harmful, these common fragile sites are conserved all the way to yeast and bacteria. These ubiquitous sites are characterized by trinucleotide repeats, most commonly CGG, CAG, GAA, and GCN. These trinucleotide repeats can form into hairpins, leading to difficulty of replication. Under replication stress, such as defective machinery or further DNA damage, DNA breaks and gaps can form at these fragile sites. Using a sister chromatid as repair is not a fool-proof backup as the surrounding DNA information of the n and n+1 repeat is virtually the same, leading to copy number variation. For example, the 16th copy of CGG might be mapped to the 13th copy of CGG in the sister chromatid since the surrounding DNA is both CGGCGGCGG…, leading to 3 extra copies of CGG in the final DNA sequence.
In both E. coli and Saccromyces pombe, transcription sites tend to have higher recombination and mutation rates. The coding or non-transcribed strand accumulates more mutations than the template strand. This is due to the fact that the coding strand is single-stranded during transcription, which is chemically more unstable than double-stranded DNA. During elongation of transcription, supercoiling can occur behind an elongating RNA polymerase, leading to single-stranded breaks. When the coding strand is single-stranded, it can also hybridize with itself, creating DNA secondary structures that can compromise replication. In E. coli, when attempting to transcribe GAA triplets such as those found in Friedrich’s ataxia, the resulting RNA and template strand can form mismatched loops between different repeats, leading the complementary segment in the coding-strand available to form its own loops which impede replication.[10] Furthermore, replication of DNA and transcription of DNA are not temporally independent; they can occur at the same time and lead to collisions between the replication fork and RNA polymerase complex. In S. cerevisiae, Rrm3 helicase is found at highly transcribed genes in the yeast genome, which is recruited to stabilize a stalling replication fork as described above. This suggests that transcription is an obstacle to replication, which can lead to increased stress in the chromatin spanning the short distance between the unwound replication fork and transcription start site, potentially causing single-stranded DNA breaks. In yeast, proteins act as barriers at the 3’ of the transcription unit to prevent further travel of the DNA replication fork.[11]
In some portions of the genome, variability is essential to survival. One such locale is the Ig genes. In a pre-B cell, the region consists of all V, D, and J segments. During development of the B cell, a specific V, D, and J segment is chosen to be spliced together to form the final gene, which is catalyzed by RAG1 and RAG2 recombinases.  Activation-Induced Cytidine Deaminase (AID) then converts cytidine into uracil. Uracil normally does not exist in DNA, and thus the base is excised and the nick is converted into a double-stranded break which is repaired by non-homologous end joining (NHEJ). This procedure is very error-prone and leads to somatic hypermutation. This genomic instability is crucial in ensuring mammalian survival against infection. V, D, J recombination can ensure millions of unique B-cell receptors; however, random repair by NHEJ introduces variation which can create a receptor that can bind with higher affinity to antigens.[12]
Of about 200 neurological and neuromuscular disorders, 15 have a clear link to an inherited or acquired defect in one of the DNA repair pathways or excessive genotoxic oxidative stress.[13][14]  Five of them (xeroderma pigmentosum, Cockayne's syndrome, trichothiodystrophy, Down's syndrome, and triple-A syndrome) have a defect in the DNA nucleotide excision repair pathway.  Six (spinocerebellar ataxia with axonal neuropathy-1, Huntington's disease, Alzheimer's disease, Parkinson's disease, Down's syndrome and amyotrophic lateral sclerosis) seem to result from increased oxidative stress, and the inability of the base excision repair pathway to handle the damage to DNA that this causes.  Four of them (Huntington's disease, various spinocerebellar ataxias, Friedreich’s ataxia and myotonic dystrophy types 1 and 2) often have an unusual expansion of repeat sequences in DNA, likely attributable to genome instability.  Four (ataxia-telangiectasia, ataxia-telangiectasia-like disorder, Nijmegen breakage syndrome and Alzheimer's disease) are defective in genes involved in repairing DNA double-strand breaks. Overall, it seems that oxidative stress is a major cause of genomic instability in the brain.  A particular neurological disease arises when a pathway that normally prevents oxidative stress is deficient, or a DNA repair pathway that normally repairs damage caused by oxidative stress is deficient.
In cancer, genome instability can occur prior to or as a consequence of transformation.[15] Genome instability can refer to the accumulation of extra copies of DNA or chromosomes, chromosomal translocations, chromosomal inversions, chromosome deletions, single-strand breaks in DNA, double-strand breaks in DNA, the intercalation of foreign substances into the DNA double helix, or any abnormal changes in DNA tertiary structure that can cause either the loss of DNA, or the misexpression of genes. Situations of genome instability (as well as aneuploidy) are common in cancer cells, and they are considered a "hallmark" for these cells. The unpredictable nature of these events are also a main contributor to the heterogeneity observed among tumour cells.
It is currently accepted that sporadic tumors (non-familial ones) are originated due to the accumulation of several genetic errors.[16]  An average cancer of the breast or colon can have about 60 to 70 protein altering mutations, of which about 3 or 4 may be "driver" mutations, and the remaining ones may be "passenger" mutations[17] Any genetic or epigenetic lesion increasing the mutation rate will have as a consequence an increase in the acquisition of new mutations, increasing then the probability to develop a tumor.[18] During the process of tumorogenesis, it is known that diploid cells acquire mutations in genes responsible for maintaining genome integrity  (caretaker genes), as well as in genes that are directly controlling cellular proliferation  (gatekeeper genes).[19] Genetic instability can originate due to deficiencies in DNA repair, or due to loss or gain of chromosomes, or due to large scale chromosomal reorganizations. Losing genetic stability will favour tumor development, because it favours the generation of mutants that can be selected by the environment.[20]
The tumor microenvironment has an inhibitory effect on DNA repair pathways contributing to genomic instability, which promotes tumor survival, proliferation, and malignant transformation.[21]
The protein coding regions of the human genome, collectively called the exome, constitutes only 1.5% of the total genome.[22]  As pointed out above, ordinarily there are only an average of 0.35 mutations in the exome per generation (parent to child) in humans.  In the entire genome (including non-protein coding regions) there are only about 70 new mutations per generation in humans.[23][24]
The likely major underlying cause of mutations in cancer is DNA damage.[25]  For example, in the case of lung cancer,  DNA damage is caused by agents in exogenous genotoxic tobacco smoke (e.g. acrolein, formaldehyde, acrylonitrile, 1,3-butadiene, acetaldehyde, ethylene oxide and isoprene).[26] Endogenous (metabolically-caused) DNA damage is also very frequent, occurring on average more than 60,000 times a day in the genomes of human cells (see DNA damage (naturally occurring)).  Externally and endogenously caused damages may be converted into mutations by inaccurate translesion synthesis or inaccurate DNA repair (e.g. by non-homologous end joining).  In addition, DNA damages can also give rise to epigenetic alterations during DNA repair.[27][28][29] Both mutations and epigenetic alterations (epimutations) can contribute to progression to cancer.
As noted above, about 3 or 4 driver mutations and 60 passenger mutations occur in the exome (protein coding region) of a cancer.[17]  However, a much larger number of mutations occur in the non-protein-coding regions of DNA.  The average number of DNA sequence mutations in the entire genome of a breast cancer tissue sample is about 20,000.[30]  In an average melanoma tissue sample (where melanomas have a higher exome mutation frequency[17]) the total number of DNA sequence mutations is about 80,000.[31]
The high frequency of mutations in the total genome within cancers suggests that, often, an early carcinogenic alteration may be a deficiency in DNA repair.  Mutation rates substantially increase (sometimes by 100-fold) in cells defective in DNA mismatch repair[32][33] or in homologous recombinational DNA repair.[34]  Also, chromosomal rearrangements and aneuploidy increase in humans defective in DNA repair gene BLM.[35]
A deficiency in DNA repair, itself, can allow DNA damages to accumulate, and error-prone translesion synthesis past some of those damages may give rise to mutations. In addition, faulty repair of these accumulated DNA damages may give rise to epigenetic alterations or epimutations.  While a mutation or epimutation in a DNA repair gene, itself, would not confer a selective advantage, such a repair defect may be carried along as a passenger in a cell when the cell acquires an additional mutation/epimutation that does provide a proliferative advantage.  Such cells, with both proliferative advantages and one or more DNA repair defects (causing a very high mutation rate), likely give rise to the 20,000 to 80,000 total genome mutations frequently seen in cancers.
In somatic cells, deficiencies in DNA repair sometimes arise by mutations in DNA repair genes, but much more often are due to epigenetic reductions in expression of DNA repair genes.  Thus, in a sequence of 113 colorectal cancers, only four had somatic missense mutations in the DNA repair gene MGMT, while the majority of these cancers had reduced MGMT expression due to methylation of the MGMT promoter region.[36]  Five reports, listed in the article Epigenetics (see section "DNA repair epigenetics in cancer") presented evidence that between 40% and 90% of colorectal cancers have reduced MGMT expression due to methylation of the MGMT promoter region.
Similarly, for 119 cases of colorectal cancers classified as mismatch repair deficient and lacking DNA repair gene PMS2 expression, Pms2 was deficient in 6 due to mutations in the PMS2 gene, while in 103 cases PMS2 expression was deficient because its pairing partner MLH1 was repressed due to promoter methylation (PMS2 protein is unstable in the absence of MLH1).[37]  The other 10 cases of loss of PMS2 expression were likely due to epigenetic overexpression of the microRNA, miR-155, which down-regulates MLH1.[38]
In cancer epigenetics (see section Frequencies of epimutations in DNA repair genes), there is a partial listing of epigenetic deficiencies found in DNA repair genes in sporadic cancers.  These include frequencies of between 13–100% of epigenetic defects in genes BRCA1, WRN, FANCB, FANCF, MGMT, MLH1, MSH2, MSH4, ERCC1, XPF, NEIL1 and ATM located in cancers including breast, ovarian, colorectal and head and neck.  Two or three epigenetic deficiencies in expression of ERCC1, XPF and/or PMS2 were found to occur simultaneously in the majority of the 49 colon cancers evaluated.[39]  Some of these DNA repair deficiencies can be caused by epimutations in microRNAs as summarized in the MicroRNA article section titled miRNA, DNA repair and cancer.
Cancers usually result from disruption of a tumor repressor or dysregulation of an oncogene. Knowing that B-cells experience DNA breaks through development can give insight to the genome of lymphomas. Many types of lymphoma are caused by chromosomal translocation, which can arise from breaks in DNA leading to incorrect joining. In Burkitt’s lymphoma, c-myc, an oncogene encoding a transcription factor, is translocated after the promoter of the immunoglobulin gene, leading dysregulation of c-myc transcription. Since immunoglobulins are essential to a lymphocyte and highly expressed to increase detection of antigens, c-myc is then also highly expressed leading to transcription of its targets which are involved in cell proliferation. Mantle cell lymphoma is characterized by fusion of cyclin D1 to the immunoglobulin locus. Cyclin D1 inhibits Rb, a tumor suppressor, leading to tumorigenesis. Follicular lymphoma results from the translocation of immunoglobulin promoter to the Bcl-2 gene, giving rise to large amounts of Bcl-2 protein which inhibits apoptosis. DNA-damaged B-cells no longer undergo apoptosis leading to further mutations which could affect driver genes leading to tumorigenesis.[40] The location of translocation in the oncogene shares structural properties of the target regions of AID, suggesting that the oncogene was a potential target of AID, leading to a double-stranded break that was translocated to the immunoglobulin gene locus through NHEJ repair.[41]



DNA mismatch repair - Wikipedia
DNA mismatch repair (MMR) is a system for recognizing and repairing erroneous insertion, deletion, and mis-incorporation of bases that can arise during DNA replication and recombination, as well as repairing some forms of DNA damage.[1][2]
Mismatch repair is strand-specific. During DNA synthesis the newly synthesised (daughter) strand will commonly include errors.  In order to begin repair, the mismatch repair machinery distinguishes the newly synthesised strand from the template (parental). In gram-negative bacteria, transient hemimethylation distinguishes the strands (the parental is methylated and daughter is not). However, in other prokaryotes and eukaryotes, the exact mechanism is not clear.  It is suspected that, in eukaryotes, newly synthesized lagging-strand DNA transiently contains nicks (before being sealed by DNA ligase) and provides a signal that directs mismatch proofreading systems to the appropriate strand. This implies that these nicks must be present in the leading strand, and evidence for this has recently been found.[3]
Recent work[4] has shown that nicks are sites for RFC-dependent loading of the replication sliding clamp PCNA, in an orientation-specific manner, such that one face of the donut-shape protein is juxtaposed toward the 3'-OH end at the nick.  Oriented PCNA then directs the action of the MutLalpha endonuclease to one strand in the presence of a mismatch and MutSalpha or MutSbeta.
Any mutational event that disrupts the superhelical structure of DNA carries with it the potential to compromise the genetic stability of a cell. The fact that the damage detection and repair systems are as complex as the replication machinery itself highlights the importance evolution has attached to DNA fidelity.
Examples of mismatched bases include a G/T or A/C pairing (see DNA repair).  Mismatches are commonly due to tautomerization of bases during DNA replication. The damage is repaired by recognition of the deformity caused by the mismatch, determining the template and non-template strand, and excising the wrongly incorporated base and replacing it with the correct nucleotide. The removal process involves more than just the mismatched nucleotide itself.  A few or up to thousands of base pairs of the newly synthesized DNA strand can be removed.
Mismatch repair is a highly conserved process from prokaryotes to eukaryotes. The first evidence for mismatch repair was obtained from S. pneumoniae (the hexA and hexB genes). Subsequent work on E. coli has identified a number of genes that, when mutationally inactivated, cause hypermutable strains. The gene products are, therefore, called the "Mut" proteins, and are the major active components of the mismatch repair system. Three of these proteins are essential in detecting the mismatch and directing repair machinery to it: MutS, MutH and MutL (MutS is a homologue of HexA and MutL of HexB).
MutS forms a dimer (MutS2) that recognises the mismatched base on the daughter strand and binds the mutated DNA. MutH binds at hemimethylated sites along the daughter DNA, but its action is latent, being activated only upon contact by a MutL dimer (MutL2), which binds the MutS-DNA complex and acts as a mediator between MutS2 and MutH, activating the latter. The DNA is looped out to search for the nearest d(GATC) methylation site to the mismatch, which could be up to 1 kb away. Upon activation by the MutS-DNA complex, MutH nicks the daughter strand near the hemimethylated site. MutL recruits UvrD helicase (DNA Helicase II) to separate the two strands with a specific 3' to 5' polarity. The entire MutSHL complex then slides along the DNA in the direction of the mismatch, liberating the strand to be excised as it goes. An exonuclease trails the complex and digests the ss-DNA tail. The exonuclease recruited is dependent on which side of the mismatch MutH incises the strand – 5' or 3'. If the nick made by MutH is on the 5' end of the mismatch, either RecJ or ExoVII (both 5' to 3' exonucleases) is used. If, however, the nick is on the 3' end of the mismatch, ExoI (a 3' to 5' enzyme) is used.
The entire process ends past the mismatch site - i.e., both the site itself and its surrounding nucleotides are fully excised. The single-strand gap created by the exonuclease can then be repaired by DNA Polymerase III (assisted by single-strand-binding protein), which uses the other strand as a template, and finally sealed by DNA ligase. DNA methylase then rapidly methylates the daughter strand.
When bound, the MutS2 dimer bends the DNA helix and shields approximately 20 base pairs. It has weak ATPase activity, and binding of ATP leads to the formation of tertiary structures on the surface of the molecule. The crystal structure of MutS reveals that it is exceptionally asymmetric, and, while its active conformation is a dimer, only one of the two halves interacts with the mismatch site.
In eukaryotes, MutS homologs form two major heterodimers: Msh2/Msh6 (MutSα) and Msh2/Msh3 (MutSβ). The MutSα pathway is involved primarily in base substitution and small-loop mismatch repair. The MutSβ pathway is also involved in small-loop repair, in addition to large-loop (~10 nucleotide loops) repair. However, MutSβ does not repair base substitutions.
MutL also has weak ATPase activity (it uses ATP for purposes of movement). It forms a complex with MutS and MutH, increasing the MutS footprint on the DNA.
However, the processivity (the distance the enzyme can move along the DNA before dissociating) of UvrD is only ~40–50 bp. Because the distance between the nick created by MutH and the mismatch can average ~600 bp, if there is not another UvrD loaded the unwound section is then free to re-anneal to its complementary strand, forcing the process to start over. However, when assisted by MutL, the rate of UvrD loading is greatly increased. While the processivity (and ATP utilisation) of the individual UvrD molecules remains the same, the total effect on the DNA is boosted considerably; the DNA has no chance to re-anneal, as each UvrD unwinds 40-50 bp of DNA, dissociates, and then is immediately replaced by another UvrD, repeating the process. This exposes large sections of DNA to exonuclease digestion, allowing for quick excision (and later replacement) of the incorrect DNA.
Eukaryotes have MutL homologs designated Mlh1 and Pms1. They form a heterodimer that mimics MutL in E. coli. The human homologue of prokaryotic MutL has three forms designated as MutLα, MutLβ, and MutLγ. The MutLα complex is made of two subunits MLH1 and PMS2, the MutLβ heterodimer is made of MLH1 and PMS1, whereas MutLγ is made of MLH1 and MLH3. MutLα acts as the matchmaker or facilitator, coordinating events in mismatch repair. It has recently been shown to be a DNA endonuclease that introduces strand breaks in DNA upon activation by mismatch and other required proteins, MutSa and PCNA. These strand interruptions serve as entry points for an exonuclease activity that removes mismatched DNA. Roles played by MutLβ and MutLγ in mismatch repair are less-understood.
MutH is a very weak endonuclease that is activated once bound to MutL (which itself is bound to MutS). It nicks unmethylated DNA and the unmethylated strand of hemimethylated DNA but does not nick fully methylated DNA. Experiments have shown that mismatch repair is random if neither strand is methylated.[citation needed] These behaviours led to the proposal that MutH determines which strand contains the mismatch. 
MutH has no eukaryotic homolog. Its endonuclease function is taken up by MutL homologs, which have some specialized 5'-3' exonuclease activity. The strand bias for removing mismatches from the newly synthesized daughter strand in eukaryotes may be provided by the free 3' ends of Okazaki fragments in the new strand created during replication.
PCNA and the β-sliding clamp associate with MutSα/β and MutS, respectively.  Although initial reports suggested that the PCNA-MutSα complex may enhance mismatch recognition,[5] it has been recently demonstrated[6] that there is no apparent change in affinity of MutSα for a mismatch in the presence or absence of PCNA.  Furthermore, mutants of MutSα that are unable to interact with PCNA in vitro exhibit the capacity to carry out mismatch recognition and mismatch excision to near wild type levels.  Such mutants are defective in the repair reaction directed by a 5' strand break, suggesting for the first time MutSα function in a post-excision step of the reaction.
Mutations in the human homologues of the Mut proteins affect genomic stability, which can result in microsatellite instability (MSI), implicated in some human cancers. In specific, the hereditary nonpolyposis colorectal cancers (HNPCC or Lynch syndrome) are attributed to damaging germline variants in the genes encoding the MutS and MutL homologues MSH2 and MLH1 respectively, which are thus classified as tumour suppressor genes. One subtype of HNPCC, the Muir-Torre Syndrome (MTS), is associated with skin tumors. If both inherited copies (alleles) of a MMR gene bear damaging genetic variants, this results in a very rare and severe condition: the mismatch repair cancer syndrome (or constitutional mismatch repair deficiency, CMMR-D), manifesting as multiple occurrences of tumors at an early age, often colon and brain tumors.[7]
Sporadic cancers with a DNA repair deficiency only rarely have a mutation in a DNA repair gene, but they instead tend to have epigenetic alterations that reduce DNA repair gene expression.[8]  About 13% of colorectal cancers are deficient in DNA mismatch repair, commonly due to loss of MLH1 (9.8%), or sometimes MSH2, MSH6 or PMS2 (all ≤1.5%).[9]  For most MLH1-deficient sporadic colorectal cancers, the deficiency was due to methylation of the MLH1 promoter.[9]  Other cancer types have higher frequencies of MLH1 loss (see table below), which are again largely a result of methylation of the promoter of the MLH1 gene. A different epigenetic mechanism underlying MMR deficiencies might involve over-expression of a microRNA, for example miR-155 levels inversely correlate with expression of MLH1 or MSH2 in colorectal cancer.[10]
A field defect (field cancerization) is an area of epithelium that has been preconditioned by epigenetic or genetic changes, predisposing it towards development of cancer. As pointed out by Rubin " ...there is evidence that more than 80% of the somatic mutations found in mutator phenotype human colorectal tumors occur before the onset of terminal clonal expansion."[19][20] Similarly, Vogelstein et al.[21] point out that more than half of somatic mutations identified in tumors occurred in a pre-neoplastic phase (in a field defect), during growth of apparently normal cells.
MLH1 deficiencies were common in the field defects (histologically normal tissues) surrounding tumors; see Table above. Epigenetically silenced or mutated MLH1 would likely not confer a selective advantage upon a stem cell, however, it would cause increased mutation rates, and one or more of the mutated genes may provide the cell with a selective advantage. The deficientMLH1 gene could then be carried along as a selectively near-neutral passenger (hitch-hiker) gene when the mutated stem cell generates an expanded clone. The continued presence of a clone with an epigenetically repressed MLH1 would continue to generate further mutations, some of which could produce a tumor.
In humans, seven DNA mismatch repair (MMR) proteins (MLH1, MLH3, MSH2, MSH3, MSH6, PMS1 and PMS2) work coordinately in sequential steps to initiate repair of DNA mismatches.[22]  In addition, there are Exo1-dependent and Exo1-independent MMR subpathways.[23]
Other gene products involved in mismatch repair (subsequent to initiation by MMR genes) in humans include DNA polymerase delta, PCNA, RPA, HMGB1, RFC and DNA ligase I, plus histone and chromatin modifying factors.[24][25]
In certain circumstances, the MMR pathway may recruit an error-prone DNA polymerase eta (POLH). This happens in B-lymphocytes during somatic hypermutation, where POLH is used to introduce genetic variation into antibody genes.[26] However, this error-prone MMR pathway may be triggered in other types of human cells upon exposure to genotoxins [27] and indeed it is broadly active in various human cancers, causing mutations that bear a signature of POLH activity.[28]
Recognizing and repairing mismatches and indels is important for cells because failure to do so results in microsatellite instability (MSI) and an elevated spontaneous mutation rate (mutator phenotype).  In comparison to other cancer types, MMR-deficient (MSI) cancer has a very high frequency of mutations, close to melanoma and lung cancer,[29] cancer types caused by much exposure to UV radiation and mutagenic chemicals.
In addition to a very high mutation burden, MMR deficiencies result in an unusual distribution of somatic mutations across the human genome: this suggests that MMR preferentially protects the gene-rich, early-replicating euchromatic regions.[30] In contrast, the gene-poor, late-replicating heterochromatic genome regions exhibit high mutation rates in many human tumors.[31]
The histone modification H3K36me3, an epigenetic mark of active chromatin, has the ability to recruit the MSH2-MSH6 (hMutSα) complex.[32] Consistently, regions of the human genome with high levels of H3K36me3 accumulate less mutations due to MMR activity.[28]
Lack of MMR often occurs in coordination with loss of other DNA repair genes.[8] For example, MMR genes MLH1 and MLH3 as well as 11 other DNA repair genes (such as MGMT and many NER pathway genes) were significantly down-regulated in lower grade as well as in higher grade astrocytomas, in contrast to normal brain tissue.[33]  Moreover, MLH1 and MGMT expression was closely correlated in 135 specimens of gastric cancer and loss of MLH1 and MGMT appeared to be synchronously accelerated during tumor progression.[34]
Deficient expression of multiple DNA repair genes is often found in cancers,[8] and may contribute to the thousands of mutations usually found in cancers (see Mutation frequencies in cancers).



DNA repair-deficiency disorder - Wikipedia
A DNA repair-deficiency disorder is a medical condition due to reduced functionality of DNA repair.
DNA repair defects can cause an accelerated aging disease or an increased risk of cancer, or sometimes both.
DNA repair defects are seen in nearly all of the diseases described as  accelerated aging disease, in which various tissues, organs or systems of the human body age prematurely. Because the accelerated aging diseases display different aspects of aging, but never every aspect, they are often called segmental progerias by biogerontologists.
Some examples of DNA repair defects causing progeroid syndromes in humans or mice are shown in Table 1.
Most of the DNA repair deficiency diseases show varying degrees of "accelerated aging" or cancer (often some of both).[37] But elimination of any gene essential for  base excision repair kills the embryo—it is too lethal to display symptoms (much less symptoms of cancer or "accelerated aging").[38] 
Rothmund-Thomson syndrome and xeroderma pigmentosum display symptoms dominated by vulnerability to cancer, whereas progeria and Werner syndrome show the most features of "accelerated aging". Hereditary nonpolyposis colorectal cancer (HNPCC) is very often caused by a defective MSH2 gene leading to defective mismatch repair, but displays no symptoms of "accelerated aging".[39] On the other hand, Cockayne Syndrome and trichothiodystrophy show mainly features of accelerated aging, but apparently without an increased risk of cancer[40] Some DNA repair defects manifest as neurodegeneration rather than as cancer or "accelerated aging".[41]  (Also see the "DNA damage theory of aging" for a discussion of the evidence that DNA damage is the primary underlying cause of aging.)
Some biogerontologists question that such a thing as "accelerated aging" actually exists, at least partly on the grounds that all of the so-called accelerated aging diseases are segmental progerias. Many disease conditions such as diabetes, high blood pressure, etc., are associated with increased mortality. Without reliable biomarkers of aging it is hard to support the claim that a disease condition represents more than accelerated mortality.[42]
Against this position other biogerontologists argue that premature aging phenotypes are identifiable symptoms associated with mechanisms of molecular damage.[37] The fact that these phenotypes are widely recognized justifies classification of the relevant diseases as "accelerated aging".[43] Such conditions, it is argued, are readily distinguishable from genetic diseases associated with increased mortality, but not associated with an aging phenotype, such as cystic fibrosis and sickle cell anemia. It is further argued that segmental aging phenotype is a natural part of aging insofar as genetic variation leads to some people being more disposed than others to aging-associated diseases such as cancer and Alzheimer's disease.[44]
Individuals with an inherited impairment in DNA repair capability are often at increased risk of cancer.[45]  When a mutation is present in a DNA repair gene, the repair gene will either not be expressed or be expressed in an altered form.  Then the repair function will likely be deficient, and, as a consequence, damages will tend to accumulate.  Such DNA damages can cause errors during DNA synthesis leading to mutations, some of which may give rise to cancer.  Germ-line DNA repair mutations that increase the risk of cancer are listed in the Table. 
postmeiotic segregation increased 2 (S. cerevisiae)
see also Template:Congenital malformations and deformations of skin appendages, Template:Phakomatoses, Template:Pigmentation disorders, Template:DNA replication and repair-deficiency disorder



microRNA - Wikipedia
A microRNA (abbreviated miRNA) is a small non-coding RNA molecule (containing about 22 nucleotides) found in plants, animals and some viruses, that functions in RNA silencing and post-transcriptional regulation of gene expression.[1][2][3] miRNAs function via base-pairing with complementary sequences within mRNA molecules.[4] As a result, these mRNA molecules are silenced, by one or more of the following processes: (1) Cleavage of the mRNA strand into two pieces, (2) Destabilization of the mRNA through shortening of its poly(A) tail, and (3) Less efficient translation of the mRNA into proteins by ribosomes.[4][5]
miRNAs resemble the small interfering RNAs (siRNAs) of the RNA interference (RNAi) pathway, except miRNAs derive from regions of RNA transcripts that fold back on themselves to form short hairpins, whereas siRNAs derive from longer regions of double-stranded RNA.[2] The human genome may encode over 1900 miRNAs,[6] although more recent analysis indicates that the number is closer to 600.[7]
miRNAs are abundant in many mammalian cell types[8][9] and appear to target about 60% of the genes of humans and other mammals.[10][11] Many miRNAs are evolutionarily conserved, which implies that they have important biological functions.[7][3] For example, 90 families of miRNAs have been conserved since at least the common ancestor of mammals and fish, and most of these conserved miRNAs have important functions, as shown by studies in which genes for one or more members of a family have been knocked out in mouse.[3]
The first miRNA was discovered in the early 1990s.[12] However, miRNAs were not recognized as a distinct class of biological regulators until the early 2000s.[13][14][15][16][17] miRNA research revealed different sets of miRNAs expressed in different cell types and tissues[9][18] and multiple roles for miRNAs in plant and animal development and in many other biological processes.[19][20][21][22][23][24][25][26] Aberrant miRNA expression are implicated in disease states. MiRNA-based therapies are under investigation.[27][28][29][30]
The first miRNA was discovered in 1993 by a group led by Ambros and including Lee and Feinbaum. However, additional insight into its mode of action required simultaneously published work by Ruvkun's team, including Wightman and Ha.[12][31] These groups published back-to-back papers on the lin-4 gene, which was known to control the timing of C. elegans larval development by repressing the lin-14 gene. When Lee et al. isolated the lin-4 miRNA, they found that instead of producing an mRNA encoding a protein, it produced short non-coding RNAs, one of which was a ~22-nucleotide RNA that contained sequences partially complementary to multiple sequences in the 3' UTR of the lin-14 mRNA.[12] This complementarity was proposed to inhibit the translation of the lin-14 mRNA into the LIN-14 protein. At the time, the lin-4 small RNA was thought to be a nematode idiosyncrasy.
In 2000, a second small RNA was characterized: let-7 RNA, which represses lin-41 to promote a later developmental transition in C. elegans.[13] The let-7 RNA was found to be conserved in many species, leading to the suggestion that let-7 RNA and additional "small temporal RNAs" might regulate the timing of development in diverse animals, including humans.[14]
A year later, the lin-4 and let-7 RNAs were found to be part of a large class of small RNAs present in C. elegans, Drosophila and human cells.[15][16][17] The many RNAs of this class resembled the lin-4 and let-7 RNAs, except their expression patterns were usually inconsistent with a role in regulating the timing of development. This suggested that most might function in other types of regulatory pathways. At this point, researchers started using the term "microRNA" to refer to this class of small regulatory RNAs.[15][16][17]
The first human disease associated with deregulation of miRNAs was chronic lymphocytic leukemia.[32]
Under a standard nomenclature system, names are assigned to experimentally confirmed miRNAs before publication.[33][34] The prefix "miR" is followed by a dash and a number, the latter often indicating order of naming. For example, miR-124 was named and likely discovered prior to miR-456. A capitalized "miR-" refers to the mature form of the miRNA, while the uncapitalized "mir-" refers to the pre-miRNA and the pri-miRNA, and "MIR" refers to the gene that encodes them.[35]
miRNAs with nearly identical sequences except for one or two nucleotides are annotated with an additional lower case letter. For example, miR-124a is closely related to miR-124b.
Pre-miRNAs, pri-miRNAs and genes that lead to 100% identical mature miRNAs but that are located at different places in the genome are indicated with an additional dash-number suffix. For example, the pre-miRNAs hsa-mir-194-1 and hsa-mir-194-2 lead to an identical mature miRNA (hsa-miR-194) but are from genes located in different genome regions.
Species of origin is designated with a three-letter prefix, e.g., hsa-miR-124 is a human (Homo sapiens) miRNA and oar-miR-124 is a sheep (Ovis aries) miRNA. Other common prefixes include 'v' for viral (miRNA encoded by a viral genome) and 'd' for Drosophila miRNA (a fruit fly commonly studied in genetic research).
When two mature microRNAs originate from opposite arms of the same pre-miRNA and are found in roughly similar amounts, they are denoted with a -3p or -5p suffix. (In the past, this distinction was also made with 's' (sense) and 'as' (antisense)). However, the mature microRNA found from one arm of the hairpin is usually much more abundant than that found from the other arm,[2] in which case, an asterisk following the name indicates the mature species found at low levels from the opposite arm of a hairpin. For example, miR-124 and miR-124* share a pre-miRNA hairpin, but much more miR-124 is found in the cell.
Plant miRNAs usually have near-perfect pairing with their mRNA targets, which induces gene repression through cleavage of the target transcripts.[19] In contrast, animal miRNAs are able to recognize their target mRNAs by using as little as 6–8 nucleotides (the seed region) at the 5' end of the miRNA,[10][36][37] which is not enough pairing to induce cleavage of the target mRNAs.[4] Combinatorial regulation is a feature of miRNA regulation in animals.[4][38] A given miRNA may have hundreds of different mRNA targets, and a given target might be regulated by multiple miRNAs.[11][39]
Estimates of the average number of unique messenger RNAs that are targets for repression by a typical miRNA vary, depending on the estimation method,[40] but multiple approaches show that mammalian miRNAs can have many unique targets. For example, an analysis of the miRNAs highly conserved in vertebrates shows that each has, on average, roughly 400 conserved targets.[11] Likewise, experiments show that a single miRNA species can reduce the stability of hundreds of unique messenger RNAs.[41] Other experiments show that a single miRNA species may repress the production of hundreds of proteins, but that this repression often is relatively mild (much less than 2-fold).[42][43] The first human disease associated with deregulation of miRNAs was chronic lymphocytic leukemia. Other B cell malignancies followed.[32]
As many as 40% of miRNA genes may lie in the introns or even exons of other genes.[44] These are usually, though not exclusively, found in a sense orientation,[45][46] and thus usually are regulated together with their host genes.[44][47][48]
The DNA template is not the final word on mature miRNA production: 6% of human miRNAs show RNA editing (IsomiRs), the site-specific modification of RNA sequences to yield products different from those encoded by their DNA. This increases the diversity and scope of miRNA action beyond that implicated from the genome alone.
miRNA genes are usually transcribed by RNA polymerase II (Pol II).[49][50] The polymerase often binds to a promoter found near the DNA sequence, encoding what will become the hairpin loop of the pre-miRNA. The resulting transcript is capped with a specially modified nucleotide at the 5' end, polyadenylated with multiple adenosines (a poly(A) tail),[49][45] and spliced. Animal miRNAs are initially transcribed as part of one arm of an ∼80 nucleotide RNA stem-loop that in turn forms part of a several hundred nucleotide-long miRNA precursor termed a primary miRNA (pri-miRNA).[49][45] When a stem-loop precursor is found in the 3' UTR, a transcript may serve as a pri-miRNA and a mRNA.[45] RNA polymerase III (Pol III) transcribes some miRNAs, especially those with upstream Alu sequences, transfer RNAs (tRNAs), and mammalian wide interspersed repeat (MWIR) promoter units.[51]
A single pri-miRNA may contain from one to six miRNA precursors. These hairpin loop structures are composed of about 70 nucleotides each. Each hairpin is flanked by sequences necessary for efficient processing.
The double-stranded RNA (dsRNA) structure of the hairpins in a pri-miRNA is recognized by a nuclear protein known as DiGeorge Syndrome Critical Region 8 (DGCR8 or "Pasha" in invertebrates), named for its association with DiGeorge Syndrome. DGCR8 associates with the enzyme Drosha, a protein that cuts RNA, to form the Microprocessor complex.[52][53] In this complex, DGCR8 orients the catalytic RNase III domain of Drosha to liberate hairpins from pri-miRNAs by cleaving RNA about eleven nucleotides from the hairpin base (one helical dsRNA turn into the stem).[54][55] The product resulting has a two-nucleotide overhang at its 3' end; it has 3' hydroxyl and 5' phosphate groups. It is often termed as a pre-miRNA (precursor-miRNA). Sequence motifs downstream of the pre-miRNA that are important for efficient processing have been identified.[56][57][58]
Pre-miRNAs that are spliced directly out of introns, bypassing the Microprocessor complex, are known as "Mirtrons." Originally thought to exist only in Drosophila and C. elegans, mirtrons have now been found in mammals.[59]
As many as 16% of pre-miRNAs may be altered through nuclear RNA editing.[60][61][62] Most commonly, enzymes known as adenosine deaminases acting on RNA (ADARs) catalyze adenosine to inosine (A to I) transitions. RNA editing can halt nuclear processing (for example, of pri-miR-142, leading to degradation by the ribonuclease Tudor-SN) and alter downstream processes including cytoplasmic miRNA processing and target specificity (e.g., by changing the seed region of miR-376 in the central nervous system).[60]
Pre-miRNA hairpins are exported from the nucleus in a process involving the nucleocytoplasmic shuttler Exportin-5. This protein, a member of the karyopherin family, recognizes a two-nucleotide overhang left by the RNase III enzyme Drosha at the 3' end of the pre-miRNA hairpin. Exportin-5-mediated transport to the cytoplasm is energy-dependent, using GTP bound to the Ran protein.[63]
In the cytoplasm, the pre-miRNA hairpin is cleaved by the RNase III enzyme Dicer.[64] This endoribonuclease interacts with 5' and 3' ends of the hairpin[65] and cuts away the loop joining the 3' and 5' arms, yielding an imperfect miRNA:miRNA* duplex about 22 nucleotides in length.[64] Overall hairpin length and loop size influence the efficiency of Dicer processing. The imperfect nature of the miRNA:miRNA* pairing also affects cleavage.[64][66] Some of the G-rich pre-miRNAs can potentially adopt the G-quadruplex structure as an alternative to the canonical stem-loop structure. For example, human pre-miRNA 92b adopts a G-quadruplex structure which is resistant to the Dicer mediated cleavage in the cytoplasm.[67] Although either strand of the duplex may potentially act as a functional miRNA, only one strand is usually incorporated into the RNA-induced silencing complex (RISC) where the miRNA and its mRNA target interact.
While the majority of miRNAs are located within the cell, some miRNAs, commonly known as circulating miRNAs or extracellular miRNAs, have also been found in extracellular environment, including various biological fluids and cell culture media.[68][69]
miRNA biogenesis in plants differs from animal biogenesis mainly in the steps of nuclear processing and export. Instead of being cleaved by two different enzymes, once inside and once outside the nucleus, both cleavages of the plant miRNA are performed by a Dicer homolog, called Dicer-like1 (DL1). DL1 is expressed only in the nucleus of plant cells, which indicates that both reactions take place inside the nucleus. Before plant miRNA:miRNA* duplexes are transported out of the nucleus, its 3' overhangs are methylated by a RNA methyltransferaseprotein called Hua-Enhancer1 (HEN1). The duplex is then transported out of the nucleus to the cytoplasm by a protein called Hasty (HST), an Exportin 5 homolog, where they disassemble and the mature miRNA is incorporated into the RISC.[70]
The mature miRNA is part of an active RNA-induced silencing complex (RISC) containing Dicer and many associated proteins.[71] RISC is also known as a microRNA ribonucleoprotein complex (miRNP);[72] A RISC with incorporated miRNA is sometimes referred to as a "miRISC."
Dicer processing of the pre-miRNA is thought to be coupled with unwinding of the duplex. Generally, only one strand is incorporated into the miRISC, selected on the basis of its thermodynamic instability and weaker base-pairing on the 5' end relative to the other strand.[73][74][75] The position of the stem-loop may also influence strand choice.[76] The other strand, called the passenger strand due to its lower levels in the steady state, is denoted with an asterisk (*) and is normally degraded. In some cases, both strands of the duplex are viable and become functional miRNA that target different mRNA populations.[77]
Members of the Argonaute (Ago) protein family are central to RISC function. Argonautes are needed for miRNA-induced silencing and contain two conserved RNA binding domains: a PAZ domain that can bind the single stranded 3' end of the mature miRNA and a PIWI domain that structurally resembles ribonuclease-H and functions to interact with the 5' end of the guide strand. They bind the mature miRNA and orient it for interaction with a target mRNA. Some argonautes, for example human Ago2, cleave target transcripts directly; argonautes may also recruit additional proteins to achieve translational repression.[78] The human genome encodes eight argonaute proteins divided by sequence similarities into two families: AGO (with four members present in all mammalian cells and called E1F2C/hAgo in humans), and PIWI (found in the germ line and hematopoietic stem cells).[72][78]
Additional RISC components include TRBP [human immunodeficiency virus (HIV) transactivating response RNA (TAR) binding protein],[79] PACT (protein activator of the interferon-induced protein kinase), the SMN complex, fragile X mental retardation protein (FMRP), Tudor staphylococcal nuclease-domain-containing protein (Tudor-SN), the putative DNA helicase MOV10, and the RNA recognition motif containing protein TNRC6B.[63][80][81]
Gene silencing may occur either via mRNA degradation or preventing mRNA from being translated. For example, miR16 contains a sequence complementary to the AU-rich element found in the 3'UTR of many unstable mRNAs, such as TNF alpha or GM-CSF.[82] It has been demonstrated that given complete complementarity between the miRNA and target mRNA sequence, Ago2 can cleave the mRNA and lead to direct mRNA degradation. Absent complementarity, silencing is achieved by preventing translation.[41] The relation of miRNA and its target mRNA(s) can be based on the simple negative regulation of a target mRNA, but it seems that a common scenario is the use of a "coherent feed-forward loop", "mutual negative feedback loop" (also termed double negative loop) and "positive feedback/feed-forward loop". Some miRNAs work as buffers of random gene expression changes arising due to stochastic events in transcription, translation and protein stability. Such regulation is typically achieved by the virtue of negative feedback loops or incoherent feed-forward loop uncoupling protein output from mRNA transcription.[32]
Turnover of mature miRNA is needed for rapid changes in miRNA expression profiles. During miRNA maturation in the cytoplasm, uptake by the Argonaute protein is thought to stabilize the guide strand, while the opposite (* or "passenger") strand is preferentially destroyed. In what has been called a "Use it or lose it" strategy, Argonaute may preferentially retain miRNAs with many targets over miRNAs with few or no targets, leading to degradation of the non-targeting molecules.[83]
Decay of mature miRNAs in Caenorhabditis elegans is mediated by the 5´-to-3´ exoribonuclease XRN2, also known as Rat1p.[84] In plants, SDN (small RNA degrading nuclease) family members degrade miRNAs in the opposite (3'-to-5') direction. Similar enzymes are encoded in animal genomes, but their roles have not been described.[83]
Several miRNA modifications affect miRNA stability. As indicated by work in the model organism Arabidopsis thaliana (thale cress), mature plant miRNAs appear to be stabilized by the addition of methyl moieties at the 3' end. The 2'-O-conjugated methyl groups block the addition of uracil (U) residues by uridyltransferase enzymes, a modification that may be associated with miRNA degradation. However, uridylation may also protect some miRNAs; the consequences of this modification are incompletely understood. Uridylation of some animal miRNAs has been reported. Both plant and animal miRNAs may be altered by addition of adenine (A) residues to the 3' end of the miRNA. An extra A added to the end of mammalian miR-122, a liver-enriched miRNA important in hepatitis C, stabilizes the molecule and plant miRNAs ending with an adenine residue have slower decay rates.[83]
The function of miRNAs appears to be in gene regulation. For that purpose, a miRNA is complementary to a part of one or more messenger RNAs (mRNAs). Animal miRNAs are usually complementary to a site in the 3' UTR whereas plant miRNAs are usually complementary to coding regions of mRNAs.[86] Perfect or near perfect base pairing with the target RNA promotes cleavage of the RNA.[87] This is the primary mode of plant miRNAs.[88] In animals the match-ups are imperfect.
For partially complementary microRNAs to recognise their targets, nucleotides 2–7 of the miRNA (its 'seed region'[10][36]) must be perfectly complementary.[89] Animal miRNAs inhibit protein translation of the target mRNA[90] (this is present but less common in plants).[88] Partially complementary microRNAs can also speed up deadenylation, causing mRNAs to be degraded sooner.[91] While degradation of miRNA-targeted mRNA is well documented, whether or not translational repression is accomplished through mRNA degradation, translational inhibition, or a combination of the two is hotly debated. Recent work on miR-430 in zebrafish, as well as on bantam-miRNA and miR-9 in Drosophila cultured cells, shows that translational repression is caused by the disruption of translation initiation, independent of mRNA deadenylation.[92][93]
miRNAs occasionally also cause histone modification and DNA methylation of promoter sites, which affects the expression of target genes.[94][95]
Nine mechanisms of miRNA action are described and assembled in a unified mathematical model:[85]
It is often impossible to discern these mechanisms using experimental data about stationary reaction rates. Nevertheless, they are differentiated in dynamics and have different kinetic signatures.[85]
Unlike plant microRNAs, the animal microRNAs target diverse genes.[36] However, genes involved in functions common to all cells, such as gene expression, have relatively fewer microRNA target sites and seem to be under selection to avoid targeting by microRNAs.[96]
dsRNA can also activate gene expression, a mechanism that has been termed "small RNA-induced gene activation" or RNAa. dsRNAs targeting gene promoters can induce potent transcriptional activation of associated genes. This was demonstrated in human cells using synthetic dsRNAs termed small activating RNAs (saRNAs),[97] but has also been demonstrated for endogenous microRNA.[98]
Interactions between microRNAs and complementary sequences on genes and even pseudogenes that share sequence homology are thought to be a back channel of communication regulating expression levels between paralogous genes. Given the name "competing endogenous RNAs" (ceRNAs), these microRNAs bind to "microRNA response elements" on genes and pseudogenes and may provide another explanation for the persistence of non-coding DNA.[99]
miRNAs are well conserved in both plants and animals, and are thought to be a vital and evolutionarily ancient component of gene regulation.[100][101][102][103][104] While core components of the microRNA pathway are conserved between plants and animals, miRNA repertoires in the two kingdoms appear to have emerged independently with different primary modes of action.[105][106]
microRNAs are useful phylogenetic markers because of their apparently low rate of evolution.[107] microRNAs' origin as a regulatory mechanism developed from previous RNAi machinery that was initially used as a defense against exogenous genetic material such as viruses.[108] Their origin may have permitted the development of morphological innovation, and by making gene expression more specific and 'fine-tunable', permitted the genesis of complex organs[109] and perhaps, ultimately, complex life.[104] Rapid bursts of morphological innovation are generally associated with a high rate of microRNA accumulation.[107][109]
New microRNAs are created in multiple ways. Novel microRNAs can originate from the random formation of hairpins in "non-coding" sections of DNA (i.e. introns or intergene regions), but also by the duplication and modification of existing microRNAs.[110] microRNAs can also form from inverted duplications of protein-coding sequences, which allows for the creation of a foldback hairpin structure.[111] The rate of evolution (i.e. nucleotide substitution) in recently originated microRNAs is comparable to that elsewhere in the non-coding DNA, implying evolution by neutral drift; however, older microRNAs have a much lower rate of change (often less than one substitution per hundred million years),[104] suggesting that once a microRNA gains a function, it undergoes purifying selection.[110] Individual regions within an miRNA gene face different evolutionary pressures, where regions that are vital for processing and function have higher levels of conservation.[112] At this point, a microRNA is rarely lost from an animal's genome,[104] although newer microRNAs (thus presumably non-functional) are frequently lost.[110] In Arabidopsis thaliana, the net flux of miRNA genes has been predicted to be between 1.2 and 3.3 genes per million years.[113] This makes them a valuable phylogenetic marker, and they are being looked upon as a possible solution to outstanding phylogenetic problems such as the relationships of arthropods.[114] On the other hand, in multiple cases microRNAs correlate poorly with phylogeny, and it is possible that their phylogenetic concordance largely reflects a limited sampling of microRNAs.[115]
microRNAs feature in the genomes of most eukaryotic organisms, from the brown algae[116] to the animals. However, the difference in how these microRNAs function and the way they are processed suggests that microRNAs arose independently in plants and animals.[117]
Focusing on the animals, the genome of Mnemiopsis leidyi[118] appears to lack recognizable microRNAs, as well as the nuclear proteins Drosha and Pasha, which are critical to canonical microRNA biogenesis. It is the only animal thus far reported to be missing Drosha. MicroRNAs play a vital role in the regulation of gene expression in all non-ctenophore animals investigated thus far except for Trichoplax adhaerens, the only known member of the phylum Placozoa.[119]
Across all species, in excess of 5000 different miRNAs had been identified by March 2010.[120] Whilst short RNA sequences (50 – hundreds of base pairs) of a broadly comparable function occur in bacteria, bacteria lack true microRNAs.[121]
While researchers focused on miRNA expression in physiological and pathological processes, various technical variables related to microRNA isolation emerged. The stability of stored miRNA samples has been questioned.[122][69] microRNAs degrade much more easily than mRNAs, partly due to their length, but also because of ubiquitously present RNases. This makes it necessary to cool samples on ice and use RNase-free equipment.[123]
microRNA expression can be quantified in a two-step polymerase chain reaction process of modified RT-PCR followed by quantitative PCR. Variations of this method achieve absolute or relative quantification.[124] miRNAs can also be hybridized to microarrays, slides or chips with probes to hundreds or thousands of miRNA targets, so that relative levels of miRNAs can be determined in different samples.[125] microRNAs can be both discovered and profiled by high-throughput sequencing methods (microRNA sequencing).[126] The activity of an miRNA can be experimentally inhibited using a locked nucleic acid (LNA) oligo, a Morpholino oligo[127][128] or a 2'-O-methyl RNA oligo.[129] A specific miRNA can be silenced by a complementary antagomir. microRNA maturation can be inhibited at several points by steric-blocking oligos.[130][131] The miRNA target site of an mRNA transcript can also be blocked by a steric-blocking oligo.[132] For the "in situ" detection of miRNA, LNA[133] or Morpholino[134] probes can be used. The locked conformation of LNA results in enhanced hybridization properties and increases sensitivity and selectivity, making it ideal for detection of short miRNA.[135]
High-throughput quantification of miRNAs is error prone, for the larger variance (compared to mRNAs) that comes with methodological problems. mRNA-expression is therefore often analyzed to check for miRNA-effects in their levels (e.g. in[136][137]). Databases can be used to pair mRNA- and miRNA-data that predict miRNA-targets based on their base sequence.[138][139] While this is usually done after miRNAs of interest have been detected (e. g. because of high expression levels), ideas for analysis tools that integrate mRNA- and miRNA-expression information have been proposed.[140][141]
Just as miRNA is involved in the normal functioning of eukaryotic cells, so has dysregulation of miRNA been associated with disease.[142] A manually curated, publicly available database, miR2Disease, documents known relationships between miRNA dysregulation and human disease.[143]
A mutation in the seed region of miR-96, causes hereditary progressive hearing loss.[144]
A mutation in the seed region of miR-184, causes hereditary keratoconus with anterior polar cataract.[145]
Deletion of the miR-17~92 cluster, causes skeletal and growth defects.[146]
The first human disease known to be associated with miRNA deregulation was chronic lymphocytic leukemia.[32] Many other miRNAs also have links with cancer[32] and accordingly are sometimes referred to as "oncomirs". In malignant B cells miRNAs participate in pathways fundamental to B cell development like B-cell receptor (BCR) signalling, B-cell migration/adhesion, cell-cell interactions in immune niches and the production and class-switching of immunoglobulins. MiRNAs influence B cell maturation, generation of pre-, marginal zone, follicular, B1, plasma and memory B cells.[32]
A study of mice altered to produce excess c-Myc — a protein with mutated forms implicated in several cancers — shows that miRNA affects the cancer development. Mice engineered to produce a surplus of types of miRNA found in lymphoma cells developed the disease within 50 days and died two weeks later. In contrast, mice without the surplus miRNA lived over 100 days.[32] Leukemia can be caused by the insertion of a viral genome next to the 17-92 array of microRNAs, leading to increased expression of this microRNA.[32]
Another study found that two types of miRNA inhibit the E2F1 protein, which regulates cell proliferation. miRNA appears to bind to messenger RNA before it can be translated to proteins that switch genes on and off.[32]
By measuring activity among 217 genes encoding miRNAs, patterns of gene activity that can distinguish types of cancers were identified. miRNA profiling can determine whether patients with chronic lymphocytic leukemia had slow growing or aggressive forms of the cancer.[32]
A novel miRNA-profiling-based screening assay for the detection of early-stage colorectal cancer is undergoing a clinical trial. Early results showed that blood plasma samples collected from patients with early, resectable (Stage II) colorectal cancer could be distinguished from those of sex-and age-matched healthy volunteers. Sufficient selectivity and specificity could be achieved using small (less than 1 mL) samples of blood.[147][148]
Another role for miRNA in cancers is to use their expression level for prognosis. For example, one study on NSCLC samples found that low miR-324a levels could serve as an indicator of poor survival.[149] Either high miR-185 or low miR-133b levels may correlate with metastasis and poor survival in colorectal cancer.[150]
Furthermore, specific miRNAs may be associated with certain histological subtypes of colorectal cancer. For instance, expression levels of miR-205 and miR-373 have been shown to be increased in mucinous colorectal cancers and mucin-producing Ulcerative Colitis-associated colon cancers, but not in sporadic colonic adenocarcinoma that lack mucinous components.[151] In-vitro studies suggested that miR-205 and miR-373 may functionally induce different features of mucinous-associated neoplastic progression in intestinal epithelial cells.[151]
Hepatocellular carcinoma cell proliferation may arise from miR-21 interaction with MAP2K3, a tumor repressor gene.[152] Optimal treatment for cancer involves accurately identifying patients for risk-stratified therapy. Those with a rapid response to initial treatment may benefit from truncated treatment regimens, showing the value of accurate disease response measures. Cell-free miRNA are highly stable in blood, are overexpressed in cancer and are quantifiable within the diagnostic laboratory. In classical Hodgkin lymphoma, plasma miR-21, miR-494, and miR-1973 are promising disease response biomarkers.[153] Circulating miRNAs have the potential to assist clinical decision making and aid interpretation of positron emission tomography combined with computerized tomography. They can be performed at each consultation to assess disease response and detect relapse.
A 2009 study explored miR-205 targeted for inhibiting the metastatic nature of breast cancer.[154] Five members of the microRNA-200 family (miR-200a, miR-200b, miR-200c, miR-141 and miR-429) are down-regulated in tumour progression of breast cancer.[155]
A 2017 study reported miR-1246, miR-196a and miR-196b are potential localized pancreatic cancer biomarker.[156]
The specific microRNA, miR-506 has been found to work as a tumor antagonist in several studies.[157]  In a 2014 study, a significant number of cervical cancer samples were found to have down-regulated expression of miR-506.  Additionally, studies found that miR-506 works to promote apoptosis of cervical cancer cells, through its direct target hedgehog pathway transcription factor, Gli3.[158]
A 2015 study used a triple helix of three miRNAs embedded in a dextran aldehyde/dendrimer gel in a mouse model of triple negative breast cancer. mir-205 and mir-212 targeted specific RNAs, while the other miRNA stabilized the others. The treatment reduced tumor sizes by 90% with survival times of 75 days.[159][160]
MicroRNAs have the potential to be used as targets for treatment of different cancers.  The specific microRNA, miR-506 has been found to work as a tumor antagonist in several studies.  In a 2014 study, a significant number of cervical cancer samples were found to have downregulated expression of miR-506.  Additionally, studies found that miR-506 works to promote apoptosis of cervical cancer cells, through its direct target hedgehog pathway transcription factor, Gli3.[157][158]
DNA damage is considered to be the primary underlying cause of cancer.[161] If DNA repair is deficient, damage can accumulate. Such damage can cause mutational errors during DNA replication due to error-prone translesion synthesis. Accumulated damage can also cause epigenetic alterations due to errors during DNA repair.[162][163] Such mutations and epigenetic alterations can give rise to cancer (see malignant neoplasms).
Germ line mutations in DNA repair genes cause only 2–5% of colon cancer cases.[164] However, altered expression of microRNAs, causing DNA repair deficiencies, are frequently associated with cancers and may be an important causal factor.
Among 68 sporadic colon cancers with reduced expression of the DNA mismatch repair protein MLH1, most were found to be deficient due to epigenetic methylation of the CpG island of the MLH1 gene.[165] However, up to 15% of MLH1-deficiencies in sporadic colon cancers appeared to be due to over-expression of the microRNA miR-155, which represses MLH1 expression.[166]
In 29–66%[167][168] of glioblastomas, DNA repair is deficient due to epigenetic methylation of the MGMT gene, which reduces protein expression of MGMT. However, for 28% of glioblastomas, the MGMT protein is deficient, but the MGMT promoter is not methylated.[167] In glioblastomas without methylated MGMT promoters, the level of microRNA miR-181d is inversely correlated with protein expression of MGMT and the direct target of miR-181d is the MGMT mRNA 3'UTR (the three prime untranslated region of MGMT mRNA).[167] Thus, in 28% of glioblastomas, increased expression of miR-181d and reduced expression of DNA repair enzyme MGMT may be a causal factor.
HMGA proteins (HMGA1a, HMGA1b and HMGA2) are implicated in cancer, and expression of these proteins is regulated by microRNAs. HMGA expression is almost undetectable in differentiated adult tissues, but is elevated in many cancers. HMGA proteins are polypeptides of ~100 amino acid residues characterized by a modular sequence organization. These proteins have three highly positively charged regions, termed AT hooks, that bind the minor groove of AT-rich DNA stretches in specific regions of DNA. Human neoplasias, including thyroid, prostatic, cervical, colorectal, pancreatic and ovarian carcinomas, show a strong increase of HMGA1a and HMGA1b proteins.[169] Transgenic mice with HMGA1 targeted to lymphoid cells develop aggressive lymphoma, showing that high HMGA1 expression is associated with cancers and that HMGA1 can act as an oncogene.[170] A 2003 study[171] showed that HMGA1 protein binds to the promoter region of DNA repair gene BRCA1 and inhibits BRCA1 promoter activity. They also showed that while only 11% of breast tumors had hypermethylation of the BRCA1 gene, 82% of aggressive breast cancers have low BRCA1 protein expression, and most of these reductions were due to chromatin remodeling by high levels of HMGA1 protein.
HMGA2 protein specifically targets the promoter of ERCC1, thus reducing expression of this DNA repair gene.[172] ERCC1 protein expression was deficient in 100% of 47 evaluated colon cancers (though the extent to which HGMA2 was involved is not known).[173] A 2012 study[174] showed that in normal tissues, HGMA1 and HMGA2 genes are targeted (and thus strongly reduced in expression) by miR-15, miR-16, miR-26a, miR-196a2 and Let-7a. However, each of these HMGA-targeting miRNAs are drastically reduced in almost all human pituitary adenomas studied, when compared with the normal pituitary gland. Consistent with the down-regulation of these HMGA-targeting miRNAs, an increase in the HMGA1 and HMGA2-specific mRNAs was observed. Three of these microRNAs (miR-16, miR-196a and Let-7a)[175][176] have methylated promoters and therefore low expression in colon cancer. For two of these, miR-15 and miR-16, the coding regions are epigenetically silenced in cancer due to histone deacetylase activity.[177] When these microRNAs are expressed at a low level, then HMGA1 and HMGA2 proteins are expressed at a high level. HMGA1 and HMGA2 target (reduce expression of) BRCA1 and ERCC1 DNA repair[178] genes. Thus DNA repair can be reduced, likely contributing to cancer progression.[161]
In contrast to the previous example, where under-expression of miRNAs indirectly caused reduced expression of DNA repair genes, in some cases over-expression of certain miRNAs may directly reduce expression of specific DNA repair proteins. A 2011 study[179] referred to 6 DNA repair genes that are directly targeted by the miRNAs indicated: ATM (miR-421), RAD52 (miR-210, miR-373), RAD23B (miR-373), MSH2 (miR-21), BRCA1 (miR-182) and P53 (miR-504, miR-125b). More recently, A 2014 study[180] listed multiple DNA repair genes directly targeted by these additional miRNAs: ATM (miR-100, miR18a, miR-101), DNA-PK (miR-101), ATR (mir-185), Wip1 (miR-16), MLH1, MSH2, MSH6 (miR-155), ERCC3, ERCC4 (miR-192) and UNG2 (miR-16, miR-34c). Among these miRNAs, miR-16, miR-18a, miR-21, miR-34c, miR-101, miR-125b, miR-155, miR-182, miR-185, miR-192 and miR-373 were identified[176] as over-expressed in colon cancer through epigenetic hypomethylation. Over expression of any one of these miRNAs can cause reduced expression of its target DNA repair gene.
The global role of miRNA function in the heart has been addressed by conditionally inhibiting miRNA maturation in the murine heart. This revealed that miRNAs play an essential role during its development.[181][182] miRNA expression profiling studies demonstrate that expression levels of specific miRNAs change in diseased human hearts, pointing to their involvement in cardiomyopathies.[183][184][185] Furthermore, animal studies on specific miRNAs identified distinct roles for miRNAs both during heart development and under pathological conditions, including the regulation of key factors important for cardiogenesis, the hypertrophic growth response and cardiac conductance.[182][186][187][188][189][190] Another role for miRNA in cardiovascular diseases is to use their expression levels for diagnosis, prognosis or risk stratification.[191]  miRNA's in animal models have also been linked to cholesterol metabolism and regulation.[192]
Murine microRNA-712 is a potential biomarker (i.e. predictor) for atherosclerosis, a cardiovascular disease of the arterial wall associated with lipid retention and inflammation.[193] Non-laminar blood flow also correlates with development of atherosclerosis as mechanosenors of endothelial cells respond to the shear force of disturbed flow (d-flow).[178] A number of pro-atherogenic genes including matrix metalloproteinases (MMPs) are upregulated by d-flow ,[178] mediating pro-inflammatory and pro-angiogenic signals. These findings were observed in ligated carotid arteries of mice to mimic the effects of d-flow. Within 24 hours, pre-existing immature miR-712 formed mature miR-712 suggesting that miR-712 is flow-sensitive.[178] Coinciding with these results, miR-712 is also upregulated in endothelial cells exposed to naturally occurring d-flow in the greater curvature of the aortic arch.[178]
Pre-mRNA sequence of miR-712 is generated from the murine ribosomal RN45s gene at the internal transcribed spacer region 2 (ITS2).[178] XRN1 is an exonuclease that degrades the ITS2 region during processing of RN45s.[178] Reduction of XRN1 under d-flow conditions therefore leads to the accumulation of miR-712.[178]
MiR-712 targets tissue inhibitor of metalloproteinases 3 (TIMP3).[178] TIMPs normally regulate activity of matrix metalloproteinases (MMPs) which degrade the extracellular matrix (ECM). Arterial ECM is mainly composed of collagen and elastin fibers, providing the structural support and recoil properties of arteries.[194] These fibers play a critical role in regulation of vascular inflammation and permeability, which are important in the development of atherosclerosis.[195] Expressed by endothelial cells, TIMP3 is the only ECM-bound TIMP.[194] A decrease in TIMP3 expression results in an increase of ECM degradation in the presence of d-flow. Consistent with these findings, inhibition of pre-miR712 increases expression of TIMP3 in cells, even when exposed to turbulent flow.[178]
TIMP3 also decreases the expression of TNFα (a pro-inflammatory regulator) during turbulent flow.[178]  Activity of TNFα in turbulent flow was measured by the expression of TNFα-converting enzyme (TACE) in blood. TNFα decreased if miR-712 was inhibited or TIMP3 overexpressed,[178] suggesting that miR-712 and TIMP3 regulate TACE activity in turbulent flow conditions.
Anti-miR-712 effectively suppresses d-flow-induced miR-712 expression and increases TIMP3 expression.[178] Anti-miR-712 also inhibits vascular hyperpermeability, thereby significantly reducing atherosclerosis lesion development and immune cell infiltration.[178]
The human homolog of miR-712 was found on the RN45s homolog gene, which maintains similar miRNAs to mice.[178] MiR-205 of humans sshare similar sequences with miR-712 of mice and is conserved across most vertebrates.[178] MiR-205 and miR-712 also share more than 50% of the cell signaling targets, including TIMP3.[178]
When tested, d-flow decreased the expression of XRN1 in humans as it did in mice endothelial cells, indicating a potentially common role of XRN1 in humans.[178]
Targeted deletion of Dicer in the FoxD1-derived renal progenitor cells in a murine model resulted in a complex renal phenotype including expansion of nephron progenitors, fewer renin cells, smooth muscle arterioles, progressive mesangial loss and glomerular aneurysms.[196] High throughput whole transcriptome profiling of the FoxD1-Dicer knockout mouse model revealed ectopic upregulation of pro-apoptotic gene, Bcl2L11 (Bim) and dysregulation of the p53 pathway with increase in p53 effector genes including Bax, Trp53inp1, Jun, Cdkn1a, Mmp2, and Arid3a. p53 protein levels remained unchanged, suggesting that FoxD1 stromal miRNAs directly repress p53-effector genes. Using a lineage tracing approach followed by Fluorescent-activated cell sorting, miRNA profiling of the FoxD1-derived cells not only comprehensively defined the transcriptional landscape of miRNAs that are critical for vascular development, but also identified key miRNAs that are likely to modulate the renal phenotype in its absence. These miRNAs include miRs‐10a, 18a, 19b, 24, 30c, 92a, 106a, 130a, 152, 181a, 214, 222, 302a, 370, and 381 that regulate Bcl2L11 (Bim) and miRs‐15b, 18a, 21, 30c, 92a, 106a, 125b‐5p, 145, 214, 222, 296‐5p and 302a that regulate p53-effector genes. Consistent with the profiling results, ectopic apoptosis was observed in the cellular derivatives of the FoxD1 derived progenitor lineage and reiterates the importance of renal stromal miRNAs in cellular homeostasis.[196]
miRNAs appear to regulate the development and function of the nervous system.[197] Neural miRNAs are involved at various stages of synaptic development, including dendritogenesis (involving miR-132, miR-134 and miR-124), synapse formation[198] and synapse maturation (where miR-134 and miR-138 are thought to be involved).[199] Some studies find altered miRNA expression in schizophrenia, as well as bipolar disorder and major depression and anxiety disorders.[200][201][202]
The vital role of miRNAs in gene expression is significant to addiction, specifically alcoholism.[203] Chronic alcohol abuse results in persistent changes in brain function mediated in part by alterations in gene expression.[203] miRNA global regulation of many downstream genes deems significant regarding the reorganization or synaptic connections or long term neuroadaptions involving the behavioral change from alcohol consumption to withdrawal and/or dependence.[204] Up to 35 different miRNAs have been found to be altered in the alcoholic post-mortem brain, all of which target genes that include the regulation of the cell cycle, apoptosis, cell adhesion, nervous system development and cell signaling.[203] Altered miRNA levels were found in the medial prefrontal cortex of alcohol-dependent mice, suggesting the role of miRNA in orchestrating translational imbalances and the creation of differentially expressed proteins within an area of the brain where complex cognitive behavior and decision making likely originate.[205]
miRNAs can be either upregulated or downregulated in response to chronic alcohol use. miR-206 expression increased in the prefrontal cortex of alcohol-dependent rats, targeting the transcription factor brain-derived neurotrophic factor (BDNF) and ultimately reducing its expression. BDNF plays a critical role in the formation and maturation of new neurons and synapses, suggesting a possible implication in synapse growth/synaptic plasticity in alcohol abusers.[206] miR-155, important in regulating alcohol-induced neuroinflammation responses, was found to be upregulated, suggesting the role of microglia and inflammatory cytokines in alcohol pathophysiology.[207] Downregulation of miR-382 was found in the nucleus accumbens, a structure in the basal forebrain significant in regulating feelings of reward that power motivational habits. miR-382 is the target for the dopamine receptor D1 (DRD1), and its overexpression results in the upregulation of DRD1 and delta fosB, a transcription factor that activates a series of transcription events in the nucleus accumbens that ultimately result in addictive behaviors.[208] Alternatively, overexpressing miR-382 resulted in attenuated drinking and the inhibition of DRD1 and delta fosB upregulation in rat models of alcoholism, demonstrating the possibility of using miRNA-targeted pharmaceuticals in treatments.[208]
miRNAs play crucial roles in the regulation of stem cell progenitors differentiating into adipocytes.[209] Studies to determine what role pluripotent stem cells play in adipogenesis, were examined in the immortalized human bone marrow-derived stromal cell line hMSC-Tert20.[210] Decreased expression of miR-155, miR-221, and miR-222, have been found during the adipogenic programming of both immortalized and primary hMSCs, suggesting that they act as negative regulators of differentiation. Conversely, ectopic expression of the miRNAs 155,221, and 222 significantly inhibited adipogenesis and repressed induction of the master regulators PPARγ and CCAAT/enhancer-binding protein alpha (CEBPA).[211] This paves the way for possible genetic obesity treatments.
Another class of miRNAs that regulate insulin resistance, obesity, and diabetes, is the let-7 family. Let-7 accumulates in human tissues during the course of aging.[212] When let-7 was ectopically overexpressed to mimic accelerated aging, mice became insulin-resistant, and thus more prone to high fat diet-induced obesity and diabetes.[213] In contrast when let-7 was inhibited by injections of let-7-specific antagomirs, mice become more insulin-sensitive and remarkably resistant to high fat diet-induced obesity and diabetes. Not only could let-7 inhibition prevent obesity and diabetes, it could also reverse and cure the condition.[214] These experimental findings suggest that let-7 inhibition could represent a new therapy for obesity and type 2 diabetes.
miRNAs also play crucial roles in the regulation of complex enzymatic cascades including the hemostatic blood coagulation system.[215] Large scale studies of functional miRNA targeting have recently uncovered rationale therapeutic targets in the hemostatic system.[216]
When the human genome project mapped its first chromosome in 1999, it was predicted the genome would contain over 100,000 protein coding genes. However, only around 20,000 were eventually identified.[217] Since then, the advent of bioinformatics approaches combined with genome tiling studies examining the transcriptome,[218] systematic sequencing of full length cDNA libraries[219] and experimental validation[220] (including the creation of miRNA derived antisense oligonucleotides called antagomirs) have revealed that many transcripts are non protein-coding RNA, including several snoRNAs and miRNAs.[221]
Viral microRNAs play an important role in the regulation of gene expression of viral and/or host genes to benefit the virus. Hence, miRNAs play a key role in host–virus interactions and pathogenesis of viral diseases.[222][223] The expression of transcription activators by human herpesvirus-6 DNA is believed to be regulated by viral miRNA.[224]
miRNAs can bind to target messenger RNA (mRNA) transcripts of protein-coding genes and negatively control their translation or cause mRNA degradation. It is of key importance to identify the miRNA targets accurately.[225] A comparison of the predictive performance of eighteen in silico algorithms is available.[226]. Large scale studies of functional miRNA targeting suggest that many functional miRNAs can be missed by target prediction algorithms.[216]





Neoplasm - Wikipedia
Neoplasia is a type of abnormal and excessive growth of tissue.  The growth of a neoplasia is uncoordinated with that of the normal surrounding tissue, and it persists growing abnormally, even if the original trigger is removed.[1][2][3] This abnormal growth usually (but not always) forms a mass.[4]  When it forms a mass, it may be called a tumor.  
ICD-10 classifies neoplasms into four main groups: benign neoplasms, in situ neoplasms, malignant neoplasms, and neoplasms of uncertain or unknown behavior.[5] Malignant neoplasms are also simply known as cancers and are the focus of oncology.
Prior to the abnormal growth of tissue, as neoplasia, cells often undergo an abnormal pattern of growth, such as metaplasia or dysplasia.[6] However, metaplasia or dysplasia does not always progress to neoplasia.[1] The word is from Ancient Greek νέος- neo "new" and πλάσμα plasma "formation, creation".
A neoplasm can be benign, potentially malignant, or malignant (cancer).[7]
Neoplastic tumors are often heterogeneous and contain more than one type of cell, but their initiation and continued growth is usually dependent on a single population of neoplastic cells. These cells are presumed to be clonal – that is, they are derived from the same cell,[8]
and all carry the same genetic or epigenetic anomaly – evident of clonality. For lymphoid neoplasms, e.g. lymphoma and leukemia, clonality is proven by the amplification of a single rearrangement of their immunoglobulin gene (for B cell lesions) or T cell receptor gene (for T cell lesions). The demonstration of clonality is now considered to be necessary to identify a lymphoid cell proliferation as neoplastic.[9]
It is tempting to define neoplasms as clonal cellular proliferations but the demonstration of clonality is not always possible. Therefore, clonality is not required in the definition of neoplasia.
Tumor (American English) or tumour (British English), Latin for swelling, one of the cardinal signs of inflammation, originally meant any form of swelling, neoplastic or not. Current English, however, both medical and non-medical, uses tumor as a synonym for a neoplasm (a solid or fluid-filled cystic lesion that may or may not be formed by an abnormal growth of neoplastic cells) that appears enlarged in size.[10][11] Some neoplasms do not form a tumor; these include leukemia and most forms of carcinoma in situ. Tumor is also not synonymous with cancer. While cancer is by definition malignant, a tumor can be benign, precancerous, or malignant.
The terms mass and nodule are often used synonymously with tumor. Generally speaking, however, the term tumor is used generically, without reference to the physical size of the lesion.[1] More specifically, the term mass is often used when the lesion has a maximal diameter of at least 20 millimeters (mm) in greatest direction, while the term nodule is usually used when the size of the lesion is less than 20 mm in its greatest dimension (25.4 mm = 1 inch).[12]
A neoplasm can be caused by an abnormal proliferation of tissues, which can be caused by genetic mutations. Not all types of neoplasms cause a tumorous overgrowth of tissue, however (such as leukemia or carcinoma in situ) and similarities between neoplasmic growths and regenerative processes, e.g., dedifferentiation and rapid cell proliferation, have been pointed out [13].
Recently, tumor growth has been studied using mathematics and continuum mechanics. Vascular tumors (formed from blood vessels) are thus looked at as being amalgams of a solid skeleton formed by sticky cells and an organic liquid filling the spaces in which cells can grow.[14] Under this type of model, mechanical stresses and strains can be dealt with and their influence on the growth of the tumor and the surrounding tissue and vasculature elucidated. Recent findings from experiments that use this model show that active growth of the tumor is restricted to the outer edges of the tumor, and that stiffening of the underlying normal tissue inhibits tumor growth as well.[15]
Benign conditions that are not associated with an abnormal proliferation of tissue (such as sebaceous cysts) can also present as tumors, however, but have no malignant potential. Breast cysts (as occur commonly during pregnancy and at other times) are another example, as are other encapsulated glandular swellings (thyroid, adrenal gland, pancreas).
Encapsulated hematomas, encapsulated necrotic tissue (from an insect bite, foreign body, or other noxious mechanism), keloids (discrete overgrowths of scar tissue) and granulomas may also present as tumors.
Discrete localized enlargements of normal structures (ureters, blood vessels, intrahepatic or extrahepatic biliary ducts, pulmonary inclusions, or gastrointestinal duplications) due to outflow obstructions or narrowings, or abnormal connections, may also present as a tumor. Examples are arteriovenous fistulae or aneurysms (with or without thrombosis), biliary fistulae or aneurysms, sclerosing cholangitis, cysticercosis or hydatid cysts, intestinal duplications, and pulmonary inclusions as seen with cystic fibrosis. It can be dangerous to biopsy a number of types of tumor in which the leakage of their contents would potentially be catastrophic. When such types of tumors are encountered, diagnostic modalities such as ultrasound, CT scans, MRI, angiograms, and nuclear medicine scans are employed prior to (or during) biopsy or surgical exploration/excision in an attempt to avoid such severe complications.
The nature of a tumor is determined by imaging, by surgical exploration, or by a pathologist after examination of the tissue from a biopsy or a surgical specimen.
DNA damage is considered to be the primary underlying cause of malignant neoplasms known as cancers.[16][17]  Its central role in progression to cancer is illustrated in the figure in this section, in the box near the top. (The central features of DNA damage, epigenetic alterations and deficient DNA repair in progression to cancer are shown in red.)  DNA damage is very common.  Naturally occurring DNA damages (mostly due to cellular metabolism and the properties of DNA in water at body temperatures) occur at a rate of more than 60,000 new damages, on average, per human cell, per day[16] [also see article DNA damage (naturally occurring) ].  Additional DNA damages can arise from exposure to exogenous agents.  Tobacco smoke causes increased exogenous DNA damage, and these DNA damages are the likely cause of lung cancer due to smoking.[18] UV light from solar radiation causes DNA damage that is important in melanoma.[19] Helicobacter pylori infection produces high levels of reactive oxygen species that damage DNA and contributes to gastric cancer.[20] Bile acids, at high levels in the colons of humans eating a high fat diet, also cause DNA damage and contribute to colon cancer.[21] Katsurano et al. indicated that macrophages and neutrophils in an inflamed colonic epithelium are the source of reactive oxygen species causing the DNA damages that initiate colonic tumorigenesis.[22]  Some sources of DNA damage are indicated in the boxes at the top of the figure in this section.
Individuals with a germ line mutation causing deficiency in any of 34 DNA repair genes (see article DNA repair-deficiency disorder) are at increased risk of cancer.  Some germ line mutations in DNA repair genes cause up to 100% lifetime chance of cancer (e.g., p53 mutations).[23] These germ line mutations are indicated in a box at the left of the figure with an arrow indicating their contribution to DNA repair deficiency.
About 70% of malignant neoplasms have no hereditary component and are called "sporadic cancers".[24] Only a minority of sporadic cancers have a deficiency in DNA repair due to mutation in a DNA repair gene.  However, a majority of sporadic cancers have deficiency in DNA repair due to epigenetic alterations that reduce or silence DNA repair gene expression.  For example, of 113 sequential colorectal cancers, only four had a missense mutation in the DNA repair gene MGMT, while the majority had reduced MGMT expression due to methylation of the MGMT promoter region (an epigenetic alteration).[25]  Five reports present evidence that between 40% and 90% of colorectal cancers have reduced MGMT expression due to methylation of the MGMT promoter region.[26][27][28][29][30]
Similarly, out of 119 cases of mismatch repair-deficient colorectal cancers that lacked DNA repair gene PMS2 expression, PMS2 was deficient in 6 due to mutations in the PMS2 gene, while in 103 cases PMS2 expression was deficient because its pairing partner MLH1 was repressed due to promoter methylation (PMS2 protein is unstable in the absence of MLH1).[31] In the other 10 cases, loss of PMS2 expression was likely due to epigenetic overexpression of the microRNA, miR-155, which down-regulates MLH1.[32]
In further examples, epigenetic defects were found at frequencies of between 13%-100% for the DNA repair genes BRCA1, WRN, FANCB, FANCF, MGMT, MLH1, MSH2, MSH4, ERCC1, XPF, NEIL1 and ATM.  These epigenetic defects occurred in various cancers (e.g. breast, ovarian, colorectal and head and neck).  Two or three deficiencies in expression of ERCC1, XPF or PMS2 occur simultaneously in the majority of the 49 colon cancers evaluated by Facista et al.[33] Epigenetic alterations causing reduced expression of DNA repair genes is shown in a central box at the third level from the top of the figure in this section, and the consequent DNA repair deficiency is shown at the fourth level.
When expression of DNA repair genes is reduced, DNA damages accumulate in cells at a higher than normal level, and these excess damages cause increased frequencies of mutation or epimutation. Mutation rates strongly increase in cells defective in DNA mismatch repair[34][35] or in homologous recombinational repair (HRR).[36]
During repair of DNA double strand breaks, or repair of other DNA damages, incompletely cleared sites of repair can cause epigenetic gene silencing.[37][38] DNA repair deficiencies (level 4 in the figure) cause increased DNA damages (level 5 in the figure) which result in increased somatic mutations and epigenetic alterations (level 6 in the figure).
Field defects, normal appearing tissue with multiple alterations (and discussed in the section below), are common precursors to development of the disordered and improperly proliferating clone of tissue in a malignant neoplasm.  Such field defects (second level from bottom of figure) may have multiple mutations and epigenetic alterations.
Once a cancer is formed, it usually has genome instability.  This instability is likely due to reduced DNA repair or excessive DNA damage.  Because of such instability, the cancer continues to evolve and to produce sub clones.  For example, a renal cancer, sampled in 9 areas, had 40 ubiquitous mutations, demonstrating tumour heterogeneity  (i.e. present in all areas of the cancer), 59 mutations shared by some (but not all areas), and 29 “private” mutations only present in one of the areas of the cancer.[39]
Various other terms have been used to describe this phenomenon, including "field effect", "field cancerization", and "field carcinogenesis". The term "field cancerization" was first used in 1953 to describe an area or "field" of epithelium that has been preconditioned by (at that time) largely unknown processes so as to predispose it towards development of cancer.[40] Since then, the terms "field cancerization" and "field defect" have been used to describe pre-malignant tissue in which new cancers are likely to arise.
Field defects are important in progression to cancer.[41][42] However, in most cancer research, as pointed out by Rubin[43] “The vast majority of studies in cancer research has been done on well-defined tumors in vivo, or on discrete neoplastic foci in vitro.  Yet there is evidence that more than 80% of the somatic mutations found in mutator phenotype human colorectal tumors occur before the onset of terminal clonal expansion.[44] Similarly, Vogelstein et al.[45] point out that more than half of somatic mutations identified in tumors occurred in a pre-neoplastic phase (in a field defect), during growth of apparently normal cells.  Likewise, epigenetic alterations present in tumors may have occurred in pre-neoplastic field defects.
An expanded view of field effect has been termed "etiologic field effect", which encompasses not only molecular and pathologic changes in pre-neoplastic cells but also influences of exogenous environmental factors and molecular changes in the local microenvironment on neoplastic evolution from tumor initiation to patient death.[46]
In the colon, a field defect probably arises by natural selection of a mutant or epigenetically altered cell among the stem cells at the base of one of the intestinal crypts on the inside surface of the colon. A mutant or epigenetically altered stem cell may replace the other nearby stem cells by natural selection. Thus, a patch of abnormal tissue may arise.  The figure in this section includes a photo of a freshly resected and lengthwise-opened segment of the colon showing a colon cancer and four polyps. Below the photo there is a schematic diagram of how a large patch of mutant or epigenetically altered cells may have formed, shown by the large area in yellow in the diagram.  Within this first large patch in the diagram (a large clone of cells), a second such mutation or epigenetic alteration may occur so that a given stem cell acquires an advantage compared to other stem cells within the patch, and this altered stem cell may expand clonally forming a secondary patch, or sub-clone, within the original patch. This is indicated in the diagram by four smaller patches of different colors within the large yellow original area. Within these new patches (sub-clones), the process may be repeated multiple times, indicated by the still smaller patches within the four secondary patches (with still different colors in the diagram) which clonally expand, until stem cells arise that generate either small polyps or else a malignant neoplasm (cancer).
In the photo, an apparent field defect in this segment of a colon has generated four polyps (labeled with the size of the polyps, 6mm, 5mm, and two of 3mm, and a cancer about 3 cm across in its longest dimension). These neoplasms are also indicated, in the diagram below the photo, by 4 small tan circles (polyps) and a larger red area (cancer).  The cancer in the photo occurred in the cecal area of the colon, where the colon joins the small intestine (labeled) and where the appendix occurs (labeled).  The fat in the photo is external to the outer wall of the colon.  In the segment of colon shown here, the colon was cut open lengthwise to expose the inner surface of the colon and to display the cancer and polyps occurring within the inner epithelial lining of the colon.
If the general process by which sporadic colon cancers arise is the formation of a pre-neoplastic clone that spreads by natural selection, followed by formation of internal sub-clones within the initial clone, and sub-sub-clones inside those, then colon cancers generally should be associated with, and be preceded by, fields of increasing abnormality reflecting the succession of premalignant events. The most extensive region of abnormality (the outermost yellow irregular area in the diagram) would reflect the earliest event in formation of a malignant neoplasm.
In experimental evaluation of specific DNA repair deficiencies in cancers, many specific DNA repair deficiencies were also shown to occur in the field defects surrounding those cancers.  The Table, below, gives examples for which the DNA repair deficiency in a cancer was shown to be caused by an epigenetic alteration, and the somewhat lower frequencies with which the same epigenetically caused DNA repair deficiency was found in the surrounding field defect.
Some of the small polyps in the field defect shown in the photo of the opened colon segment may be relatively benign neoplasms.  Of polyps less than 10mm in size, found during colonoscopy and followed with repeat colonoscopies for 3 years, 25% were unchanged in size, 35% regressed or shrank in size while 40% grew in size.[54]
Cancers are known to exhibit genome instability or a mutator phenotype.[55] The protein-coding DNA within the nucleus is about 1.5% of the total genomic DNA.[56] Within this protein-coding DNA (called the exome), an average cancer of the breast or colon can have about 60 to 70 protein altering mutations, of which about 3 or 4 may be “driver” mutations, and the remaining ones may be “passenger” mutations[45]  However, the average number of DNA sequence mutations in the entire genome (including non-protein-coding regions) within a breast cancer tissue sample is about 20,000.[57] In an average melanoma tissue sample (where melanomas have a higher exome mutation frequency[45]) the total number of DNA sequence mutations is about 80,000.[58] This compares to the very low mutation frequency of about 70 new mutations in the entire genome between generations (parent to child) in humans.[59][60]
The high frequencies of mutations in the total nucleotide sequences within cancers suggest that often an early alteration in the field defects giving rise to a cancer (e.g. yellow area in the diagram in this section) is a deficiency in DNA repair.  The large field defects surrounding colon cancers (extending to at about 10 cm on each side of a cancer) were shown by Facista et al.[33] to frequently have epigenetic defects in 2 or 3 DNA repair proteins (ERCC1, XPF or PMS2) in the entire area of the field defect.  Deficiencies in DNA repair cause increased mutation rates.[34][35][36] A deficiency in DNA repair, itself, can allow DNA damages to accumulate, and error-prone translesion synthesis past some of those damages may give rise to mutations.  In addition, faulty repair of these accumulated DNA damages may give rise to epimutations.  These new mutations or epimutations may provide a proliferative advantage, generating a field defect.  Although the mutations/epimutations in DNA repair genes do not, themselves, confer a selective advantage, they may be carried along as passengers in cells when the cells acquire additional mutations/epimutations that do provide a proliferative advantage.
The term 'neoplasm' is a synonym of "tumor". 'Neoplasia' denotes the process of the formation of neoplasms/tumors, the process is referred to as a 'neoplastic' process. 'Neoplastic' itself comes from the Greek 'neo' ('new') and 'plastic' ('formed, molded').
The term tumor is derived from the Latin noun tumor "a swelling" ultimately from the verb tumēre "to swell". In the Commonwealth the spelling "tumour" is commonly used, whereas in the U.S. it is usually spelled "tumor".
In its medical sense it has traditionally meant an abnormal swelling of the flesh. The Roman medical encyclopedist Celsus (ca 30 BC–38 AD) described the four cardinal signs of acute inflammation as tumor, dolor, calor, and rubor (swelling, pain, increased heat, and redness). His treatise, De Medicina, was the first medical book printed in 1478 following the invention of the movable-type printing press.
In contemporary English, the word tumor is often used as a synonym for a cystic (liquid-filled) growth or solid neoplasm (cancerous or non-cancerous),[61] with other forms of swelling often referred to as swellings.[62]
Related terms are common in the medical literature, where the nouns tumefaction and tumescence (derived from the adjective tumefied), are current medical terms for non-neoplastic swelling. This type of swelling is most often caused by inflammation caused by trauma, infection, and other factors.
Tumors may be caused by conditions other than an overgrowth of neoplastic cells, however. Cysts (such as sebaceous cysts) are also referred to as tumors, even though they have no neoplastic cells. This is standard in medical billing terminology (especially when billing for a growth whose pathology has yet to be determined).



Metastasis - Wikipedia
Metastasis is a pathogenic agent's spread from an initial or primary site to a different or secondary site within the host's body;[1] it is typically spoken of as such spread by a cancerous tumor.[2]  The newly pathological sites, then, are metastases (mets).[3][4]
Cancer occurs after cells are genetically altered to proliferate rapidly and indefinitely. This uncontrolled proliferation by mitosis produces a primary heterogeneic tumour. The cells which constitute the tumor eventually undergo metaplasia, followed by dysplasia then anaplasia, resulting in a malignant phenotype. This malignancy allows for invasion into the circulation, followed by invasion to a second site for tumorigenesis.
Some cancer cells known as circulating tumor cells acquire the ability to penetrate the walls of lymphatic or blood vessels, after which they are able to circulate through the bloodstream to other sites and tissues in the body.[5] This process is known (respectively) as lymphatic or hematogenous spread.  After the tumor cells come to rest at another site, they re-penetrate the vessel or walls and continue to multiply, eventually forming another clinically detectable tumor.[citation needed] This new tumor is known as a metastatic (or secondary) tumor. Metastasis is one of the hallmarks of cancer, distinguishing it from benign tumors.[6] Most cancers can metastasize, although in varying degrees. Basal cell carcinoma for example rarely metastasizes.[6]
When tumor cells metastasize, the new tumor is called a secondary or metastatic tumor, and its cells are similar to those in the original or primary tumor. This means that if breast cancer metastasizes to the lungs, the secondary tumor is made up of abnormal breast cells, not of abnormal lung cells. The tumor in the lung is then called metastatic breast cancer, not lung cancer.  Metastasis is a key element in cancer staging systems such as the TNM staging system, where it represents the "M".  In overall stage grouping, metastasis places a cancer in Stage IV.  The possibilities of curative treatment are greatly reduced, or often entirely removed, when a cancer has metastasized.
Initially, nearby lymph nodes are struck early.[7] The lungs, liver, brain, and bones are the most common metastasis locations from solid tumors.[7]
Although advanced cancer may cause pain, it is often not the first symptom.
Some patients, however, do not show any symptoms.[7]
When the organ gets a metastatic disease it begins to shrink until its lymph nodes burst, or undergo lysis.
Metastatic tumors are very common in the late stages of cancer. The spread of metastasis may occur via the blood or the lymphatics or through both routes. The most common places for the metastases to occur are the lungs, liver, brain, and the bones.[8]
Metastasis involves a complex series of steps in which cancer cells leave the original tumor site and migrate to other parts of the body via the bloodstream, via the lymphatic system, or by direct extension. To do so, malignant cells break away from the primary tumor and attach to and degrade proteins that make up the surrounding extracellular matrix (ECM), which separates the tumor from adjoining tissues. By degrading these proteins, cancer cells are able to breach the ECM and escape. The location of the metastases is not always random, with different types of cancer tending to spread to particular organs and tissues at a rate that is higher than expected by statistical chance alone.[9] Breast cancer, for example, tends to metastasize to the bones and lungs. This specificity seems to be mediated by soluble signal molecules such as chemokines[10] and transforming growth factor beta.[11] The body resists metastasis by a variety of mechanisms through the actions of a class of proteins known as metastasis suppressors, of which about a dozen are known.[12]
Human cells exhibit three kinds of motion: collective motility, mesenchymal-type movement, and amoeboid movement. Cancer cells often opportunistically switch between different kinds of motion. Some cancer researchers hope to find treatments that can stop or at least slow down the spread of cancer by somehow blocking some necessary step in one or more kinds of motion.[13]
Cancer researchers studying the conditions necessary for cancer metastasis have discovered that one of the critical events required is the growth of a new network of blood vessels, called tumor angiogenesis.[14] It has been found that angiogenesis inhibitors would therefore prevent the growth of metastases.[6]
Several different cell types are critical to tumor growth. In particular, endothelial progenitor cells have been shown to have a strong influence on the growth of tumor blood-vessels. Endothelial progenitor cells are also critical for metastasis and angiogenesis.[15][16] Endothelial progenitor cells are important in tumor growth, angiogenesis and metastasis, and can be marked using the Inhibitor of DNA Binding 1 (ID1). This novel finding meant that investigators gained the ability to track endothelial progenitor cells from the bone marrow to the blood to the tumor-stroma and even incorporated in tumor vasculature. Endothelial progenitor cells incorporated in tumor vasculature suggests that this cell type in blood-vessel development is important in a tumor setting and metastasis. Furthermore, ablation of the endothelial progenitor cells in the bone marrow can lead to a significant decrease in tumor growth and vasculature development. Therefore, endothelial progenitor cells are important in tumor biology and present novel therapeutic targets.[17]
NFAT transcription factors are implicated in breast cancer, more specifically in the process of cell motility as the basis of metastasis formation. Indeed, NFAT1 (NFATC2) and NFAT5 are pro-invasive and pro-migratory in breast carcinoma[18][19]  and NFAT3 (NFATc4) is an inhibitor of cell motility.[20]
NFAT1 regulates the expression of the TWEAKR and its ligand TWEAK with the Lipocalin 2 to increase breast-cancer cell invasion [21] and NFAT3 inhibits Lipocalin 2 expression to blunt the cell invasion.[20]
Epigenetic regulation also plays an important role in the metastatic outgrowth of disseminated tumor cells.  Metastases display alterations in histone modifications, such as H3K4-methylation and H3K9-methylation, when compared to matching primary tumors.[22]  These epigenetic modifications in metastases may allow the proliferation and survival of disseminated tumor cells in distant organs.[23]
A recent study shows that PKC-iota promotes melanoma cell invasion by activating Vimentin during EMT. PKC-iota inhibition or knockdown resulted an increase E-cadherin and RhoA levels while decreasing total Vimentin, phophorylated Vimentin (S39) and Par6 in metastatic melanoma cells. These results suggested that PKC-ι is involved in signaling pathways which upregulate EMT in melanoma thereby directly stimulates metastasis.[24]
Recently, a series of high-profile experiments suggests that the co-option of intercellular cross-talk mediated by exosome vesicles is a critical factor involved in all steps of the invasion-metastasis cascade.[25]
Metastasis occurs by the following four routes:
The spread of a malignancy into body cavities can occur via penetrating the surface of the peritoneal, pleural, pericardial, or subarachnoid spaces. For example, ovarian tumors can spread transperitoneally to the surface of the liver.
Lymphatic spread allows the transport of tumor cells to regional lymph nodes near the primary tumor and ultimately, to other parts of the body. This is called nodal involvement, positive nodes, or regional disease. "Positive nodes" is a term that would be used by medical specialists to describe regional lymph nodes that tested positive for malignancy. It is common medical practice to test by biopsy at least one lymph node near a tumor site when carrying out surgery to examine or remove a tumor. This lymph node is then called a sentinel lymph node.
Lymphatic spread is the most common route of initial metastasis for carcinomas.[6] In contrast, it is uncommon for a sarcoma to metastasize via this route. Localized spread to regional lymph nodes near the primary tumor is not normally counted as a metastasis, although this is a sign of a worse outcome. 
The lymphatic system does eventually drain from the thoracic duct and right lymphatic duct into the systemic venous system at the venous angle and into the brachiocephalic veins, and therefore these metastatic cells can also eventually spread through the haematogenous route.
This is typical route of metastasis for sarcomas, but it is also the favored route for certain types of carcinoma, such as renal cell carcinoma originating in the kidney. Because of their thinner walls, veins are more frequently invaded than are arteries, and metastasis tends to follow the pattern of venous flow. That is, hematogenous spread often follows distinct patterns depending on the location of the primary tumor. For example, colorectal cancer spreads primarily through the portal vein to the liver.
Some tumors, especially carcinomas may metastasize along anatomical canalicular spaces. These spaces include for example the bile ducts, the urinary system, the airways and the subarachnoid space. The process is similar to that of transcoelomic spread. However, often it remains unclear whether simultaneously diagnosed tumors of a canalicular system are one metastatic process or in fact independent tumors caused by the same agent (field cancerization).
There is a propensity for certain tumors to seed in particular organs. This was first discussed as the "seed and soil" theory by Stephen Paget over a century ago, in 1889. The propensity for a metastatic cell to spread to a particular organ is termed 'organotropism'. For example, prostate cancer usually metastasizes to the bones. In a similar manner, colon cancer has a tendency to metastasize to the liver. Stomach cancer often metastasises to the ovary in women, then it is called a Krukenberg tumor.
According to the "seed and soil" theory, it is difficult for cancer cells to survive outside their region of origin, so in order to metastasize they must find a location with similar characteristics.[27] For example, breast tumor cells, which gather calcium ions from breast milk, metastasize to bone tissue, where they can gather calcium ions from bone. Malignant melanoma spreads to the brain, presumably because neural tissue and melanocytes arise from the same cell line in the embryo.[28]
In 1928, James Ewing challenged the "seed and soil" theory and proposed that metastasis occurs purely by anatomic and mechanical routes.  This hypothesis has been recently utilized to suggest several hypotheses about the life cycle of circulating tumor cells (CTCs) and to postulate that the patterns of spread could be better understood through a 'filter and flow' perspective.[29] However, contemporary evidences indicate that the primary tumour may dictate organotropic metastases by inducing the formation of pre-metastatic niches at distant sites, where incoming metastatic cells may engraft and colonise.[30] Specifically, exosome vesicles secreted by tumours have been shown to home to pre-metastatic sites, where they activate pro-metastatic processes such as angiogenesis and modify the immune contexture, so as to foster a favourable microenvironment for secondary tumour growth.[30]
It is theorized that metastasis always coincides with a primary cancer, and, as such, is a tumor that started from a cancer cell or cells in another part of the body. However, over 10% of patients presenting to oncology units will have metastases without a primary tumor found. In these cases, doctors refer to the primary tumor as "unknown" or "occult," and the patient is said to have cancer of unknown primary origin (CUP) or unknown primary tumors (UPT).[31] It is estimated that 3% of all cancers are of unknown primary origin.[32] Studies have shown that, if simple questioning does not reveal the cancer's source (coughing up blood—"probably lung", urinating blood—"probably bladder"), complex imaging will not either.[32] In some of these cases a primary tumor may appear later.
The use of immunohistochemistry has permitted pathologists to give an identity to many of these metastases. However, imaging of the indicated area only occasionally reveals a primary.  In rare cases (e.g., of melanoma), no primary tumor is found, even on autopsy. It is therefore thought that some primary tumors can regress completely, but leave their metastases behind. In other cases, the tumor might just be too small and/or in an unusual location to be diagnosed.
The cells in a metastatic tumor resemble those in the primary tumor. Once the cancerous tissue is examined under a microscope to determine the cell type, a doctor can usually tell whether that type of cell is normally found in the part of the body from which the tissue sample was taken.
For instance, breast cancer cells look the same whether they are found in the breast or have spread to another part of the body. So, if a tissue sample taken from a tumor in the lung contains cells that look like breast cells, the doctor determines that the lung tumor is a secondary tumor. Still, the determination of the primary tumor can often be very difficult, and the pathologist may have to use several adjuvant techniques, such as immunohistochemistry, FISH (fluorescent in situ hybridization), and others. Despite the use of techniques, in some cases the primary tumor remains unidentified.
Metastatic cancers may be found at the same time as the primary tumor, or months or years later. When a second tumor is found in a patient that has been treated for cancer in the past, it is more often a metastasis than another primary tumor.
It was previously thought that most cancer cells have a low metastatic potential and that there are rare cells that develop the ability to metastasize through the development of somatic mutations.[33]  According to this theory, diagnosis of metastatic cancers is only possible after the event of metastasis.  Traditional means of diagnosing cancer (e.g. a biopsy) would only investigate a subpopulation of the cancer cells and would very likely not sample from the subpopulation with metastatic potential.[34]
The somatic mutation theory of metastasis development has not been substantiated in human cancers.  Rather, it seems that the genetic state of the primary tumor reflects the ability of that cancer to metastasize.[34]  Research comparing gene expression between primary and metastatic adenocarcinomas identified a subset of genes whose expression could distinguish primary tumors from metastatic tumors, dubbed a "metastatic signature."[34]  Up-regulated genes in the signature include:  SNRPF, HNRPAB, DHPS and securin.  Actin, myosin and MHC class II down-regulation was also associated with the signature.  Additionally, the metastatic-associated expression of these genes was also observed in some primary tumors, indicating that cells with the potential to metastasize could be identified concurrently with diagnosis of the primary tumor.[35] Recent work identified a form of genetic instability in cancer called chromosome instability (CIN) as a driver of metastasis.[36] In aggressive cancer cells, loose DNA fragments from unstable chromosomes spill in the cytosol leading to the chronic activation of innate immune pathways, which are hijacked by cancer cells to spread to distant organs.
Expression of this metastatic signature has been correlated with a poor prognosis and has been shown to be consistent in several types of cancer.  Prognosis was shown to be worse for individuals whose primary tumors expressed the metastatic signature.[34]  Additionally, the expression of these metastatic-associated genes was shown to apply to other cancer types in addition to adenocarcinoma.  Metastases of breast cancer, medulloblastoma and prostate cancer all had similar expression patterns of these metastasis-associated genes.[34]
The identification of this metastasis-associated signature provides promise for identifying cells with metastatic potential within the primary tumor and hope for improving the prognosis of these metastatic-associated cancers.  Additionally, identifying the genes whose expression is changed in metastasis offers potential targets to inhibit metastasis.[34]
Cut surface of a humerus sawn lengthwise, showing a large cancerous metastasis (the whitish tumor between the head and the shaft of the bone)
Micrograph of thyroid cancer (papillary thyroid carcinoma) in a lymph node of the neck. H&E stain
CT image  of multiple liver metastases
CT image of a lung metastasis
Metastasis proven by liver biopsy (tumor (adenocarcinoma) - lower two-thirds of image). H&E stain.
Metastatic cancer in the lungs
Metastases from the lungs to the brain
Metastases from the lungs to the pancreas
Treatment and survival is determined, to a great extent, by whether or not a cancer remains localized or spreads to other locations in the body. If the cancer metastasizes to other tissues or organs it usually dramatically increases a patient's likelihood of death. Some cancers—such as some forms of leukemia, a cancer of the blood, or malignancies in the brain—can kill without spreading at all.
Once a cancer has metastasized it may still be treated with radiosurgery, chemotherapy, radiation therapy, biological therapy, hormone therapy, surgery, or a combination of these interventions ("multimodal therapy"). The choice of treatment depends on a large number of factors, including the type of primary cancer, the size and location of the metastases, the patient's age and general health, and the types of treatments used previously. In patients diagnosed with CUP it is often still possible to treat the disease even when the primary tumor cannot be located.
Current treatments are rarely able to cure metastatic cancer though some tumors, such as testicular cancer and thyroid cancer, are usually curable.
Palliative care, care aimed at improving the quality of life of people with major illness, has been recommended as part of management programs for metastasis.[37]
Although metastasis is widely accepted to be the result of the tumor cells migration, there is a hypothesis saying that some metastases are the result of inflammatory processes by abnormal immune cells.[38]  The existence of metastatic cancers in the absence of primary tumors also suggests that metastasis is not always caused by malignant cells that leave primary tumors.[39]
In March 2014 researchers discovered the oldest complete example of a human with metastatic cancer. The tumors had developed in a 3,000-year-old skeleton found in 2013 in a tomb in Sudan dating back to 1200 BC. The skeleton was analyzed using radiography and a scanning electron microscope. These findings were published in the Public Library of Science journal.[40][41][42]
Metastasis is a Greek word meaning "displacement", from μετά, meta, "next", and στάσις, stasis, "placement".
Medical information about metastatic cancer
Charities and advocacy groups dealing with metastatic cancer



Lymphatic system - Wikipedia
The lymphatic system is part of the vascular system and an important part of the immune system, comprising a network of lymphatic vessels that carry a clear fluid called lymph (from Latin, lympha meaning "water"[1]) directionally towards the heart. The lymphatic system was first described in the seventeenth century independently by Olaus Rudbeck and Thomas Bartholin. Unlike the circulatory system, the lymphatic system is not a closed system. The human circulatory system processes an average of 20 litres of blood per day through capillary filtration, which removes plasma while leaving the blood cells. Roughly 17 litres of the filtered plasma is reabsorbed directly into the blood vessels, while the remaining three litres remain in the interstitial fluid.  One of the main functions of the lymph system is to provide an accessory return route to the blood for the surplus three litres.[2]
The other main function is that of defense in the immune system. Lymph is very similar to blood plasma: it contains lymphocytes. It also contains waste products and cellular debris together with bacteria and proteins. Associated organs composed of lymphoid tissue are the sites of lymphocyte production. Lymphocytes are concentrated in the lymph nodes. The spleen and the thymus are also lymphoid organs of the immune system. The tonsils are lymphoid organs that are also associated with the digestive system. Lymphoid tissues contain lymphocytes, and also contain other types of cells for support.[3] The system also includes all the structures dedicated to the circulation and production of lymphocytes (the primary cellular component of lymph), which also includes the bone marrow, and the lymphoid tissue associated with the digestive system.[4]
The blood does not come into direct contact with the parenchymal cells and tissues in the body (except in case of an injury causing rupture of one or more blood vessels), but constituents of the blood first exit the microvascular exchange blood vessels to become interstitial fluid, which comes into contact with the parenchymal cells of the body. Lymph is the fluid that is formed when interstitial fluid enters the initial lymphatic vessels of the lymphatic system. The lymph is then moved along the lymphatic vessel network by either intrinsic contractions of the lymphatic passages or by extrinsic compression of the lymphatic vessels via external tissue forces (e.g., the contractions of skeletal muscles), or by lymph hearts in some animals. The organization of lymph nodes and drainage follows the organization of the body into external and internal regions; therefore, the lymphatic drainage of the head, limbs, and body cavity walls follows an external route, and the lymphatic drainage of the thorax, abdomen, and pelvic cavities follows an internal route.[5]  Eventually, the lymph vessels empty into the lymphatic ducts, which drain into one of the two subclavian veins, near their junction with the internal jugular veins.
The lymphatic system consists of lymphatic organs, a conducting network of lymphatic vessels, and the circulating lymph.
The primary  or central lymphoid organs generate lymphocytes from immature progenitor cells.
The thymus and the bone marrow constitute the primary lymphoid organs involved in the production and early clonal selection of lymphocyte tissues. Bone marrow is responsible for both the creation of T cells and the production and maturation of B cells. From the bone marrow, B cells immediately join the circulatory system and travel to secondary lymphoid organs in search of pathogens. T cells, on the other hand, travel from the bone marrow to the thymus, where they develop further. Mature T cells join B cells in search of pathogens. The other 95% of T cells begin a process of apoptosis, a form of programmed cell death.
Secondary or peripheral lymphoid organs, which include lymph nodes and the spleen, maintain mature naive lymphocytes and initiate an adaptive immune response. The peripheral lymphoid organs are the sites of lymphocyte activation by antigens. Activation leads to clonal expansion and affinity maturation. Mature lymphocytes recirculate between the blood and the peripheral lymphoid organs until they encounter their specific antigen.
Secondary lymphoid tissue provides the environment for the foreign or altered native molecules (antigens) to interact with the lymphocytes. It is exemplified by the lymph nodes, and the lymphoid follicles in tonsils, Peyer's patches, spleen, adenoids, skin, etc. that are associated with the mucosa-associated lymphoid tissue (MALT).
In the gastrointestinal wall the appendix has mucosa resembling that of the colon, but here it is heavily infiltrated with lymphocytes.
The tertiary lymphoid tissue[clarification needed] typically contains far fewer lymphocytes, and assumes an immune role only when challenged with antigens that result in inflammation. It achieves this by importing the lymphocytes from blood and lymph.[6]
The thymus is a primary lymphoid organ and the site of maturation for T cells, the lymphocytes of the adaptive immune system. The thymus increases in size from birth in response to postnatal antigen stimulation, then to puberty and regresses thereafter.[7] The loss or lack of the thymus results in severe immunodeficiency and subsequent high susceptibility to infection.[7] In most species, the thymus consists of lobules divided by septa which are made up of epithelium and is therefore an epithelial organ. T cells mature from thymocytes, proliferate and undergo selection process in the thymic cortex before entering the medulla to interact with epithelial cells.
The thymus provides an inductive environment for development of T cells from hematopoietic progenitor cells.  In addition, thymic stromal cells allow for the selection of a functional and self-tolerant T cell repertoire. Therefore, one of the most important roles of the thymus is the induction of central tolerance.
The thymus is largest and most active during the neonatal and pre-adolescent periods. By the early teens, the thymus begins to atrophy and thymic stroma is mostly replaced by adipose tissue. Nevertheless, residual T lymphopoiesis continues throughout adult life.
The main functions of the spleen are:
The spleen synthesizes antibodies in its white pulp and removes antibody-coated bacteria and antibody-coated blood cells by way of blood and lymph node circulation. A study published in 2009 using mice found that the spleen contains, in its reserve, half of the body's monocytes within the red pulp.[8] These monocytes, upon moving to injured tissue (such as the heart), turn into dendritic cells and macrophages while promoting tissue healing.[8][9][10] The spleen is a center of activity of the mononuclear phagocyte system and can be considered analogous to a large lymph node, as its absence causes a predisposition to certain infections.
Like the thymus, the spleen has only efferent lymphatic vessels. Both the short gastric arteries and the splenic artery supply it with blood.[11]
The germinal centers are supplied by arterioles called penicilliary radicles.[12]
Up to the fifth month of  prenatal development the spleen creates red blood cells. After birth the bone marrow is solely responsible for hematopoiesis. As a major lymphoid organ and a central player in the reticuloendothelial system, the spleen retains the ability to produce lymphocytes. The spleen stores red blood cells and lymphocytes. It can store enough blood cells to help in an emergency. Up to 25% of lymphocytes can be stored at any one time.[13]
A lymph node is an organized collection of lymphoid tissue, through which the lymph passes on its way back to the blood. Lymph nodes are located at intervals along the lymphatic system. Several afferent lymph vessels bring in lymph, which percolates through the substance of the lymph node, and is then drained out by an efferent lymph vessel. There are between five and six hundred lymph nodes in the human body, many of which are grouped in clusters in different regions as in the underarm and abdominal areas. Lymph node clusters are commonly found at the base of limbs (groin, armpits) and in the neck, where lymph is collected from regions of the body likely to sustain pathogen contamination from injuries.
The substance of a lymph node consists of lymphoid follicles in an outer portion called the cortex. The inner portion of the node is called the medulla, which is surrounded by the cortex on all sides except for a portion known as the hilum. The hilum presents as a depression on the surface of the lymph node, causing the otherwise spherical lymph node to be bean-shaped or ovoid. The efferent lymph vessel directly emerges from the lymph node at the hilum. The arteries and veins supplying the lymph node with blood enter and exit through the hilum.
The region of the lymph node called the paracortex immediately surrounds the medulla. Unlike the cortex, which has mostly immature T cells, or thymocytes, the paracortex has a mixture of immature and mature T cells. Lymphocytes enter the lymph nodes through specialised high endothelial venules found in the paracortex.
A lymph follicle is a dense collection of lymphocytes, the number, size and configuration of which change in accordance with the functional state of the lymph node. For example, the follicles expand significantly when encountering a foreign antigen. The selection of B cells, or B lymphocytes, occurs in the germinal centre of the lymph nodes.
Lymph nodes are particularly numerous in the mediastinum in the chest, neck, pelvis, axilla, inguinal region, and in association with the blood vessels of the intestines.[4]
Lymphoid tissue associated with the lymphatic system is concerned with immune functions in defending the body against infections and the spread of tumours. It consists of connective tissue formed of reticular fibers, with various types of leukocytes, (white blood cells), mostly lymphocytes enmeshed in it, through which the lymph passes.[14] Regions of the lymphoid tissue that are densely packed with lymphocytes are known as lymphoid follicles. Lymphoid tissue can either be structurally well organized as lymph nodes or may consist of loosely organized lymphoid follicles known as the mucosa-associated lymphoid tissue.
The central nervous system also has lymphatic vessels, as discovered by the University of Virginia Researchers. The search for T-cell gateways into and out of the meninges uncovered functional meningeal lymphatic vessels lining the dural sinuses, anatomically integrated into the membrane surrounding the brain.[15]
The lymphatic vessels, also called lymph vessels, conduct lymph between different parts of the body. They include the tubular vessels of the lymph capillaries, and the larger collecting vessels–the right lymphatic duct and the thoracic duct (the left lymphatic duct). The lymph capillaries are mainly responsible for the absorption of interstitial fluid from the tissues, while lymph vessels propel the absorbed fluid forward into the larger collecting ducts, where it ultimately returns to the bloodstream via one of the subclavian veins. These vessels are also called the lymphatic channels or simply lymphatics.[16]
The lymphatics are responsible for maintaining the balance of the body fluids. Its network of capillaries and collecting lymphatic vessels work to efficiently drain and transport extravasated fluid, along with proteins and antigens, back to the circulatory system. Numerous intraluminal valves in the vessels ensure a unidirectional flow of lymph without reflux.[17] Two valve systems are used to achieve this one directional flow—a primary and a secondary valve system.[18]
The capillaries are blind-ended, and the valves at the ends of capillaries use specialised junctions together with anchoring filaments to allow a unidirectional flow to the primary vessels. The collecting lymphatics, however, act to propel the lymph by the combined actions of the intraluminal valves and lymphatic muscle cells.[19]
Lymphatic tissues begin to develop by the end of the fifth week of embryonic development.[20] Lymphatic vessels develop from lymph sacs that arise from developing veins, which are derived from mesoderm.
The first lymph sacs to appear are the paired jugular lymph sacs at the junction of the internal jugular and subclavian veins.[20] From the jugular lymph sacs, lymphatic capillary plexuses spread to the thorax, upper limbs, neck and head.[20] Some of the plexuses enlarge and form lymphatic vessels in their respective regions. Each jugular lymph sac retains at least one connection with its jugular vein, the left one developing into the superior portion of the thoracic duct.
The next lymph sac to appear is the unpaired retroperitoneal lymph sac at the root of the mesentery of the intestine. It develops from the primitive vena cava and mesonephric veins. Capillary plexuses and lymphatic vessels spread from the retroperitoneal lymph sac to the abdominal viscera and diaphragm. The sac establishes connections with the cisterna chyli but loses its connections with neighbouring veins.
The last of the lymph sacs, the paired posterior lymph sacs, develop from the iliac veins. The posterior lymph sacs produce capillary plexuses and lymphatic vessels of the abdominal wall, pelvic region, and lower limbs. The posterior lymph sacs join the cisterna chyli and lose their connections with adjacent veins.
With the exception of the anterior part of the sac from which the cisterna chyli develops, all lymph sacs become invaded by mesenchymal cells and are converted into groups of lymph nodes.
The spleen develops from mesenchymal cells between layers of the dorsal mesentery of the stomach.[20] The thymus arises as an outgrowth of the third pharyngeal pouch.
The lymphatic system has multiple interrelated functions:[21]
Lymph vessels called lacteals are at the beginning of the gastrointestinal tract, predominantly in the small intestine. While most other nutrients absorbed by the small intestine are passed on to the portal venous system to drain via the portal vein into the liver for processing, fats (lipids) are passed on to the lymphatic system to be transported to the blood circulation via the thoracic duct. (There are exceptions, for example medium-chain triglycerides are fatty acid esters of glycerol that passively diffuse from the GI tract to the portal system.) The enriched lymph originating in the lymphatics of the small intestine is called chyle. The nutrients that are released into the circulatory system are processed by the liver, having passed through the systemic circulation.
The lymphatic system plays a major role in the body's immune system, as the primary site for cells relating to adaptive immune system including T-cells and B-cells. Cells in the lymphatic system react to antigens presented or found by the cells directly or by other dendritic cells. When an antigen is recognized, an immunological cascade begins involving the activation and recruitment of more and more cells, the production of antibodies and cytokines and the recruitment of other immunological cells such as macrophages.
The study of lymphatic drainage of various organs is important in the diagnosis, prognosis, and treatment of cancer.  The lymphatic system, because of its closeness to many tissues of the body, is responsible for carrying cancerous cells between the various parts of the body in a process called metastasis. The intervening lymph nodes can trap the cancer cells.  If they are not successful in destroying the cancer cells the nodes may become sites of secondary tumours.
Lymphadenopathy refers to one or more enlarged lymph nodes. Small groups or individually enlarged lymph nodes are generally reactive in response to infection or inflammation. This is called local lymphadenopathy. When many lymph nodes in different areas of the body are involved, this is called generalised lymphadenopathy. Generalised lymphadenopathy may be caused by infections such as infectious mononucleosis, tuberculosis and HIV, connective tissue diseases such as SLE and rheumatoid arthritis, and cancers, including both cancers of tissue within lymph nodes, discussed below, and metastasis of cancerous cells from other parts of the body, that have arrived via the lymphatic system.[22]
Lymphedema is the swelling caused by the accumulation of lymph, which may occur if the lymphatic system is damaged or has malformations. It usually affects limbs, though the face, neck and abdomen may also be affected. In an extreme state, called elephantiasis, the edema progresses to the extent that the skin becomes thick with an appearance similar to the skin on elephant limbs.[23]
Causes are unknown in most cases, but sometimes there is a previous history of severe infection, usually caused by a parasitic disease, such as lymphatic filariasis.
Lymphangiomatosis is a disease involving multiple cysts or lesions formed from lymphatic vessels.[relevant to this paragraph?  – discuss]
Lymphedema can also occur after surgical removal of lymph nodes in the armpit (causing the arm to swell due to poor lymphatic drainage) or groin (causing swelling of the leg). Treatment is by manual lymphatic drainage. There is no evidence to suggest that the effects of manual lymphatic drainage are permanent.[24]
Cancer of the lymphatic system can be primary or secondary. Lymphoma refers to cancer that arises from lymphatic tissue. Lymphoid leukaemias and lymphomas are now considered to be tumours of the same type of cell lineage. They are called "leukaemia" when in the blood or marrow and "lymphoma" when in lymphatic tissue. They are grouped together under the name "lymphoid malignancy".[25]
Lymphoma is generally considered as either Hodgkin lymphoma or non-Hodgkin lymphoma. Hodgkin lymphoma is characterised by a particular type of cell, called a Reed–Sternberg cell, visible under microscope. It is associated with past infection with the Epstein-Barr Virus, and generally causes a painless "rubbery" lymphadenopathy. It is staged, using Ann Arbor staging. Chemotherapy generally involves the ABVD and may also involve radiotherapy.[22] Non-Hodgkin lymphoma is a cancer characterised by increased proliferation of B-cells or T-cells, generally occurs in an older age group than Hodgkin lymphoma. It is treated according to whether it is high-grade or low-grade, and carries a poorer prognosis than Hodgkin lymphoma.[22]
Lymphangiosarcoma is a malignant soft tissue tumour, whereas lymphangioma is a benign tumour occurring frequently in association with Turner syndrome. Lymphangioleiomyomatosis is a benign tumour of the smooth muscles of the lymphatics that occurs in the lungs.
Lymphoid leukaemia is another form of cancer where the host is devoid of different lymphatic cells.
Hippocrates, in the 5th century BC, was one of the first people to mention the lymphatic system. In his work On Joints, he briefly mentioned the lymph nodes in one sentence. Rufus of Ephesus, a Roman physician, identified the axillary, inguinal and mesenteric lymph nodes as well as the thymus during the 1st to 2nd century AD.[26] The first mention of lymphatic vessels was in the 3rd century BC by Herophilos, a Greek anatomist living in Alexandria, who incorrectly concluded that the "absorptive veins of the lymphatics," by which he meant the lacteals (lymph vessels of the intestines), drained into the hepatic portal veins, and thus into the liver.[26] The findings of Ruphus and Herophilos were further propagated by the Greek physician Galen, who described the lacteals and mesenteric lymph nodes which he observed in his dissection of apes and pigs in the 2nd century AD.[26][27]
In the mid 16th century, Gabriele Falloppio (discoverer of the fallopian tubes), described what is now known as the lacteals as "coursing over the intestines full of yellow matter."[26] In about 1563 Bartolomeo Eustachi, a professor of anatomy, described the thoracic duct in horses as vena alba thoracis.[26] The next breakthrough came when in 1622 a physician, Gaspare Aselli, identified lymphatic vessels of the intestines in dogs and termed them venue alba et lacteal, which is now known as simply the lacteals. The lacteals were termed the fourth kind of vessels (the other three being the artery, vein and nerve, which was then believed to be a type of vessel), and disproved Galen's assertion that chyle was carried by the veins. But, he still believed that the lacteals carried the chyle to the liver (as taught by Galen).[28] He also identified the thoracic duct but failed to notice its connection with the lacteals.[26] This connection was established by Jean Pecquet in 1651, who found a white fluid mixing with blood in a dog's heart. He suspected that fluid to be chyle as its flow increased when abdominal pressure was applied. He traced this fluid to the thoracic duct, which he then followed to a chyle-filled sac he called the chyli receptaculum, which is now known as the cisternae chyli; further investigations led him to find that lacteals' contents enter the venous system via the thoracic duct.[26][28] Thus, it was proven convincingly that the lacteals did not terminate in the liver, thus disproving Galen's second idea: that the chyle flowed to the liver.[28] Johann Veslingius drew the earliest sketches of the lacteals in humans in 1647.[27]
The idea that blood recirculates through the body rather than being produced anew by the liver and the heart was first accepted as a result of works of William Harvey—a work he published in 1628. In 1652, Olaus Rudbeck (1630–1702), a Swede, discovered certain transparent vessels in the liver that contained clear fluid (and not white), and thus named them hepatico-aqueous vessels. He also learned that they emptied into the thoracic duct and that they had valves.[28] He announced his findings in the court of Queen Christina of Sweden, but did not publish his findings for a year,[29] and in the interim similar findings were published by Thomas Bartholin, who additionally published that such vessels are present everywhere in the body, not just in the liver. He is also the one to have named them "lymphatic vessels."[28] This had resulted in a bitter dispute between one of Bartholin's pupils, Martin Bogdan,[30] and Rudbeck, whom he accused of plagiarism.[29]
Galen's ideas prevailed in medicine until the 17th century. It was thought that blood was produced by the liver from chyle contaminated with ailments by the intestine and stomach, to which various spirits were added by other organs, and that this blood was consumed by all the organs of the body. This theory required that the blood be consumed and produced many times over. Even in the 17th century, his ideas were defended by some physicians.[27]
Alexander Monro, of the University of Edinburgh Medical School, was the first to describe the function of the lymphatic system in detail.[31]
"Claude Galien". Lithograph by Pierre Roche Vigneron. (Paris: Lith de Gregoire et Deneux, ca. 1865)
Gabriele Falloppio
Portrait of Eustachius
Olaus Rudbeck in 1696.
Thomas Bartholin
Lymph originates in the Classical Latin word lympha "water",[32] which is also the source of the English word limpid. The spelling with y and ph was influenced by folk etymology with Greek νύμϕη (nýmphē) "nymph".[33]
The adjective used for the lymph-transporting system is lymphatic. The adjective used for the tissues where lymphocytes are formed is lymphoid. Lymphatic comes from the Latin word lymphaticus, meaning "connected to water."



Cancer screening - Wikipedia
Cancer screening aims to detect cancer before symptoms appear.[1] This may involve blood tests, urine tests, DNA tests other tests, or medical imaging.[1][2] The benefits of screening in terms of cancer prevention, early detection and subsequent treatment must be weighed against any harms.
Universal screening, also known as mass screening or population screening, involves screening everyone, usually within a specific age group.[3] Selective screening identifies people who are known to be at higher risk of developing cancer, such as people with a family history of cancer.[3]
Screening can lead to false positive results and subsequent invasive procedures.[4] Screening can also lead to false negative results, where an existing cancer is missed. Controversy arises when it is not clear if the benefits of screening outweigh the risks of the screening procedure itself, and any follow-up diagnostic tests and treatments.
Screening tests must be effective, safe, well-tolerated with acceptably low rates of false positive and false negative results. If signs of cancer are detected, more definitive and invasive follow-up tests are performed to reach a diagnosis. Screening for cancer can lead to cancer prevention and earlier diagnosis. Early diagnosis may lead to higher rates of successful treatment and extended life. However, it may also falsely appear to increase the time to death through lead time bias or length time bias.
The goal of cancer screening is to provide useful health information which can guide medical treatment.[5][medical citation needed] A good cancer screening is one which would detect when a person has cancer so that the person could seek treatment to protect their health.[5] Good cancer screening would not be more likely to cause harm than to provide useful information.[5] In general, cancer screening has risks and should not be done except with a medical indication.[5]
Different kinds of cancer screening procedures have different risks, but good tests share some characteristics.[5] If a test detects cancer, then that test result should also lead to options for treatment.[5] Good tests come with a patient explanation of why that person has high enough risk of cancer to justify the test.[5] Part of the testing experience is for the health care provider to explain how common false positive results are so that the patient can understand the context of their results.[5] If multiple tests are available, then any test should be presented along with other options.[5]
Screening for cancer is controversial in cases when it is not yet known if the test actually saves lives.[6] Screening can lead to substantial false positive result and subsequent invasive procedures.[4] The controversy arises when it is not clear if the benefits of screening outweigh the risks of follow-up diagnostic tests and cancer treatments. Cancer screening is not indicated unless life expectancy is greater than five years and the benefit is uncertain over the age of 70.[7]
Several factors are considered to determine whether the benefits of screening outweigh the risks and the costs of screening.[1] These factors include:
Breast cancer screening is the medical screening of asymptomatic, apparently healthy women for breast cancer in an attempt to achieve an earlier diagnosis. The assumption is that early detection will improve outcomes.  A number of screening tests have been employed, including clinical and self breast exams, mammography, genetic screening, ultrasound, and magnetic resonance imaging. The use of mammography in universal screening for breast cancer is controversial as it may not reduce all-cause mortality and for causing harms through unnecessary treatments and medical procedures. Many national organizations recommend it for most older women.
Cervical screening by the Pap test or other methods is highly effective at detecting and preventing cervical cancer, although there is a serious risk of overtreatment in young women up to the age of 20 or beyond, who are prone to have many abnormal cells which clear up naturally.[10] There is a considerable range in the recommended age at which to begin screening around the world. According to the 2010 European guidelines for cervical cancer screening, the age at which to commence screening ranges between 20–30 years of age, "but preferentially not before age 25 or 30 years", depending on the burden of the disease in the population and the available resources.[11]
In the United States the rate of cervical cancer is 0.1% among women under 20 years of age, so the American Cancer Society as well as the American College of Obstetricians and Gynecologists strongly recommend that screening begin at age 21, regardless of age at sexual initiation or other risk-related behaviors.[12][13][14] For healthy women aged 21–29 who have never had an abnormal Pap smear, cervical cancer screening with cervical cytology (Pap smear) should occur every 3 years, regardless of HPV vaccination status.[15] The preferred screening for women aged 30–65 is "co-testing", which includes a combination of cervical cytology screening and HPV testing, every 5 years.[15] However, it is acceptable to screen this age group with a Pap smear alone every 3 years.[15] In women over the age of 65, screening for cervical cancer may be discontinued in the absence of abnormal screening results within the prior 10 years and no history of CIN 2 or higher.[15]
Screening for colorectal cancer, if done early enough, is preventive because almost all[16][17] colorectal cancers originate from benign growths called polyps, which can be located and removed during a colonoscopy (see colonic polypectomy).
The US Preventive Services Task Force recommends screening for colorectal cancer using fecal occult blood testing, sigmoidoscopy, or colonoscopy, in adults, beginning at age 50 years and continuing until age 75 years.[18] For people over 75 or those with a life expectancy of less than 10 years screening is not recommended. A new enzyme method for colorectal cancer screening is the M2-PK Test,[19] which is able to detect bleeding and non-bleeding colorectal cancers and polyps.[18] In 2008, Kaiser Permanente Colorado implemented a program that used automated calls and sends fecal immunochemical test kits to patients who are overdue for colorectal cancer screenings. The program has increased the proportion of all eligible members screened by 25 percent.[20] DNA testing with Cologuard test has been FDA-approved.[21]
In England, adults are screened biennially via faecal occult blood testing between the ages of 60 and 74 years.[22]
When screening for prostate cancer, the PSA test may detect small cancers that would never become life-threatening, but once detected will lead to treatment. This situation, called overdiagnosis, puts men at risk for complications from unnecessary treatment such as surgery or radiation. Follow up procedures used to diagnose prostate cancer (prostate biopsy) may cause side effects, including bleeding and infection. Prostate cancer treatment may cause incontinence (inability to control urine flow) and erectile dysfunction (erections inadequate for intercourse).[23] The U.S. Preventative Services Task Force (USPSTF) recommends against prostate-specific antigen (PSA) based screening for prostate cancer finding, "there is a very small potential benefit and significant potential harms" and concluding, "while everyone wants to help prevent deaths from prostate cancer, current methods of PSA screening and treatment of screen-detected cancer are not the answer."[24][25] Most North American medical groups recommend individualized decisions about screening, taking into consideration the risks, benefits, and the patients' personal preferences.[26]
Screening studies for lung cancer have only been done in high risk populations, such as smokers and workers with occupational exposure to certain substances.[27] In the 2010s recommendations by medical authorities are turning in favour of lung cancer screening, which is likely to become more widely available in the advanced economies.
In December 2013 the U.S. Preventative Services Task Force (USPSTF) changed its long-standing recommendation that there is insufficient evidence to recommend for or against screening for lung cancer to the following: "The USPSTF recommends annual screening for lung cancer with low-dose computed tomography in adults ages 55 to 80 years who have a 30 pack-year smoking history and currently smoke or have quit within the past 15 years. Screening should be discontinued once a person has not smoked for 15 years or develops a health problem that substantially limits life expectancy or the ability or willingness to have curative lung surgery".[28]
It is generally agreed that general screening of large groups for pancreatic cancer is not at present likely to be effective, and outside clinical trials there are no programmes for this. The European Society for Medical Oncology recommends regular screening with endoscopic ultrasound and MRI/CT imaging for those at high risk from inherited genetics,[29] in line with other recommendations,[30][31] which may also include CT.[30]
The US Preventive Services Task Force (USPSTF) in 2013 found that evidence was insufficient to determine the balance of benefits and harms of screening for oral cancer in adults without symptoms by primary care providers.[32] The American Academy of Family Physicians comes to similar conclusions while the American Cancer Society recommends that adults over 20 years who have periodic health examinations should have the oral cavity examined for cancer.[32] The American Dental Association recommends that providers remain alert for signs of cancer during routine examinations.[32] Oral cancer screening is also recommended by some groups of dental hygienists.[33]
There is insufficient evidence to recommend for or against screening for skin cancer,[34] and bladder cancer.[35] Routine screening is not recommended for testicular cancer,[36] and ovarian cancer.[37]
Full body CT scans are available for cancer screening, but this type of medical imaging to search for cancer in people without clear symptoms can create problems such as increased exposure to ionizing radiation. However, magnetic resonance imaging (MRI) scans are not associated with a radiation risk, and MRI scans are being evaluated for their use in cancer screening.[38] There is a significant risk of detection of what has been called incidentalomas - benign lesions that may be interpreted as a cancer and be subjected to potentially dangerous investigations.[citation needed]



Medical diagnosis - Wikipedia
Medical diagnosis (abbreviated Dx[1] or DS) is the process of determining which disease or condition explains a person's symptoms and signs. It is most often referred to as diagnosis with the medical context being implicit. The information required for diagnosis is typically collected from a history and physical examination of the person seeking medical care. Often, one or more diagnostic procedures, such as diagnostic tests, are also done during the process. Sometimes posthumous diagnosis is considered a kind of medical diagnosis.
Diagnosis is often challenging, because many signs and symptoms are nonspecific. For example, redness of the skin (erythema), by itself, is a sign of many disorders and thus does not tell the healthcare professional what is wrong. Thus differential diagnosis, in which several possible explanations are compared and contrasted, must be performed. This involves the correlation of various pieces of information followed by the recognition and differentiation of patterns. Occasionally the process is made easy by a sign or symptom (or a group of several) that is pathognomonic.
Diagnosis is a major component of the procedure of a doctor's visit. From the point of view of statistics, the diagnostic procedure involves classification tests.
The first recorded examples of medical diagnosis are found in the writings of Imhotep (2630–2611 BC) in ancient Egypt (the Edwin Smith Papyrus).[2] A Babylonian medical textbook, the Diagnostic Handbook written by Esagil-kin-apli (fl.1069–1046 BC), introduced the use of empiricism, logic and rationality in the diagnosis of an illness or disease.[3] Traditional Chinese Medicine, as described in the Yellow Emperor's Inner Canon or Huangdi Neijing, specified four diagnostic methods: inspection, auscultation-olfaction, interrogation,  and palpation.[4] Hippocrates was known to make diagnoses by tasting his patients' urine and smelling their sweat.[5]
A diagnosis, in the sense of diagnostic procedure, can be regarded as an attempt at classification of an individual's condition into separate and distinct categories that allow medical decisions about treatment and prognosis to be made. Subsequently, a diagnostic opinion is often described in terms of a disease or other condition, but in the case of a wrong diagnosis, the individual's actual disease or condition is not the same as the individual's diagnosis.
A diagnostic procedure may be performed by various health care professionals such as a physician, physical therapist, optometrist, healthcare scientist, chiropractor, dentist, podiatrist, nurse practitioner, or physician assistant. This article uses diagnostician as any of these person categories.
A diagnostic procedure (as well as the opinion reached thereby) does not necessarily involve elucidation of the etiology of the diseases or conditions of interest, that is, what caused the disease or condition. Such elucidation can be useful to optimize treatment, further specify the prognosis or prevent recurrence of the disease or condition in the future.
The initial task is to detect a medical indication to perform a diagnostic procedure. Indications include:
Even during an already ongoing diagnostic procedure, there can be an indication to perform another, separate, diagnostic procedure for another, potentially concomitant, disease or condition. This may occur as a result of an incidental finding of a sign unrelated to the parameter of interest, such as can occur in comprehensive tests such as radiological studies like magnetic resonance imaging or blood test panels that also include blood tests that are not relevant for the ongoing diagnosis.
General components which are present in a diagnostic procedure in most of the various available methods include:
There are a number of methods or techniques that can be used in a diagnostic procedure, including performing a differential diagnosis or following medical algorithms.[7] In reality, a diagnostic procedure may involve components of multiple methods.[8]
The method of differential diagnosis is based on finding as many candidate diseases or conditions as possible that can possibly cause the signs or symptoms, followed by a process of elimination or at least of rendering the entries more or less probable by further medical tests and other processing until, aiming to reach the point where only one candidate disease or condition remains as probable. The final result may also remain a list of possible conditions, ranked in order of probability or severity.
The resultant diagnostic opinion by this method can be regarded more or less as a diagnosis of exclusion. Even if it does not result in a single probable disease or condition, it can at least rule out any imminently life-threatening conditions.
Unless the provider is certain of the condition present, further medical tests, such as medical imaging, are performed or scheduled in part to confirm or disprove the diagnosis but also to document the patient's status and keep the patient's medical history up to date.
If unexpected findings are made during this process, the initial hypothesis may be ruled out and the provider must then consider other hypotheses.
In a pattern recognition method the provider uses experience to recognize a pattern of clinical characteristics.[7] It is mainly based on certain symptoms or signs being associated with certain diseases or conditions, not necessarily involving the more cognitive processing involved in a differential diagnosis.
This may be the primary method used in cases where diseases are "obvious", or the provider's experience may enable him or her to recognize the condition quickly. Theoretically, a certain pattern of signs or symptoms can be directly associated with a certain therapy, even without a definite decision regarding what is the actual disease, but such a compromise carries a substantial risk of missing a diagnosis which actually has a different therapy so it may be limited to cases where no diagnosis can be made.
The term diagnostic criteria designates the specific combination of signs, symptoms, and test results that the clinician uses to attempt to determine the correct diagnosis.
Some examples of diagnostic criteria, also known as clinical case definitions, are:
Clinical decision support systems are interactive computer programs designed to assist health professionals with decision-making tasks. The clinician interacts with the software utilizing both the clinician’s knowledge and the software to make a better analysis of the patients data than either human or software could make on their own.  Typically the system makes suggestions for the clinician to look through and the clinician picks useful information and removes erroneous suggestions.[9]
Other methods that can be used in performing a diagnostic procedure include:
Diagnosis problems are the dominant cause of medical malpractice payments, accounting for 35% of total payments in a study of 25 years of data and 350,000 claims.[13]
Overdiagnosis is the diagnosis of "disease" that will never cause symptoms or death during a patient's lifetime. It is a problem because it turns people into patients unnecessarily and because it can lead to economic waste (overutilization) and treatments that may cause harm. Overdiagnosis occurs when a disease is diagnosed correctly, but the diagnosis is irrelevant. A correct diagnosis may be irrelevant because treatment for the disease is not available, not needed, or not wanted.
Most people will experience at least one diagnostic error in their lifetime, according to a 2015 report by the National Academies of Sciences, Engineering, and Medicine.[14]
Causes and factors of error in diagnosis are:[15]
When making a medical diagnosis, a lag time is a delay in time until a step towards diagnosis of a disease or condition is made. Types of lag times are mainly:
The plural of diagnosis is diagnoses. The verb is to diagnose, and a person who diagnoses is called a diagnostician. The word diagnosis /daɪ.əɡˈnoʊsɪs/ is derived through Latin from the Greek word διάγνωσις (diágnōsis) from διαγιγνώσκειν (diagignṓskein), meaning "to discern, distinguish".[17]
Medical diagnosis or the actual process of making a diagnosis is a cognitive process. A clinician uses several sources of data and puts the pieces of the puzzle together to make a diagnostic impression. The initial diagnostic impression can be a broad term describing a category of diseases instead of a specific disease or condition. After the initial diagnostic impression, the clinician obtains follow up tests and procedures to get more data to support or reject the original diagnosis and will attempt to narrow it down to a more specific level. Diagnostic procedures are the specific tools that the clinicians use to narrow the diagnostic possibilities.
Diagnosis can take many forms.[18] It might be a matter of naming the disease, lesion, dysfunction or disability. It might be a management-naming or prognosis-naming exercise. It may indicate either degree of abnormality on a continuum or kind of abnormality in a classification. It’s influenced by non-medical factors such as power, ethics and financial incentives for patient or doctor. It can be a brief summation or an extensive formulation, even taking the form of a story or metaphor. It might be a means of communication such as a computer code through which it triggers payment, prescription, notification, information or advice. It might be pathogenic or salutogenic. It’s generally uncertain and provisional.
Once a diagnostic opinion has been reached, the provider is able to propose a management plan, which will include treatment as well as plans for follow-up. From this point on, in addition to treating the patient's condition, the provider can educate the patient about the etiology, progression, prognosis, other outcomes, and possible treatments of her or his ailments, as well as providing advice for maintaining health.
A treatment plan is proposed which may include therapy and follow-up consultations and tests to monitor the condition and the progress of the treatment, if needed, usually according to the medical guidelines provided by the medical field on the treatment of the particular illness.
Relevant information should be added to the medical record of the patient.
A failure to respond to treatments that would normally work may indicate a need for review of the diagnosis.
Sub-types of diagnoses include:
Medical signSymptomSyndrome
Medical diagnosisDifferential diagnosisPrognosis
AcuteChronicCure/Remission
DiseaseEponymous diseaseAcronym or abbreviation



Cytogenetics - Wikipedia
Cytogenetics is a branch of genetics that is concerned with how the chromosomes relate to cell behaviour, particularly to their behaviour during mitosis and meiosis.[1] Techniques used include karyotyping, analysis of G-banded chromosomes, other cytogenetic banding techniques, as well as molecular cytogenetics such as fluorescent in situ hybridization (FISH) and comparative genomic hybridization (CGH).
Chromosomes were first observed in plant cells by Karl Wilhelm von Nägeli in 1842. Their behavior in animal (salamander) cells was described by Walther Flemming, the discoverer of mitosis, in 1882. The name was coined by another German anatomist, von Waldeyer in 1888.
The next stage took place after the development of genetics in the early 20th century, when it was appreciated that the set of chromosomes (the karyotype) was the carrier of the genes. Levitsky seems to have been the first to define the karyotype as the phenotypic appearance of the somatic chromosomes, in contrast to their genic contents.[2][3] Investigation into the human karyotype took many years to settle the most basic question: how many chromosomes does a normal diploid human cell contain?[4] In 1912, Hans von Winiwarter reported 47 chromosomes in spermatogonia and 48 in oogonia, concluding an XX/XO sex determination mechanism.[5] Painter in 1922 was not certain whether the diploid number of man was 46 or 48, at first favoring 46.[6] He revised his opinion later from 46 to 48, and he correctly insisted on man having an XX/XY system.[7] Considering their techniques, these results were quite remarkable.
In science books, the number of human chromosomes remained at 48 for over thirty years. New techniques were needed to correct this error. Joe Hin Tjio working in Albert Levan's lab[8][9] was responsible for finding the approach:
It took until 1956 until it became generally accepted that the karyotype of man included only 46 chromosomes.[10][11][12] The great apes have 48 chromosomes. Human chromosome 2 was formed by a merger of ancestral chromosomes, reducing the number.[13]
Barbara McClintock began her career as a maize cytogeneticist. In 1931, McClintock and Harriet Creighton demonstrated that cytological recombination of marked chromosomes correlated with recombination of genetic traits (genes). McClintock, while at the Carnegie Institution, continued previous studies on the mechanisms of chromosome breakage and fusion flare in maize. She identified a particular chromosome breakage event that always occurred at the same locus on maize chromosome 9, which she named the "Ds" or "dissociation" locus.[14] McClintock continued her career in cytogenetics studying the mechanics and inheritance of broken and ring (circular) chromosomes of maize. During her cytogenetic work, McClintock discovered transposons, a find which eventually led to her Nobel Prize in 1983.
In the 1930s, Dobzhansky and his coworkers collected Drosophila pseudoobscura and D. persimilis from wild populations in California and neighboring states. Using Painter's technique[15] they studied the polytene chromosomes and discovered that the wild populations were polymorphic for chromosomal inversions. All the flies look alike whatever inversions they carry: this is an example of a cryptic polymorphism.
Evidence rapidly accumulated to show that natural selection was responsible. Using a method invented by L'Héritier and Teissier, Dobzhansky bred populations in population cages, which enabled feeding, breeding and sampling whilst preventing escape. This had the benefit of eliminating migration as a possible explanation of the results. Stocks containing inversions at a known initial frequency can be maintained in controlled conditions. It was found that the various chromosome types do not fluctuate at random, as they would if selectively neutral, but adjust to certain frequencies at which they become stabilised. By the time Dobzhansky published the third edition of his book in 1951[16] he was persuaded that the chromosome morphs were being maintained in the population by the selective advantage of the heterozygotes, as with most polymorphisms.[17][18]
The lily is a favored organism for the cytological examination of meiosis since the chromosomes are large and each morphological stage of meiosis can be easily identified microscopically. Hotta et al.[19]  presented evidence for a common pattern of DNA nicking and repair synthesis in male meiotic cells of lilies and rodents during the zygotene–pachytene stages of meiosis when crossing over was presumed to occur.  The presence of a common pattern between organisms as phylogenetically distant as lily and mouse led the authors to conclude that the organization for meiotic crossing-over in at least higher eukaryotes is probably universal in distribution.
In the event of procedures which allowed easy enumeration of chromosomes, discoveries were quickly made related to aberrant chromosomes or chromosome number. In some congenital disorders, such as Down syndrome, cytogenetics revealed the nature of the chromosomal defect: a "simple" trisomy.  Abnormalities arising from nondisjunction events can cause cells with aneuploidy (additions or deletions of entire chromosomes) in one of the parents or in the fetus. In 1959, Lejeune[20] discovered patients with Down syndrome had an extra copy of chromosome 21. Down syndrome is also referred to as trisomy 21.
Other numerical abnormalities discovered include sex chromosome abnormalities. A female with only one X chromosome has Turner syndrome, whereas an additional X chromosome in a male, resulting in 47 total chromosomes, has Klinefelter syndrome. Many other sex chromosome combinations are compatible with live birth including XXX, XYY, and XXXX. The ability for mammals to tolerate aneuploidies in the sex chromosomes arises from the ability to inactivate them, which is required in normal females to compensate for having two copies of the chromosome. Not all genes on the X chromosome are inactivated, which is why there is a phenotypic effect seen in individuals with extra X chromosomes.
Trisomy 13 was associated with Patau syndrome and trisomy 18 with Edwards syndrome.
In 1960, Peter Nowell and David Hungerford[21] discovered a small chromosome in the white blood cells of patients with Chronic myelogenous leukemia (CML). This abnormal chromosome was dubbed the Philadelphia chromosome - as both scientists were doing their research in Philadelphia, Pennsylvania. Thirteen years later, with the development of more advanced techniques, the abnormal chromosome was shown by Janet Rowley to be the result of a translocation of chromosomes 9 and 22. Identification of the Philadelphia chromosome by cytogenetics is diagnostic for CML.
In the late 1960s, Torbjörn Caspersson developed a quinicrine fluorescent staining technique (Q-banding) which revealed unique banding patterns for each chromosome pair. This allowed chromosome pairs of otherwise equal size to be differentiated by distinct horizontal banding patterns. Banding patterns are now used to elucidate the breakpoints and constituent chromosomes involved in chromosome translocations. Deletions and inversions within an individual chromosome can also be identified and described more precisely using standardized banding nomenclature. G-banding (utilizing trypsin and Giemsa/ Wright stain) was concurrently developed in the early 1970s and allows visualization of banding patterns using a bright field microscope.
Diagrams identifying the chromosomes based on the banding patterns are known as idiograms. These maps became the basis for both prenatal and oncological fields to quickly move cytogenetics into the clinical lab where karyotyping allowed scientists to look for chromosomal alterations. Techniques were expanded to allow for culture of free amniocytes recovered from amniotic fluid, and elongation techniques for all culture types that allow for higher-resolution banding.
In the 1980s, advances were made in molecular cytogenetics. While radioisotope-labeled probes had been hybridized with DNA since 1969, movement was now made in using fluorescent labeled probes. Hybridizing them to chromosomal preparations using existing techniques came to be known as fluorescence in situ hybridization (FISH).[22] This change significantly increased the usage of probing techniques as fluorescent labeled probes are safer. Further advances in micromanipulation and examination of chromosomes led to the technique of chromosome microdissection whereby aberrations in chromosomal structure could be isolated, cloned and studied in ever greater detail.
The routine chromosome analysis (Karyotyping) refers to analysis of metaphase chromosomes which have been banded using trypsin followed by Giemsa, Leishmanns, or a mixture of the two. This creates unique banding patterns on the chromosomes. The molecular mechanism and reason for these patterns is unknown, although it likely related to replication timing and chromatin packing.
Several chromosome-banding techniques are used in cytogenetics laboratories. Quinacrine banding (Q-banding) was the first staining method used to produce specific banding patterns. This method requires a fluorescence microscope and is no longer as widely used as Giemsa banding (G-banding). Reverse banding, or R-banding, requires heat treatment and reverses the usual black-and-white pattern that is seen in G-bands and Q-bands. This method is particularly helpful for staining the distal ends of chromosomes. Other staining techniques include C-banding and nucleolar organizing region stains (NOR stains). These latter methods specifically stain certain portions of the chromosome. C-banding stains the constitutive heterochromatin, which usually lies near the centromere, and NOR staining highlights the satellites and stalks of acrocentric chromosomes.
High-resolution banding involves the staining of chromosomes during prophase or early metaphase (prometaphase), before they reach maximal condensation. Because prophase and prometaphase chromosomes are more extended than metaphase chromosomes, the number of bands observable for all chromosomes increases from about 300 to 450 to as many as 800. This allows the detection of less obvious abnormalities usually not seen with conventional banding.
Cells from bone marrow, blood, amniotic fluid, cord blood, tumor, and tissues (including skin, umbilical cord, chorionic villi, liver, and many other organs) can be cultured using standard cell culture techniques in order to increase their number. A mitotic inhibitor (colchicine, colcemid) is then added to the culture. This stops cell division at mitosis which allows an increased yield of mitotic cells for analysis. The cells are then centrifuged and media and mitotic inhibitor are removed, and replaced with a hypotonic solution. This causes the white blood cells or fibroblasts to swell so that the chromosomes will spread when added to a slide as well as lyses the red blood cells. After the cells have been allowed to sit in hypotonic solution, Carnoy's fixative (3:1 methanol to glacial acetic acid) is added. This kills the cells and hardens the nuclei of the remaining white blood cells. The cells are generally fixed repeatedly to remove any debris or remaining red blood cells. The cell suspension is then dropped onto specimen slides. After aging the slides in an oven or waiting a few days they are ready for banding and analysis.
Analysis of banded chromosomes is done at a microscope by a clinical laboratory specialist in cytogenetics (CLSp(CG)). Generally 20 cells are analyzed which is enough to rule out mosaicism to an acceptable level. The results are summarized and given to a board-certified cytogeneticist for review, and to write an interpretation taking into account the patient's previous history and other clinical findings. The results are then given out reported in an International System for Human Cytogenetic Nomenclature 2009 (ISCN2009).
Fluorescent in situ hybridization (FISH) refers to using fluorescently labeled probe to hybridize to cytogenetic cell preparations.
In addition to standard preparations FISH can also be performed on:
This section refers to preparation of standard cytogenetic preparations
The slide is aged using a salt solution usually consisting of 2X SSC (salt, sodium citrate). The slides are then dehydrated in ethanol, and the probe mixture is added. The sample DNA and the probe DNA are then co-denatured using a heated plate and allowed to re-anneal for at least 4 hours. The slides are then washed to remove excess unbound probe, and counterstained with 4',6-Diamidino-2-phenylindole (DAPI) or propidium iodide.
Analysis of FISH specimens is done by fluorescence microscopy by a clinical laboratory specialist in cytogenetics. For oncology generally a large number of interphase cells are scored in order to rule out low-level residual disease, generally between 200 and 1,000 cells are counted and scored. For congenital problems usually 20 metaphase cells are scored.
Advances now focus on molecular cytogenetics including automated systems for counting the results of standard FISH preparations and techniques for virtual karyotyping, such as comparative genomic hybridization arrays, CGH and Single nucleotide polymorphism arrays.



List of distinct cell types in the adult human body - Wikipedia
There are many different types of cell in the human body.
There are nerve cells, also known as neurons, present in our human body. They are branched out. These cells make up nervous tissue.
A neuron consists of a cell body with a nucleus and cytoplasm, from which long thin hair-like parts arise.




Organ (anatomy) - Wikipedia
Organs are collections of tissues with similar functions. Plant and animal life relies on many organs that coexist in organ systems.[2]
Organs are composed of main tissue, parenchyma, and "sporadic" tissues, stroma. The main tissue is that which is unique for the specific organ, such as the myocardium, the main tissue of the heart, while sporadic tissues include the nerves, blood vessels, and connective tissues. The main tissues that make up an organ tend to have common embryologic origins, such as arising from the same germ layer. Functionally-related organs often cooperate to form whole organ systems. Organs exist in all organisms. In single-celled organisms such as bacteria, the functional analogue of an organ is known as an organelle. In plants there are three main organs.[3] A hollow organ is an internal organ that forms a hollow tube, or pouch such as the stomach, intestine, or bladder.
In the study of anatomy, the term viscus is used to refer to an internal organ, and viscera is the plural form.[4][5]
79 organs have been identified in the human body.[6]
In biology, tissue is a cellular organizational level between cells and complete organs. A tissue is an ensemble of similar cells and their extracellular matrix from the same origin that together carry out a specific function. Organs are then formed by the functional grouping together of multiple tissues.
The study of human and animal tissues is known as histology or, in connection with disease, histopathology. For plants, the discipline is called plant anatomy. The classical tools for studying tissues are the paraffin block in which tissue is embedded and then sectioned, the histological stain, and the optical microscope. In the last couple of decades, developments in electron microscopy, immunofluorescence, and the use of frozen tissue sections have enhanced the detail that can be observed in tissues. With these tools, the classical appearances of tissues can be examined in health and disease, enabling considerable refinement of medical diagnosis and prognosis.
Two or more organs working together in the execution of a specific body function form an organ system, also called a biological system or body system. The functions of organ systems often share significant overlap. For instance, the nervous and endocrine system both operate via a shared organ, the hypothalamus. For this reason, the two systems are combined and studied as the neuroendocrine system. The same is true for the musculoskeletal system because of the relationship between the muscular and skeletal systems.
Animals such as humans have a variety of organ systems. These specific systems are also widely studied in human anatomy.
The study of plant organs is referred to as plant morphology, rather than anatomy, as in animal systems. Organs of plants can be divided into vegetative and reproductive. Vegetative plant organs are roots, stems, and leaves. The reproductive organs are variable. In flowering plants, they are represented by the flower, seed and fruit. In conifers, the organ that bears the reproductive structures is called a cone. In other divisions (phyla) of plants, the reproductive organs are called strobili, in Lycopodiophyta, or simply gametophores in mosses.
The vegetative organs are essential for maintaining the life of a plant. While there can be 11 organ systems in animals, there are far fewer in plants, where some perform the vital functions, such as photosynthesis, while the reproductive organs are essential in reproduction. However, if there is asexual vegetative reproduction, the vegetative organs are those that create the new generation of plants (see clonal colony).
Many societies have a system for Organ donation, in which a living or deceased donor's organ is transplanted into a person with a failing organ. The transplantation of larger solid organs often requires immunosuppression to prevent organ rejection or graft vs host disease.
There is considerable interest throughout the world in creating laboratory-grown or artificial organs.[citation needed]
The English word "organism" is a neologism coined in the 17th century, probably formed from the verb to organize. At first the word referred to an organization or social system. The meaning of a living animal or plant is first recorded in 1842.[7] Plant organs are made from tissue built up from different types of tissue. When there are three or more organs it is called an organ system.[8]
The adjective visceral, also splanchnic, is used for anything pertaining to the internal organs. Historically, viscera of animals were examined by Roman pagan priests like the haruspices or the augurs in order to divine the future by their shape, dimensions or other factors. This practice remains an important ritual in some remote, tribal societies.
The term "visceral" is contrasted with the term "parietal", meaning "of or relating to the wall of a body part, organ or cavity"[9] The two terms are often used in describing a membrane or piece of connective tissue, referring to the opposing sides.[citation needed]
Aristotle used the word frequently in his philosophy, both to describe the organs of plants or animals (e.g. the roots of a tree, the heart or liver of an animal), and to describe more abstract "parts" of an interconnected whole (e.g. his philosophical works, taken as a whole, are referred to as the "organon").
Some alchemists (e.g. Paracelsus) adopted the Hermetic Qabalah assignment between the seven vital organs and the seven classical planets as follows:[10]
The variations in natural language definitions of what constitutes an organ, their degree of precision, and the variations in how they map to ontologies and taxonomies in information science (for example, to count how many organs exist in a typical human body) are topics explored by writer Carl Engelking of Discover magazine in 2017 as he analyzed the science journalism coverage of the evolving scientific understanding of the mesentery.[11] He explored a challenge now faced by anatomists: as human understanding of ontology generally (that is, how things are defined, and how the relationship of one thing to another is defined) meets applied ontology and ontology engineering, unification of varying views is in higher demand.[11] However, such unification always faces epistemologic frontiers, as humans can only declare computer ontologies with certainty and finality to the extent that their own cognitive taxonomy (that is, science's understanding of the universe) is certain and final. For example, the fact that the tissues of the mesentery are continuous was something that was simply not known for sure until it was demonstrated with microscopy.[12] Because humans cannot predict all future scientific discoveries, they cannot build a unified ontology that is totally certain and will never again change. However, one of the points made by an anatomist interviewed by Engelking is that, finality aside, much more could be done even now to represent existing human knowledge more clearly for computing purposes.
The organ level of organisation in animals can be first detected in flatworms and the more derived phyla. The less-advanced taxa (like Placozoa, Sponges and Radiata) do not show consolidation of their tissues into organs.
Complex animals are composed of organs and many of these organs evolved a very long time ago. For example, the liver evolved in the stem vertebrates more than 500 million years ago, while the gut and brain are even more ancient, arising in the ancestor of vertebrates, insects, and worms more than 600 million years ago.
Given the ancient origin of most vertebrate organs, researchers have looked for model systems, where organs have evolved more recently, and ideally have evolved multiple times independently. An outstanding model for this kind of research is the placenta, which has evolved more than 100 times independently in vertebrates, has evolved relatively recently in some lineages, and exists in intermediate forms in extant taxa.[13] Studies on the evolution of the placenta have identified a variety of genetic and physiological processes that contribute to the origin and evolution of organs, these include the re-purposing of existing animal tissues, the acquisition of new functional properties by these tissues, and novel interactions of distinct tissue types.[13]



Benign tumor - Wikipedia
A benign tumor is a mass of cells (tumor) that lacks the ability to invade neighboring tissue or metastasize. These do not spread into, or invade, nearby tissues; however, they can sometimes be quite large. When removed, benign tumors usually do not grow back, whereas malignant tumors sometimes do. Unlike most benign tumors elsewhere in the body, benign brain tumors can be life threatening.[1] Benign tumors generally have a slower growth rate than malignant tumors and the tumor cells are usually more differentiated (cells have normal features).[2][3][4] They are typically surrounded by an outer surface (fibrous sheath of connective tissue) or remain with the epithelium.[5] Common examples of benign tumors include moles and uterine fibroids.
Although benign tumors will not metastasize or locally invade tissues, some types may still produce negative health effects. The growth of benign tumors produces a "mass effect" that can compress tissues and may cause nerve damage, reduction of blood to an area of the body (ischaemia), tissue death (necrosis) and organ damage. The health effects of the tumor may be more prominent if the tumor is within an enclosed space such as the cranium, respiratory tract, sinus or inside bones. Tumors of endocrine tissues may overproduce certain hormones, especially when the cells are well differentiated.  Examples include thyroid adenomas and adrenocortical adenomas.[2]
Although most benign tumors are not life-threatening, many types of benign tumors have the potential to become cancerous (malignant) through a process known as tumour progression.[6] For this reason and other possible negative health effects, some benign tumors are removed by surgery.[7]
Benign neoplasms are typically but not always composed of cells which bear a strong resemblance to a normal cell type in their organ of origin. These tumors are named for the cell or tissue type from which they originate, followed by the suffix "-oma" (but not -carcinoma, -sarcoma, or -blastoma, which are generally cancers). For example, a lipoma is a common benign tumor of fat cells (lipocytes), and a chondroma is a benign tumor of cartilage-forming cells (chondrocytes). Adenomas are benign tumors of gland-forming cells, and are usually specified further by their cell or organ of origin, as in hepatic adenoma (a benign tumor of hepatocytes, or liver cells). Teratomas contain many cell types such as skin, nerve, brain and thyroid, among others, because they are derived from germ cells.[4] Hamartomas are a group of benign tumors that have relatively normal cellular differentiation but the architecture of the tissue is disorganised.[9] There are a few cancers with 'benign-sounding' names which have been retained for historical reasons, including melanoma (a cancer of pigmented skin cells, or melanocytes) and seminoma (a cancer of male reproductive cells).[10] Skin tags, vocal chord polyps and hyperplastic polyps of the colon are often referred to as benign but they are actually overgrowths of normal tissue rather than neoplasms.[4]
Benign tumors are very diverse, and may be asymptomatic or may cause specific symptoms depending on their anatomic location and tissue type. They grow outwards, producing large rounded masses, which can cause what is known as a "mass effect". This growth can cause compression of local tissues or organs, which can cause many effects such as blockage of ducts, reduced blood flow (ischaemia), tissue death (necrosis) and nerve pain or damage.[2] Some tumors also produce hormones that can lead to life-threatening situations. Insulinomas can produce large amounts of insulin leading to hypoglycemia.[11][12] Pituitary adenomas can cause elevated levels of hormones such as growth hormone and insulin-like growth factor-1, which cause acromegaly; prolactin; ACTH and cortisol, which cause Cushings disease; TSH, which causes hyperthyroidism; and FSH and LH.[13] Bowel intussusception can occur with various benign colonic tumors.[14] Cosmetic effects can be caused by tumors, especially those of the skin, possibly causing psychological effects on the person with the tumor.[15] Vascular tumors can bleed, which in some cases can be substantial, leading to anemia.[16]
PTEN hamartoma syndrome comprises four distinct hamartomatous disorders characterised by genetic mutations in the PTEN gene; Cowden syndrome, Bannayan-Riley-Ruvalcaba syndrome, Proteus syndrome and Proteus-like syndrome. Although they all have distinct clinical features, the formation of hamartomas is present in all four syndromes. PTEN is a tumor suppressor gene that is involved in cellular signalling. Absent or dysfunctional PTEN protein allows cells to over-proliferate, causing hamartomas.[17]
Cowden syndrome is an autosomal dominant genetic disorder characterised by multiple benign hamartomas (trichilemmomas and mucocutaneous papillomatous papules) as well as a predisposition for cancers of multiple organs including the breast and thyroid.[18][19] Bannayan-Riley-Ruvalcaba syndrome is a congenital disorder characterised by hamartomatous intestinal polyposis, macrocephaly, lipomatosis, hemangiomatosis and glans penis macules.[17][20] Proteus syndrome is characterised by nevi, asymmetric overgrowth of various body parts, adipose tissue dysregulation, cystadenomas, adenomas, vascular malformation.[21][22]
Familial adenomatous polyposis (FAP) is a familial cancer syndrome caused by mutations in the APC gene. In this disorder adenomatous polyps are present in the colon that invariably progress into colon cancer.[23] The APC gene is a tumor suppressor and its product is involved in many cellular processes. Inactivation of the APC gene leads to a buildup of a protein called β-catenin, which activates two transcription factors; T-cell factor (TCF) and lymphoid enhancer factor (LEF). These cause the upregulation of many genes involved in cell proliferation, differentiation, migration and apoptosis (programmed cell death), causing the growth of benign tumors.[24]
Tuberous sclerosis complex (TSC) is an autosomal dominant genetic disorder caused by mutations in the genesTSC1 and TSC2, which produce the proteins hamartin and tuberin, respectively. This disorder presents with many benign hamartomatous tumors including angiofibromas, renal angiomyolipomas, pulmonary lymphangiomyomatosis. Tuberin and hamartin inhibit the mTOR protein in normal cellular physiology and the inactivation of the TSC tumour suppressors causes an increase in mTOR activity. This leads to the activation of genes and the production of proteins that increase cell growth.[9][25][26]
Von Hippel-Lindau disease is a dominantly inherited cancer syndrome that massively increases the risk of various tumors including benign hemangioblastomas and malignant pheochromocytomas, renal cell carcinomas, pancreatic endocrine tumors and endolymphatic sac tumors. It is caused by genetic mutations in the Von Hippel–Lindau tumor suppressor gene. The VHL protein (pVHL) is involved in cellular signalling in oxygen starved (hypoxic) cells. One role of pVHL is to cause the cellular degradation of another protein, HIF1α. Dysfunctional pVHL leads to accumulation of HIF1α, which in turn activates the production of several genes involved in cell growth and blood vessel production (VEGF, PDGFβ, TGFα and erythropoietin).[27]
One of the most important factors in classifying a tumor as benign or malignant is its invasive potential. If a tumor lacks the ability to invade adjacent tissues or spread to distant sites by metastasizing then it is benign, whereas invasive or metastatic tumours are malignant.[2] For this reason, benign tumours are not classed as cancer.[3] Benign tumors will grow in a contained area usually encapsulated in a fibrous connective tissue capsule. The growth rates of benign and malignant tumors also differ; benign tumors generally grow more slowly than malignant tumors. Although benign tumors pose a lower health risk than malignant tumors, they both can be life-threatening in certain situations. There are many general characteristics which apply to either benign or malignant tumors, but sometimes one type may show characteristics of the other. For example, benign tumors are mostly well differentiated and malignant tumors are often undifferentiated. However, undifferentiated benign tumors and differentiated malignant tumors can occur.[28][29] Although benign tumors generally grow slowly, cases of fast-growing benign tumors have also been documented.[30] Some malignant tumors are mostly non-metastatic such as in the case of basal cell carcinoma.[4] CT and chest radiography can be a useful diagnostic exam in visualizing a benign tumour and differentiating it from a malignant tumor. The smaller the tumor on a radiograph the more likely it is to be benign as 80% of lung nodules less than 2 cm in diameter are benign. Most benign nodules are smoothed radiopaque densities with clear margins but these are not exclusive signs of benign tumors.[31]
Tumours are formed by carcinogenesis, a process in which cellular alterations lead to the formation of cancer. Multistage carcinogenesis involves the sequential genetic or epigenetic changes to a cell's DNA, where each step produces a more advanced tumour. It is often broken down into three stages; initiation, promotion and progression, and several mutations may occur at each stage. Initiation is where the first genetic mutation occurs in a cell. Promotion is the clonal expansion (repeated division) of this transformed cell into a visible tumour that is usually benign. Following promotion, progression may take place where more genetic mutations are acquired in a sub-population of tumor cells. Progression changes the benign tumor into a malignant tumor.[6][32] A prominent and well studied example of this phenomenon is the tubular adenoma, a common type of colon polyp which is an important precursor to colon cancer. The cells in tubular adenomas, like most tumors which frequently progress to cancer, show certain abnormalities of cell maturation and appearance collectively known as dysplasia. These cellular abnormalities are not seen in benign tumors that rarely or never turn cancerous, but are seen in other pre-cancerous tissue abnormalities which do not form discrete masses, such as pre-cancerous lesions of the uterine cervix. Some authorities[who?] prefer to refer to dysplastic tumors as "pre-malignant", and reserve the term "benign" for tumors which rarely or never give rise to cancer.[citation needed]
Some benign tumors need no treatment; others may be removed if they cause problems such as seizures, discomfort or cosmetic concerns. Surgery is usually the most effective approach and is used to treat most benign tumors. In some case other treatments may be of use. Adenomas of the rectum may be treated with sclerotherapy, a treatment in which chemicals are used to shrink blood vessels in order to cut off the blood supply.[16] Most benign tumors do not respond to chemotherapy or radiation therapy, although there are exceptions; benign intercranial tumors are sometimes treated with radiation therapy and chemotherapy under certain circumstances.[33][34] Radiation can also be used to treat hemangiomas in the rectum.[16] Benign skin tumors are usually surgically resected but other treatments such as cryotherapy, curettage, electrodesiccation, laser therapy, dermabrasion, chemical peels and topical medication are used.[35][36]



Ductal carcinoma - Wikipedia
Ductal carcinoma is a type of tumor that primarily presents in the ducts of a gland.
Types include:




Colorectal cancer - Wikipedia

Colorectal cancer (CRC), also known as bowel cancer and colon cancer, is the development of cancer from the colon or rectum (parts of the large intestine).[6] A cancer is the abnormal growth of cells that have the ability to invade or spread to other parts of the body.[10] Signs and symptoms may include blood in the stool, a change in bowel movements, weight loss, and feeling tired all the time.[1]
Most colorectal cancers are due to old age and lifestyle factors, with only a small number of cases due to underlying genetic disorders.[2][3] Some risk factors include diet, obesity, smoking, and lack of physical activity.[2][3] Dietary factors that increase the risk include red meat, processed meat, and alcohol.[2][4] Another risk factor is inflammatory bowel disease, which includes Crohn's disease and ulcerative colitis.[2] Some of the inherited genetic disorders that can cause colorectal cancer include familial adenomatous polyposis and hereditary non-polyposis colon cancer; however, these represent less than 5% of cases.[2][3] It typically starts as a benign tumor, often in the form of a polyp, which over time becomes cancerous.[2]
Bowel cancer may be diagnosed by obtaining a sample of the colon during a sigmoidoscopy or colonoscopy.[1] This is then followed by medical imaging to determine if the disease has spread.[6] Screening is effective for preventing and decreasing deaths from colorectal cancer.[5] Screening, by one of a number of methods, is recommended starting from the age of 50 to 75.[5] During colonoscopy, small polyps may be removed if found.[2] If a large polyp or tumor is found, a biopsy may be performed to check if it is cancerous. Aspirin and other non-steroidal anti-inflammatory drugs decrease the risk.[2][11] Their general use is not recommended for this purpose, however, due to side effects.[12]
Treatments used for colorectal cancer may include some combination of surgery, radiation therapy, chemotherapy and targeted therapy.[6] Cancers that are confined within the wall of the colon may be curable with surgery, while cancer that has spread widely are usually not curable, with management being directed towards improving quality of life and symptoms.[6] The five-year survival rate in the United States is around 65%.[7] The individual likelihood of survival depends on how advanced the cancer is, whether or not all the cancer can be removed with surgery and the person's overall health.[1] Globally, colorectal cancer is the third most common type of cancer, making up about 10% of all cases.[13] In 2012, there were 1.4 million new cases and 694,000 deaths from the disease.[13] It is more common in developed countries, where more than 65% of cases are found.[2] It is less common in women than men.[2]
The signs and symptoms of colorectal cancer depend on the location of the tumor in the bowel, and whether it has spread elsewhere in the body (metastasis). The classic warning signs include: worsening constipation, blood in the stool, decrease in stool caliber (thickness), loss of appetite, loss of weight, and nausea or vomiting in someone over 50 years old.[14] While rectal bleeding or anemia are high-risk features in those over the age of 50,[15] other commonly described symptoms including weight loss and change in bowel habit are typically only concerning if associated with bleeding.[15][16]
Greater than 75–95% of colorectal cancer occurs in people with little or no genetic risk.[17][18] Risk factors include older age, male sex,[18] high intake of fat, alcohol, red meat, processed meats, obesity, smoking, and a lack of physical exercise.[17][19] Approximately 10% of cases are linked to insufficient activity.[20] The risk from alcohol appears to increase at greater than one drink per day.[21] Drinking 5 glasses of water a day is linked to a decrease in the risk of colorectal cancer and adenomatous polyps.[22] Streptococcus gallolyticus is associated with colorectal cancer.[23] Some strains of Streptococcus bovis/Streptococcus equinus complex are consumed by millions of people daily and thus may be safe.[24] 25 to 80% of people with Streptococcus bovis/gallolyticus bacteremia have concomitant colorectal tumors.[25] Seroprevalence of Streptococcus bovis/gallolyticus is considered as a candidate practical marker for the early prediction of an underlying bowel lesion at high risk population.[25] It has been suggested that the presence of antibodies to Streptococcus bovis/gallolyticus antigens or the antigens themselves in the bloodstream may act as markers for the carcinogenesis in the colon.[25]
People with inflammatory bowel disease (ulcerative colitis and Crohn's disease) are at increased risk of colon cancer.[26][27] The risk increases the longer a person has the disease,[28] and the worse the severity of inflammation.[29] In these high risk groups, both prevention with aspirin and regular colonoscopies are recommended.[28] People with inflammatory bowel disease account for less than 2% of colon cancer cases yearly.[29] In those with Crohn's disease, 2% get colorectal cancer after 10 years, 8% after 20 years, and 18% after 30 years.[29] In those with ulcerative colitis, approximately 16% develop either a cancer precursor or cancer of the colon over 30 years.[29]
Those with a family history in two or more first-degree relatives (such as a parent or sibling) have a two to threefold greater risk of disease and this group accounts for about 20% of all cases. A number of genetic syndromes are also associated with higher rates of colorectal cancer. The most common of these is hereditary nonpolyposis colorectal cancer (HNPCC or Lynch syndrome) which is present in about 3% of people with colorectal cancer.[18] Other syndromes that are strongly associated with colorectal cancer include Gardner syndrome,[30] and familial adenomatous polyposis (FAP). For people with these syndromes, cancer almost always occurs and makes up 1% of the cancer cases.[31] A total proctocolectomy may be recommended for people with FAP as a preventative measure due to the high risk of malignancy. Colectomy, removal of the colon, may not suffice as a preventative measure because of the high risk of rectal cancer if the rectum remains.[32]
Most deaths due to colon cancer are associated with metastatic disease. A gene that appears to contribute to the potential for metastatic disease, metastasis associated in colon cancer 1 (MACC1), has been isolated.[33] It is a transcriptional factor that influences the expression of hepatocyte growth factor. This gene is associated with the proliferation, invasion and scattering of colon cancer cells in cell culture, and tumor growth and metastasis in mice. MACC1 may be a potential target for cancer intervention, but this possibility needs to be confirmed with clinical studies.[34]
Epigenetic factors, such as abnormal DNA methylation of tumor suppressor promoters play a role in the development of colorectal cancer.[35]
Colorectal cancer is a disease originating from the epithelial cells lining the colon or rectum of the gastrointestinal tract, most frequently as a result of mutations in the Wnt signaling pathway that increase signaling activity. The mutations can be inherited or acquired, and most probably occur in the intestinal crypt stem cell.[36][37][38] The most commonly mutated gene in all colorectal cancer is the APC gene, which produces the APC protein. The APC protein prevents the accumulation of β-catenin protein. Without APC, β-catenin accumulates to high levels and translocates (moves) into the nucleus, binds to DNA, and activates the transcription of proto-oncogenes. These genes are normally important for stem cell renewal and differentiation, but when inappropriately expressed at high levels, they can cause cancer. While APC is mutated in most colon cancers, some cancers have increased β-catenin because of mutations in β-catenin (CTNNB1) that block its own breakdown, or have mutations in other genes with function similar to APC such as AXIN1, AXIN2, TCF7L2, or NKD1.[39]
Beyond the defects in the Wnt signaling pathway, other mutations must occur for the cell to become cancerous. The p53 protein, produced by the TP53 gene, normally monitors cell division and kills cells if they have Wnt pathway defects. Eventually, a cell line acquires a mutation in the TP53 gene and transforms the tissue from a benign epithelial tumor into an invasive epithelial cell cancer. Sometimes the gene encoding p53 is not mutated, but another protective protein named BAX is mutated instead.[39]
Other proteins responsible for programmed cell death that are commonly deactivated in colorectal cancers are TGF-β and DCC (Deleted in Colorectal Cancer). TGF-β has a deactivating mutation in at least half of colorectal cancers. Sometimes TGF-β is not deactivated, but a downstream protein named SMAD is deactivated.[39] DCC commonly has a deleted segment of a chromosome in colorectal cancer.[40]
Approximately 70% of all human genes are expressed in colorectal cancer, with just over 1% of having increased expression in colorectal cancer compared to other forms of cancer.[41] Some genes are oncogenes: they are overexpressed in colorectal cancer. For example, genes encoding the proteins KRAS, RAF, and PI3K, which normally stimulate the cell to divide in response to growth factors, can acquire mutations that result in over-activation of cell proliferation. The chronological order of mutations is sometimes important. If a previous APC mutation occurred, a primary KRAS mutation often progresses to cancer rather than a self-limiting hyperplastic or borderline lesion.[42] PTEN, a tumor suppressor, normally inhibits PI3K, but can sometimes become mutated and deactivated.[39]
Comprehensive, genome-scale analysis has revealed that colorectal carcinomas can be categorized into hypermutated and non-hypermutated tumor types.[43] In addition to the oncogenic and inactivating mutations described for the genes above, non-hypermutated samples also contain mutated  CTNNB1, FAM123B, SOX9, ATM, and ARID1A. Progressing through a distinct set of genetic events, hypermutated tumors display mutated forms of ACVR2A, TGFBR2, MSH3, MSH6, SLC9A9, TCF7L2, and BRAF. The common theme among these genes, across both tumor types, is their involvement in WNT and TGF-β signaling pathways, which results in increased activity of MYC, a central player in colorectal cancer.[43]
The term "field cancerization" was first used in 1953 to describe an area or "field" of epithelium that has been preconditioned (by what were largely unknown processes at the time) to predispose it towards development of cancer.[44] Since then, the terms "field cancerization", "field carcinogenesis", "field defect", and "field effect" have been used to describe pre-malignant or pre-neoplastic tissue in which new cancers are likely to arise.[45]
Field defects are important in progression to colon cancer.[46][47][48]
However, in most cancer research, as pointed out by Rubin[49] "The vast majority of studies in cancer research has been done on well-defined tumors in vivo, or on discrete neoplastic foci in vitro. Yet there is evidence that more than 80% of the somatic mutations found in mutator phenotype human colorectal tumors occur before the onset of terminal clonal expansion."[50] Similarly, Vogelstein et al.[51] pointed out that more than half of somatic mutations identified in tumors occurred in a pre-neoplastic phase (in a field defect), during growth of apparently normal cells. Likewise, epigenetic alterations present in tumors may have occurred in pre-neoplastic field defects.
An expanded view of field effect has been termed "etiologic field effect", which encompasses not only molecular and pathologic changes in pre-neoplastic cells but also influences of exogenous environmental factors and molecular changes in the local microenvironment on neoplastic evolution from tumor initiation to death.[52]
Epigenetic alterations are much more frequent in colon cancer than genetic (mutational) alterations.  As described by Vogelstein et al.,[51] an average cancer of the colon has only 1 or 2 oncogene mutations and 1 to 5 tumor suppressor mutations (together designated “driver mutations”), with about 60 further “passenger” mutations. The oncogenes and tumor suppressor genes are well studied and are described above under Pathogenesis.
However, by comparison, epigenetic alterations in colon cancers are frequent and affect hundreds of genes. For instance, there are types of small RNAs called microRNAs that are about 22 nucleotides long. These microRNAs (or miRNAs) do not code for proteins, but they can target protein coding genes and reduce their expression. Expression of these miRNAs can be epigenetically altered. As one example, the epigenetic alteration consisting of CpG island methylation of the DNA sequence encoding miR-137 reduces its expression. This is a frequent early epigenetic event in colorectal carcinogenesis, occurring in 81% of colon cancers and in 14% of the normal appearing colonic mucosa adjacent to the cancers. The altered adjacent tissues associated with these cancers are called field defects. Silencing of miR-137 can affect expression of about 500 genes, the targets of this miRNA.[53]
Changes in the level of miR-137 expression result in changed mRNA expression of the target genes by 2 to 20-fold and corresponding, though often smaller, changes in expression of the protein products of the genes. Other microRNAs, with likely comparable numbers of target genes, are even more frequently epigenetically altered in colonic field defects and in the colon cancers that arise from them. These include miR-124a, miR-34b/c and miR-342 which are silenced by CpG island methylation of their encoding DNA sequences in primary tumors at rates of 99%, 93% and 86%, respectively, and in the adjacent normal appearing mucosa at rates of 59%, 26% and 56%, respectively.[54][55]
In addition to epigenetic alteration of expression of miRNAs, other common types of epigenetic alterations in cancers that change gene expression levels include direct hypermethylation or hypomethylation of CpG islands of protein-encoding genes and alterations in histones and chromosomal architecture that influence gene expression.[56][57] As an example, 147 hypermethylations and 27 hypomethylations of protein coding genes were frequently associated with colorectal cancers. Of the hypermethylated genes, 10 were hypermethylated in 100% of colon cancers, and many others were hypermethylated in more than 50% of colon cancers.[58] In addition, 11 hypermethylations and 96 hypomethylations of miRNAs were also associated with colorectal cancers.[58]
Recent evidence indicates that early epigenetic reductions of DNA repair enzyme expression likely lead to the genomic and epigenomic instability characteristic of cancer.[46][59][60][61]
As summarized in the articles Carcinogenesis and Neoplasm, for sporadic cancers in general, a deficiency in DNA repair is occasionally due to a mutation in a DNA repair gene, but is much more frequently due to epigenetic alterations that reduce or silence expression of DNA repair genes.
Colorectal cancer diagnosis is performed by sampling of areas of the colon suspicious for possible tumor development, typically during colonoscopy or sigmoidoscopy, depending on the location of the lesion.[18] It is confirmed by microscopical examination of a tissue sample.
Disease extent is usually determined by a CT scan of the chest, abdomen and pelvis.[18] Other potential imaging tests such as PET and MRI may be used in certain cases.[18]
Colon cancer staging is done next and is based on radiology and pathology. As for all other forms of cancer, tumor staging is based on the TNM system which considers how much the initial tumor has spread, if and where there are lymph node metastasis and if there are metastases in more distant organs, usually liver.[18]
The microscopic cellular characteristics of the tumor are reported from the analysis of tissue taken from a biopsy or surgery. A pathology report contains a description of the microscopical characteristics of the tumor tissue, including both tumor cells and how the tumor invades into healthy tissues and finally if the tumor appears to be completely removed. The most common form of colon cancer  is adenocarcinoma. Other, rarer types include lymphoma, adenosquamous and squamous cell carcinoma. Some subtypes have been found to be more aggressive.[62]
Cancers on the right side of the large intestine (ascending colon and cecum) tend to be exophytic, that is, the tumor grows outwards from one location in the bowel wall. This very rarely causes obstruction of feces, and presents with symptoms such as anemia. Left-sided tumors tend to be circumferential, and can obstruct the bowel lumen, much like a napkin ring, and results in thinner caliber stools.
Adenocarcinoma is a malignant epithelial tumor, originating from superficial glandular epithelial cells lining the colon and rectum. It invades the wall, infiltrating the muscularis mucosae layer, the submucosa, and then the muscularis propria. Tumor cells describe irregular tubular structures, harboring pluristratification, multiple lumens, reduced stroma ("back to back" aspect).
Sometimes, tumor cells are discohesive and secrete mucus, which invades the interstitium producing large pools of mucus. This occurs in mucinous adenocarcinoma, in which cells are poorly differentiated. If the mucus remains inside the tumor cell, it pushes the nucleus at the periphery, this occurs in "signet-ring cell." Depending on glandular architecture, cellular pleomorphism, and mucosecretion of the predominant pattern, adenocarcinoma may present three degrees of differentiation: well, moderately, and poorly differentiated.[63]
In cases where a metastasis from colorectal cancer is suspected, immunohistochemistry is used to ascertain correct diagnosis. Proteins that are more specifically expressed in  colorectal cancer and can be used as diagnostic markers are cytokeratin 20, CDX2, SATB2 and CDH17. Most (50%) colorectal adenomas and (80–90%) colorectal cancer tumors are thought to over express the cyclooxygenase-2 (COX-2) enzyme.[64] This enzyme is generally not found in healthy colon tissue, but is thought to fuel abnormal cell growth.
Macroscopy 
Appearance of the inside of the colon showing one invasive colorectal carcinoma (the crater-like, reddish, irregularly shaped tumor)
Gross appearance of a colectomy specimen containing two adenomatous polyps (the brownish oval tumors above the labels, attached to the normal beige lining by a stalk) and one invasive colorectal carcinoma (the crater-like, reddish, irregularly shaped tumor located above the label)
Endoscopic image of colon cancer identified in sigmoid colon on screening colonoscopy in the setting of Crohn's disease
PET/CT of a staging exam of colon carcinoma. Besides the primary tumor a lot of lesions can be seen. On cursor position: lung nodule.
Fungating carcinoma of the colon
Micrographs (H&E stain)
Cancer — Invasive adenocarcinoma (the most common type of colorectal cancer). The cancerous cells are seen in the center and at the bottom right of the image (blue). Near normal colon-lining cells are seen at the top right of the image.
Cancer — Histopathologic image of colonic carcinoid
Precancer — Tubular adenoma (left of image), a type of colonic polyp and a precursor of colorectal cancer. Normal colorectal mucosa is seen on the right.
Precancer — Colorectal villous adenoma
Staging is typically made according to the TNM staging system from the WHO organization, the UICC and the AJCC. The Astler-Coller classification (1954) and the Dukes classification (1932) are now less used.
The T stages of bowel cancer.
Dukes stage A bowel cancer; the cancer is only in the inner lining of the bowel.
Dukes stage B bowel cancer; the cancer has invaded the muscle.
Dukes stage C bowel cancer; the cancer has invaded the nearby lymph nodes.
Dukes stage D bowel cancer; the cancer has metastasized.
The most common metastasis sites for colorectal cancer are the liver, the lung and the peritoneum.[65]
Tumor budding in colorectal cancer is loosely defined by the presence of individual cells and small clusters of tumor cells at the invasive front of carcinomas. It has been postulated to represent an epithelial–mesenchymal transition (EMT). Tumor budding is a well-established independent marker of a potentially poor outcome in colorectal carcinoma that may allow for dividing people into risk categories more meaningful than those defined by TNM staging, and also potentially guide treatment decisions, especially in T1 and T3 N0 (Stage II, Dukes’ B) colorectal carcinoma. Unfortunately, its universal acceptance as a reportable factor has been held back by a lack of definitional uniformity with respect to both qualitative and quantitative aspects of tumor budding.[66]
It has been estimated that about half of colorectal cancer cases are due to lifestyle factors, and about a quarter of all cases are preventable.[67] Increasing surveillance, engaging in physical activity, consuming a diet high in fiber, and reducing smoking and alcohol consumption decrease the risk.[68][69]
Current dietary recommendations to prevent colorectal cancer include increasing the consumption of whole grains, fruits and vegetables, and reducing the intake of red meat and processed meats.[19][70] Higher physical activity is also recommended.[19][71] Physical exercise is associated with a modest reduction in colon but not rectal cancer risk.[72][73] High levels of physical activity reduce the risk of colon cancer by about 21%.[74] Sitting regularly for prolonged periods is associated with higher mortality from colon cancer. The risk is not negated by regular exercise, though it is lowered.[75] The risk of colon cancer can be reduced by maintaining a normal body weight.[76] The evidence for any protective effect conferred by fiber and fruits and vegetables is, however, poor.[19][77]
Aspirin and celecoxib appear to decrease the risk of colorectal cancer in those at high risk.[78][79] Aspirin is recommended in those who are 50 to 60 years old, do not have an increased risk of bleeding, and are at risk for cardiovascular disease to prevent colorectal cancer.[80] It is not recommended in those at average risk.[81] There is tentative evidence for calcium supplementation, but it is not sufficient to make a recommendation.[82] Vitamin D intake and blood levels are associated with a lower risk of colon cancer.[83][84]
As more than 80% of colorectal cancers arise from adenomatous polyps, screening for this cancer is effective for both early detection and for prevention.[18][85] Diagnosis of cases of colorectal cancer through screening tends to occur 2–3 years before diagnosis of cases with symptoms.[18] Any polyps that are detected can be removed, usually by colonoscopy or sigmoidoscopy, and thus prevent them from turning into cancer. Screening has the potential to reduce colorectal cancer deaths by 60%.[86]
The three main screening tests are colonoscopy, fecal occult blood testing, and flexible sigmoidoscopy.[87] Of the three, only sigmoidoscopy cannot screen the right side of the colon where 42% of cancers are found.[88] Flexible sigmoidoscopy, however, has the best evidence for decreasing the risk of death from any cause.[89]
Fecal occult blood testing (FOBT) of the stool is typically recommended every two years and can be either guaiac-based or immunochemical.[18] If abnormal FOBT results are found, participants are typically referred for a follow-up colonoscopy examination. Yearly to every two year FOBT screening reduces colorectal cancer deaths by 16% and among those participating in screening colorectal cancer deaths can be reduced up to 23%, although it has not been proven to reduce all-cause mortality.[90] Immunochemical tests are accurate and do not require dietary or medication changes before testing.[91]
Other options include virtual colonoscopy and stool DNA screening testing (FIT-DNA).[87] Virtual colonoscopy via a CT scan appears as good as standard colonoscopy for detecting cancers and large adenomas but is expensive, associated with radiation exposure, and cannot remove any detected abnormal growths like standard colonoscopy can.[18] Stool DNA screening test looks for biomarkers associated with colorectal cancer and precancerous lesions, including altered DNA and blood hemoglobin. A positive result should be followed by colonoscopy. FIT-DNA has more false positives than FIT and thus results in more adverse effects.[5] Further study is required as of 2016 to determine if a three year screening interval is correct.[5]
In the United States, screening is typically recommended between ages 50 to 75 years.[5] The American Cancer Society recommends starting at the age of 45.[92] For those between 76 and 85 years old, the decision to screen should be individualized.[5] Several screening methods can be used, including stool based tests every 3 years, sigmoidoscopy every 5 years and colonoscopy every 10 years. For those at high risk, screenings usually begin at around 40.[18][93] It is unclear which of these two methods is better.[94] Colonoscopy may find more cancers in the first part of the colon, but is associated with greater cost and more complications.[94] For people with average risk who have had a high-quality colonoscopy with normal results, the American Gastroenterological Association does not recommend any type of screening in the 10 years following the colonoscopy.[95][96] For people over 75 or those with a life expectancy of less than 10 years, screening is not recommended.[97] It takes about 10 years after screening for one out of a 1000 people to benefit.[98] The USPSTF list seven potential strategies for screening, with the most important thing being that at least one of these strategies is appropriately used.[5]
In Canada, among those 50 to 75 years old at normal risk, fecal immunochemical testing or FOBT is recommended every two years or sigmoidoscopy every 10 years.[99] Colonoscopy is less preferred.[99]
Some countries have national colorectal screening programs which offer FOBT screening for all adults within a certain age group, typically starting between ages 50 to 60. Examples of countries with organised screening include the United Kingdom,[100] Australia,[101] and the Netherlands.[102]
The treatment of colorectal cancer can be aimed at cure or palliation. The decision on which aim to adopt depends on various factors, including the person's health and preferences, as well as the stage of the tumor.[103] When colorectal cancer is caught early, surgery can be curative.  However, when it is detected at later stages (for which metastases are present), this is less likely and treatment is often directed at palliation, to relieve symptoms caused by the tumour and keep the person as comfortable as possible.[18]
If the cancer is found at a very early stage, it may be removed during a colonoscopy.[6] For people with localized cancer, the preferred treatment is complete surgical removal with adequate margins, with the attempt of achieving a cure. This can either be done by an open laparotomy or sometimes laparoscopically.[18] The colon may then be reconnected or a person may have a colostomy.[6]
If there are only a few metastases in the liver or lungs they may also be removed. Sometimes chemotherapy is used before surgery to shrink the cancer before attempting to remove it. The two most common sites of recurrence of colorectal cancer are the liver and lungs.[18]
In both cancer of the colon and rectum, chemotherapy may be used in addition to surgery in certain cases. The decision to add chemotherapy in management of colon and rectal cancer depends on the stage of the disease.
In Stage I colon cancer, no chemotherapy is offered, and surgery is the definitive treatment. The role of chemotherapy in Stage II colon cancer is debatable, and is usually not offered unless risk factors such as T4 tumor, undifferentiated tumor, vascular and perineural invasion or inadequate lymph node sampling is identified.[104] It is also known that the people who carry abnormalities of the mismatch repair genes do not benefit from chemotherapy. For stage III and Stage IV colon cancer, chemotherapy is an integral part of treatment.[18]
If cancer has spread to the lymph nodes or distant organs, which is the case with stage III and stage IV colon cancer respectively, adding chemotherapy agents fluorouracil, capecitabine or oxaliplatin increases life expectancy. If the lymph nodes do not contain cancer, the benefits of chemotherapy are controversial. If the cancer is widely metastatic or unresectable, treatment is then palliative. Typically in this setting, a number of different chemotherapy medications may be used.[18] Chemotherapy drugs for this condition may include capecitabine, fluorouracil, irinotecan, oxaliplatin and UFT.[105] The drugs capecitabine and fluorouracil are interchangeable, with capecitabine being an oral medication while fluorouracil being an intravenous medicine. Some specific regimens used for CRC are FOLFOX, FOLFOXIRI, and FOLFIRI.[106] Antiangiogenic drugs such as bevacizumab are often added in first line therapy. Another class of drugs used in the second line setting are epidermal growth factor receptor inhibitors, of which the two FDA approved ones are cetuximab and panitumumab.[107]
The primary difference in the approach to low stage rectal cancer is the incorporation of radiation therapy. Often, it is used in conjunction with chemotherapy in a neoadjuvant fashion to enable surgical resection, so that ultimately as colostomy is not required. However, it may not be possible in low lying tumors, in which case, a permanent colostomy may be required. Stage IV rectal cancer is treated similar to stage IV colon cancer.
While a combination of radiation and chemotherapy may be useful for rectal cancer,[18] its use in colon cancer is not routine due to the sensitivity of the bowels to radiation.[108] Just as for chemotherapy, radiotherapy can be used in the neoadjuvant and adjuvant setting for some stages of rectal cancer.
Immunotherapy with immune checkpoint inhibitors has been found to be useful for a type of colorectal cancer with mismatch repair deficiency and microsatellite instability.[109][110] Most people who do improve, however, still worsen after months or years.[110] Other types of colorectal cancer as of 2017 is still being studied.[109][110]
Palliative care is medical care which focuses on treatment of symptoms from serious illness, like cancer, and improving quality of life.[111]  Palliative care is recommended for any person who has advanced colon cancer or has significant symptoms.[112]
Involvement of palliative care may be beneficial to improve the quality of life for both the person and his or her family, by improving symptoms, anxiety and preventing admissions to the hospital.[113]
In people with incurable colorectal cancer, palliative care can consist of procedures that relieve symptoms or complications from the cancer but do not attempt to cure the underlying cancer, thereby improving quality of life. Surgical options may include non-curative surgical removal of some of the cancer tissue, bypassing part of the intestines, or stent placement. These procedures can be considered to improve symptoms and reduce complications such as bleeding from the tumor, abdominal pain and intestinal obstruction.[114] Non-operative methods of symptomatic treatment include radiation therapy to decrease tumor size as well as pain medications.[115]
The aims of follow-up are to diagnose, in the earliest possible stage, any metastasis or tumors that develop later, but did not originate from the original cancer (metachronous lesions).
The U.S. National Comprehensive Cancer Network and American Society of Clinical Oncology provide guidelines for the follow-up of colon cancer.[116][117] A medical history and physical examination are recommended every 3 to 6 months for 2 years, then every 6 months for 5 years. Carcinoembryonic antigen blood level measurements follow the same timing, but are only advised for people with T2 or greater lesions who are candidates for intervention. A CT-scan of the chest, abdomen and pelvis can be considered annually for the first 3 years for people who are at high risk of recurrence (for example, those who had poorly differentiated tumors or venous or lymphatic invasion) and are candidates for curative surgery (with the aim to cure). A colonoscopy can be done after 1 year, except if it could not be done during the initial staging because of an obstructing mass, in which case it should be performed after 3 to 6 months. If a villous polyp, a polyp >1 centimeter or high grade dysplasia is found, it can be repeated after 3 years, then every 5 years. For other abnormalities, the colonoscopy can be repeated after 1 year.
Routine PET or ultrasound scanning, chest X-rays, complete blood count or liver function tests are not recommended.[116][117] A 2016 systematic review concluded that more intense surveillance and close follow-up does not provide additional survival benefits in non-metastatic colorectal cancers.[118]
Exercise may be recommended in the future as secondary therapy to cancer survivors. In epidemiological studies, exercise may decrease colorectal cancer-specific mortality and all-cause mortality. Results for the specific amounts of exercise needed to observe a benefit were conflicting. These differences may reflect differences in tumour biology and expression of biomarkers. Patients with tumors that lacked CTNNB1 expression (β-catenin), involved in Wnt signalling pathway, required more than 18 Metabolic equivalent (MET) hours per week, a measure of exercise, to observe a reduction in colorectal cancer mortality. The mechanism of how exercise benefits survival may be involved in immune surveillance and inflammation pathways. In clinical studies, a pro-inflammatory response was found in patients with stage II-III colorectal cancer who underwent 2 weeks of moderate exercise after completing their primary therapy. Oxidative balance may be another possible mechanism for benefits observed. A significant decrease in 8-oxo-dG was found in the urine of patients who underwent 2 weeks of moderate exercise after primary therapy. Other possible mechanisms may involve metabolic hormone and sex-steroid hormones, although these pathways may be involved in other types of cancers[119][120]
Another potential biomarker may be p27. Survivors with tumors that expressed p27 and performed greater and equal to 18 MET hours per week were found to have reduced colorectal-cancer mortality survival compared to those with less than 18 MET hours per week. Survivors without p27 expression who exercised were shown to have worse outcomes. The constitutive activation of PI3K/AKT/mTOR pathway may explain the loss of p27 and excess energy balance may up-regulate p27 to stop cancer cells from dividing.[120]
In Europe the five-year survival rate for colorectal cancer is less than 60%. In the developed world about a third of people who get the disease die from it.[18]
Survival is directly related to detection and the type of cancer involved, but overall is poor for symptomatic cancers, as they are typically quite advanced. Survival rates for early stage detection are about five times that of late stage cancers. People with a tumor that has not breached the muscularis mucosa (TNM stage Tis, N0, M0) have a five-year survival rate of 100%, while those with invasive cancer of T1 (within the submucosal layer) or T2 (within the muscular layer) have an average five-year survival rate of approximately 90%. Those with a more invasive tumor yet without node involvement (T3-4, N0, M0) have an average five-year survival rate of approximately 70%. Patients with positive regional lymph nodes (any T, N1-3, M0) have an average five-year survival rate of approximately 40%, while those with distant metastases (any T, any N, M1) have an average five-year survival rate of approximately 5%.[121]
The average five-year recurrence rate in people where surgery is successful is 5% for stage I cancers, 12% in stage II and 33% in stage III. However, depending on the number of risk factors it ranges from 9 – 22% in stage II and 17 – 44% in stage III.[122]
According to American Cancer Society statistics in 2006,[123] over 20% of people with colorectal cancer come to medical attention when the disease is already advanced (stage IV), and up to 25% of this group will have isolated liver metastasis that is potentially resectable. In this selective group, those who undergo curative resection experience a five-year survival outcome in a third of the cases.[124]
Less than 600 genes are linked to outcomes in colorectal cancer.[41] These include both unfavorable genes, where high expression is related to poor outcome, for example the heat shock 70 kDa protein 1 (HSPA1A), and favorable genes where high expression is associated with better survival, for example the putative RNA-binding protein 3 (RBM3).[41]
Globally more than 1 million people get colorectal cancer every year[18] resulting in about 715,000 deaths as of 2010 up from 490,000 in 1990.[125]
As of  2012[update], it is the second most common cause of cancer in women (9.2% of diagnoses) and the third most common in men (10.0%)[126] with it being the fourth most common cause of cancer death after lung, stomach, and liver cancer.[127] It is more common in developed than developing countries.[128] Globally incidences vary 10-fold with highest rates in Australia, New Zealand, Europe and the US and lowest rates in Africa and South-Central Asia.[129]
Colorectal cancer is the second highest cause of cancer occurrence and death for men and women in the United States combined. An estimated 141,210 cases were diagnosed in 2011.[130]
Based on rates from 2007 to 2009, 4.96% of US men and women born today will be diagnosed with colorectal cancer during their lifetime.[131] From 2005 to 2009, the median age at diagnosis for cancer of the colon and rectum in the US was 69 years of age. Approximately 0.1% were diagnosed under age 20; 1.1% between 20 and 34; 4.0% between 35 and 44; 13.4% between 45 and 54; 20.4% between 55 and 64; 24.0% between 65 and 74; 25.0% between 75 and 84; and 12.0% 85+ years of age.  Rates are higher among males (54 per 100,000 c.f. 40 per 100,000 for females).
In the UK about 41,000 people a year get colon cancer making it the fourth most common type.[132]
One in 19 men and one in 28 women in Australia will develop colorectal cancer before the age of 75; one in 10 men and one in 15 women will develop it by 85 years of age.[133]
Rectal cancer has been diagnosed in an Ancient Egyptian mummy who had lived in the Dakhleh Oasis during the Ptolemaic period.[134]
The Biblical king Jehoram of Judah was recorded in 2 Chronicles 21 to be cursed with an incurable disease of the bowel, leading to his death, due to his supposed evil deeds. Modern scholarship indicates that his condition was most likely colon cancer.[135]
In the United States, March is colorectal cancer awareness month.[86]
Preliminary in-vitro evidence suggests lactic acid bacteria (e.g., lactobacilli, streptococci or lactococci) may be protective against the development and progression of colorectal cancer through several mechanisms such as antioxidant activity, immunomodulation, promoting programmed cell death, antiproliferative effects, and epigenetic modification of cancer cells.[136]
Large-scale genome sequencing studies have been done to identify mutations in colorectal cancer patients' genome.[137]
The bacteria clostridium novyi-NT, is also being studied.[138]



Squamous cell carcinoma - Wikipedia
Squamous cell carcinomas (SCCs),  also known as epidermoid carcinoma, comprise a number of different types of cancer that result from squamous cells.[1] These cells form the surface of the skin and lining of hollow organs in the body and line the respiratory and digestive tracts.[1]
Common types include:
Despite sharing the name "squamous cell carcinoma", the SCCs of different body sites can show differences in their presented symptoms, natural history, prognosis, and response to treatment.
Human papillomavirus infection has been associated with SCCs of the oropharynx, lung,[2] fingers,[3] and anogenital region.
Ninety percent[4]  of cases of head and neck cancer (cancer of the mouth, nasal cavity, nasopharynx, throat and associated structures) are due to SCC.
Primary squamous cell thyroid carcinoma shows an aggressive biological phenotype resulting in poor prognosis for patients.[5]
Esophageal cancer may be due to either Esophageal squamous cell carcinoma (ESCC) or adenocarcinoma (EAC). SCCs tend to occur closer to the mouth, while adenocarcinomas occur closer to the stomach. Dysphagia (difficulty swallowing, solids worse than liquids) and painful swallowing are common initial symptoms. If the disease is localized, surgical removal of the affected esophagus may offer the possibility of a cure. If the disease has spread, chemotherapy and radiotherapy are commonly used.
When associated with the lung, it is typically a centrally located large-cell cancer (nonsmall cell lung cancer). It often has a paraneoplastic syndrome causing ectopic production of parathyroid hormone-related protein, resulting in hypercalcemia, but paraneoplastic syndrome is more commonly associated with small-cell lung cancer. It is primarily due to smoking.[6]
Human papillomavirus (HPV), primarily HPV 16 and 18, are strongly implicated in the development of SCC of the penis.
Three carcinomas in situ are associated with SCCs of the penis:
When associated with the prostate, squamous cell carcinoma is very aggressive in nature. It is difficult to detect as no increase in prostate-specific antigen levels is seen, meaning that the cancer is often diagnosed at an advanced stage.
Vaginal SCC spreads slowly and usually stays near the vagina, but may spread to the lungs and liver. This is the most common type of vaginal cancer.
Most bladder cancer is transitional cell, but bladder cancer associated with schistosomiasis is often SCC.
Cancer can be considered a very large and exceptionally heterogeneous family of malignant diseases, with squamous cell carcinomas comprising one of the largest subsets.[8][9][10] All SCC lesions are thought to begin via the repeated, uncontrolled division of cancer stem cells of epithelial lineage or characteristics.[citation needed] SCCs arise from squamous cells, which are flat cells that line many areas of the body.  Accumulation of these cancer cells causes a microscopic focus of abnormal cells that are, at least initially, locally confined within the specific tissue in which the progenitor cell resided. This condition is called squamous cell carcinoma in situ, and it is diagnosed when the tumor has not yet penetrated the basement membrane or other delimiting structure to invade adjacent tissues. Once the lesion has grown and progressed to the point where it has breached, penetrated, and infiltrated adjacent structures, it is referred to as "invasive" squamous cell carcinoma. Once a carcinoma becomes invasive, it is able to spread to other organs and cause the formation of a metastasis, or "secondary tumor".
The International Classification of Diseases for Oncology (ICD-O) system lists a number of morphological subtypes and variants of malignant squamous cell neoplasms, including:[11]
Other variants of SCCs are recognized under other systems, such as keratoacanthoma.
One method of classifying squamous cell carcinomas is by their appearance under microscope. Subtypes may include:
SCC is a histologically distinct form of cancer. It arises from the uncontrolled multiplication of cells of epithelium, or cells showing particular cytological or tissue architectural characteristics of squamous cell differentiation, such as the presence of keratin, tonofilament bundles, or desmosomes, structures involved in cell-to-cell adhesion.
Adenoid squamous cell carcinoma
Basaloid squamous cell carcinoma
Clear-cell squamous cell carcinoma
Spindle-cell squamous cell carcinoma




Invasive carcinoma of no special type - Wikipedia
Invasive carcinoma of no special type (NST) also known as invasive ductal carcinoma or ductal NOS and previously known as invasive ductal carcinoma, not otherwise specified (NOS) is a group of breast cancers that do not have the "specific differentiating features".[1] Those that have these features belong to other types.[1]
In this group are: pleomorphic carcinoma, carcinoma with osteoclast-like stromal giant cells, carcinoma with choriocarcinomatous features, and carcinoma with melanotic features.[1] It is a diagnosis of exclusion, which means that for the diagnosis to be made all the other specific types must be ruled out.[1]
Invasive carcinoma of no special type (NST) is the most common form of invasive breast cancer. It accounts for 55% of breast cancer incidence upon diagnosis, according to statistics from the United States in 2004.[2] On a mammogram, it is usually visualized as a mass with fine spikes radiating from the edges. On physical examination, this lump usually feels much harder or firmer than benign breast lesions such as fibroadenoma. On microscopic examination, the cancerous cells invade and replace the surrounding normal tissues. IDC is divided in several histological subtypes.
In many cases, ductal carcinoma is asymptomatic, and detected as abnormal results on mammography. When symptoms occur, a painless, enlarging mass that does not fluctuate with the menstrual period may be felt.[3] :274–275 Pinching of the overlying skin may also be seen. Certain subtypes, such as inflammatory carcinomas, may result in a swollen, enlarged and tender breast. All variants of cancer, if there is metastatic spread, may cause enlarged lymph nodes and affect other organs.[4] :746–747
The cancer may form from the precancerous lesion called ductal carcinoma in situ.[1]
Tumors under 1 cm in diameter are unlikely to spread systemically.  Tumors are staged by size.[5]
Absence of cancer cells in the lymph nodes is a good indication that the cancer has not spread systemically.  Presence of cancer in the lymph nodes indicates the cancer may have spread.  In studies, some women have had presence of cancer in the lymph nodes, were not treated with chemotherapy, and still did not have a systemic spread.  Therefore, lymph node involvement is not a positive predictor of spread.[5]
Tumor size staging and node involvement staging can be combined into a single clinical staging number.
The appearance of cancer cells under a microscope is another predictor of systemic spread.  The more different the cancer cells look compared to normal duct cells, the greater the risk of systemic spread.  There are three characteristics that differentiate cancer cells from normal cells.
The histologic appearance of cancer cells can be scored on these three parameters on a scale from one to three.  The sum of these grades is a number between 3 and 9.  The score is called a Bloom Richardson Grade (BR) and is expressed [sum of the grades]/9.  For example, cells that were graded 2 on all three parameters would result in a BR score of 6/9.
A score of 5 and under is considered Low.  6 to 7 is considered Intermediate.  8 to 9 is considered High.[5]
Invasive ductal carcinoma of the Breast assayed with anti Mucin 1 antibody.
Breast cancer (Infiltrating ductal carcinoma of the breast) assayed with anti HER-2 (ErbB2) antibody.
Histopathology of invasive ductal carcinoma of the breast representing a scirrhous growth. Core needle biopsy. Hematoxylin and eosin stain.
Invasive ductal carcinoma of the breast. H&E stain.
Histopathology of invasive ductal carcinoma of the breast representing a scirrhous growth. Core needle biopsy. HER-2/neu oncoprotein expression by Ventana immunostaining system.
Histopathology of invasive ductal carcinoma of the breast. H&E stain.
The presence of cancer cell in small blood vessels is called vascular invasion.  The presence of vascular invasion increases the probability of systemic spread.[5]
DNA analysis indicates the amount of DNA in cancer cells and how fast the cancer is growing.
Cells with the normal amount of DNA are called diploid.  Cells with too much or too little DNA are called aneuploid.  Aneuploid cells are more likely to spread than diploid cells.
DNA testings indicates the rate of growth by determining the number of cells in the synthetic phase (S Phase).  An S Phase > 10% means a higher chance of spreading.
The results of DNA testing are considered less reliable predictors of spread than size, histology, and lymph node involvement.[5]
According to the NIH Consensus Conference[where?], if DCIS is allowed to go untreated, the natural course or natural history varies according to the grade of the DCIS. Unless treated, approximately 60 percent of low-grade DCIS lesions will have become invasive at 40 years follow-up.[6] High-grade DCIS lesions that have been inadequately resected and not given radiotherapy have a 50 percent risk of becoming invasive breast cancer within seven years.  Approximately half of low-grade DCIS detected at screening will represent overdiagnosis, but overdiagnosis of high-grade DCIS is rare. The natural history of intermediate-grade DCIS is difficult to predict. Approximately one-third of malignant calcification clusters detected at screening mammography already have an invasive focus.
The prognosis of IDC depends, in part, on its histological subtype. Mucinous, papillary, cribriform, and tubular carcinomas have longer survival, and lower recurrence rates. The prognosis of the most common form of IDC, called "IDC Not Otherwise Specified", is intermediate. Finally, some rare forms of breast cancer (e.g., sarcomatoid carcinoma, inflammatory carcinoma) have a poor prognosis. Regardless of the histological subtype, the prognosis of IDC depends also on tumor size, presence of cancer in the lymph nodes, histological grade, presence of cancer in small vessels (vascular invasion), expression of hormone receptors and of oncogenes like HER2/neu.
These parameters can be entered into models that provide a statistical probability of systemic spread.  The probability of systemic spread is a key factor in determining whether radiation and chemotherapy are worthwhile.  The individual parameters are important also because they can predict how well a cancer will respond to specific chemotherapy agents.
Overall, the 5-year survival rate of invasive ductal carcinoma was approximately 85% in 2003.[7]
Treatment of invasive carcinoma of no special type (NST) depends on the size of the mass (size of the tumor measured in its longest direction):
The treatment options offered to an individual patient are determined by the form, stage and location of the cancer, and also by the age, history of prior disease and general health of the patient. Not all patients are treated the same way.



Tobacco - Wikipedia
Tobacco is a product prepared from the leaves of the tobacco plant by curing them. The plant is part of the genus Nicotiana and of the Solanaceae (nightshade) family. While more than 70 species of tobacco are known, the chief commercial crop is N. tabacum. The more potent variant N. rustica is also used around the world.
Tobacco contains the alkaloid nicotine, which is a stimulant, and harmala alkaloids.[2] Dried tobacco leaves are mainly used for smoking in cigarettes, cigars, pipe tobacco, and flavored shisha tobacco. They can also be consumed as snuff, chewing tobacco, dipping tobacco and snus.
Tobacco use is a risk factor for many diseases, especially those affecting the heart, liver, and lungs, as well as many cancers. In 2008, the World Health Organization named tobacco as the world's single greatest preventable cause of death.[3]
The English word "tobacco" originates from the Spanish and Portuguese word "tabaco". The precise origin of this word is disputed, but it is generally thought to have derived at least in part, from Taino, the Arawakan language of the Caribbean. In Taino, it was said to mean either a roll of tobacco leaves (according to Bartolomé de las Casas, 1552) or to tabago, a kind of Y-shaped pipe used for sniffing tobacco smoke (according to Oviedo; with the leaves themselves being referred to as cohiba).[4][5]
However, perhaps coincidentally, similar words in Spanish, Portuguese and Italian were used from 1410 to define medicinal herbs believed to have originated from the Arabic طُبّاق ṭubbāq (also طُباق ṭubāq), a word reportedly dating to the 9th century, as a name for various herbs.[6][7]
Tobacco has long been used in the Americas, with some cultivation sites in Mexico dating back to 1400–1000 BC.[8] Many Native American tribes have traditionally grown and used tobacco. Eastern North American tribes historically carried tobacco in pouches as a readily accepted trade item, as well as smoking it, both socially and ceremonially, such as to seal a peace treaty or trade agreement.[9][10] In some populations, tobacco is seen as a gift from the Creator, with the ceremonial tobacco smoke carrying one's thoughts and prayers to the Creator.[11]
Following the arrival of the Europeans to the Americas, tobacco became increasingly popular as a trade item. Hernández de Boncalo, Spanish chronicler of the Indies, was the first European to bring tobacco seeds to the Old World in 1559 following orders of King Philip II of Spain. These seeds were planted in the outskirts of Toledo, more specifically in an area known as "Los Cigarrales" named after the continuous plagues of cicadas (cigarras in Spanish). Before the development of the lighter Virginia and white burley strains of tobacco, the smoke was too harsh to be inhaled. Small quantities were smoked at a time, using a pipe like the midwakh or kiseru or smoking newly invented waterpipes such as the bong or the hookah (see thuốc lào for a modern continuance of this practice). Tobacco became so popular that the English colony of Jamestown used it as currency and began exporting it as a cash crop; tobacco is often credited as being the export that saved Virginia from ruin.[12]
The alleged benefits of tobacco also account for its considerable success. The astronomer Thomas Harriot, who accompanied Sir Richard Grenville on his 1585 expedition to Roanoke Island, explains that the plant "openeth all the pores and passages of the body" so that the natives’ "bodies are notably preserved in health, and know not many grievous diseases, wherewithal we in England are often times afflicted." [13]
Tobacco smoking, chewing, and snuffing became a major industry in Europe and its colonies by 1700.[14][15]
Tobacco has been a major cash crop in Cuba and in other parts of the Caribbean since the 18th century. Cuban cigars are world-famous.[16]
In the late 19th century, cigarettes became popular. James Bonsack created a machine that automated cigarette production. This increase in production allowed tremendous growth in the tobacco industry until the health revelations of the late-20th century.[17][18]
Following the scientific revelations of the mid-20th century, tobacco became condemned as a health hazard, and eventually became encompassed as a cause for cancer, as well as other respiratory and circulatory diseases. In the United States, this led to the Tobacco Master Settlement Agreement, which settled the lawsuit in exchange for a combination of yearly payments to the states and voluntary restrictions on advertising and marketing of tobacco products.
In the 1970s, Brown & Williamson cross-bred a strain of tobacco to produce Y1. This strain of tobacco contained an unusually high amount of nicotine, nearly doubling its content from 3.2-3.5% to 6.5%. In the 1990s, this prompted the Food and Drug Administration to use this strain as evidence that tobacco companies were intentionally manipulating the nicotine content of cigarettes.[citation needed]
In 2003, in response to growth of tobacco use in developing countries, the World Health Organization[19] successfully rallied 168 countries to sign the Framework Convention on Tobacco Control. The convention is designed to push for effective legislation and its enforcement in all countries to reduce the harmful effects of tobacco. This led to the development of tobacco cessation products.
Many species of tobacco are in the genus of herbs Nicotiana. It is part of the nightshade family (Solanaceae) indigenous to North and South America, Australia, south west Africa, and the South Pacific.[20]
Most nightshades contain varying amounts of nicotine, a powerful neurotoxin to insects. However, tobaccos tend to contain a much higher concentration of nicotine than the others. Unlike many other Solanaceae species, they do not contain tropane alkaloids, which are often poisonous to humans and other animals.
Despite containing enough nicotine and other compounds such as germacrene and anabasine and other piperidine alkaloids (varying between species) to deter most herbivores,[21] a number of such animals have evolved the ability to feed on Nicotiana species without being harmed. Nonetheless, tobacco is unpalatable to many species due to its other attributes. For example, although the cabbage looper is a generalist pest, tobacco's gummosis and trichomes can harm early larvae survival.[22] As a result, some tobacco plants (chiefly N. glauca) have become established as invasive weeds in some places.
The types of tobacco include:
Tobacco is cultivated similarly to other agricultural products. Seeds were at first quickly scattered onto the soil. However, young plants came under increasing attack from flea beetles (Epitrix cucumeris or E. pubescens), which caused destruction of half the tobacco crops in United States in 1876. By 1890, successful experiments were conducted that placed the plant in a frame covered by thin cotton fabric. Today, tobacco is sown in cold frames or hotbeds, as their germination is activated by light.[citation needed]
In the United States, tobacco is often fertilized with the mineral apatite, which partially starves the plant of nitrogen, to produce a more desired flavor.
After the plants are about 8 inches (20 cm)  tall, they are transplanted into the fields. Farmers used to have to wait for rainy weather to plant. A hole is created in the tilled earth with a tobacco peg, either a curved wooden tool or deer antler. After making two holes to the right and left, the planter would move forward two feet, select plants from his/her bag, and repeat. Various mechanical tobacco planters like Bemis, New Idea Setter, and New Holland Transplanter were invented in the late 19th and 20th centuries to automate the process: making the hole, watering it, guiding the plant in — all in one motion.[24]
Tobacco is cultivated annually, and can be harvested in several ways. In the oldest method still used today, the entire plant is harvested at once by cutting off the stalk at the ground with a tobacco knife. It is then speared onto sticks, four to six plants a stick and hung in a curing barn. In the 19th century, bright tobacco began to be harvested by pulling individual leaves off the stalk as they ripened. The leaves ripen from the ground upwards, so a field of tobacco harvested in this manner involves the serial harvest of a number of "primings", beginning with the volado leaves near the ground, working to the seco leaves in the middle of the plant, and finishing with the potent ligero leaves at the top. Before this, the crop must be topped when the pink flowers develop. Topping always refers to the removal of the tobacco flower before the leaves are systematically removed, and eventually, entirely harvested. As the industrial revolution took hold, harvesting wagons used to transport leaves were equipped with man-powered stringers, an apparatus that used twine to attach leaves to a pole. In modern times, large fields are harvested mechanically, although topping the flower and in some cases the plucking of immature leaves is still done by hand. Most tobacco in the U.S. is grown in North Carolina, Kentucky, and Virginia.[25]
Curing and subsequent aging allow for the slow oxidation and degradation of carotenoids in tobacco leaf. This produces certain compounds in the tobacco leaves, and gives a sweet hay, tea, rose oil, or fruity aromatic flavor that contributes to the "smoothness" of the smoke. Starch is converted to sugar, which glycates protein, and is oxidized into advanced glycation endproducts (AGEs), a caramelization process that also adds flavor. Inhalation of these AGEs in tobacco smoke contributes to atherosclerosis and cancer.[26] Levels of AGEs are dependent on the curing method used.
Tobacco can be cured through several methods, including:
Some tobaccos go through a second stage of curing, known as fermenting or sweating.[28] Cavendish undergoes fermentation pressed in a casing solution containing sugar and/or flavoring.[29]
Production of tobacco leaf increased by 40% between 1971, when 4.2 million tons of leaf were produced, and 1997, when 5.9 million tons of leaf were produced.[30] According to the Food and Agriculture organization of the UN, tobacco leaf production was expected to hit 7.1 million tons by 2010. This number is a bit lower than the record-high production of 1992, when 7.5 million tons of leaf were produced.[31] The production growth was almost entirely due to increased productivity by developing nations, where production increased by 128%.[32] During that same time, production in developed countries actually decreased.[31] China's increase in tobacco production was the single biggest factor in the increase in world production. China's share of the world market increased from 17% in 1971 to 47% in 1997.[30] This growth can be partially explained by the existence of a high import tariff on foreign tobacco entering China. While this tariff has been reduced from 64% in 1999 to 10% in 2004,[33] it still has led to local, Chinese cigarettes being preferred over foreign cigarettes because of their lower cost.
Every year, about 6.7 million tons of tobacco are produced throughout the world. The top producers of tobacco are China (39.6%), India (8.3%), Brazil (7.0%) and the United States (4.6%).[35]
Around the peak of global tobacco production, 20 million rural Chinese households were producing tobacco on 2.1 million hectares of land.[36] While it is the major crop for millions of Chinese farmers, growing tobacco is not as profitable as cotton or sugarcane, because the Chinese government sets the market price. While this price is guaranteed, it is lower than the natural market price, because of the lack of market risk. To further control tobacco in their borders, China founded a State Tobacco Monopoly Administration (STMA) in 1982. The STMA controls tobacco production, marketing, imports, and exports, and contributes 12% to the nation's national income.[37] As noted above, despite the income generated for the state by profits from state-owned tobacco companies and the taxes paid by companies and retailers, China's government has acted to reduce tobacco use.[38]
India's Tobacco Board is headquartered in Guntur in the state of Andhra Pradesh.[39] India has 96,865 registered tobacco farmers[40] and many more who are not registered. In 2010, 3,120 tobacco product manufacturing facilities were operating in all of India.[41] Around 0.25% of India's cultivated land is used for tobacco production.[42]
Since 1947, the Indian government has supported growth in the tobacco industry. India has seven tobacco research centers, located in Tamil Nadu, Andhra Pradesh, Punjab, Bihar, Mysore, and West Bengal houses the core research institute.
In Brazil, around 135,000 family farmers cite tobacco production as their main economic activity.[36] Tobacco has never exceeded 0.7% of the country's total cultivated area.[43] In the southern regions of Brazil, Virginia, and Amarelinho, flue-cured tobacco, as well as burley and Galpão Comum air-cured tobacco, are produced. These types of tobacco are used for cigarettes. In the northeast, darker, air- and sun-cured tobacco is grown. These types of tobacco are used for cigars, twists, and dark cigarettes.[43]
Brazil's government has made attempts to reduce the production of tobacco, but has not had a successful systematic antitobacco farming initiative. Brazil's government, however, provides small loans for family farms, including those that grow tobacco, through the Programa Nacional de Fortalecimento da Agricultura Familiar.[44]
The International Labour Office reported that the most child-laborers work in agriculture, which is one of the most hazardous types of work.[45][not in citation given (See discussion.)] The tobacco industry houses some of these working children. Use of children is widespread on farms in Argentina, Brazil, China, India, Indonesia, Malawi, and Zimbabwe.[46] While some of these children work with their families on small, family-owned farms, others work on large plantations.
In late 2009, reports were released by the London-based human-rights group Plan International, claiming that child labor was common on Malawi (producer of 1.8% of the world's tobacco[30]) tobacco farms. The organization interviewed 44 teens, who worked full-time on farms during the 2007-8 growing season. The child-laborers complained of low pay and long hours, as well as physical and sexual abuse by their supervisors.[47] They also reported suffering from Green tobacco sickness, a form of nicotine poisoning. When wet leaves are handled, nicotine from the leaves gets absorbed in the skin and causes nausea, vomiting, and dizziness. Children were exposed to 50-cigarettes-worth of nicotine through direct contact with tobacco leaves. This level of nicotine in children can permanently alter brain structure and function.[45][not in citation given (See discussion.)]
Major tobacco companies have encouraged global tobacco production. Philip Morris, British American Tobacco, and Japan Tobacco each own or lease tobacco-manufacturing facilities in at least 50 countries and buy crude tobacco leaf from at least 12 more countries.[48] This encouragement, along with government subsidies, has led to a glut in the tobacco market. This surplus has resulted in lower prices, which are devastating to small-scale tobacco farmers. According to the World Bank, between 1985 and 2000, the inflation-adjusted price of tobacco dropped 37%.[49] Tobacco is the most widely smuggled legal product.[50]
Tobacco production requires the use of large amounts of pesticides. Tobacco companies recommend up to 16 separate applications of pesticides just in the period between planting the seeds in greenhouses and transplanting the young plants to the field.[51] Pesticide use has been worsened by the desire to produce larger crops in less time because of the decreasing market value of tobacco. Pesticides often harm tobacco farmers because they are unaware of the health effects and the proper safety protocol for working with pesticides. These pesticides, as well as fertilizers, end up in the soil, waterways, and the food chain.[52] Coupled with child labor, pesticides pose an even greater threat. Early exposure to pesticides may increase a child's lifelong cancer risk, as well as harm his or her nervous and immune systems.[53]
Tobacco crops extract nutrients (such as phosphorus, nitrogen, and potassium) from soil, decreasing its fertility.[54]
Furthermore, the wood used to cure tobacco in some places leads to deforestation. While some big tobacco producers such as China and the United States have access to petroleum, coal, and natural gas, which can be used as alternatives to wood, most developing countries still rely on wood in the curing process.[54] Brazil alone uses the wood of 60 million trees per year for curing, packaging, and rolling cigarettes.[51]
In 2017 WHO released a study on the environmental effects of tobacco.[55]
Several tobacco plants have been used as model organisms in genetics. Tobacco BY-2 cells, derived from N. tabacum cultivar 'Bright Yellow-2', are among the most important research tools in plant cytology.[56] Tobacco has played a pioneering role in callus culture research and the elucidation of the mechanism by which kinetin works, laying the groundwork for modern agricultural biotechnology. The first genetically modified plant was produced in 1982, using Agrobacterium tumefaciens to create an antibiotic-resistant tobacco plant.[57] This research laid the groundwork for all genetically modified crops.[58]
Because of its importance as a research tool, transgenic tobacco was the first GM crop to be tested in field trials, in the United States and France in 1986; China became the first country in the world to approve commercial planting of a GM crop in 1993, which was tobacco.[59]
Many varieties of transgenic tobacco have been intensively tested in field trials. Agronomic traits such as resistance to pathogens (viruses, particularly to the tobacco mosaic virus (TMV); fungi; bacteria and nematodes); weed management via herbicide tolerance; resistance against insect pests; resistance to drought and cold; and production of useful products such as pharmaceuticals; and use of GM plants for bioremediation, have all been tested in over 400 field trials using tobacco.[60]
Currently, only the US is producing GM tobacco.[59][60] The Chinese virus-resistant tobacco was withdrawn from the market in China in 1997.[61]:3 In the US, cigarettes made with GM tobacco with reduced nicotine content are available under the market name Quest.[60]
Tobacco is consumed in many forms and through a number of different methods. Some examples are:
Smoking in public was, for a long time, reserved for men, and when done by women was sometimes associated with promiscuity; in Japan, during the Edo period, prostitutes and their clients often approached one another under the guise of offering a smoke. The same was true in 19th-century Europe.[65]
Following the American Civil War, the use of tobacco, primarily in cigars, became associated with masculinity and power. Today, tobacco use is often stigmatized; this has spawned quitting associations and antismoking campaigns.[66][67] Bhutan is the only country in the world where tobacco sales are illegal.[68] Due to its propensity for causing detumescence and erectile dysfunction, some studies have described tobacco as an anaphrodisiacal substance.[69]
Research on tobacco use is limited mainly to smoking, which has been studied more extensively than any other form of consumption. An estimated 1.1 billion people, and up to one-third of the adult population, use tobacco in some form.[70] Smoking is more prevalent among men[71] (however, the gender gap declines with age),[72][73] the poor, and in transitional or developing countries.[74]
Rates of smoking continue to rise in developing countries, but have leveled off or declined in developed countries.[75] Smoking rates in the United States have dropped by half from 1965 to 2006, falling from 42% to 20.8% in adults.[76] In the developing world, tobacco consumption is rising by 3.4% per year.[77]
Tobacco smoking poses a risk to health due to the inhalation of poisonous chemicals in tobacco smoke such as carbon monoxide, cyanide, and carcinogens which have been proven to cause heart and lung diseases and Cancer.
According to the World Health Organization (WHO), tobacco is the single greatest cause of preventable death globally.[78] The WHO estimates that tobacco caused 5.4 million deaths in 2004[79] and 100 million deaths over the course of the 20th century.[80] Similarly, the United States Centers for Disease Control and Prevention describe tobacco use as "the single most important preventable risk to human health in developed countries and an important cause of premature death worldwide."[81]
The harms caused by inhalation of poisonous chemicals such as carbon monoxide in tobacco smoke include diseases affecting the heart and lungs, with smoking being a major risk factor for heart attacks, strokes, chronic obstructive pulmonary disease (emphysema), and cancer (particularly lung cancer, cancers of the larynx and mouth, and pancreatic cancers). Cancer is caused by inhaling carcinogenic substances present in tobacco smoke.
Inhaling secondhand tobacco smoke can cause lung cancer in nonsmoking adults. In the United States, about 3,000 adults die each year due to lung cancer from secondhand smoke exposure. Heart disease caused by secondhand smoke kills around 46,000 nonsmokers every year.[82]
The addictive alkaloid nicotine is a stimulant, and popularly known as the most characteristic constituent of tobacco. Nicotine is known to produce conditioned place preference, a sign of enforcement value.[83] Nicotine scores almost as highly as opioids on drug effect questionnaire liking scales, which are a rough indicator of addictive potential.[84] Users may develop tolerance and dependence.[85][86] Thousands of different substances in cigarette smoke, including polycyclic aromatic hydrocarbons (such as benzopyrene), formaldehyde, cadmium, nickel, arsenic, tobacco-specific nitrosamines, and phenols contribute to the harmful effects of smoking.[87] Tobacco's overall harm to user and self score as determined by a multi-criteria decision analysis was determined at 3 percent below cocaine, and 13 percent above amphetamines, ranking 6th most harmful of the 20 drugs assessed.[88]
Polonium 210 is a natural contaminant of tobacco, providing additional evidence for the link between smoking and bronchial cancer.[89] It is also extremely toxic, with one microgram being enough to kill the average adult (250,000 times more toxic than hydrogen cyanide by weight).[90]
Tobacco has a significant economic impact. The global tobacco market has been approximated to be US$760 billion (excluding China).[91] Statistica estimates that in the U.S. alone the tobacco industry has a market of US$121 billion[92] despite the fact the CDC reports that US smoking rates are declining steadily.[93] In the US, the decline in the number of smokers, the end of the Tobacco Transition Payment Program in 2014, and competition from growers in other countries, made tobacco farming economics more challenging.[94]
"Much of the disease burden and premature mortality attributable to tobacco use disproportionately affect the poor", and of the 1.22 billion smokers, 1 billion of them live in developing or transitional economies.[74]
Smoking of tobacco is practised worldwide by over one billion people. However, while smoking prevalence has declined in many developed countries, it remains high in others and is increasing among women and in developing countries. Between one-fifth and two-thirds of men in most populations smoke. Women's smoking rates vary more widely but rarely equal male rates.[95]
In Indonesia, the lowest income group spends 15% of its total expenditures on tobacco. In Egypt, more than 10% of households' expenditure in low-income homes is on tobacco. The poorest 20% of households in Mexico spend 11% of their income on tobacco.[96]
Tobacco advertising of tobacco products by the tobacco industry is through a variety of media, including sponsorship, particularly of sporting events. It is now one of the most highly regulated forms of marketing. Some or all forms of tobacco advertising are banned in many countries.



Vegetable - Wikipedia

Vegetables are parts of plants that are consumed by humans as food as part of a meal.  The original meaning is still commonly used and is applied to plants collectively to refer to all edible plant matter, including the flowers, fruits, stems, leaves, roots, and seeds. The alternate definition of the term vegetable is applied somewhat arbitrarily, often by culinary and cultural tradition. It may exclude foods derived from some plants that are fruits, nuts, and cereal grains, but include fruits from others such as tomatoes and courgettes and seeds such as pulses.
Originally, vegetables were collected from the wild by hunter-gatherers and entered cultivation in several parts of the world, probably during the period 10,000 BC to 7,000 BC, when a new agricultural way of life developed. At first, plants which grew locally would have been cultivated, but as time went on, trade brought exotic crops from elsewhere to add to domestic types. Nowadays, most vegetables are grown all over the world as climate permits, and crops may be cultivated in protected environments in less suitable locations. China is the largest producer of vegetables and global trade in agricultural products allows consumers to purchase vegetables grown in faraway countries. The scale of production varies from subsistence farmers supplying the needs of their family for food, to agribusinesses with vast acreages of single-product crops. Depending on the type of vegetable concerned, harvesting the crop is followed by grading, storing, processing, and marketing.
Vegetables can be eaten either raw or cooked and play an important role in human nutrition, being mostly low in fat and carbohydrates, but high in vitamins, minerals and dietary fiber. Many nutritionists encourage people to consume plenty of fruit and vegetables, five or more portions a day often being recommended.
The word vegetable was first recorded in English in the early 15th century. It comes from Old French,[1] and was originally applied to all plants; the word is still used in this sense in biological contexts.[2] It derives from Medieval Latin vegetabilis "growing, flourishing" (i.e. of a plant), a semantic change from a Late Latin meaning "to be enlivening, quickening".[1]
The meaning of "vegetable" as a "plant grown for food" was not established until the 18th century.[3] In 1767, the word was specifically used to mean a "plant cultivated for food, an edible herb or root". The year 1955 saw the first use of the shortened, slang term "veggie".[4]
As an adjective, the word vegetable is used in scientific and technical contexts with a different and much broader meaning, namely of "related to plants" in general, edible or not—as in vegetable matter, vegetable kingdom, vegetable origin, etc.[2]
The exact definition of "vegetable" may vary simply because of the many parts of a plant consumed as food worldwide—roots, stems, leaves, flowers, fruits, and seeds. The broadest definition is the word's use adjectivally to mean "matter of plant origin". More specifically, a vegetable may be defined as "any plant, part of which is used for food",[5] a secondary meaning then being "the edible part of such a plant".[5] A more precise definition is "any plant part consumed for food that is not a fruit or seed, but including mature fruits that are eaten as part of a main meal".[6][7] Falling outside these definitions are edible fungi (such as edible mushrooms)  and edible seaweed which, although not parts of plants, are often treated as vegetables.[8]
In the latter-mentioned definition of "vegetable", which is used in everyday language, the words "fruit" and "vegetable" are mutually exclusive. "Fruit" has a precise botanical meaning, being a part that developed from the ovary of a flowering plant. This is considerably different from the word's culinary meaning. While peaches, plums, and oranges are "fruit" in both senses, many items commonly called "vegetables", such as eggplants, bell peppers, and tomatoes, are botanically fruits. The question of whether the tomato is a fruit or a vegetable found its way into the United States Supreme Court in 1893. The court ruled unanimously in Nix v. Hedden that a tomato is correctly identified as, and thus taxed as, a vegetable, for the purposes of the Tariff of 1883 on imported produce. The court did acknowledge, however, that, botanically speaking, a tomato is a fruit.[9]
Before the advent of agriculture, humans were hunter-gatherers. They foraged for edible fruit, nuts, stems, leaves, corms, and tubers, scavenged for dead animals and hunted living ones for food.[10] Forest gardening in a tropical jungle clearing is thought to be the first example of agriculture; useful plant species were identified and encouraged to grow while undesirable species were removed. Plant breeding through the selection of strains with desirable traits such as large fruit and vigorous growth soon followed.[11] While the first evidence for the domestication of grasses such as wheat and barley has been found in the Fertile Crescent in the Middle East, it is likely that various peoples around the world started growing crops in the period 10,000 BC to 7,000 BC.[12] Subsistence agriculture continues to this day, with many rural farmers in Africa, Asia, South America, and elsewhere using their plots of land to produce enough food for their families, while any surplus produce is used for exchange for other goods.[13]
Throughout recorded history, the rich have been able to afford a varied diet including meat, vegetables and fruit, but for poor people, meat was a luxury and the food they ate was very dull, typically comprising mainly some staple product made from rice, rye, barley, wheat, millet or maize. The addition of vegetable matter provided some variety to the diet. The staple diet of the Aztecs in Central America was maize and they cultivated tomatoes, avocados, beans, peppers, pumpkins, squashes, peanuts, and amaranth seeds to supplement their tortillas and porridge.  In Peru, the Incas subsisted on maize in the lowlands and potatoes at higher altitudes. They also used seeds from quinoa, supplementing their diet with peppers, tomatoes, and avocados.[14]
In Ancient China, rice was the staple crop in the south and wheat in the north, the latter made into dumplings, noodles, and pancakes. Vegetables used to accompany these included yams, soybeans, broad beans, turnips, spring onions, and garlic. The diet of the ancient Egyptians was based on bread, often contaminated with sand which wore away their teeth. Meat was a luxury but fish was more plentiful. These were accompanied by a range of vegetables including marrows, broad beans, lentils, onions, leeks, garlic, radishes, and lettuces.[14]
The mainstay of the Ancient Greek diet was bread, and this was accompanied by goat's cheese, olives, figs, fish, and occasionally meat. The vegetables grown included onions, garlic, cabbages, melons, and lentils.[15] In Ancient Rome, a thick porridge was made of emmer wheat or beans, accompanied by green vegetables but little meat, and fish was not esteemed. The Romans grew broad beans, peas, onions and turnips and ate the leaves of beets rather than their roots.[16]
Vegetables play an important role in human nutrition. Most are low in fat and calories but are bulky and filling.[18] They supply dietary fiber and are important sources of essential vitamins, minerals, and trace elements. Particularly important are the antioxidant vitamins A, C, and E. When vegetables are included in the diet, there is found to be a reduction in the incidence of cancer, stroke, cardiovascular disease, and other chronic ailments.[19][20][21] Research has shown that, compared with individuals who eat less than three servings of fruits and vegetables each day, those that eat more than five servings have an approximately twenty percent lower risk of developing coronary heart disease or stroke.[22]
The nutritional content of vegetables varies considerably; some contain useful amounts of protein though generally they contain little fat,[23] and varying proportions of vitamins such as vitamin A, vitamin K, and vitamin B6; provitamins; dietary minerals; and carbohydrates.
However, vegetables often also contain toxins and antinutrients which interfere with the absorption of nutrients. These include α-solanine, α-chaconine,[24] enzyme inhibitors (of cholinesterase, protease, amylase, etc.), cyanide and cyanide precursors, oxalic acid, tannins and others.[citation needed] These toxins are natural defenses, used to ward off the insects, predators and fungi that might attack the plant. Some beans contain phytohaemagglutinin, and cassava roots contain cyanogenic glycoside as do bamboo shoots. These toxins can be deactivated by adequate cooking. Green potatoes contain glycoalkaloids and should be avoided.[25]
Fruit and vegetables, particularly leafy vegetables, have been implicated in nearly half the gastrointestinal infections caused by norovirus in the United States. These foods are commonly eaten raw and may become contaminated during their preparation by an infected food handler. Hygiene is important when handling foods to be eaten raw, and such products need to be properly cleaned, handled, and stored to limit contamination.[26]
The USDA Dietary Guidelines for Americans recommends consuming five to nine servings of fruit and vegetables daily.[27] The total amount consumed will vary according to age and gender, and is determined based upon the standard portion sizes typically consumed, as well as general nutritional content. Potatoes are not included in the count as they are mainly providers of starch. For most vegetables and vegetable juices, one serving is half of a cup and can be eaten raw or cooked. For leafy greens, such as lettuce and spinach, a single serving is typically a full cup.[28] A variety of products should be chosen as no single fruit or vegetable provides all the nutrients needed for health.[22]
International dietary guidelines are similar to the ones established by the USDA. Japan, for example, recommends the consumption of five to six servings of vegetables daily.[29] French recommendations provide similar guidelines and set the daily goal at five servings.[30] In India, the daily recommendation for adults is 275 grams (9.7 oz) of vegetables per day.[19]
Vegetables have been part of the human diet from time immemorial. Some are staple foods but most are accessory foodstuffs, adding variety to meals with their unique flavors and at the same time, adding nutrients necessary for health. Some vegetables are perennials but most are annuals and biennials, usually harvested within a year of sowing or planting. Whatever system is used for growing crops, cultivation follows a similar pattern; preparation of the soil by loosening it, removing or burying weeds, and adding organic manures or fertilisers; sowing seeds or planting young plants; tending the crop while it grows to reduce weed competition, control pests, and provide sufficient water; harvesting the crop when it is ready; sorting, storing, and marketing the crop or eating it fresh from the ground.[31]
Different soil types suit different crops, but in general in temperate climates, sandy soils dry out fast but warm up quickly in the spring and are suitable for early crops, while heavy clays retain moisture better and are more suitable for late season crops. The growing season can be lengthened by the use of fleece, cloches, plastic mulch, polytunnels, and greenhouses.[31] In hotter regions, the production of vegetables is constrained by the climate, especially the pattern of rainfall, while in temperate zones, it is constrained by the temperature and day length.[32]
On a domestic scale, the spade, fork, and hoe are the tools of choice while on commercial farms a range of mechanical equipment is available. Besides tractors, these include ploughs, harrows, drills, transplanters, cultivators, irrigation equipment, and harvesters. New techniques are changing the cultivation procedures involved in growing vegetables with computer monitoring systems, GPS locators, and self-steer programs for driverless machines giving economic benefits.[32]
When a vegetable is harvested, it is cut off from its source of water and nourishment. It continues to transpire and loses moisture as it does so, a process most noticeable in the wilting of green leafy crops. Harvesting root vegetables when they are fully mature improves their storage life, but alternatively, these root crops can be left in the ground and harvested over an extended period. The harvesting process should seek to minimise damage and bruising to the crop. Onions and garlic can be dried for a few days in the field and root crops such as potatoes benefit from a short maturation period in warm, moist surroundings, during which time wounds heal and the skin thickens up and hardens. Before marketing or storage, grading needs to be done to remove damaged goods and select produce according to its quality, size, ripeness, and color.[33]
All vegetables benefit from proper post harvest care. A large proportion of vegetables and perishable foods are lost after harvest during the storage period. These losses may be as high as thirty to fifty percent in developing countries where adequate cold storage facilities are not available. The main causes of loss include spoilage caused by moisture, moulds, micro-organisms, and vermin.[34]
Storage can be short-term or long-term. Most vegetables are perishable and short-term storage for a few days provides flexibility in marketing. During storage, leafy vegetables lose moisture, and the vitamin C in them degrades rapidly. A few products such as potatoes and onions have better keeping qualities and can be sold when higher prices may be available, and by extending the marketing season, a greater total volume of crop can be sold. If refrigerated storage is not available, the priority for most crops is to store high-quality produce, to maintain a high humidity level, and to keep the produce in the shade.[33]
Proper post-harvest storage aimed at extending and ensuring shelf life is best effected by efficient cold chain application.[35] Cold storage is particularly useful for vegetables such as cauliflower, eggplant, lettuce, radish, spinach, potatoes, and tomatoes, the optimum temperature depending on the type of produce. There are temperature-controlling technologies that do not require the use of electricity such as evaporative cooling.[6] Storage of fruit and vegetables in controlled atmospheres with high levels of carbon dioxide or high oxygen levels can inhibit microbial growth and extend storage life.[36]
The irradiation of vegetables and other agricultural produce by ionizing radiation can be used to preserve it from both microbial infection and insect damage, as well as from physical deterioration. It can extend the storage life of food without noticeably changing its properties.[37]
The objective of preserving vegetables is to extend their availability for consumption or marketing purposes. The aim is to harvest the food at its maximum state of palatability and nutritional value, and preserve these qualities for an extended period. The main causes of deterioration in vegetables after they are gathered are the actions of naturally-occurring enzymes and the spoilage caused by micro-organisms.[38] Canning and freezing are the most commonly used techniques, and vegetables preserved by these methods are generally similar in nutritional value to comparable fresh products with regards to carotenoids, vitamin E, minerals. and dietary fiber.[39]
Canning is a process during which the enzymes in vegetables are deactivated and the micro-organisms present killed by heat. The sealed can excludes air from the foodstuff to prevent subsequent deterioration. The lowest necessary heat and the minimum processing time are used in order to prevent the mechanical breakdown of the product and to preserve the flavor as far as is possible. The can is then able to be stored at ambient temperatures for a long period.[38]
Freezing vegetables and maintaining their temperature at below −10 °C (14 °F) will prevent their spoilage for a short period, whereas a temperature of −18 °C (0 °F) is required for longer-term storage. The enzyme action will merely be inhibited, and blanching of suitably sized prepared vegetables before freezing mitigates this and prevents off-flavors developing. Not all micro-organisms will be killed at these temperatures and after thawing the vegetables should be used promptly because otherwise, any microbes present may proliferate.[40]
Traditionally, sun drying has been used for some products such as tomatoes, mushrooms, and beans, spreading the produce on racks and turning the crop at intervals. This method suffers from several disadvantages including lack of control over drying rates, spoilage when drying is slow, contamination by dirt, wetting by rain, and attack by rodents, birds, and insects. These disadvantages can be alleviated by using solar powered driers.[34] The dried produce must be prevented from reabsorbing moisture during storage.[38]
High levels of both sugar and salt can preserve food by preventing micro-organisms from growing. Green beans can be salted by layering the pods with salt, but this method of preservation is unsuited to most vegetables. Marrows, beetroot, carrot, and some other vegetables can be boiled with sugar to create jams.[38] Vinegar is widely used in food preservation; a sufficient concentration of acetic acid prevents the development of destructive micro-organisms, a fact made use of in the preparation of pickles, chutneys and relishes.[38] Fermentation is another method of preserving vegetables for later use. Sauerkraut is made from chopped cabbage and relies on lactic acid bacteria which produce compounds that are inhibitory to the growth of other micro-organisms.[6]
In 2010, China was the largest vegetable producing nation, with over half the world's production. India, the United States, Turkey, Iran, and Egypt were the next largest producers. China had the highest area of land devoted to vegetable production, while the highest average yields were obtained in Spain and the Republic of Korea.[41]
The International Organization for Standardization (ISO) sets international standards to ensure that products and services are safe, reliable, and of good quality. There are a number of ISO standards regarding fruits and vegetables.[42]  ISO 1991-1:1982 lists the botanical names of sixty-one species of plants used as vegetables along with the common names of the vegetables in English, French, and Russian.[43] ISO 67.080.20 covers the storage and transport of vegetables and their derived products.[44]



Vitamin - Wikipedia

A vitamin is an organic molecule (or related set of molecules) which is an essential micronutrient, that an organism needs in small quantities for the proper functioning of its metabolism. Essential nutrients cannot be synthesized in the organism, either at all or not in sufficient quantities, and therefore must be obtained through the diet. Vitamin C can be synthesized by some species but not by others; it is not a vitamin in the first instance but is in the second. The term vitamin does not include the three other groups of essential nutrients: minerals, essential fatty acids, and essential amino acids.[2]  Most vitamins are not single molecules, but groups of related molecules called vitamers. For example, vitamin E consists of four tocopherols and four tocotrienols. The thirteen vitamins required by human metabolism are: vitamin A (retinols and carotenoids), vitamin B1 (thiamine), vitamin B2 (riboflavin), vitamin B3 (niacin), vitamin B5 (pantothenic acid), vitamin B6 (pyridoxine), vitamin B7 (biotin), vitamin B9 (folic acid or folate), vitamin B12 (cobalamins), vitamin C (ascorbic acid), vitamin D (calciferols), vitamin E (tocopherols and tocotrienols), and vitamin K (quinones).   
Vitamins have diverse biochemical functions. Some forms of vitamin A function as regulators of cell and tissue growth and differentiation. The B complex vitamins function as enzyme cofactors (coenzymes) or the precursors for them. Vitamin D has a hormone-like function as a regulator of mineral metabolism for bones and other organs. Vitamins C and E function as antioxidants.[3] Both deficient and excess intake of a vitamin can potentially cause clinically significant illness; although excess intake of water-soluble vitamins is less likely to do so.   
Before 1935, the only source of vitamins was from food. If intake of vitamins was lacking, the result was vitamin deficiency and consequent deficiency diseases. Then, commercially produced tablets of yeast-extract vitamin B complex and semi-synthetic vitamin C became available. This was followed in the 1950s by the mass production and marketing of vitamin supplements, including multivitamins, to prevent vitamin deficiencies in the general population. Governments mandated addition of vitamins to staple foods such as flour or milk, referred to as food fortification, to prevent deficiencies.[4] Recommendations for folic acid supplementation during pregnancy reduced risk of infant neural tube defects.[5] Although reducing incidence of vitamin deficiencies clearly has benefits, supplementation is thought to be of little value for healthy people who are consuming a vitamin-adequate diet.[6]
The term vitamin is derived from the word vitamine, coined in 1912 by biochemist Casimir Funk, who isolated a complex of micronutrients essential to life, all of which he presumed to be amines. When this presumption was later determined not to be true, the "e" was dropped from the name.[7]  All vitamins were discovered (identified) between 1913 and 1948.
For the most part, vitamins are obtained from the diet, but some are acquired by other means: for example, microorganisms in the gut flora produce vitamin K and biotin; and one form of vitamin D is synthesized in skin cells when they are exposed to a certain wavelength of ultraviolet light present in sunlight. Humans can produce some vitamins from precursors they consume: for example, vitamin A is synthesized from beta carotene; and niacin is synthesized from the amino acid tryptophan.[19] The Food Fortification Initiative lists countries which have mandatory fortification programs for vitamins folic acid, niacin, vitamin A and vitamins B1, B2 and B12.[4]
Vitamins are classified as either water-soluble or fat-soluble. In humans there are 13 vitamins: 4 fat-soluble (A, D, E, and K) and 9 water-soluble (8 B vitamins and vitamin C). Water-soluble vitamins dissolve easily in water and, in general, are readily excreted from the body, to the degree that urinary output is a strong predictor of vitamin consumption.[20] Because they are not as readily stored, more consistent intake is important.[21] Fat-soluble vitamins are absorbed through the intestinal tract with the help of lipids (fats). Vitamins A and D can accumulate in the body, which can result in dangerous hypervitaminosis. Fat-soluble vitamin deficiency due to malabsorption is of particular significance in cystic fibrosis.[22]
Each vitamin is typically used in multiple reactions, and therefore most have multiple functions.[23]
Vitamins are essential for the normal growth and development of a multicellular organism. Using the genetic blueprint inherited from its parents, a fetus begins to develop from the nutrients it absorbs. It requires certain vitamins and minerals to be present at certain times.[5] These nutrients facilitate the chemical reactions that produce among other things, skin, bone, and muscle. If there is serious deficiency in one or more of these nutrients, a child may develop a deficiency disease. Even minor deficiencies may cause permanent damage.[24]
Once growth and development are completed, vitamins remain essential nutrients for the healthy maintenance of the cells, tissues, and organs that make up a multicellular organism; they also enable a multicellular life form to efficiently use chemical energy provided by food it eats, and to help process the proteins, carbohydrates, and fats required for cellular respiration.[3]
The USDA has conducted extensive studies on the percentage losses of various nutrients from different food types and cooking methods.[25] Some vitamins may become more "bio-available" – that is, usable by the body – when foods are cooked.[26] The table below shows whether various vitamins are susceptible to loss from heat—such as heat from boiling, steaming, frying, etc. The effect of cutting vegetables can be seen from exposure to air and light. Water-soluble vitamins such as B and C dissolve into the water when a vegetable is boiled, and are then lost when the water is discarded.[27]
The body's stores for different vitamins vary widely; vitamins A, D, and B12 are stored in significant amounts, mainly in the liver,[15] and an adult's diet may be deficient in vitamins A and D for many months and B12 in some cases for years, before developing a deficiency condition. However, vitamin B3 (niacin and niacinamide) is not stored in significant amounts, so stores may last only a couple of weeks.[9][15] For vitamin C, the first symptoms of scurvy in experimental studies of complete vitamin C deprivation in humans have varied widely, from a month to more than six months, depending on previous dietary history that determined body stores.[28]
Deficiencies of vitamins are classified as either primary or secondary. A primary deficiency occurs when an organism does not get enough of the vitamin in its food. A secondary deficiency may be due to an underlying disorder that prevents or limits the absorption or use of the vitamin, due to a "lifestyle factor", such as smoking, excessive alcohol consumption, or the use of medications that interfere with the absorption or use of the vitamin.[15] People who eat a varied diet are unlikely to develop a severe primary vitamin deficiency. In contrast, restrictive diets have the potential to cause prolonged vitamin deficits, which may result in often painful and potentially deadly diseases.
Well-known human vitamin deficiencies involve thiamine (beriberi), niacin (pellagra),[29] vitamin C (scurvy), and vitamin D (rickets).[30] In much of the developed world, such deficiencies are rare; this is due to (1) an adequate supply of food and (2) the addition of vitamins and minerals to common foods (fortification).[15] In addition to these classical vitamin deficiency diseases, some evidence has also suggested links between vitamin deficiency and a number of different disorders.[31][32]
Some vitamins have documented acute or chronic toxicity at larger intakes. The European Union and the governments of several countries have established Tolerable upper intake levels (ULs) for those vitamins which have documented toxicity (see table).[8][33][34] The likelihood of consuming too much of any vitamin from food is remote, but excessive intake (vitamin poisoning) from dietary supplements does occur. In 2016, overdose exposure to all formulations of vitamins and multi-vitamin/mineral formulations was reported by 63,931 individuals to the American Association of Poison Control Centers with 72% of these exposures in children under the age of five.[35]
In setting human nutrient guidelines, government organizations do not necessarily agree on amounts needed to avoid deficiency or maximum amounts to avoid the risk of toxicity.[33][8][34] For example, for vitamin C, recommended intakes range from 40 mg/day in India[36] to 155 mg/day for the European Union.[37] The table below shows U.S. Estimated Average Requirements (EARs) and Recommended Dietary Allowances (RDAs) for vitamins, PRIs for the European Union (same concept as RDAs), followed by what three government organizations deem to be the safe upper intake. RDAs are set higher than EARs to cover people with higher than average needs. Adequate Intakes (AIs) are set when there is not sufficient information to establish EARs and RDAs. Governments are slow to revise information of this nature. For the U.S. values, with the exception of calcium and vitamin D, all of the data date to 1997-2004.[38]
EAR US Estimated Average Requirements.
RDA US Recommended Dietary Allowances; higher for adults than for children, and may be even higher for women who are pregnant or lactating.
AI  US and EFSA Adequate Intake; AIs established when there is not sufficient information to set EARs and RDAs.
PRI Population Reference Intake is European Union equivalent of RDA; higher for adults than for children, and may be even higher for women who are pregnant or lactating. For Thiamin and Niacin the PRIs are expressed as amounts per MJ of calories consumed. MJ = megajoule = 239 food calories.
Upper Limit  Tolerable upper intake levels.
ND  ULs have not been determined.
NE  EARs have not been established.
In those who are otherwise healthy, there is little evidence that supplements have any benefits with respect to cancer or heart disease.[6][39] Vitamin A and E supplements not only provide no health benefits for generally healthy individuals, but they may increase mortality, though the two large studies that support this conclusion included smokers for whom it was already known that beta-carotene supplements can be harmful.[39][40]
The European Union and other countries of Europe have regulations that define limits of vitamin (and mineral) dosages for their safe use as dietary supplements. Most vitamins that are sold as dietary supplements are not supposed to exceed a maximum daily dosage referred to as the tolerable upper intake level (UL). Vitamin products above these regulatory limits are not considered supplements and should be registered as prescription or non-prescription (over-the-counter drugs) due to their potential side effects. The European Union, United States, Japan and some other countries each set ULs.[33][8][34]
Dietary supplements often contain vitamins, but may also include other ingredients, such as minerals, herbs, and botanicals. Scientific evidence supports the benefits of dietary supplements for persons with certain health conditions.[41] In some cases, vitamin supplements may have unwanted effects, especially if taken before surgery, with other dietary supplements or medicines, or if the person taking them has certain health conditions.[41] They may also contain levels of vitamins many times higher, and in different forms, than one may ingest through food.
Most countries place dietary supplements in a special category under the general umbrella of foods, not drugs. As a result, the manufacturer, and not the government, has the responsibility of ensuring that its dietary supplement products are safe before they are marketed. Regulation of supplements varies widely by country. In the United States, a dietary supplement is defined under the Dietary Supplement Health and Education Act of 1994.[42] There is no FDA approval process for dietary supplements, and no requirement that manufacturers prove the safety or efficacy of supplements introduced before 1994.[29][30] The Food and Drug Administration must rely on its Adverse Event Reporting System to monitor adverse events that occur with supplements.[43] In 2007, the US Code of Federal Regulations (CFR) Title 21, part III took effect, regulating Good Manufacturing Practices (GMPs) in the manufacturing, packaging, labeling, or holding operations for dietary supplements. Even though product registration is not required, these regulations mandate production and quality control standards (including testing for identity, purity and adulterations) for dietary supplements.[44] In the European Union, the Food Supplements Directive requires that only those supplements that have been proven safe can be sold without a prescription.[45]
For most vitamins, pharmacopoeial standards have been established. In the United States, the United States Pharmacopeia (USP) sets standards for the most commonly used vitamins and preparations thereof. Likewise, monographs of the European Pharmacopoeia (Ph.Eur.) regulate aspects of identity and purity for vitamins on the European market.
The reason that the set of vitamins skips directly from E to K is that the vitamins corresponding to letters F–J were either reclassified over time, discarded as false leads, or renamed because of their relationship to vitamin B, which became a complex of vitamins.
The German-speaking scientists who isolated and described vitamin K (in addition to naming it as such) did so because the vitamin is intimately involved in the coagulation of blood following wounding (from the German word Koagulation). At the time, most (but not all) of the letters from F through to J were already designated, so the use of the letter K was considered quite reasonable.[46][49] The table nomenclature of reclassified vitamins lists chemicals that had previously been classified as vitamins, as well as the earlier names of vitamins that later became part of the B-complex.
There are other missing B vitamins which were reclassified or determined not to be vitamins. For example, B9 is folic acid and five of the folates are in the range B11 through B16, forms of other vitamins already discovered, not required as a nutrient by the entire population (like B10, PABA for internal use[50]), biologically inactive, toxic, or with unclassifiable effects in humans, or not generally recognised as vitamins by science,[51] such as the highest-numbered, which some naturopath practitioners call B21 and B22. There are also nine lettered B complex vitamins (e.g. Bm). There are other D vitamins now recognised as other substances,[50] which some sources of the same type number up to D7. The controversial cancer treatment laetrile was at one point lettered as vitamin B17. There appears to be no consensus on any vitamins Q, R, T, V, W, X, Y or Z, nor are there substances officially designated as Vitamins N or I, although the latter may have been another form of one of the other vitamins or a known and named nutrient of another type.
Once discovered, vitamins were actively promoted in articles and advertisements in McCall's, Good Housekeeping, and other media outlets.[29] Marketers enthusiastically promoted cod-liver oil, a source of Vitamin D, as "bottled sunshine", and bananas as a “natural vitality food". They promoted foods such as yeast cakes, a source of B vitamins, on the basis of scientifically-determined nutritional value, rather than taste or appearance.[52] World War II researchers focused on the need to ensure adequate nutrition, especially in processed foods.[29] Robert W. Yoder is credited with first using the term vitamania, in 1942, to describe the appeal of relying on nutritional supplements rather than on obtaining vitamins from a varied diet of foods. The continuing preoccupation with a healthy lifestyle has led to an obsessive consumption of additives the beneficial effects of which are questionable.[30]
Anti-vitamins are chemical compounds that inhibit the absorption or actions of vitamins. For example, avidin is a protein in raw egg whites that inhibits the absorption of biotin; it is deactivated by cooking.[53] Pyrithiamine, a synthetic compound, has a molecular structure similar to thiamine, vitamin B1, and inhibits the enzymes that use thiamine.[54]
The value of eating certain foods to maintain health was recognized long before vitamins were identified. The ancient Egyptians knew that feeding liver to a person may help with night blindness, an illness now known to be caused by a vitamin A deficiency.[56] The advancement of ocean voyages during the Renaissance resulted in prolonged periods without access to fresh fruits and vegetables, and made illnesses from vitamin deficiency common among ships' crews.[57]
In 1747, the Scottish surgeon James Lind discovered that citrus foods helped prevent scurvy, a particularly deadly disease in which collagen is not properly formed, causing poor wound healing, bleeding of the gums, severe pain, and death.[56] In 1753, Lind published his Treatise on the Scurvy, which recommended using lemons and limes to avoid scurvy, which was adopted by the British Royal Navy. This led to the nickname limey for British sailors. Lind's discovery, however, was not widely accepted by individuals in the Royal Navy's Arctic expeditions in the 19th century, where it was widely believed that scurvy could be prevented by practicing good hygiene, regular exercise, and maintaining the morale of the crew while on board, rather than by a diet of fresh food.[56] As a result, Arctic expeditions continued to be plagued by scurvy and other deficiency diseases. In the early 20th century, when Robert Falcon Scott made his two expeditions to the Antarctic, the prevailing medical theory at the time was that scurvy was caused by "tainted" canned food.[56]
During the late 18th and early 19th centuries, the use of deprivation studies allowed scientists to isolate and identify a number of vitamins. Lipid from fish oil was used to cure rickets in rats, and the fat-soluble nutrient was called "antirachitic A". Thus, the first "vitamin" bioactivity ever isolated, which cured rickets, was initially called "vitamin A"; however, the bioactivity of this compound is now called vitamin D.[58] In 1881, Russian medical doctor Nikolai I. Lunin (ru) studied the effects of scurvy at the University of Tartu .[59] He fed mice an artificial mixture of all the separate constituents of milk known at that time, namely the proteins, fats, carbohydrates, and salts. The mice that received only the individual constituents died, while the mice fed by milk itself developed normally. He made a conclusion that "a natural food such as milk must therefore contain, besides these known principal ingredients, small quantities of unknown substances essential to life."[59] However, his conclusions were rejected by his advisor, Gustav von Bunge, even after other students reproduced his results.[60] A similar result by Cornelius Pekelharing appeared in a Dutch medical journal  in 1905, but it was not widely reported.[60]
In East Asia, where polished white rice was the common staple food of the middle class, beriberi resulting from lack of vitamin B1 was endemic. In 1884, Takaki Kanehiro, a British-trained medical doctor of the Imperial Japanese Navy, observed that beriberi was endemic among low-ranking crew who often ate nothing but rice, but not among officers who consumed a Western-style diet. With the support of the Japanese navy, he experimented using crews of two battleships; one crew was fed only white rice, while the other was fed a diet of meat, fish, barley, rice, and beans. The group that ate only white rice documented 161 crew members with beriberi and 25 deaths, while the latter group had only 14 cases of beriberi and no deaths. This convinced Takaki and the Japanese Navy that diet was the cause of beriberi, but they mistakenly believed that sufficient amounts of protein prevented it.[61] That diseases could result from some dietary deficiencies was further investigated by Christiaan Eijkman, who in 1897 discovered that feeding unpolished rice instead of the polished variety to chickens helped to prevent beriberi in the chickens.[29] The following year, Frederick Hopkins postulated that some foods contained "accessory factors" — in addition to proteins, carbohydrates, fats etc. — that are necessary for the functions of the human body.[56] Hopkins and Eijkman were awarded the Nobel Prize for Physiology or Medicine in 1929 for their discoveries.[62]
In 1910, the first vitamin complex was isolated by Japanese scientist Umetaro Suzuki, who succeeded in extracting a water-soluble complex of micronutrients from rice bran and named it aberic acid (later Orizanin). He published this discovery in a Japanese scientific journal.[63] When the article was translated into German, the translation failed to state that it was a newly discovered nutrient, a claim made in the original Japanese article, and hence his discovery failed to gain publicity. In 1912 Polish-born biochemist Casimir Funk, working in London, isolated the same complex of micronutrients and proposed the complex be named "vitamine". It was later to be known as vitamin B3 (niacin), though he described it as "anti-beri-beri-factor" (which would today be called thiamine or vitamin B1). Funk proposed the hypothesis that other diseases, such as rickets, pellagra, coeliac disease, and scurvy could also be cured by vitamins. Max Nierenstein a friend and reader of Biochemistry at Bristol University reportedly suggested the "vitamine" name (from "vital amine").[64][65] The name soon became synonymous with Hopkins' "accessory factors", and, by the time it was shown that not all vitamins are amines, the word was already ubiquitous. In 1920, Jack Cecil Drummond proposed that the final "e" be dropped to deemphasize the "amine" reference, after researchers began to suspect that not all "vitamines" (in particular, vitamin A) have an amine component.[61]
In 1930, Paul Karrer elucidated the correct structure for beta-carotene, the main precursor of vitamin A, and identified other carotenoids. Karrer and Norman Haworth confirmed Albert Szent-Györgyi's discovery of ascorbic acid and made significant contributions to the chemistry of flavins, which led to the identification of lactoflavin. For their investigations on carotenoids, flavins and vitamins A and B2, they both received the Nobel Prize in Chemistry in 1937.[66]
In 1931, Albert Szent-Györgyi and a fellow researcher Joseph Svirbely suspected that "hexuronic acid" was actually vitamin C, and gave a sample to Charles Glen King, who proved its anti-scorbutic activity in his long-established guinea pig scorbutic assay. In 1937, Szent-Györgyi was awarded the Nobel Prize in Physiology or Medicine for his discovery. In 1943, Edward Adelbert Doisy and Henrik Dam were awarded the Nobel Prize in Physiology or Medicine for their discovery of vitamin K and its chemical structure. In 1967, George Wald was awarded the Nobel Prize (along with Ragnar Granit and Haldan Keffer Hartline) for his discovery that vitamin A could participate directly in a physiological process.[62]
In 1938, Richard Kuhn was awarded the Nobel Prize in Chemistry for his work on carotenoids and vitamins, specifically B2 and B6.[67]
The term vitamin was derived from "vitamine", a compound word coined in 1912 by the Polish biochemist Casimir Funk[68] when working at the Lister Institute of Preventive Medicine. The name is from vital and amine, meaning amine of life, because it was suggested in 1912 that the organic micronutrient food factors that prevent beriberi and perhaps other similar dietary-deficiency diseases might be chemical amines. This was true of thiamine, but after it was found that other such micronutrients were not amines the word was shortened to vitamin in English.




beta-Carotene - Wikipedia
β-Carotene is an organic, strongly colored red-orange pigment abundant in plants and fruits. It is a member of the carotenes, which are terpenoids (isoprenoids), synthesized biochemically from eight isoprene units and thus having 40 carbons. Among the carotenes, β-carotene is distinguished by having beta-rings at both ends of the molecule. β-Carotene is biosynthesized from geranylgeranyl pyrophosphate.[5]
β-Carotene is the most common form of carotene in plants. When used as a food coloring, it has the E number E160a.[6]:119 The structure was deduced by Karrer et al. in 1930.[7] In nature, β-carotene is a precursor (inactive form) to vitamin A via the action of beta-carotene 15,15'-monooxygenase.[5]
Isolation of β-carotene from fruits abundant in carotenoids is commonly done using column chromatography. It can also be extracted from the beta-carotene rich algae, Dunaliella salina.[8] The separation of β-carotene from the mixture of other carotenoids is based on the polarity of a compound. β-Carotene is a non-polar compound, so it is separated with a non-polar solvent such as hexane.[9] Being highly conjugated, it is deeply colored, and as a hydrocarbon lacking functional groups, it is very lipophilic.
Plant carotenoids are the primary dietary source of provitamin A worldwide, with β-carotene as the best-known provitamin A carotenoid. Others include α-carotene and β-cryptoxanthin. Carotenoid absorption is restricted to the duodenum of the small intestine and dependent on class B scavenger receptor (SR-B1) membrane protein, which is also responsible for the absorption of vitamin E (α-tocopherol).[10] One molecule of β-carotene can be cleaved by the intestinal enzyme β,β-carotene 15,15'-monooxygenase into two molecules of vitamin A.[11]
Absorption efficiency is estimated to be between 9 and 22%. The absorption and conversion of carotenoids may depend on the form of β-carotene (e.g., cooked vs. raw vegetables, or in a supplement), the intake of fats and oils at the same time, and the current stores of vitamin A and β-carotene in the body. Researchers list these factors that determine the provitamin A activity of carotenoids:[12]
In the molecular chain between the two cyclohexyl rings, β-carotene cleaves either symmetrically or asymmetrically. Symmetric cleavage with the enzyme β,β-carotene-15,15'-dioxygenase requires an antioxidant such as α-tocopherol.[13] This symmetric cleavage gives two equivalent retinal molecules and each retinal molecule further reacts to give retinol (vitamin A) and retinoic acid. β-Carotene is also cleaved into two asymmetric products; the product is β-apocarotenal (8',10',12'). Asymmetric cleavage reduces the level of retinoic acid significantly.[14]
Since 2001, the US Institute of Medicine uses retinol activity equivalents (RAE) for their Dietary Reference Intakes, defined as follows:[15]
1 µg RAE = 1 µg retinol
1 µg RAE = 2 µg all-trans-β-carotene from supplements
1 µg RAE = 12 µg of all-trans-β-carotene from food
1 µg RAE = 24 µg α-carotene or β-cryptoxanthin from food
RAE takes into account carotenoids' variable absorption and conversion to vitamin A by humans better than and replaces the older retinol equivalent (RE) (1 µg RE = 1 µg retinol, 6 µg β-carotene, or 12 µg α-carotene or β-cryptoxanthin).[15] RE was developed 1967 by the United Nations/World Health Organization Food and Agriculture Organization (FAO/WHO).[16]
Another older unit of vitamin A activity is the international unit (IU). Like retinol equivalent, the international unit does not take into account carotenoids' variable absorption and conversion to vitamin A by humans, as well as the more modern retinol activity equivalent. Unfortunately, food and supplement labels still generally use IU, but IU can be converted to the more useful retinol activity equivalent as follows:[15]
Beta-carotene is found in many foods and is sold as a dietary supplement. β-Carotene contributes to the orange color of many different fruits and vegetables. Vietnamese gac (Momordica cochinchinensis Spreng.) and crude palm oil are particularly rich sources, as are yellow and orange fruits, such as cantaloupe, mangoes, pumpkin, and papayas, and orange root vegetables such as carrots and sweet potatoes. The color of β-carotene is masked by chlorophyll in green leaf vegetables such as spinach, kale, sweet potato leaves, and sweet gourd leaves.[17] Vietnamese gac and crude palm oil have the highest content of β-carotene of any known plant sources, 10 times higher than carrots, for example. However, gac is quite rare and unknown outside its native region of Southeast Asia, and crude palm oil is typically processed to remove the carotenoids before sale to improve the color and clarity.[18]
The average daily intake of β-carotene is in the range 2–7 mg, as estimated from a pooled analysis of 500,000 women living in the US, Canada, and some European countries.[19]
The U.S. Department of Agriculture lists these 10 foods to have the highest β-carotene content per serving.[20]
Excess β-carotene is predominantly stored in the fat tissues of the body. The most common side effect of excessive β-carotene consumption is carotenodermia, a physically harmless condition that presents as a conspicuous orange skin tint arising from deposition of the carotenoid in the outermost layer of the epidermis.[21] Adults' fat stores are often yellow from accumulated carotenoids, including β-carotene, while infants' fat stores are white. Carotenodermia is quickly reversible upon cessation of excessive intakes.[22]
The proportion of carotenoids absorbed decreases as dietary intake increases. Within the intestinal wall (mucosa), β-carotene is partially converted into vitamin A (retinol) by an enzyme, dioxygenase. This mechanism is regulated by the individual's vitamin A status. If the body has enough vitamin A, the conversion of β-carotene decreases. Therefore, β-carotene is considered a safe source of vitamin A and high intakes will not lead to hypervitaminosis A.
β-Carotene can interact with medication used for lowering cholesterol. Taking them together can lower the effectiveness of these medications and is considered only a moderate interaction.[23] β-Carotene should not be taken with orlistat, a weight-loss medication, as orlistat can reduce the absorption of β-carotene by as much as 30%.[24] Bile acid sequestrants and proton-pump inhibitors can also decrease absorption of β-carotene.[25] Consuming alcohol with β-carotene can decrease its ability to convert to retinol and could possibly result in hepatotoxicity.[26]
Chronic high doses of β-carotene supplementation increases the probability of lung cancer in smokers.[27] The effect is specific to supplementation dose as no lung damage has been detected in those who are exposed to cigarette smoke and who ingest a physiologic dose of β-carotene (6 mg), in contrast to high pharmacologic dose (30 mg). Therefore, the oncology from β-carotene is based on both cigarette smoke and high daily doses of β-carotene.[28]
Increases in lung cancer may be due to the tendency of β-carotene to oxidize,[29] and may hasten oxidation more than other food colors such as annatto. A β-carotene breakdown product suspected of causing cancer at high dose is trans-β-apo-8'-carotenal (common apocarotenal), which has been found in one study to be mutagenic and genotoxic in cell cultures which do not respond to β-carotene itself.[30]
Additionally, supplemental β-carotene may increase the risk of prostate cancer, intracerebral hemorrhage, and cardiovascular and total mortality in people who smoke cigarettes or have a history of high-level exposure to asbestos.[31]
Medical authorities generally recommend obtaining beta-carotene from food rather than dietary supplements.[32]
Research is insufficient to determine whether a minimum level of beta-carotene consumption is necessary for human health and to identify what problems might arise from insufficient beta-carotene intake,[33] although strict vegetarians rely on pro-vitamin A carotenoids to meet their vitamin A requirements. Use of beta-carotene to treat or prevent some diseases has been studied.
A 2010 systemic meta review concluded that supplementation with β-carotene does not appear to decrease the risk of cancer overall, nor specific cancers including: pancreatic, colorectal, prostate, breast, melanoma, or skin cancer generally.[34] High levels of β-carotene may increase the risk of lung cancer in current and former smokers.[35] This is likely because beta-carotene is unstable in cigarette smoke-exposed lungs where it forms oxidized metabolites that can induce carcinogen-bioactivating enzymes.[36] Results are not clear for thyroid cancer.[37] In a single, small clinical study published in 1989, natural beta-carotene appeared to reduce premalignant gastric lesions.[33]:177
A Cochrane review looked at supplementation of β-carotene, vitamin C, and vitamin E, independently and combined, on people to examine differences in risk of cataract, cataract extraction, progression of cataract, and slowing the loss of visual acuity. These studies found no evidence of any protective effects afforded by β-carotene supplementation on preventing and slowing age-related cataract.[38] A second meta-analysis compiled data from studies that measured diet-derived serum beta-carotene and reported a not statistically significant 10% decrease in cataract risk.[39]



Vaccine - Wikipedia

A vaccine is a biological preparation that provides active acquired immunity to a particular disease. A vaccine typically contains an agent that resembles a disease-causing microorganism and is often made from weakened or killed forms of the microbe, its toxins, or one of its surface proteins. The agent stimulates the body's immune system to recognize the agent as a threat, destroy it, and to further recognize and destroy any of the microorganisms associated with that agent that it may encounter in the future. Vaccines can be prophylactic (example: to prevent or ameliorate the effects of a future infection by a natural or "wild" pathogen), or therapeutic (e.g., vaccines against cancer are being investigated).[1][2][3][4]
The administration of vaccines is called vaccination. Vaccination is the most effective method of preventing infectious diseases;[5] widespread immunity due to vaccination is largely responsible for the worldwide eradication of smallpox and the restriction of diseases such as polio, measles, and tetanus from much of the world.
The effectiveness of vaccination has been widely studied and verified; for example, the influenza vaccine,[6] the HPV vaccine,[7] and the chicken pox vaccine.[8] The World Health Organization (WHO) reports that licensed vaccines are currently available for twenty-five different preventable infections.[9]
The terms vaccine and vaccination are derived from Variolae vaccinae (smallpox of the cow), the term devised by Edward Jenner to denote cowpox. He used it in 1798 in the long title of his Inquiry into the Variolae vaccinae known as the Cow Pox, in which he described the protective effect of cowpox against smallpox.[10] In 1881, to honor Jenner, Louis Pasteur proposed that the terms should be extended to cover the new protective inoculations then being developed.[11]
Vaccines have historically been the most effective means to fight and eradicate infectious diseases. Limitations to their effectiveness, nevertheless, exist.[12] Sometimes, protection fails because the host's immune system simply does not respond adequately or at all. Lack of response commonly results from clinical factors such as diabetes, steroid use, HIV infection, or age.[citation needed] It also might fail for genetic reasons if the host's immune system includes no strains of B cells that can generate antibodies suited to reacting effectively and binding to the antigens associated with the pathogen.
Even if the host does develop antibodies, protection might not be adequate; immunity might develop too slowly to be effective in time, the antibodies might not disable the pathogen completely, or there might be multiple strains of the pathogen, not all of which are equally susceptible to the immune reaction. However, even a partial, late, or weak immunity, such as a one resulting from cross-immunity to a strain other than the target strain, may mitigate an infection, resulting in a lower mortality rate, lower morbidity, and faster recovery.
Adjuvants commonly are used to boost immune response, particularly for older people (50–75 years and up), whose immune response to a simple vaccine may have weakened.[13]
The efficacy or performance of the vaccine is dependent on a number of factors:
If a vaccinated individual does develop the disease vaccinated against (breakthrough infection), the disease is likely to be less virulent than in unvaccinated victims.[16]
The following are important considerations in the effectiveness of a vaccination program:[citation needed]
In 1958, there were 763,094 cases of measles in the United States; 552 deaths resulted.[17][18] After the introduction of new vaccines, the number of cases dropped to fewer than 150 per year (median of 56).[18] In early 2008, there were 64 suspected cases of measles. Fifty-four of those infections were associated with importation from another country, although only 13% were actually acquired outside the United States; 63 of the 64 individuals either had never been vaccinated against measles or were uncertain whether they had been vaccinated.[18]
Vaccines have contributed to the eradication of smallpox, one of the most contagious and deadly diseases in humans. Other diseases such as rubella, polio, measles, mumps, chickenpox, and typhoid are nowhere near as common as they were a hundred years ago. As long as the vast majority of people are vaccinated, it is much more difficult for an outbreak of disease to occur, let alone spread. This effect is called herd immunity. Polio, which is transmitted only between humans, is targeted by an extensive eradication campaign that has seen endemic polio restricted to only parts of three countries (Afghanistan, Nigeria, and Pakistan).[19] However, the difficulty of reaching all children as well as cultural misunderstandings have caused the anticipated eradication date to be missed several times.
Vaccines also help prevent the development of antibiotic resistance.  For example, by greatly reducing the incidence of pneumonia caused by Streptococcus pneumoniae, vaccine programs have greatly reduced the prevalence of infections resistant to penicillin or other first-line antibiotics.[20]
Vaccination given during childhood is generally safe.[21] Adverse effects if any are generally mild.[22] The rate of side effects depends on the vaccine in question.[22] Some common side effects include fever, pain around the injection site, and muscle aches.[22] Additionally, some individuals may be allergic to ingredients in the vaccine.[23] MMR vaccine is rarely associated with febrile seizures.[21]
Severe side effects are extremely rare.[21] Varicella vaccine is rarely associated with complications in immunodeficient individuals and rotavirus vaccines are moderately associated with intussusception.[21]
Vaccines are dead or inactivated organisms or purified products derived from them.
There are several types of vaccines in use.[24] These represent different strategies used to try to reduce the risk of illness while retaining the ability to induce a beneficial immune response.
Some vaccines contain inactivated, but previously virulent, micro-organisms that have been destroyed with chemicals, heat, or radiation.[25] Examples include the polio vaccine, hepatitis A vaccine, rabies vaccine and some influenza vaccines.[citation needed]
Some vaccines contain live, attenuated microorganisms. Many of these are active viruses that have been cultivated under conditions that disable their virulent properties, or that use closely related but less dangerous organisms to produce a broad immune response. Although most attenuated vaccines are viral, some are bacterial in nature. Examples include the viral diseases yellow fever, measles, mumps, and rubella, and the bacterial disease typhoid. The live Mycobacterium tuberculosis  vaccine developed by Calmette and Guérin is not made of a contagious strain but contains a virulently modified strain called "BCG" used to elicit an immune response to the vaccine. The live attenuated vaccine containing strain Yersinia pestis EV is used for plague immunization. Attenuated vaccines have some advantages and disadvantages. They typically provoke more durable immunological responses and are the preferred type for healthy adults. But they may not be safe for use in immunocompromised individuals, and on rare occasions mutate to a virulent form and cause disease.[26]
Toxoid vaccines are made from inactivated toxic compounds that cause illness rather than the micro-organism.[citation needed] Examples of toxoid-based vaccines include tetanus and diphtheria. Toxoid vaccines are known for their efficacy.[citation needed] Not all toxoids are for micro-organisms; for example, Crotalus atrox toxoid is used to vaccinate dogs against rattlesnake bites.[citation needed]
Protein subunit – rather than introducing an inactivated or attenuated micro-organism to an immune system (which would constitute a "whole-agent" vaccine), a fragment of it can create an immune response.[citation needed] Examples include the subunit vaccine against Hepatitis B virus that is composed of only the surface proteins of the virus (previously extracted from the blood serum of chronically infected patients, but now produced by recombination of the viral genes into yeast)[citation needed] or as an edible algae vaccine, the virus-like particle (VLP) vaccine against human papillomavirus (HPV) that is composed of the viral major capsid protein,[citation needed] and the hemagglutinin and neuraminidase subunits of the influenza virus.[citation needed] Subunit vaccine is being used for plague immunization.[citation needed]
Conjugate – certain bacteria have polysaccharide outer coats that are poorly immunogenic. By linking these outer coats to proteins (e.g., toxins), the immune system can be led to recognize the polysaccharide as if it were a protein antigen.[citation needed] This approach is used in the Haemophilus influenzae type B vaccine.[citation needed]
A number of innovative vaccines are also in development and in use:
While most vaccines are created using inactivated or attenuated compounds from micro-organisms, synthetic vaccines are composed mainly or wholly of synthetic peptides, carbohydrates, or antigens.
Vaccines may be monovalent (also called univalent) or multivalent (also called polyvalent). A monovalent vaccine is designed to immunize against a single antigen or single microorganism.[31] A multivalent or polyvalent vaccine is designed to immunize against two or more strains of the same microorganism, or against two or more microorganisms.[32] The valency of a multivalent vaccine may be denoted with a Greek or Latin prefix (e.g., tetravalent or quadrivalent).  In certain cases, a monovalent vaccine may be preferable for rapidly developing a strong immune response.[33]
Also known as heterologous or "Jennerian" vaccines, these are vaccines that are pathogens of other animals that either do not cause disease or cause mild disease in the organism being treated. The classic example is Jenner's use of cowpox to protect against smallpox. A current example is the use of BCG vaccine made from Mycobacterium bovis to protect against human tuberculosis.[34]
Various fairly standardized abbreviations for vaccine names have developed, although the standardization is by no means centralized or global. For example, the vaccine names used in the United States have well-established abbreviations that are also widely known and used elsewhere. An extensive list of them provided in a sortable table and freely accessible, is available at a US Centers for Disease Control and Prevention web page.[35] The page explains that "The abbreviations [in] this table (Column 3) were standardized jointly by staff of the Centers for Disease Control and Prevention, ACIP Work Groups, the editor of the Morbidity and Mortality Weekly Report (MMWR), the editor of Epidemiology and Prevention of Vaccine-Preventable Diseases (the Pink Book), ACIP members, and liaison organizations to the ACIP."[35] Some examples are "DTaP" for diphtheria and tetanus toxoids and acellular pertussis vaccine, "DT" for diphtheria and tetanus toxoids, and "Td" for tetanus and diphtheria toxoids. At its page on tetanus vaccination,[36] the CDC further explains that "Upper-case letters in these abbreviations denote full-strength doses of diphtheria (D) and tetanus (T) toxoids and pertussis (P) vaccine. Lower-case "d" and "p" denote reduced doses of diphtheria and pertussis used in the adolescent/adult-formulations. The 'a' in DTaP and Tdap stands for 'acellular,' meaning that the pertussis component contains only a part of the pertussis organism."[36] Another list of established vaccine abbreviations is at the CDC's page called "Vaccine Acronyms and Abbreviations", with abbreviations used on U.S. immunization records.[37] The United States Adopted Name system has some conventions for the word order of vaccine names, placing head nouns first and adjectives postpositively. This is why the USAN for "OPV" is "poliovirus vaccine live oral" rather than "oral poliovirus vaccine".
The immune system recognizes vaccine agents as foreign, destroys them, and "remembers" them. When the virulent version of an agent is encountered, the body recognizes the protein coat on the virus, and thus is prepared to respond, by (1) neutralizing the target agent before it can enter cells, and (2) recognizing and destroying infected cells before that agent can multiply to vast numbers.
When two or more vaccines are mixed together in the same formulation, the two vaccines can interfere. This most frequently occurs with live attenuated vaccines, where one of the vaccine components is more robust than the others and suppresses the growth and immune response to the other components. This phenomenon was first noted in the trivalent Sabin polio vaccine, where the amount of serotype 2 virus in the vaccine had to be reduced to stop it from interfering with the "take" of the serotype 1 and 3 viruses in the vaccine.[38] This phenomenon has also been found to be a problem with the dengue vaccines currently being researched,[when?] where the DEN-3 serotype was found to predominate and suppress the response to DEN-1, −2 and −4 serotypes.[39]
Vaccines typically contain one or more adjuvants, used to boost the immune response. Tetanus toxoid, for instance, is usually adsorbed onto alum. This presents the antigen in such a way as to produce a greater action than the simple aqueous tetanus toxoid. People who have an adverse reaction to adsorbed tetanus toxoid may be given the simple vaccine when the time comes for a booster.[citation needed]
In the preparation for the 1990 Persian Gulf campaign, whole cell pertussis vaccine was used as an adjuvant for anthrax vaccine. This produces a more rapid immune response than giving only the anthrax vaccine, which is of some benefit if exposure might be imminent.[citation needed]
Vaccines may also contain preservatives to prevent contamination with bacteria or fungi. Until recent years, the preservative thimerosal was used in many vaccines that did not contain live virus. As of 2005, the only childhood vaccine in the U.S. that contains thimerosal in greater than trace amounts is the influenza vaccine,[40] which is currently recommended only for children with certain risk factors.[41] Single-dose influenza vaccines supplied in the UK do not list thiomersal (its UK name) in the ingredients. Preservatives may be used at various stages of production of vaccines, and the most sophisticated methods of measurement might detect traces of them in the finished product, as they may in the environment and population as a whole.[42]
In order to provide the best protection, children are recommended to receive vaccinations as soon as their immune systems are sufficiently developed to respond to particular vaccines, with additional "booster" shots often required to achieve "full immunity". This has led to the development of complex vaccination schedules. In the United States, the Advisory Committee on Immunization Practices, which recommends schedule additions for the Centers for Disease Control and Prevention, recommends routine vaccination of children against:[43] hepatitis A, hepatitis B, polio, mumps, measles, rubella, diphtheria, pertussis, tetanus, HiB, chickenpox, rotavirus, influenza, meningococcal disease and pneumonia.[44] A large number of vaccines and boosters recommended (up to 24 injections by age two) has led to problems with achieving full compliance. In order to combat declining compliance rates, various notification systems have been instituted and a number of combination injections are now marketed (e.g., Pneumococcal conjugate vaccine and MMRV vaccine), which provide protection against multiple diseases.
Besides recommendations for infant vaccinations and boosters, many specific vaccines are recommended for other ages or for repeated injections throughout life—most commonly for measles, tetanus, influenza, and pneumonia. Pregnant women are often screened for continued resistance to rubella. The human papillomavirus vaccine is recommended in the U.S. (as of 2011)[45] and UK (as of 2009).[46] Vaccine recommendations for the elderly concentrate on pneumonia and influenza, which are more deadly to that group. In 2006, a vaccine was introduced against shingles, a disease caused by the chickenpox virus, which usually affects the elderly.
Prior to the introduction of vaccination with material from cases of cowpox (heterotypic immunisation), smallpox could be prevented by deliberate inoculation of smallpox virus, later referred to as variolation to distinguish it from smallpox vaccination. The earliest hints of the practice of inoculation for smallpox in China come during the 10th century.[47] The Chinese also practiced the oldest documented use of variolation, dating back to the fifteenth century. They implemented a method of "nasal insufflation" administered by blowing powdered smallpox material, usually scabs, up the nostrils. Various insufflation techniques have been recorded throughout the sixteenth and seventeenth centuries within China.[48]:60
Two reports on the Chinese practice of inoculation were received by the Royal Society in London in 1700; one by Dr. Martin Lister who received a report by an employee of the East India Company stationed in China and another by Clopton Havers.[49]
Sometime during the late 1760s whilst serving his apprenticeship as a surgeon/apothecary Edward Jenner learned of the story, common in rural areas, that dairy workers would never have the often-fatal or disfiguring disease smallpox, because they had already had cowpox, which has a very mild effect in humans. In 1796, Jenner took pus from the hand of a milkmaid with cowpox, scratched it into the arm of an 8-year-old boy, James Phipps, and six weeks later inoculated (variolated) the boy with smallpox, afterwards observing that he did not catch smallpox.[50][51] Jenner extended his studies and in 1798 reported that his vaccine was safe in children and adults and could be transferred from arm-to-arm reducing reliance on uncertain supplies from infected cows.[10] Since vaccination with cowpox was much safer than smallpox inoculation,[52] the latter, though still widely practised in England, was banned in 1840.[53]
The second generation of vaccines was introduced in the 1880s by Louis Pasteur who developed vaccines for chicken cholera and anthrax,[11] and from the late nineteenth century vaccines were considered a matter of national prestige, and compulsory vaccination laws were passed.[50]
The twentieth century saw the introduction of several successful vaccines, including those against diphtheria, measles, mumps, and rubella. Major achievements included the development of the polio vaccine in the 1950s and the eradication of smallpox during the 1960s and 1970s. Maurice Hilleman was the most prolific of the developers of the vaccines in the twentieth century. As vaccines became more common, many people began taking them for granted. However, vaccines remain elusive for many important diseases, including herpes simplex, malaria, gonorrhea, and HIV.[50][54]
One challenge in vaccine development is economic: Many of the diseases most demanding a vaccine, including HIV, malaria and tuberculosis, exist principally in poor countries. Pharmaceutical firms and biotechnology companies have little incentive to develop vaccines for these diseases, because there is little revenue potential. Even in more affluent countries, financial returns are usually minimal and the financial and other risks are great.[55]
Most vaccine development to date has relied on "push" funding by government, universities and non-profit organizations.[56] Many vaccines have been highly cost effective and beneficial for public health.[57] The number of vaccines actually administered has risen dramatically in recent decades.[58]  This increase, particularly in the number of different vaccines administered to children before entry into schools may be due to government mandates and support, rather than economic incentive.[citation needed]
The filing of patents on vaccine development processes can also be viewed as an obstacle to the development of new vaccines. Because of the weak protection offered through a patent on the final product, the protection of the innovation regarding vaccines is often made through the patent of processes used in the development of new vaccines as well as the protection of secrecy.[59]
According to the World Health Organization, the biggest barrier to local vaccine production in less developed countries has not been patents, but the substantial financial, infrastructure, and workforce expertise requirements needed for market entry. Vaccines are complex mixtures of biological compounds, and unlike the case of drugs, there are no true generic vaccines. The vaccine produced by a new facility must undergo complete clinical testing for safety and efficacy similar to that undergone by that produced by the original manufacturer. For most vaccines, specific processes have been patented. These can be circumvented by alternative manufacturing methods, but this required R&D infrastructure and a suitably skilled workforce. In the case of a few relatively new vaccines such as the human papillomavirus vaccine, the patents may impose an additional barrier.[60]
Vaccine production has several stages. First, the antigen itself is generated. Viruses are grown either on primary cells such as chicken eggs (e.g., for influenza) or on continuous cell lines such as cultured human cells (e.g., for hepatitis A).[61]  Bacteria are grown in bioreactors (e.g., Haemophilus influenzae type b). Likewise, a recombinant protein derived from the viruses or bacteria can be generated in yeast, bacteria, or cell cultures. After the antigen is generated, it is isolated from the cells used to generate it. A virus may need to be inactivated, possibly with no further purification required. Recombinant proteins need many operations involving ultrafiltration and column chromatography. Finally, the vaccine is formulated by adding adjuvant, stabilizers, and preservatives as needed. The adjuvant enhances the immune response of the antigen, stabilizers increase the storage life, and preservatives allow the use of multidose vials.[62][63] Combination vaccines are harder to develop and produce, because of potential incompatibilities and interactions among the antigens and other ingredients involved.[64]
Vaccine production techniques are evolving. Cultured mammalian cells are expected to become increasingly important, compared to conventional options such as chicken eggs, due to greater productivity and low incidence of problems with contamination. Recombination technology that produces genetically detoxified vaccine is expected to grow in popularity for the production of bacterial vaccines that use toxoids. Combination vaccines are expected to reduce the quantities of antigens they contain, and thereby decrease undesirable interactions, by using pathogen-associated molecular patterns.[64]
In 2010, India produced 60 percent of the world's vaccine worth about $900 million(€670 million).[65]
Beside the active vaccine itself, the following excipients and residual manufacturing compounds are present or may be present in vaccine preparations:[66]
Many vaccines need preservatives to prevent serious adverse effects such as Staphylococcus infection, which in one 1928 incident killed 12 of 21 children inoculated with a diphtheria vaccine that lacked a preservative.[68] Several preservatives are available, including thiomersal, phenoxyethanol, and formaldehyde. Thiomersal is more effective against bacteria, has a better shelf-life, and improves vaccine stability, potency, and safety; but, in the U.S., the European Union, and a few other affluent countries, it is no longer used as a preservative in childhood vaccines, as a precautionary measure due to its mercury content.[69] Although controversial claims have been made that thiomersal contributes to autism, no convincing scientific evidence supports these claims.[70]
The development of new delivery systems raises the hope of vaccines that are safer and more efficient to deliver and administer. Lines of research include liposomes and ISCOM (immune stimulating complex).[71]
Notable developments in vaccine delivery technologies have included oral vaccines. Early attempts to apply oral vaccines showed varying degrees of promise, beginning early in the 20th century, at a time when the very possibility of an effective oral antibacterial vaccine was controversial.[72] By the 1930s there was increasing interest in the prophylactic value of an oral typhoid fever vaccine for example.[73]
An oral polio vaccine turned out to be effective when vaccinations were administered by volunteer staff without formal training; the results also demonstrated increased ease and efficiency of administering the vaccines. Effective oral vaccines have many advantages; for example, there is no risk of blood contamination. Vaccines intended for oral administration need not be liquid, and as solids, they commonly are more stable and less prone to damage or to spoilage by freezing in transport and storage.[74] Such stability reduces the need for a "cold chain": the resources required to keep vaccines within a restricted temperature range from the manufacturing stage to the point of administration, which, in turn, may decrease costs of vaccines.
A microneedle approach, which is still in stages of development, uses "pointed projections fabricated into arrays that can create vaccine delivery pathways through the skin".[75]
An experimental needle-free[76] vaccine delivery system is undergoing animal testing.[77][78]  A stamp-size patch similar to an adhesive bandage contains about 20,000 microscopic projections per square cm.[79]  This dermal administration potentially increases the effectiveness of vaccination, while requiring less vaccine than injection.[80]
The use of plasmids has been validated in preclinical studies as a protective vaccine strategy for cancer and infectious diseases. However, in human studies, this approach has failed to provide clinically relevant benefit. The overall efficacy of plasmid DNA immunization depends on increasing the plasmid's immunogenicity while also correcting for factors involved in the specific activation of immune effector cells.[81]
Vaccinations of animals are used both to prevent their contracting diseases and to prevent transmission of disease to humans.[82] Both animals kept as pets and animals raised as livestock are routinely vaccinated. In some instances, wild populations may be vaccinated. This is sometimes accomplished with vaccine-laced food spread in a disease-prone area and has been used to attempt to control rabies in raccoons.
Where rabies occurs, rabies vaccination of dogs may be required by law. Other canine vaccines include canine distemper, canine parvovirus, infectious canine hepatitis, adenovirus-2, leptospirosis, bordatella, canine parainfluenza virus, and Lyme disease, among others.
Cases of veterinary vaccines used in humans have been documented, whether intentional or accidental, with some cases of resultant illness, most notably with brucellosis.[83]  However, the reporting of such cases is rare and very little has been studied about the safety and results of such practices.  With the advent of aerosol vaccination in veterinary clinics for companion animals, human exposure to pathogens that are not naturally carried in humans, such as Bordetella bronchiseptica, has likely increased in recent years.[83]  In some cases, most notably rabies, the parallel veterinary vaccine against a pathogen may be as much as orders of magnitude more economical than the human one.
DIVA (Differentiating Infected from Vaccinated Animals) vaccines make it possible to differentiate between infected and vaccinated animals.
DIVA vaccines carry at least one epitope less than the microorganisms circulating in the field. An accompanying diagnostic test that detects antibody against that epitope allows us to actually make that differentiation.
The first DIVA vaccines (formerly termed marker vaccines and since 1999 coined as DIVA vaccines) and companion diagnostic tests have been developed by
J.T. van Oirschot and colleagues at the Central Veterinary Institute in Lelystad, The Netherlands.[84]
[85]
They found that some existing vaccines against pseudorabies (also termed Aujeszky's disease) had deletions in their viral genome (among which the gE gene). Monoclonal antibodies were produced against that deletion and selected to develop an ELISA that demonstrated antibodies against gE. In addition, novel genetically engineered gE-negative vaccines were constructed.[86]
Along the same lines, DIVA vaccines and companion diagnostic tests against bovine herpesvirus 1 infections have been developed.[87][88]
The DIVA strategy has been applied in various countries and successfully eradicated pseudorabies virus. Swine populations were intensively vaccinated and monitored by the companion diagnostic test and, subsequently, the infected pigs were removed from the population. Bovine herpesvirus 1 DIVA vaccines are also widely used in practice.
Scientists have put and still, are putting much effort in applying the DIVA principle to a wide range of infectious diseases, such as, for example, classical swine fever,[89] avian influenza,[90] Actinobacillus pleuropneumonia[91] and Salmonella infections in pigs.[92]
Vaccine development has several trends:[93]
Principles that govern the immune response can now be used in tailor-made vaccines against many noninfectious human diseases, such as cancers and autoimmune disorders.[97] For example, the experimental vaccine CYT006-AngQb has been investigated as a possible treatment for high blood pressure.[98] Factors that affect the trends of vaccine development include progress in translatory medicine, demographics, regulatory science, political, cultural, and social responses.[99]
Transgenic plants have been identified as promising expression systems for vaccine production. Complex plants such as tobacco, potato, tomato, and banana can have genes inserted that cause them to produce vaccines usable for humans.[100] Bananas have been developed that produce a human vaccine against Hepatitis B.[101] Another example is the expression of a fusion protein in alfalfa transgenic plants for the selective directioning to antigen presenting cells, therefore increasing vaccine potency against Bovine Viral Diarrhea Virus (BVDV).[102][103]



Symptom - Wikipedia
A symptom (from Greek σύμπτωμα, "accident, misfortune, that which befalls",[1] from συμπίπτω, "I befall", from συν- "together, with" and πίπτω, "I fall") is a departure from normal function or feeling which is noticed by a patient, reflecting the presence of an unusual state, or of a disease. A symptom is subjective,[2] observed by the patient,[3] and cannot be measured directly,[4] whereas a sign is objectively observable by others. For example, paresthesia is a symptom (only the person experiencing it can directly observe their own tingling feeling), whereas erythema is a sign (anyone can confirm that the skin is redder than usual). Symptoms and signs are often nonspecific, but often combinations of them are at least suggestive of certain diagnoses, helping to narrow down what may be wrong. In other cases they are specific even to the point of being pathognomonic.
The term is sometimes also applied to physiological states outside the context of disease, as for example when referring to "symptoms of pregnancy". Many people use the term sign and symptom interchangeably.[5]
Symptoms may be briefly acute or a more prolonged but acute or chronic, relapsing or remitting. Asymptomatic conditions also exist (e.g. subclinical infections and silent diseases like sometimes, high blood pressure).
Constitutional or general symptoms are those related to the systemic effects of a disease (e.g., fever, malaise, anorexia, and weight loss). They affect the entire body rather than a specific organ or location.
The terms "chief complaint", "presenting symptom", "iatrotropic symptom", or "presenting complaint" are used to describe the initial concern which brings a patient to a doctor. The symptom that ultimately leads to a diagnosis is called a "cardinal symptom".
Non-specific symptoms are self-reported symptoms that do not indicate a specific disease process or involve an isolated body system. For example, fatigue is a feature of many acute and chronic medical conditions, which may or may not be mental, and may be either a primary or secondary symptom. Fatigue is also a normal, healthy condition when experienced after exertion or at the end of a day.
In describing mental disorders,[6][7] especially schizophrenia, symptoms can be divided into positive and negative symptoms.[8]
Some symptoms occur in a wide range of disease processes, whereas other symptoms are fairly specific for a narrow range of illnesses. For example, a sudden loss of sight in one eye has a significantly smaller number of possible causes than nausea does.
Some symptoms can be misleading to the patient or the medical practitioner caring for them. For example, inflammation of the gallbladder often gives rise to pain in the right shoulder, which may understandably lead the patient to attribute the pain to a non-abdominal cause such as muscle strain.
A sign has the potential to be objectively observed by someone other than the patient, whereas a symptom does not. There is a correlation between this difference and the difference between the medical history and the physical examination. Symptoms belong only to the history, whereas signs can often belong to both. Clinical signs such as rash and muscle tremors are objectively observable both by the patient and by anyone else. Some signs belong only to the physical examination, because it takes medical expertise to uncover them. (For example, laboratory signs such as hypocalcaemia or neutropenia require blood tests to find.) A sign observed by the patient last week but now gone (such as a resolved rash) was a sign, but it belongs to the medical history, not the physical examination, because the physician cannot independently verify it today.
Symptomatology (also called semeiology) is a branch of medicine dealing with symptoms.[10] Also this study deals with the signs and indications of a disease.[11]
Medical signSymptomSyndrome
Medical diagnosisDifferential diagnosisPrognosis
AcuteChronicCure/Remission
DiseaseEponymous diseaseAcronym or abbreviation



Screening (medicine) - Wikipedia
Screening, in medicine, is a strategy used in a population to identify the possible presence of an as-yet-undiagnosed disease in individuals without signs or symptoms. This can include individuals with pre-symptomatic or unrecognized symptomatic disease. As such, screening tests are somewhat unusual in that they are performed on persons apparently in good health.
Screening interventions are designed to identify disease in a community early, thus enabling earlier intervention and management in the hope to reduce mortality and suffering from a disease. Although screening may lead to an earlier diagnosis, not all screening tests have been shown to benefit the person being screened; overdiagnosis, misdiagnosis, and creating a false sense of security are some potential adverse effects of screening. Additionally, some screening tests can be inappropriately overused.[1][2] For these reasons, a test used in a screening program, especially for a disease with low incidence, must have good sensitivity in addition to acceptable specificity.[3]
Several types of screening exist: universal screening involves screening of all individuals in a certain category (for example, all children of a certain age). Case finding involves screening a smaller group of people based on the presence of risk factors (for example, because a family member has been diagnosed with a hereditary disease). Screening interventions are not designed to be diagnostic, and often have significant rates of both false positive and false negative results.
In 1968, the World Health Organization published guidelines on the Principles and practice of screening for disease, which often referred to as Wilson's criteria.[4] The principles are still broadly applicable today:
In 2008, with emergence of new genomic technologies, the WHO synthesised and modified these with the new understanding as follows:
Synthesis of emerging screening criteria proposed over the past 40 years
[5]
In many countries there are population-based screening programmes. In some countries, such as the UK, these operate at a national level. Common screening programmes include:
Most public school systems in the United States screen students periodically for hearing and vision deficiencies and dental problems. Screening for spinal and posture issues such as scoliosis is sometimes carried out, but is controversial as scoliosis (unlike vision or dental issues) is found in only a very small segment of the general population and because students must remove their shirts for screening. Many states no longer mandate scoliosis screenings, or allow them to be waived with parental notification.[citation needed]
Medical equipment used in screening tests is usually different from equipment used in diagnostic tests as screening tests are used to indicate the likely presence or absence of a disease or condition in people not presenting symptoms; while diagnostic medical equipment is used to make quantitative physiological measurements to confirm and determine the progress of a suspected disease or condition. Medical screening equipment must be capable of fast processing of many cases, but may not need to be as precise as diagnostic equipment.
Screening can detect medical conditions at an early stage before symptoms present while treatment is more effective than for later detection. In the best of cases lives are saved. Like any medical test, the tests used in screening are not perfect. The test result may incorrectly show positive for those without disease (false positive), or negative for people who have the condition (false negative). Limitations of screening programmes can include:
Screening for dementia in the English NHS is controversial because it could cause undue anxiety in patients and support services would be stretched. A GP reported "The main issue really seems to be centred around what the consequences of a such a diagnosis is and what is actually available to help patients."[7]
To many people, screening instinctively seems like an appropriate thing to do, because catching something earlier seems better.  However, no screening test is perfect.  There will always be the problems with incorrect results and other issues listed above.
Before a screening program is implemented, it should ideally be looked at to ensure that putting it in place would do more good than harm. The best studies for assessing whether a screening test will increase a population's health are rigorous randomized controlled trials.
When studying a screening program using case-control or, more usually, cohort studies, various factors can cause the screening test to appear more successful than it really is. A number of different biases, inherent in the study method, will skew results.
Screening can certainly improve outcomes, but this must be confirmed with proper statistical analysis, not simplistic comparison of numbers.
The intention of screening is to diagnose a disease earlier than it would be without screening. Without screening the disease may be discovered later, when symptoms appear.
Even if in both cases a person will die at the same time, because we diagnosed the disease earlier with screening the survival time since diagnosis is longer with screening; but life span has not been prolonged, and there will be added anxiety as the patient must live with knowledge of the disease for longer.
Looking at statistics of survival time since diagnosis, screening will show an increase (this gain is called lead time). If we do not think about what survival time actually means in this context, we might attribute success to a screening test that does nothing but advance diagnosis; comparing statistics of mortality due to a disease in a screened and unscreened population gives more meaningful information.
Many screening tests involve the detection of cancers. It is often hypothesized that slower-growing tumors have better prognoses than tumors with high growth rates. Screening is more likely to detect slower-growing tumors (due to longer pre-clinical sojourn time), which may be less deadly. Thus screening may tend to detect cancers that would not have killed the patient or even been detected prior to death from other causes.
Not everyone will partake in a screening program. There are factors that differ between those willing to get tested and those who are not.
If people with a higher risk of a disease are more likely to be screened, for instance women with a family history of breast cancer are more likely than other women to join a mammography program, then a screening test will look worse than it really is: negative outcomes among the screened population will be higher than for a random sample.
Selection bias may also make a test look better than it really is. If a test is more available to young and healthy people (for instance if people have to travel a long distance to get checked) then fewer people in the screening population will have negative outcomes than for a random sample, and the test will seem to make a positive difference.
Screening may identify abnormalities that would never cause a problem in a person's lifetime. An example of this is prostate cancer screening; it has been said that "more men die with prostate cancer than of it".[8] Autopsy studies have shown that a high proportion of elderly men who have died of other causes are found to have had prostate cancer.
Aside from issues with unnecessary treatment (prostate cancer treatment is by no means without risk), overdiagnosis makes a study look good at picking up abnormalities, even though they are sometimes harmless.
Overdiagnosis occurs when all of these people with harmless abnormalities are counted as "lives saved" by the screening, rather than as "healthy people needlessly harmed by overdiagnosis".
The best way to minimize these biases is to use a randomized controlled trial, though observational, naturalistic, or retrospective studies can be of some value and are typically easier to conduct. Any study must be sufficiently large (include many patients) and sufficiently long (follow patients for many years) to have the statistical power to assess the true value of a screening program. For rare diseases, hundreds of thousands of patients may be needed to realize the value of screening (find enough treatable disease), and to assess the effect of the screening program on mortality a study may have to follow the cohort for decades. Such studies take a long time and are expensive, but can provide the most useful data with which to evaluate the screening program and practice evidence-based medicine.



United States Preventive Services Task Force - Wikipedia
The United States Preventive Services Task Force (USPSTF) is "an independent panel of experts in primary care and prevention that systematically reviews the evidence of effectiveness and develops recommendations for clinical preventive services".[1] The task force, a panel of primary care physicians and epidemiologists, is funded, staffed, and appointed by the U.S. Department of Health and Human Services' Agency for Healthcare Research and Quality.[2][3]

The USPSTF evaluates scientific evidence to determine whether medical screenings, counseling, and preventive medications work for adults and children who have no symptoms.
The methods of evidence synthesis used by the Task Force have been described in detail.[4] In 2007, their methods were revised.[5][6]
The USPSTF explicitly does not consider cost as a factor in its recommendations, and it does not perform cost-effectiveness analyses.[7] American health insurance groups are required to cover, at no charge to the patient, any service that the USPSTF recommends, regardless of how much it costs or how small the benefit is.[8]
The task force assigns the letter grades A, B, C, D, or I to each of its recommendations, and includes "suggestions for practice" for each grade. The Task Force also defined levels of certainty regarding net benefit.[9]
Levels of certainty vary from high to low according to the evidence.
The USPSTF has evaluated many interventions for prevention and found several have an expected net benefit in the general population.[10]
In 2009, the USPSTF updated its advice for screening mammograms.[11] Screening mammograms, or routine mammograms, are X-rays given to apparently healthy women with no symptoms or evidence of breast cancer in the hope of detecting the disease in an early, easily treatable stage.  The advice about using mammography in the presence of symptoms (such as a lump in the breast that can be felt) is unchanged.
The previous advice was for all women over the age of 40 to receive a mammogram every one to two years.[12]  The new advice is more detailed. For women between the ages of 50 and 74, they have recommended routine mammograms once every two years in the absence of symptoms.  Most American women who are diagnosed with breast cancer are diagnosed after age 60.[13]
The USPSTF declared that there is insufficient evidence to make any statement about the use of mammograms in women over the age of 75, as very little research has been performed in this age group.
The Task Force made no recommendation about routine mammography to screen asymptomatic women aged 40 to 49 years for breast cancer.  Patients in this age group should be educated about the risks and benefits of screening, and the decision whether to screen or not should be based on the individual situation and preferences.[14] The old advice was based on "weak" evidence for this age group.[12] The new advice is based on improved scientific evidence about the benefits and harms associated with mammography and is consistent with recommendations by the World Health Organization and other major medical bodies.  Their recommendation against routine, suspicion-less mammograms for younger women does not change the advice for screening women at above-average risk for developing breast cancer or for testing women who have a suspicious lump or any other symptoms that might be related to breast cancer.
The change in the recommendation for younger women has been criticized by some physicians and cancer advocacy groups, such as Otis Brawley, the chief medical officer for the American Cancer Society,<ref."Task force opposes routine mammograms for women age 40-49" - Danielle Dellorto, CNN Medical Producer</ref> and praised by physicians and medical organizations that support individualized and evidence-based medicine, such as Donna Sweet, the former chair of the American College of Physicians, who currently serves on its Clinical Efficacy Assessment Subcommittee.[15]
The USPSTF recommendation, which focuses solely on clinical effectiveness without regard to cost,[16] formally reduces the grade give[2]n for evidence quality from "B" to "C" (limited evidence prevents a one-size-fits-all recommendation) for routine mammograms in women under the age of 50.[17] With a grade C recommendation, physicians are required to consider additional factors, such as the individual woman's personal risk of breast cancer. Pending health care legislation would require insurance companies to cover any and all preventive services that receive an "A" or "B" grade, but permit them to use discretion on preventive services that receive a worse grade.[17]
The Vitter amendment to the Mikulski amendment to pending legislation in the U.S. Senate instructs insurers to disregard the task force's recommendation against frequent routine mammograms in asymptomatic younger women, and requires them to provide free annual mammograms, even for low-risk women, based on the outdated 2002 report.[17] This proposal is not yet law and may change. [needs update] The efforts by politicians to reject the committee's scientific findings have been condemned as an example of unwarranted political interference in scientific research.[16]
In the current recommendation published in 2012, the Task Force recommended against prostate-specific antigen (PSA)-based screening for prostate cancer.[18] The Task Force gave PCa screening a D recommendation.[18]
A final statement published in 2018 recommends basing the decision to screen on shared decision making in those 55 to 69 years old.[19] It continues to recommend against screening in those 70 and older.[19]
From 1984 to 1989, the task force's stated purpose was to "develop recommendations for primary care clinicians on the appropriate content of periodic health examinations."[20]



Stomach cancer - Wikipedia
Stomach cancer, also known as gastric cancer, is a cancer which develops from the lining of the stomach.[10] Early symptoms may include heartburn, upper abdominal pain, nausea and loss of appetite.[1] Later signs and symptoms may include weight loss, yellowing of the skin and whites of the eyes, vomiting, difficulty swallowing and blood in the stool among others.[1] The cancer may spread from the stomach to other parts of the body, particularly the liver, lungs, bones, lining of the abdomen and lymph nodes.[11]
The most common cause is infection by the bacterium Helicobacter pylori, which accounts for more than 60% of cases.[2][3][12] Certain types of H. pylori have greater risks than others.[2] Smoking, dietary factors such as pickled vegetables and obesity are other risk factors.[2][4] About 10% of cases run in families, and between 1% and 3% of cases are due to genetic syndromes inherited from a person's parents such as hereditary diffuse gastric cancer.[2] Most cases of stomach cancers are gastric carcinomas.[2] This type can be divided into a number of subtypes.[2] Lymphomas and mesenchymal tumors may also develop in the stomach.[2] Most of the time, stomach cancer develops in stages over years.[2] Diagnosis is usually by biopsy done during endoscopy.[1] This is followed by medical imaging to determine if the disease has spread to other parts of the body.[1] Japan and South Korea, two countries that have high rates of the disease, screen for stomach cancer.[2]
A Mediterranean diet lowers the risk of cancer as does the stopping of smoking.[2][5] There is tentative evidence that treating H. pylori decreases the future risk.[2][5] If cancer is treated early, many cases can be cured.[2] Treatments may include some combination of surgery, chemotherapy, radiation therapy and targeted therapy.[1][13] If treated late, palliative care may be advised.[2] Outcomes are often poor with a less than 10% five-year survival rate globally.[6] This is largely because most people with the condition present with advanced disease.[6] In the United States, five-year survival is 28%,[7] while in South Korea it is over 65%, partly due to screening efforts.[2]
Globally, stomach cancer is the fifth leading cause of cancer and the third leading cause of death from cancer, making up 7% of cases and 9% of deaths.[14] In 2012, it newly occurred in 950,000 people and caused 723,000 deaths.[14] Before the 1930s, in much of the world, including most Western developed countries, it was the most common cause of death from cancer.[15][16][17] Rates of death have been decreasing in many areas of the world since then.[2] This is believed to be due to the eating of less salted and pickled foods as a result of the development of refrigeration as a method of keeping food fresh.[18] Stomach cancer occurs most commonly in East Asia and Eastern Europe.[2] It occurs twice as often in males as in females.[2]
Stomach cancer is often either asymptomatic (producing no noticeable symptoms) or it may cause only nonspecific symptoms (symptoms that may also be present in other related or unrelated disorders) in its early stages. By the time symptoms occur, the cancer has often reached an advanced stage (see below) and may have metastasized (spread to other, perhaps distant, parts of the body), which is one of the main reasons for its relatively poor prognosis.[19] Stomach cancer can cause the following signs and symptoms:
Early cancers may be associated with indigestion or a burning sensation (heartburn). However, less than 1 in every 50 people referred for endoscopy due to indigestion has cancer.[20] Abdominal discomfort and loss of appetite, especially for meat, can occur.
Gastric cancers that have enlarged and invaded normal tissue can cause weakness, fatigue, bloating of the stomach after meals, abdominal pain in the upper abdomen, nausea and occasional vomiting, diarrhea or constipation. Further enlargement may cause weight loss or bleeding with vomiting blood or having blood in the stool, the latter apparent as black discolouration (melena) and sometimes leading to anemia. Dysphagia suggests a tumour in the cardia or extension of the gastric tumour into the esophagus.
These can be symptoms of other problems such as a stomach virus, gastric ulcer, or tropical sprue.
Gastric cancer occurs as a result of many factors.[21] It occurs twice as commonly in males as females. Estrogen may protect women against the development of this form of cancer.[22][23]
Helicobacter pylori infection is an essential risk factor in 65–80% of gastric cancers, but only 2% of people with Helicobacter infections develop stomach cancer.[24][4] The mechanism by which H. pylori induces stomach cancer potentially involves chronic inflammation, or the action of H. pylori virulence factors such as CagA.[25] It was estimated that Epstein–Barr virus is responsible for 84,000 cases per year.[26] AIDS is also associated with elevated risk.[4]
Smoking increases the risk of developing gastric cancer significantly, from 40% increased risk for current smokers to 82% increase for heavy smokers. Gastric cancers due to smoking mostly occur in the upper part of the stomach near the esophagus.[27][28][29] Some studies show increased risk with alcohol consumption as well.[4][30]
Dietary factors are not proven causes and the association between stomach cancer and various foods and beverages is weak.[32] Some foods including smoked foods, salt and salt-rich foods, red meat, processed meat, pickled vegetables, and bracken are associated with a higher risk of stomach cancer.[33][4][34] Nitrates and nitrites in cured meats can be converted by certain bacteria, including H. pylori, into compounds that have been found to cause stomach cancer in animals.
Fresh fruit and vegetable intake, citrus fruit intake, and antioxidant intake are associated with a lower risk of stomach cancer.[4][27] A Mediterranean diet is associated with lower rates of stomach cancer,[35] as is regular aspirin use.[4]
Obesity is a physical risk factor that has been found to increase the risk of gastric adenocarcinoma by contributing to the development of gastroesophageal reflux disease (GERD).[36] The exact mechanism by which obesity causes GERD is not completely known. Studies hypothesize that increased dietary fat leading to increased pressure on the stomach and the lower esophageal sphincter, due to excess adipose tissue, could play a role, yet no statistically significant data has been collected.[37] However, the risk of gastric cardia adenocarcinoma, with GERD present, has been found to increase more than 2 times for an obese person.[36] There is a correlation between iodine deficiency and gastric cancer.[38][39][40]
About 10% of cases run in families and between 1% and 3% of cases are due to genetic syndromes inherited from a person's parents such as hereditary diffuse gastric cancer.[2]
A genetic risk factor for gastric cancer is a genetic defect of the CDH1 gene known as hereditary diffuse gastric cancer (HDGC). The CDH1 gene, which codes for E-cadherin, lies on the 16th chromosome.[41] When the gene experiences a particular mutation, gastric cancer develops through a mechanism that is not fully understood.[41] This mutation is considered autosomal dominant meaning that half of a carrier’s children will likely experience the same mutation.[41] Diagnosis of hereditary diffuse gastric cancer usually takes place when at least two cases involving a family member, such as a parent or grandparent, are diagnosed, with at least one diagnosed before the age of 50.[41] The diagnosis can also be made if there are at least three cases in the family, in which case age is not considered.[41]
The International Cancer Genome Consortium is leading efforts to identify genomic changes involved in stomach cancer.[42][43] A very small percentage of diffuse-type gastric cancers (see Histopathology below) arise from an inherited abnormal CDH1 gene. Genetic testing and treatment options are available for families at risk.[44]
Other risks include diabetes,[45]
pernicious anemia,[30] chronic atrophic gastritis,[46] Menetrier's disease (hyperplastic, hypersecretory gastropathy),[47]
and intestinal metaplasia.[48]
To find the cause of symptoms, the doctor asks about the patient's medical history, does a physical exam, and may order laboratory studies. The patient may also have one or all of the following exams:
In 2013, Chinese and Israeli scientists reported a successful pilot study of a breathalyzer-style breath test intended to diagnose stomach cancer by analyzing exhaled chemicals without the need for an intrusive endoscopy.[50] A larger-scale clinical trial of this technology was completed in 2014.[51]
Abnormal tissue seen in a gastroscope examination will be biopsied by the surgeon or gastroenterologist. This tissue is then sent to a pathologist for histological examination under a microscope to check for the presence of cancerous cells. A biopsy, with subsequent histological analysis, is the only sure way to confirm the presence of cancer cells.[30]
Various gastroscopic modalities have been developed to increase yield of detected mucosa with a dye that accentuates the cell structure and can identify areas of dysplasia. Endocytoscopy involves ultra-high magnification to visualise cellular structure to better determine areas of dysplasia. Other gastroscopic modalities such as optical coherence tomography are being tested investigationally for similar applications.[52]
A number of cutaneous conditions are associated with gastric cancer. A condition of darkened hyperplasia of the skin, frequently of the axilla and groin, known as acanthosis nigricans, is associated with intra-abdominal cancers such as gastric cancer. Other cutaneous manifestations of gastric cancer include tripe palms (a similar darkening hyperplasia of the skin of the palms) and the Leser-Trelat sign, which is the rapid development of skin lesions known as seborrheic keratoses.[53]
Various blood tests may be done including a complete blood count (CBC) to check for anaemia, and a fecal occult blood test to check for blood in the stool.
If cancer cells are found in the tissue sample, the next step is to stage, or find out the extent of the disease. Various tests determine whether the cancer has spread and, if so, what parts of the body are affected. Because stomach cancer can spread to the liver, the pancreas, and other organs near the stomach as well as to the lungs, the doctor may order a CT scan, a PET scan,[56] an endoscopic ultrasound exam, or other tests to check these areas. Blood tests for tumor markers, such as carcinoembryonic antigen (CEA) and carbohydrate antigen (CA) may be ordered, as their levels correlate to extent of metastasis, especially to the liver, and the cure rate.
Staging may not be complete until after surgery. The surgeon removes nearby lymph nodes and possibly samples of tissue from other areas in the abdomen for examination by a pathologist.
The clinical stages of stomach cancer are:[57][58]
The TNM staging system is also used.[59]
In a study of open-access endoscopy in Scotland, patients were diagnosed 7% in Stage I 17% in Stage II, and 28% in Stage III.[60] A Minnesota population was diagnosed 10% in Stage I, 13% in Stage II, and 18% in Stage III.[61] However, in a high-risk population in the Valdivia Province of southern Chile, only 5% of patients were diagnosed in the first two stages and 10% in stage III.[62]
Getting rid of H. pylori in those who are infected decreases the risk of stomach cancer, at least in those who are Asian.[63] A 2014 meta-analysis of observational studies found that a diet high in fruits, mushrooms, garlic, soybeans, and green onions was associated with a lower risk of stomach cancer in the Korean population.[64] Low doses of vitamins, especially from a healthy diet, decrease the risk of stomach cancer.[65] A previous review of antioxidant supplementation did not find supporting evidence and possibly worse outcomes.[66][67]
Cancer of the stomach is difficult to cure unless it is found at an early stage (before it has begun to spread). Unfortunately, because early stomach cancer causes few symptoms, the disease is usually advanced when the diagnosis is made.[68]
Treatment for stomach cancer may include surgery,[69] chemotherapy,[13] and/or radiation therapy.[70] New treatment approaches such as immunotherapy or gene therapy and improved ways of using current methods are being studied in clinical trials.[71]
Surgery remains the only curative therapy for stomach cancer.[6] Of the different surgical techniques, endoscopic mucosal resection (EMR) is a treatment for early gastric cancer (tumor only involves the mucosa) that was pioneered in Japan and is available in the United States at some centers.[6] In this procedure, the tumor, together with the inner lining of stomach (mucosa), is removed from the wall of the stomach using an electrical wire loop through the endoscope. The advantage is that it is a much smaller operation than removing the stomach.[6] Endoscopic submucosal dissection (ESD) is a similar technique pioneered in Japan, used to resect a large area of mucosa in one piece.[6] If the pathologic examination of the resected specimen shows incomplete resection or deep invasion by tumor, the patient would need a formal stomach resection.[6] A 2016 Cochrane review found low quality evidence of no difference in short-term mortality between laparoscopic and open gastrectomy (removal of stomach), and that benefits or harms of laparoscopic gastrectomy cannot be ruled out.[72]
Those with metastatic disease at the time of presentation may receive palliative surgery and while it remains controversial, due to the possibility of complications from the surgery itself and the fact that it may delay chemotherapy the data so far is mostly positive, with improved survival rates being seen in those treated with this approach.[6][73]
The use of chemotherapy to treat stomach cancer has no firmly established standard of care.[13] Unfortunately, stomach cancer has not been particularly sensitive to these drugs, and chemotherapy, if used, has usually served to palliatively reduce the size of the tumor, relieve symptoms of the disease and increase survival time. Some drugs used in stomach cancer treatment have included: 5-FU (fluorouracil) or its analog capecitabine, BCNU (carmustine), methyl-CCNU (semustine) and doxorubicin (Adriamycin), as well as mitomycin C, and more recently cisplatin and taxotere, often using drugs in various combinations.[13] The relative benefits of these different drugs, alone and in combination, are unclear.[74][13] Clinical researchers are exploring the benefits of giving chemotherapy before surgery to shrink the tumor, or as adjuvant therapy after surgery to destroy remaining cancer cells.[6]
Recently, treatment with human epidermal growth factor receptor 2 (HER2) inhibitor, trastuzumab, has been demonstrated to increase overall survival in inoperable locally advanced or metastatic gastric carcinoma over-expressing the HER2/neu gene.[6] In particular, HER2 is overexpressed in 13–22% of patients with gastric cancer.[71][75] Of note, HER2 overexpression in gastric neoplasia is heterogeneous and comprises a minority of tumor cells (less than 10% of gastric cancers overexpress HER2 in more than 5% of tumor cells). Hence, this heterogeneous expression should be taken into account for HER2 testing, particularly in small samples such as biopsies, requiring the evaluation of more than one bioptic sample.[75]
Radiation therapy (also called radiotherapy) may be used to treat stomach cancer, often as an adjuvant to chemotherapy and/or surgery.[6]
The prognosis of stomach cancer is generally poor, due to the fact the tumour has often metastasised by the time of discovery and the fact that most people with the condition are elderly (median age is between 70 and 75 years) at presentation.[76] The five-year survival rate for stomach cancer is reported to be less than 10 percent.[6]
Almost 300 genes are related to outcomes in stomach cancer with both unfavorable genes where high expression related to poor survival and favorable genes where high expression associated with longer survival times.[77][78] Examples of poor prognosis genes include ITGAV and DUSP1.
Worldwide, stomach cancer is the fifth most-common cancer with 952,000 cases diagnosed in 2012.[14] It is more common both in men and in developing countries.[79][80] In 2012, it represented 8.5% of cancer cases in men, making it the fourth most-common cancer in men.[81] Also in 2012, the number of deaths was 700,000 having decreased slightly from 774,000 in 1990, making it the third-leading cause of cancer-related death (after lung cancer and liver cancer).[82][83]
Less than 5% of stomach cancers occur in people under 40 years of age with 81.1% of that 5% in the age-group of 30 to 39 and 18.9% in the age-group of 20 to 29.[84]
In 2014, stomach cancer resulted in 0.61% of deaths (13,303 cases) in the U.S.[85] In China, stomach cancer accounted for 3.56% of all deaths (324,439 cases).[86] The highest rate of stomach cancer was in Mongolia, at 28 cases per 100,000 people.[87]
In the United Kingdom, stomach cancer is the fifteenth most-common cancer (around 7,100 people were diagnosed with stomach cancer in 2011), and it is the tenth most-common cause of cancer-related deaths (around 4,800 people died in 2012).[88]
Incidence and mortality rates of gastric cancer vary greatly in Africa. The GLOBOCAN system is currently the most widely-used method to compare these rates between countries, but African incidence and mortality rates are seen to differ among countries, possibly due to the lack of universal access to a registry system for all countries.[89] Variation as drastic as estimated rates from 0.3/100000 in Botswana to 20.3/100000 in Mali have been observed.[89] In Uganda, the incidence of gastric cancer has increased from the 1960s measurement of 0.8/100000 to 5.6/100000.[89] Gastric cancer, though present, is relatively low when compared to countries with high incidence like Japan and China. One suspected cause of the variation within Africa and between other countries is due to different strains of the Helicobacter pylori bacteria. The trend commonly-seen is that H. pylori infection increases the risk for gastric cancer. However, this is not the case in Africa, giving this phenomenon the name the “African enigma.”[90] Although this bacteria is found in Africa, evidence has supported that different strains with mutations in the bacterial genotype may contribute to the difference in cancer development between African countries and others outside the continent.[90] However, increasing access to health care and treatment measures have been commonly-associated with the rising incidence, particularly in Uganda.[89]
The stomach is a muscular organ of the gastrointestinal tract that holds food and begins the digestive process by secreting gastric juice. The most common cancers of the stomach are adenocarcinomas but other histological types have been reported. Signs vary but may include vomiting (especially if blood is present), weight loss, anemia, and lack of appetite. Bowel movements may be dark and tarry in nature. In order to determine whether cancer is present in the stomach, special X-rays and/or abdominal ultrasound may be performed. Gastroscopy, a test using an instrument called endoscope to examine the stomach, is a useful diagnostic tool that can also take samples of the suspected mass for histopathological analysis to confirm or rule out cancer. The most definitive method of cancer diagnosis is through open surgical biopsy.[91] Most stomach tumors are malignant with evidence of spread to lymph nodes or liver, making treatment difficult. Except for lymphoma, surgery is the most frequent treatment option for stomach cancers but it is associated with significant risks.




Genetic testing - Wikipedia
Genetic testing, also known as DNA testing, allows the determination of bloodlines and the genetic diagnosis of vulnerabilities to inherited diseases. In agriculture, a form of genetic testing known as progeny testing can be used to evaluate the quality of breeding stock. In population ecology, genetic testing can be used to track genetic strengths and vulnerabilities of species populations.
In humans, genetic testing can be used to determine a child's parentage (genetic mother and father) or in general a person's ancestry or biological relationship between people. In addition to studying chromosomes to the level of individual genes, genetic testing in a broader sense includes biochemical tests for the possible presence of genetic diseases, or mutant forms of genes associated with increased risk of developing genetic disorders.
Genetic testing identifies changes in chromosomes, genes, or proteins.[1] The variety of genetic tests has expanded throughout the years. In the past, the main genetic tests searched for abnormal chromosome numbers and mutations that lead to rare, inherited disorders. Today, tests involve analyzing multiple genes to determine the risk of developing specific diseases or disorders, with the more common diseases consisting of heart disease and cancer.[2] The results of a genetic test can confirm or rule out a suspected genetic condition or help determine a person's chance of developing or passing on a genetic disorder. Several hundred genetic tests are currently in use, and more are being developed.[3][4]
Because genetic mutations can directly affect the structure of the proteins they code for, testing for specific genetic diseases can also be accomplished by looking at those proteins or their metabolites, or looking at stained or fluorescent chromosomes under a microscope.[5]
Genetic testing is "the analysis of chromosomes (DNA), proteins, and certain metabolites in order to detect heritable disease-related genotypes, mutations, phenotypes, or karyotypes for clinical purposes."[6] It can provide information about a person's genes and chromosomes throughout life. Available types of testing include:
Non-diagnostic testing includes:
Many diseases have a genetic component with tests already available. This list is continuously changing with additions of new test availabilities. This list below is just a few of the thousands of tests available.
Over-absorption of iron; accumulation of iron in vital organs (heart, liver, pancreas); organ damage; heart disease; cancer; liver disease; arthritis; diabetes; infertility; impotence[17]
Obstructive lung disease in adults; liver cirrhosis during childhood; when a newborn or infant has jaundice that lasts for an extended period of time (more than a week or two), an enlarged spleen, ascites (fluid accumulation in the abdominal cavity), pruritus (itching), and other signs of liver injury; persons under 40 years of age that develops wheezing, a chronic cough or bronchitis, is short of breath after exertion and/or shows other signs of emphysema (especially when the patient is not a smoker, has not been exposed to known lung irritants, and when the lung damage appears to be located low in the lungs); when you have a close relative with alpha-1 antitrypsin deficiency; when a patient has a decreased level of A1AT.
Elevation of both serum cholesterol and triglycerides; accelerated atherosclerosis, coronary heart disease; cutaneous xanthomas; peripheral vascular disease; diabetes mellitus, obesity or hypothyroidism. The APOE gene is also related to the development of complex genetic disorders like Alzheimer's disease.
Muscle weakness (rapidly progressive); frequent falls; difficulty with motor skills (running, hopping, jumping); progressive difficulty walking (ability to walk may be lost by age 12); fatigue; intellectual retardation (possible); skeletal deformities; chest and back (scoliosis); muscle deformities (contractures of heels, legs; pseudohypertrophy of calf muscles)
Reduced synthesis of the hemoglobin-beta chain; microcytic hypochromic anemia[18]
Venous thrombosis; certain arterial thrombotic conditions; patients with deep vein thrombosis, pulmonary embolism, cerebral vein thrombosis, and premature ischemic stroke and also of women with premature myocardial infarction; family history of early onset stroke, deep vein thrombosis, thromboembolism, pregnancy associated with thrombosis/embolism, hyperhomocysteinemia, and multiple miscarriage. Individuals with the mutation are at increased risk of thrombosis in the setting of oral contraceptive use, trauma, and surgery.
Venous thrombosis; pulmonary embolism; transient ischemic attack or premature stroke; peripheral vascular disease, particularly lower extremity; occlusive disease; cerebral vein thrombosis; multiple spontaneous abortions; intrauterine fetal demise
Venous thrombosis; increased plasma homocysteine levels
Independent risk factor for coronary artery disease, ischemic stroke, venous thrombosis (including osteonecrosis)
Uncontrolled division of cancer cells
Inflammation confined to the colon; abdominal pain and bloody diarrhea; anal fistulae and peri-rectal abscesses can also occur
Large amount of abnormally thick mucus in the lungs and intestines; leads to congestioni, pneumonia, diarrhea and poor growth
Congenital loss of hearing; -prelingual, non-syndromic deafness
Tendon xanthomas; elevated LDL cholesterol; premature heart disease
Predisposition of acute myeloid leukemia; skeletal abnormalities; radial hypoplasia and vertebral defect and other physical abnormalities, bone marrow failure (pancytopenia), endocrine dysfunction, early onset osteopenia/osteoporosis and lipid abnormalities, spontaneous chromosomal breakage exacerbated by exposure to DNA cross-linking agents.
Mental retardation or learning disabilities of unknown etiology; autism or autistic-like characteristics; women with premature menopause.
Subtle dysmorphism, log face with prominent mandible and large ears, macroorchidism in postpubertal males, behavioral abnormalities, due to lack of FMR1 in areas such as the cerebral cortex, amygdala, hippocampus and cerebellum
Characterized by slowly progressive ataxia; typically associated with depressed tendon reflexes, dysarthria, Babinski responses, and loss of position and vibration senses
Over-absorption of iron; accumulation of iron in vital organs (heart, liver, pancreas); organ damage; heart disease; cancer; liver disease; arthritis; diabetes; infertility; impotence
Absence of ganglia in the gut
Progressive disorder of motor, cognitive, and psychiatric disturbances.
Hypolactasia; persistent diarrhea; abdominal cramps; bloating; nausea; flatus
MEN2A (which affects 60% to 90% of MEN2 families):Medullary thyroid carcinoma; Pheochromocytoma (tumor of the adrenal glands); Parathyroid adenomas (benign [noncancerous] tumors) or hyperplasia (increased size) of the parathyroid gland; MEN2B (which affects 5% of MEN2 families): Medullary thyroid carcinoma; Pheochromocytoma; Mucosal neuromas (benign tumors of nerve tissue on the tongue and lips); Digestive problems; Muscle, joint, and spinal problems; Typical facial features; Familial medullary thyroid carcinoma (FMTC) (which affects 5% to 35% of MEN2 families):Medullary thyroid carcinoma only
Affects skeletal and smooth muscle as well as the eye, heart, endocrine system, and central nervous system; clinical findings, which span a continuum from mild to severe, have been categorized into three somewhat overlapping phenotypes: mild, classic, and congenital.
Pseudocholinesterase (also called butyrylcholinesterase or "BCHE") hydrolyzes a number of choline-based compounds including cocaine, heroin, procaine, and succinylcholine, mivacurium, and other fast-acting muscle relaxants.[20] Mutations in the BCHE gene lead to deficiency in the amount or function of the protein, which in turn results in a delay in the metabolism of these compounds, which prolongs their effects. Succinylcholine is commonly used as an anaesthetic in surgical procedures, and a person with BCHE mutations may suffer prolonged paraylasis. Between 1 in 3200 and 1 in 5000 people carry BCHE mutations; they are most prevalent in Persian Jews and Alaska Natives.[20][21] As of 2013 there are 9 genetic tests available.[22]
Variable degrees of hemolysis and intermittent episodes of vascular occlusion resulting in tissue ischemia and acute and chronic organ dysfunction; complications include anemia, jaundice, predisposition to aplastic crisis, sepsis, cholelithiasis, and delayed growth. Diagnosis suspected in infants or young children with painful swelling of the hands and feet, pallor, jaundice, pneumococcal sepsis or meningitis, severe anemia with splenic enlargement, or acute chest syndrome.
Lipids accumulate in the brain; neurological dysfunction; progressive weakness and loss of motor skills; decreased social interaction, seizures, blindness, and total debilitation
Cutaneous photosensitivity; acute neurovisceral crises
Genetic testing is often done as part of a genetic consultation and as of mid-2008 there were more than 1,200 clinically applicable genetic tests available.[23] Once a person decides to proceed with genetic testing, a medical geneticist, genetic counselor, primary care doctor, or specialist can order the test after obtaining informed consent.
Genetic tests are performed on a sample of blood, hair, skin, amniotic fluid (the fluid that surrounds a fetus during pregnancy), or other tissue. For example, a medical procedure called a buccal smear uses a small brush or cotton swab to collect a sample of cells from the inside surface of the cheek. Alternatively, a small amount of saline mouthwash may be swished in the mouth to collect the cells. The sample is sent to a laboratory where technicians look for specific changes in chromosomes, DNA, or proteins, depending on the suspected disorders, often using DNA sequencing. The laboratory reports the test results in writing to a person's doctor or genetic counselor.
Routine newborn screening tests are done on a small blood sample obtained by pricking the baby's heel with a lancet.
The physical risks associated with most genetic tests are very small, particularly for those tests that require only a blood sample or buccal smear (a procedure that samples cells from the inside surface of the cheek). The procedures used for prenatal testing carry a small but non-negligible risk of losing the pregnancy (miscarriage) because they require a sample of amniotic fluid or tissue from around the fetus.[24]
Many of the risks associated with genetic testing involve the emotional, social, or financial consequences of the test results. People may feel angry, depressed, anxious, or guilty about their results. The potential negative impact of genetic testing has led to an increasing recognition of a "right not to know".[25] In some cases, genetic testing creates tension within a family because the results can reveal information about other family members in addition to the person who is tested.[26] The possibility of genetic discrimination in employment or insurance is also a concern. Some individuals avoid genetic testing out of fear it will affect their ability to purchase insurance or find a job.[27] Health insurers do not currently require applicants for coverage to undergo genetic testing, and when insurers encounter genetic information, it is subject to the same confidentiality protections as any other sensitive health information.[28] In the United States, the use of genetic information is governed by the Genetic Information Nondiscrimination Act (GINA) (see discussion below in the section on government regulation).
Genetic testing can provide only limited information about an inherited condition. The test often can't determine if a person will show symptoms of a disorder, how severe the symptoms will be, or whether the disorder will progress over time. Another major limitation is the lack of treatment strategies for many genetic disorders once they are diagnosed.[24]
Another limitation to genetic testing for a hereditary linked cancer, is the variants of unknown clinical significance. Because the human genome has over 22,000 genes, there are 3.5 million variants in the average person's genome. These variants of unknown clinical significance means there is a change in the DNA sequence, however the increase for cancer is unclear because it is unknown if the change affects the gene's function.[29]
A genetics professional can explain in detail the benefits, risks, and limitations of a particular test. It is important that any person who is considering genetic testing understand and weigh these factors before making a decision.[24]
Other risks include accidental findings—a discovery of some possible problem found while looking for something else.[30] In 2013 the American College of Medical Genetics and Genomics (ACMG) that certain genes always be included any time a genomic sequencing was done, and that labs should report the results.[31]
Direct-to-consumer (DTC) genetic testing is a type of genetic test that is accessible directly to the consumer without having to go through a health care professional. Usually, to obtain a genetic test, health care professionals (such as doctors) acquire their patient's permission and then order the desired test. DTC genetic tests, however, allow consumers to bypass this process and order DNA tests themselves.
There is a variety of DTC tests, ranging from tests for breast cancer alleles to mutations linked to cystic fibrosis. Benefits of DTC testing are the accessibility of tests to consumers, promotion of proactive healthcare, and the privacy of genetic information. Possible additional risks of DTC testing are the lack of governmental regulation, the potential misinterpretation of genetic information, issues related to testing minors, privacy of data, and downstream expenses for the public health care system.[32]
DTC genetic testing has been controversial due to outspoken opposition within the medical community. Critics of DTC testing argue against the risks involved, the unregulated advertising and marketing claims, and the overall lack of governmental oversight.[33][34]
DTC testing involves many of the same risks associated with any genetic test. One of the more obvious and dangerous of these is the possibility of misreading of test results. Without professional guidance, consumers can potentially misinterpret genetic information, causing them to be deluded about their personal health.
Some advertising for DTC genetic testing has been criticized as conveying an exaggerated and inaccurate message about the connection between genetic information and disease risk, utilizing emotions as a selling factor. An advertisement for a BRCA-predictive genetic test for breast cancer stated: “There is no stronger antidote for fear than information.”[35]
Ancestry.com, a company providing DTC DNA tests for genealogy purposes, has reportedly allowed the warrantless search of their database by police investigating a murder.[36] The warrantless search led to a search warrant to force the gathering of a DNA sample from a New Orleans filmmaker; however he turned out not to be a match for the suspected killer.[37]
Currently, the U.S. has no strong federal regulation moderating the DTC market. Though there are several hundred tests available, only a handful are approved by the Food and Drug Administration (FDA); these are sold as at-home test kits, and are therefore considered "medical devices" over which the FDA may assert jurisdiction. Other types of DTC tests require customers to mail in DNA samples for testing; it is difficult for the FDA to exercise jurisdiction over these types of tests, because the actual testing is completed in the laboratories of providers. As of 2007, the FDA had not yet officially substantiated with scientific evidence the claimed accuracy of the majority of direct-to-consumer genetic tests.[38]
With regard to genetic testing and information in general, legislation in the United States called the Genetic Information Nondiscrimination Act prohibits group health plans and health insurers from denying coverage to a healthy individual or charging that person higher premiums based solely on a genetic predisposition to developing a disease in the future. The legislation also bars employers from using individuals’ genetic information when making hiring, firing, job placement, or promotion decisions.[39] The legislation, the first of its kind in the U.S.,[40] was passed by the United States Senate on April 24, 2008, on a vote of 95-0, and was signed into law by President George W. Bush on May 21, 2008.[41][42] It went into effect on November 21, 2009.
In June 2013 the US Supreme Court issued two rulings on human genetics. The Court struck down patents on human genes, opening up competition in the field of genetic testing.[43] The Supreme Court also ruled that police were allowed to collect DNA from people arrested for serious offenses.[44]
Some possible future ethical problems of genetic testing were considered in the science fiction film Gattaca, the novel Next, and the science fiction anime series "Gundam Seed". Also, some films which include the topic of genetic testing include The Island, Halloween: The Curse of Michael Myers, and the Resident Evil series.
The American Academy of Pediatrics (AAP) and the American College of Medical Genetics (ACMG) have provided new guidelines for the ethical issue of pediatrics genetic testing and screening of children in the United States.[45][46]  Their guidelines state that performing pediatric genetic testing should be in the best interest of the child. In hypothetical situations for adults getting genetically tested 84-98% expressing interest in getting genetically tested for cancer predisposition.[47]  Though only half who are at risk of would get tested. AAP and ACMG recommend holding off on genetic testing for late-onset conditions until adulthood. Unless diagnosing genetic disorders during childhood and start early intervention can reduce morbidity or mortality. They also state that with parents or guardians permission testing for asymptomatic children who are at risk of childhood onset conditions are ideal reasons for pediatrics genetic testing. Testing for pharmacogenetics and newborn screening is found to be acceptable by AAP and ACMG guidelines. 
Histocompatibility testing guideline states that it’s permissible for children of all ages to have tissue compatibility testing for immediate family members but only after the psychosocial, emotional and physical implications has been explored. With a donor advocate or similar mechanism should be in place to protect the minors from coercion and to safeguard the interest of said minor. 
Both AAP and ACMG discourage the use of direct-to-consumer and home kit genetic because of the accuracy, interpretation and oversight of test content. 
Guidelines also state that if parents or guardians should be encouraged to inform their child of the results from the genetic test if the minor is of appropriate age. If minor is of mature appropriate age and request results, the request should be honored. Though for ethical and legal reasons health care providers should be cautions in providing minors with predictive genetic testing without the involvement of parents or guardians. Within the guidelines AAP and ACMG state that health care provider have an obligation to inform parents or guardians on the implication of test results. To encourage patients and families to share information and even offer help in explain results to extend family or refer them to genetic counseling. 
AAP and ACMG state any type of predictive genetic testing for all types is best offer with genetic counseling being offer by Clinical genetics, genetic counselors or health care providers.[46]
Israel uses DNA testing to determine if people are eligible for immigration. The policy where "many Jews from the Former Soviet Union (‘FSU’) are asked to provide DNA confirmation of their Jewish heritage in the form of paternity tests in order to immigrate as Jews and become citizens under Israel's Law of Return" has generated controversy.[48][49][50][51]
The cost of genetic testing can range from under $100 to more than $2,000. This depends on the complexity of the test. The cost will increase if more than one test is necessary or if multiple family members are getting tested to obtain additional results. Costs can vary by state and some states cover part of the total cost.
From the date that a sample is taken, results may take weeks to months, depending upon the complexity and extent of the tests being performed. Results for prenatal testing are usually available more quickly because time is an important consideration in making decisions about a pregnancy. Prior to the testing, the doctor or genetic counselor who is requesting a particular test can provide specific information about the cost and time frame associated with that test.[52]
 This article incorporates public domain material from the United States Department of Health and Human Services document "What are the risks and limitations of genetic testing?".
 This article incorporates public domain material from the United States Department of Health and Human Services document "What is the cost of genetic testing, and how long does it take to get the results?".



Chemotherapy - Wikipedia

Chemotherapy (often abbreviated to chemo and sometimes CTX or CTx) is a type of cancer treatment that uses one or more anti-cancer drugs (chemotherapeutic agents) as part of a standardized chemotherapy regimen. Chemotherapy may be given with a curative intent (which almost always involves combinations of drugs), or it may aim to prolong life or to reduce symptoms (palliative chemotherapy). Chemotherapy is one of the major categories of the medical discipline specifically devoted to pharmacotherapy for cancer, which is called medical oncology.
The term chemotherapy has come to connote non-specific usage of intracellular poisons to inhibit mitosis, cell division. The connotation excludes more selective agents that block extracellular signals (signal transduction). The development of therapies with specific molecular or genetic targets, which inhibit growth-promoting signals from classic endocrine hormones (primarily estrogens for breast cancer and androgens for prostate cancer) are now called hormonal therapies. By contrast, other inhibitions of growth-signals like those associated with receptor tyrosine kinases are referred to as targeted therapy.
Importantly, the use of drugs (whether chemotherapy, hormonal therapy or targeted therapy) constitutes systemic therapy for cancer in that they are introduced into the blood stream and are therefore in principle able to address cancer at any anatomic location in the body. Systemic therapy is often used in conjunction with other modalities that constitute local therapy (i.e. treatments whose efficacy is confined to the anatomic area where they are applied) for cancer such as radiation therapy, surgery or hyperthermia therapy.
Traditional chemotherapeutic agents are cytotoxic by means of interfering with cell division (mitosis) but cancer cells vary widely in their susceptibility to these agents. To a large extent, chemotherapy can be thought of as a way to damage or stress cells, which may then lead to cell death if apoptosis is initiated. Many of the side effects of chemotherapy can be traced to damage to normal cells that divide rapidly and are thus sensitive to anti-mitotic drugs: cells in the bone marrow, digestive tract and hair follicles. This results in the most common side-effects of chemotherapy: myelosuppression (decreased production of blood cells, hence also immunosuppression), mucositis (inflammation of the lining of the digestive tract), and alopecia (hair loss). Because of the effect on immune cells (especially lymphocytes), chemotherapy drugs often find use in a host of diseases that result from harmful overactivity of the immune system against self (so-called autoimmunity). These include rheumatoid arthritis, systemic lupus erythematosus, multiple sclerosis, vasculitis and many others.
There are a number of strategies in the administration of chemotherapeutic drugs used today. Chemotherapy may be given with a curative intent or it may aim to prolong life or to palliate symptoms.
All chemotherapy regimens require that the recipient be capable of undergoing the treatment. Performance status is often used as a measure to determine whether a person can receive chemotherapy, or whether dose reduction is required.  Because only a fraction of the cells in a tumor die with each treatment (fractional kill), repeated doses must be administered to continue to reduce the size of the tumor.[7] Current chemotherapy regimens apply drug treatment in cycles, with the frequency and duration of treatments limited by toxicity.[8]
The efficacy of chemotherapy depends on the type of cancer and the stage. The overall effectiveness ranges from being curative for some cancers, such as some leukemias,[9][10] to being ineffective, such as in some brain tumors,[11] to being needless in others, like most non-melanoma skin cancers.[12]
Dosage of chemotherapy can be difficult: If the dose is too low, it will be ineffective against the tumor, whereas, at excessive doses, the toxicity (side-effects) will be intolerable to the person receiving it.[1] The standard method of determining chemotherapy dosage is based on calculated body surface area (BSA). The BSA is usually calculated with a mathematical formula or a nomogram, using the recipient's weight and height, rather than by direct measurement of body area. This formula was originally derived in a 1916 study and attempted to translate medicinal doses established with laboratory animals to equivalent doses for humans.[13] The study only included 9 human subjects.[14] When chemotherapy was introduced in the 1950s, the BSA formula was adopted as the official standard for chemotherapy dosing for lack of a better option.[15][16]
Recently, the validity of this method in calculating uniform doses has been questioned. The reason for this is that the formula only takes into account the individual's weight and height. Drug absorption and clearance are influenced by multiple factors, including age, gender, metabolism, disease state, organ function, drug-to-drug interactions, genetics, and obesity, which has a major impact on the actual concentration of the drug in the person's bloodstream.[15][17][18] As a result, there is high variability in the systemic chemotherapy drug concentration in people dosed by BSA, and this variability has been demonstrated to be more than 10-fold for many drugs.[14][19]   In other words, if two people receive the same dose of a given drug based on BSA, the concentration of that drug in the bloodstream of one person may be 10 times higher or lower compared to that of the other person.[19] This variability is typical with many chemotherapy drugs dosed by BSA, and, as shown below, was demonstrated in a study of 14 common chemotherapy drugs.[14]
The result of this pharmacokinetic variability among people, is that many people do not receive the right dose to achieve optimal treatment effectiveness with minimized toxic side effects. Some people are overdosed while others are underdosed.[15][17][18][20][21][22][23] For example, in a randomized clinical trial, investigators found 85% of metastatic colorectal cancer patients treated with 5-fluorouracil (5-FU) did not receive the optimal therapeutic dose when dosed by the BSA standard—68% were underdosed and 17% were overdosed.[20]
There has been controversy over the use of BSA to calculate chemotherapy doses for people who are obese.[24] Because of their higher BSA, clinicians often arbitrarily reduce the dose prescribed by the BSA formula for fear of overdosing.[24] In many cases, this can result in sub-optimal treatment.[24]
Several clinical studies have demonstrated that when chemotherapy dosing is individualized to achieve optimal systemic drug exposure, treatment outcomes are improved and toxic side effects are reduced.[20][22] In the 5-FU clinical study cited above, people whose dose was adjusted to achieve a pre-determined target exposure realized an 84% improvement in treatment response rate and a six-month improvement in overall survival (OS) compared with those dosed by BSA.[20]
In the same study, investigators compared the incidence of common 5-FU-associated grade 3/4 toxicities between the dose-adjusted people and people dosed per BSA.[20] The incidence of debilitating grades of diarrhea was reduced from 18% in the BSA-dosed group to 4% in the dose-adjusted group and serious hematologic side effects were eliminated.[20]  Because of the reduced toxicity, dose-adjusted patients were able to be treated for longer periods of time.[20] BSA-dosed people were treated for a total of 680 months while people in the dose-adjusted group were treated for a total of 791 months.[20] Completing the course of treatment is an important factor in achieving better treatment outcomes.
Similar results were found in a study involving people with colorectal cancer who were treated with the popular FOLFOX regimen.[22] The incidence of serious diarrhea was reduced from 12% in the BSA-dosed group of patients to 1.7% in the dose-adjusted group, and the incidence of severe mucositis was reduced from 15% to 0.8%.[22]
The FOLFOX study also demonstrated an improvement in treatment outcomes.[22] Positive response increased from 46% in the BSA-dosed group to 70% in the dose-adjusted group.  Median progression free survival (PFS) and overall survival (OS) both improved by six months in the dose adjusted group.[22]
One approach that can help clinicians individualize chemotherapy dosing is to measure the drug levels in blood plasma over time and adjust dose according to a formula or algorithm to achieve optimal exposure. With an established target exposure for optimized treatment effectiveness with minimized toxicities, dosing can be personalized to achieve target exposure and optimal results for each person.  Such an algorithm was used in the clinical trials cited above and resulted in significantly improved treatment outcomes.
Oncologists are already individualizing dosing of some cancer drugs based on exposure. Carboplatin[25]:4 and busulfan[26][27] dosing rely upon results from blood tests to calculate the optimal dose for each person. Simple blood tests are also available for dose optimization of methotrexate,[28] 5-FU, paclitaxel, and docetaxel.[29][30]
Alkylating agents are the oldest group of chemotherapeutics in use today. Originally derived from mustard gas used in World War I, there are now many types of alkylating agents in use.[1] They are so named because of their ability to alkylate many molecules, including proteins, RNA and DNA. This ability to bind covalently to DNA via their alkyl group is the primary cause for their anti-cancer effects.[32] DNA is made of two strands and the molecules may either bind twice to one strand of DNA (intrastrand crosslink) or may bind once to both strands (interstrand crosslink). If the cell tries to replicate crosslinked DNA during cell division, or tries to repair it, the DNA strands can break. This leads to a form of programmed cell death called apoptosis.[31][33] Alkylating agents will work at any point in the cell cycle and thus are known as cell cycle-independent drugs. For this reason the effect on the cell is dose dependent; the fraction of cells that die is directly proportional to the dose of drug.[34]
The subtypes of alkylating agents are the nitrogen mustards, nitrosoureas, tetrazines, aziridines,[35] cisplatins and derivatives, and non-classical alkylating agents. Nitrogen mustards include mechlorethamine, cyclophosphamide, melphalan, chlorambucil, ifosfamide and busulfan. Nitrosoureas include N-Nitroso-N-methylurea (MNU), carmustine (BCNU), lomustine (CCNU) and semustine (MeCCNU), fotemustine and streptozotocin. Tetrazines include dacarbazine, mitozolomide and temozolomide. Aziridines include thiotepa, mytomycin and diaziquone (AZQ). Cisplatin and derivatives include cisplatin, carboplatin and oxaliplatin.[32][33] They impair cell function by forming covalent bonds with the amino, carboxyl, sulfhydryl, and phosphate groups in biologically important molecules.[36] Non-classical alkylating agents include procarbazine and hexamethylmelamine.[32][33]
Anti-metabolites are a group of molecules that impede DNA and RNA synthesis. Many of them have a similar structure to the building blocks of DNA and RNA. The building blocks are nucleotides; a molecule comprising a nucleobase, a sugar and a phosphate group. The nucleobases are divided into purines (guanine and adenine) and pyrimidines (cytosine, thymine and uracil). Anti-metabolites resemble either nucleobases or nucleosides (a nucleotide without the phosphate group), but have altered chemical groups.[37] These drugs exert their effect by either blocking the enzymes required for DNA synthesis or becoming incorporated into DNA or RNA. By inhibiting the enzymes involved in DNA synthesis, they prevent mitosis because the DNA cannot duplicate itself. Also, after misincorporation of the molecules into DNA, DNA damage can occur and programmed cell death (apoptosis) is induced. Unlike alkylating agents, anti-metabolites are cell cycle dependent. This means that they only work during a specific part of the cell cycle, in this case S-phase (the DNA synthesis phase). For this reason, at a certain dose, the effect plateaus and proportionally no more cell death occurs with increased doses. Subtypes of the anti-metabolites are the anti-folates, fluoropyrimidines, deoxynucleoside analogues and thiopurines.[32][37]
The anti-folates include methotrexate and pemetrexed. Methotrexate inhibits dihydrofolate reductase (DHFR), an enzyme that regenerates tetrahydrofolate from dihydrofolate. When the enzyme is inhibited by methotrexate, the cellular levels of folate coenzymes diminish. These are required for thymidylate and purine production, which are both essential for DNA synthesis and cell division.[3]:55–59[4]:11 Pemetrexed is another anti-metabolite that affects purine and pyrimidine production, and therefore also inhibits DNA synthesis. It primarily inhibits the enzyme thymidylate synthase, but also has effects on DHFR, aminoimidazole carboxamide ribonucleotide formyltransferase and glycinamide ribonucleotide formyltransferase.[38] The fluoropyrimidines include fluorouracil and capecitabine. Fluorouracil is a nucleobase analogue that is metabolised in cells to form at least two active products; 5-fluourouridine monophosphate (FUMP) and 5-fluoro-2'-deoxyuridine 5'-phosphate (fdUMP). FUMP becomes incorporated into RNA and fdUMP inhibits the enzyme thymidylate synthase; both of which lead to cell death.[4]:11 Capecitabine is a prodrug of 5-fluorouracil that is broken down in cells to produce the active drug.[39] The deoxynucleoside analogues include cytarabine, gemcitabine, decitabine, azacitidine, fludarabine, nelarabine, cladribine, clofarabine, and pentostatin. The thiopurines include thioguanine and mercaptopurine.[32][37]
Anti-microtubule agents are plant-derived chemicals that block cell division by preventing microtubule function. Microtubules are an important cellular structure composed of two proteins; α-tubulin and β-tubulin. They are hollow rod shaped structures that are required for cell division, among other cellular functions.[40] Microtubules are dynamic structures, which means that they are permanently in a state of assembly and disassembly. Vinca alkaloids and taxanes are the two main groups of anti-microtubule agents, and although both of these groups of drugs cause microtubule dysfunction, their mechanisms of action are completely opposite. The vinca alkaloids prevent the formation of the microtubules, whereas the taxanes prevent the microtubule disassembly. By doing so, they prevent the cancer cells from completing mitosis. Following this, cell cycle arrest occurs, which induces programmed cell death (apoptosis).[32][41] Also, these drugs can affect blood vessel growth; an essential process that tumours utilise in order to grow and metastasise.[41]
Vinca alkaloids are derived from the Madagascar periwinkle, Catharanthus roseus[42][43] (formerly known as Vinca rosea). They bind to specific sites on tubulin, inhibiting the assembly of tubulin into microtubules. The original vinca alkaloids are natural products that include vincristine and vinblastine.[44][45][46][47] Following the success of these drugs, semi-synthetic vinca alkaloids were produced: vinorelbine (used in the treatment of non-small-cell lung cancer[46][48][49]), vindesine, and vinflunine.[41] These drugs are cell cycle-specific. They bind to the tubulin molecules in S-phase and prevent proper microtubule formation required for M-phase.[34]
Taxanes are natural and semi-synthetic drugs. The first drug of their class, paclitaxel, was originally extracted from the Pacific Yew tree, Taxus brevifolia.  Now this drug and another in this class, docetaxel, are produced semi-synthetically from a chemical found in the bark of another Yew tree; Taxus baccata. These drugs promote microtubule stability, preventing their disassembly. Paclitaxel prevents the cell cycle at the boundary of G2-M, whereas docetaxel exerts its effect during S-phase. Taxanes present difficulties in formulation as medicines because they are poorly soluble in water.[41]
Podophyllotoxin is an antineoplastic lignan obtained primarily from the American Mayapple (Podophyllum peltatum) and Himalayan Mayapple (Podophyllum hexandrum or Podophyllum emodi). It has anti-microtubule activity, and its mechanism is similar to that of vinca alkaloids in that they bind to tubulin, inhibiting microtubule formation. Podophyllotoxin is used to produce two other drugs with different mechanisms of action: etoposide and teniposide.[50][51]
Topoisomerase inhibitors are drugs that affect the activity of two enzymes: topoisomerase I and topoisomerase II. When the DNA double-strand helix is unwound, during DNA replication or transcription, for example, the adjacent unopened DNA winds tighter (supercoils), like opening the middle of a twisted rope. The stress caused by this effect is in part aided by the topoisomerase enzymes. They produce single- or double-strand breaks into DNA, reducing the tension in the DNA strand. This allows the normal unwinding of DNA to occur during replication or transcription. Inhibition of topoisomerase I or II interferes with both of these processes.[52][53]
Two topoisomerase I inhibitors, irinotecan and topotecan, are semi-synthetically derived from camptothecin, which is obtained from the Chinese ornamental tree Camptotheca acuminata.[34] Drugs that target topoisomerase II can be divided into two groups. The topoisomerase II poisons cause increased levels enzymes bound to DNA. This prevents DNA replication and transcription, causes DNA strand breaks, and leads to programmed cell death (apoptosis). These agents include etoposide, doxorubicin, mitoxantrone and teniposide. The second group, catalytic inhibitors, are drugs that block the activity of topoisomerase II, and therefore prevent DNA synthesis and translation because the DNA cannot unwind properly. This group includes novobiocin, merbarone, and aclarubicin, which also have other significant mechanisms of action.[54]
The cytotoxic antibiotics are a varied group of drugs that have various mechanisms of action. The common theme that they share in their chemotherapy indication is that they interrupt cell division. The most important subgroup is the anthracyclines and the bleomycins; other prominent examples include mitomycin C, mitoxantrone, and actinomycin.[55]
Among the anthracyclines, doxorubicin and daunorubicin were the first, and were obtained from the bacterium Streptomyces peucetius.[56]   Derivatives of these compounds include epirubicin and idarubicin. Other clinically used drugs in the anthracyline group are pirarubicin, aclarubicin, and mitoxantrone. The mechanisms of anthracyclines include DNA intercalation (molecules insert between the two strands of DNA), generation of highly reactive free radicals that damage intercellular molecules and topoisomerase inhibition.[57]
Actinomycin is a complex molecule that intercalates DNA and prevents RNA synthesis.[58]
Bleomycin, a glycopeptide isolated from Streptomyces verticillus, also intercalates DNA, but produces free radicals that damage DNA. This occurs when bleomycin binds to a metal ion, becomes chemically reduced and reacts with oxygen.[59][3]:87
Mitomycin is a cytotoxic antibiotic with the ability to alkylate DNA.[60]
Most chemotherapy is delivered intravenously, although a number of agents can be administered orally (e.g., melphalan, busulfan, capecitabine).
There are many intravenous methods of drug delivery, known as vascular access devices. These include the winged infusion device, peripheral venous catheter, midline catheter, peripherally inserted central catheter (PICC), central venous catheter and implantable port. The devices have different applications regarding duration of chemotherapy treatment, method of delivery and types of chemotherapeutic agent.[4]:94–95
Depending on the person, the cancer, the stage of cancer, the type of chemotherapy, and the dosage, intravenous chemotherapy may be given on either an inpatient or an outpatient basis. For continuous, frequent or prolonged intravenous chemotherapy administration, various systems may be surgically inserted into the vasculature to maintain access.[4]:113–118 Commonly used systems are the Hickman line, the Port-a-Cath, and the PICC line. These have a lower infection risk, are much less prone to phlebitis or extravasation, and eliminate the need for repeated insertion of peripheral cannulae.[citation needed]
Isolated limb perfusion (often used in melanoma),[61] or isolated infusion of chemotherapy into the liver[62] or the lung have been used to treat some tumors. The main purpose of these approaches is to deliver a very high dose of chemotherapy to tumor sites without causing overwhelming systemic damage.[63] These approaches can help control solitary or limited metastases, but they are by definition not systemic, and, therefore, do not treat distributed metastases or micrometastases.
Topical chemotherapies, such as 5-fluorouracil, are used to treat some cases of non-melanoma skin cancer.[64]
If the cancer has central nervous system involvement, or with meningeal disease, intrathecal chemotherapy may be administered.[1]
Chemotherapeutic techniques have a range of side-effects that depend on the type of medications used. The most common medications affect mainly the fast-dividing cells of the body, such as blood cells and the cells lining the mouth, stomach, and intestines. Chemotherapy-related toxicities can occur acutely after administration, within hours or days, or chronically, from weeks to years.[3]:265
Virtually all chemotherapeutic regimens can cause depression of the immune system, often by paralysing the bone marrow and leading to a decrease of white blood cells, red blood cells, and platelets.
Anemia and thrombocytopenia may require blood transfusion. Neutropenia (a decrease of the neutrophil granulocyte count below 0.5 x 109/litre) can be improved with synthetic G-CSF (granulocyte-colony-stimulating factor, e.g., filgrastim, lenograstim).
In very severe myelosuppression, which occurs in some regimens, almost all the bone marrow stem cells (cells that produce white and red blood cells) are destroyed, meaning allogenic or autologous bone marrow cell transplants are necessary. (In autologous BMTs, cells are removed from the person before the treatment, multiplied and then re-injected afterward; in allogenic BMTs, the source is a donor.) However, some people still develop diseases because of this interference with bone marrow.[citation needed]
Although people receiving chemotherapy are encouraged to wash their hands, avoid sick people, and take other infection-reducing steps, about 85% of infections are due to naturally occurring microorganisms in the person's own gastrointestinal tract (including oral cavity) and skin.[65]:130 This may manifest as systemic infections, such as sepsis, or as localized outbreaks, such as Herpes simplex, shingles, or other members of the Herpesviridea.[66] The risk of illness and death can be reduced by taking common antibiotics such as quinolones or trimethoprim/sulfamethoxazole before any fever or sign of infection appears.[67] Quinolones show effective prophylaxis mainly with hematological cancer.[67] However, in general, for every five people who are immunosuppressed following chemotherapy who take an antibiotic, one fever can be prevented; for every 34 who take an antibiotic, one death can be prevented.[67] Sometimes, chemotherapy treatments are postponed because the immune system is suppressed to a critically low level.
In Japan, the government has approved the use of some medicinal mushrooms like Trametes versicolor, to counteract depression of the immune system in people undergoing chemotherapy.[68]
Due to immune system suppression, neutropenic enterocolitis (typhlitis) is a "life-threatening gastrointestinal complication of chemotherapy."[69] Typhlitis is an intestinal infection which may manifest itself through symptoms including nausea, vomiting, diarrhea, a distended abdomen, fever, chills, or abdominal pain and tenderness.
Typhlitis is a medical emergency. It has a very poor prognosis and is often fatal unless promptly recognized and aggressively treated.[70] Successful treatment hinges on early diagnosis provided by a high index of suspicion and the use of CT scanning, nonoperative treatment for uncomplicated cases, and sometimes elective right hemicolectomy to prevent recurrence.[70]
Nausea, vomiting, anorexia, diarrhoea, abdominal cramps, and constipation are common side-effects of chemotherapeutic medications that kill fast-dividing cells.[71] Malnutrition and dehydration can result when the recipient does not eat or drink enough, or when the person vomits frequently, because of gastrointestinal damage. This can result in rapid weight loss, or occasionally in weight gain, if the person eats too much in an effort to allay nausea or heartburn. Weight gain can also be caused by some steroid medications. These side-effects can frequently be reduced or eliminated with antiemetic drugs. Self-care measures, such as eating frequent small meals and drinking clear liquids or ginger tea, are often recommended. In general, this is a temporary effect, and frequently resolves within a week of finishing treatment. However, a high index of suspicion is appropriate, since diarrhea and bloating are also symptoms of typhlitis, a very serious and potentially life-threatening medical emergency that requires immediate treatment.
Anemia can be a combined outcome caused by myelosuppressive chemotherapy, and possible cancer-related causes such as bleeding, blood cell destruction (hemolysis), hereditary disease, kidney dysfunction, nutritional
deficiencies or anemia of chronic disease. Treatments to mitigate anemia include hormones to boost blood production (erythropoietin), iron supplements, and blood transfusions.[72][73][74] Myelosuppressive therapy can cause a tendency to bleed easily, leading to anemia. Medications that kill rapidly dividing cells or blood cells can reduce the number of platelets in the blood, which can result in bruises and bleeding. Extremely low platelet counts may be temporarily boosted through platelet transfusions and new drugs to increase platelet counts during chemotherapy are being developed.[75][76] Sometimes, chemotherapy treatments are postponed to allow platelet counts to recover.
Fatigue may be a consequence of the cancer or its treatment, and can last for months to years after treatment. One physiological cause of fatigue is anemia, which can be caused by chemotherapy, surgery, radiotherapy, primary and metastatic disease or nutritional depletion.[77][78] Anaerobic exercise has been found to be beneficial in reducing fatigue in people with solid tumours.[79]
Nausea and vomiting are two of the most feared cancer treatment-related side-effects for people with cancer and their families. In 1983, Coates et al. found that people receiving chemotherapy ranked nausea and vomiting as the first and second most severe side-effects, respectively. Up to 20% of people receiving highly emetogenic agents in this era postponed, or even refused, potentially curative treatments.[80] Chemotherapy-induced nausea and vomiting (CINV) are common with many treatments and some forms of cancer. Since the 1990s, several novel classes of antiemetics have been developed and commercialized, becoming a nearly universal standard in chemotherapy regimens, and helping to successfully manage these symptoms in many people. Effective mediation of these unpleasant and sometimes-crippling symptoms results in increased quality of life for the recipient and more efficient treatment cycles, due to less stoppage of treatment due to better tolerance and better overall health.
Hair loss (alopecia) can be caused by chemotherapy that kills rapidly dividing cells; other medications may cause hair to thin.  These are most often temporary effects: hair usually starts to regrow a few weeks after the last treatment, but sometimes with a change in colour, texture, thickness or style. Sometimes hair has a tendency to curl after regrowth, resulting in "chemo curls." Severe hair loss occurs most often with drugs such as doxorubicin, daunorubicin, paclitaxel, docetaxel, cyclophosphamide, ifosfamide and etoposide. Permanent thinning or hair loss can result from some standard chemotherapy regimens.
Chemotherapy induced hair loss occurs by a non-androgenic mechanism, and can manifest as alopecia totalis, telogen effluvium, or less often alopecia areata.[81] It is usually associated with systemic treatment due to the high mitotic rate of hair follicles, and more reversible than androgenic hair loss,[82][83] although permanent cases can occur.[84] Chemotherapy induces hair loss in women more often than men.[85]
Scalp cooling offers a means of preventing both permanent and temporary hair loss; however, concerns about this method have been raised.[86][87]
Development of secondary neoplasia after successful chemotherapy or radiotherapy treatment can occur. The most common secondary neoplasm is secondary acute myeloid leukemia, which develops primarily after treatment with alkylating agents or topoisomerase inhibitors.[88] Survivors of childhood cancer are more than 13 times as likely to get a secondary neoplasm during the 30 years after treatment than the general population.[89] Not all of this increase can be attributed to chemotherapy.
Some types of chemotherapy are gonadotoxic and may cause infertility.[90] Chemotherapies with high risk include procarbazine and other alkylating drugs such as cyclophosphamide, ifosfamide, busulfan, melphalan, chlorambucil, and chlormethine.[90] Drugs with medium risk include doxorubicin and platinum analogs such as cisplatin and carboplatin.[90] On the other hand, therapies with low risk of gonadotoxicity include plant derivatives such as vincristine and vinblastine, antibiotics such as bleomycin and dactinomycin, and antimetabolites such as methotrexate, mercaptopurine, and 5-fluorouracil.[90]
Female infertility by chemotherapy appears to be secondary to premature ovarian failure by loss of primordial follicles.[91] This loss is not necessarily a direct effect of the chemotherapeutic agents, but could be due to an increased rate of growth initiation to replace damaged developing follicles.[91]
People may choose between several methods of fertility preservation prior to chemotherapy, including cryopreservation of semen, ovarian tissue, oocytes, or embryos.[92] As more than half of cancer patients are elderly, this adverse effect is only relevant for a minority of patients. A study in France between 1999 and 2011 came to the result that embryo freezing before administration of gonadotoxic agents to females caused a delay of treatment in 34% of cases, and a live birth in 27% of surviving cases who wanted to become pregnant, with the follow-up time varying between 1 and 13 years.[93]
Potential protective or attenuating agents include GnRH analogs, where several studies have shown a protective effect in vivo in humans, but some studies show no such effect. Sphingosine-1-phosphate (S1P) has shown similar effect, but its mechanism of inhibiting the sphingomyelin apoptotic pathway may also interfere with the apoptosis action of chemotherapy drugs.[94]
In chemotherapy as a conditioning regimen in hematopoietic stem cell transplantation, a study of people conditioned with cyclophosphamide alone for severe aplastic anemia came to the result that ovarian recovery occurred in all women younger than 26 years at time of transplantation, but only in five of 16 women older than 26 years.[95]
Chemotherapy is teratogenic during pregnancy, especially during the first trimester, to the extent that abortion usually is recommended if pregnancy in this period is found during chemotherapy.[96] Second- and third-trimester exposure does not usually increase the teratogenic risk and adverse effects on cognitive development, but it may increase the risk of various complications of pregnancy and fetal myelosuppression.[96]
In males previously having undergone chemotherapy or radiotherapy, there appears to be no increase in genetic defects or congenital malformations in their children conceived after therapy.[96] The use of assisted reproductive technologies and micromanipulation techniques might increase this risk.[96] In females previously having undergone chemotherapy, miscarriage and congenital malformations are not increased in subsequent conceptions.[96] However, when in vitro fertilization and embryo cryopreservationis practised between or shortly after treatment, possible genetic risks to the growing oocytes exist, and hence it has been recommended that the babies be screened.[96]
Between 30 and 40 percent of people undergoing chemotherapy experience chemotherapy-induced peripheral neuropathy (CIPN), a progressive, enduring, and often irreversible condition, causing pain, tingling, numbness and sensitivity to cold, beginning in the hands and feet and sometimes progressing to the arms and legs.[97] Chemotherapy drugs associated with CIPN include thalidomide, epothilones, vinca alkaloids, taxanes, proteasome inhibitors, and the platinum-based drugs.[97][98] Whether CIPN arises, and to what degree, is determined by the choice of drug, duration of use, the total amount consumed and whether the person already has peripheral neuropathy. Though the symptoms are mainly sensory, in some cases motor nerves and the autonomic nervous system are affected.[99] CIPN often follows the first chemotherapy dose and increases in severity as treatment continues, but this progression usually levels off at completion of treatment. The platinum-based drugs are the exception; with these drugs, sensation may continue to deteriorate for several months after the end of treatment.[100] Some CIPN appears to be irreversible.[100] Pain can often be managed with drug or other treatment but the numbness is usually resistant to treatment.[101]
Some people receiving chemotherapy report fatigue or non-specific neurocognitive problems, such as an inability to concentrate; this is sometimes called post-chemotherapy cognitive impairment, referred to as "chemo brain" in popular and social media.[102]
In particularly large tumors and cancers with high white cell counts, such as lymphomas, teratomas, and some leukemias, some people develop tumor lysis syndrome. The rapid breakdown of cancer cells causes the release of chemicals from the inside of the cells. Following this, high levels of uric acid, potassium and phosphate are found in the blood. High levels of phosphate induce secondary hypoparathyroidism, resulting in low levels of calcium in the blood.  This causes kidney damage and the high levels of potassium can cause cardiac arrhythmia. Although prophylaxis is available and is often initiated in people with large tumors, this is a dangerous side-effect that can lead to death if left untreated.[4]:202
Cardiotoxicity (heart damage) is especially prominent with the use of anthracycline drugs (doxorubicin, epirubicin, idarubicin, and liposomal doxorubicin). The cause of this is most likely due to the production of free radicals in the cell and subsequent DNA damage. Other chemotherapeutic agents that cause cardiotoxicity, but at a lower incidence, are cyclophosphamide, docetaxel and clofarabine.[103]
Hepatotoxicity (liver damage) can be caused by many cytotoxic drugs. The susceptibility of an individual to liver damage can be altered by other factors such as the cancer itself, viral hepatitis, immunosuppression and nutritional deficiency. The liver damage can consist of damage to liver cells, hepatic sinusoidal syndrome (obstruction of the veins in the liver), cholestasis (where bile does not flow from the liver to the intestine) and liver fibrosis.[104][105]
Nephrotoxicity (kidney damage) can be caused by tumor lysis syndrome and also due direct effects of drug clearance by the kidneys. Different drugs will affect different parts of the kidney and the toxicity may be asymptomatic (only seen on blood or urine tests) or may cause acute renal failure.[106][107]
Ototoxicity (damage to the inner ear) is a common side effect of platinum based drugs that can produce symptoms such as dizziness and vertigo.[108][109]
Less common side-effects include red skin (erythema), dry skin, damaged fingernails, a dry mouth (xerostomia), water retention, and sexual impotence. Some medications can trigger allergic or pseudoallergic reactions.
Specific chemotherapeutic agents are associated with organ-specific toxicities, including cardiovascular disease (e.g., doxorubicin), interstitial lung disease (e.g., bleomycin) and occasionally secondary neoplasm (e.g., MOPP therapy for Hodgkin's disease).
Hand-foot syndrome is another side effect to cytotoxic chemotherapy.
Chemotherapy does not always work, and even when it is useful, it may not completely destroy the cancer. People frequently fail to understand its limitations. In one study of people who had been newly diagnosed with incurable, stage 4 cancer, more than two-thirds of people with lung cancer and more than four-fifths of people with colorectal cancer still believed that chemotherapy was likely to cure their cancer.[110]
The blood–brain barrier poses an obstacle to delivery of chemotherapy to the brain. This is because the brain has an extensive system in place to protect it from harmful chemicals. Drug transporters can pump out drugs from the brain and brain's blood vessel cells into the cerebrospinal fluid and blood circulation. These transporters pump out most chemotherapy drugs, which reduces their efficacy for treatment of brain tumors. Only small lipophilic alkylating agents such as lomustine or temozolomide are able to cross this blood–brain barrier.[111][112][113]
Blood vessels in tumors are very different from those seen in normal tissues. As a tumor grows, tumor cells furthest away from the blood vessels become low in oxygen (hypoxic). To counteract this they then signal for new blood vessels to grow. The newly formed tumor vasculature is poorly formed and does not deliver an adequate blood supply to all areas of the tumor. This leads to issues with drug delivery because many drugs will be delivered to the tumor by the circulatory system.[114]
Resistance is a major cause of treatment failure in chemotherapeutic drugs. There are a few possible causes of resistance in cancer, one of which is the presence of small pumps on the surface of cancer cells that actively move chemotherapy from inside the cell to the outside. Cancer cells produce high amounts of these pumps, known as p-glycoprotein, in order to protect themselves from chemotherapeutics. Research on p-glycoprotein and other such chemotherapy efflux pumps is currently ongoing. Medications to inhibit the function of p-glycoprotein are undergoing investigation, but due to toxicities and interactions with anti-cancer drugs their development has been difficult.[115][116] Another mechanism of resistance is gene amplification, a process in which multiple copies of a gene are produced by cancer cells. This overcomes the effect of drugs that reduce the expression of genes involved in replication. With more copies of the gene, the drug can not prevent all expression of the gene and therefore the cell can restore its proliferative ability. Cancer cells can also cause defects in the cellular pathways of apoptosis (programmed cell death). As most chemotherapy drugs kill cancer cells in this manner, defective apoptosis allows survival of these cells, making them resistant. Many chemotherapy drugs also cause DNA damage, which can be repaired by enzymes in the cell that carry out DNA repair. Upregulation of these genes can overcome the DNA damage and prevent the induction of apoptosis. Mutations in genes that produce drug target proteins, such as tubulin, can occur which prevent the drugs from binding to the protein, leading to resistance to these types of drugs.[117] Drugs used in chemotherapy can induce cell stress, which can kill a cancer cell; however, under certain conditions, cells stress can induce changes in gene expression that enables resistance to several types of drugs.[118]
Targeted therapies are a relatively new class of cancer drugs that can overcome many of the issues seen with the use of cytotoxics. They are divided into two groups: small molecule and antibodies. The massive toxicity seen with the use of cytotoxics is due to the lack of cell specificity of the drugs. They will kill any rapidly dividing cell, tumor or normal. Targeted therapies are designed to affect cellular proteins or processes that are utilised by the cancer cells. This allows a high dose to cancer tissues with a relatively low dose to other tissues. Although the side effects are often less severe than that seen of cytotoxic chemotherapeutics, life-threatening effects can occur. Initially, the targeted therapeutics were supposed to be solely selective for one protein. Now it is clear that there is often a range of protein targets that the drug can bind. An example target for targeted therapy is the protein produced by the Philadelphia chromosome, a genetic lesion found commonly in chronic myelomonocytic leukemia. This fusion protein has enzyme activity that can be inhibited by imatinib, a small molecule drug.[119][120][121][122]
Cancer is the uncontrolled growth of cells coupled with malignant behaviour: invasion and metastasis (among other features).[123] It is caused by the interaction between genetic susceptibility and environmental factors.[124][125] These factors lead to accumulations of genetic mutations in oncogenes (genes that control the growth rate of cells) and tumor suppressor genes (genes that help to prevent cancer), which gives cancer cells their malignant characteristics, such as uncontrolled growth.[126]:93–94
In the broad sense, most chemotherapeutic drugs work by impairing mitosis (cell division), effectively targeting fast-dividing cells. As these drugs cause damage to cells, they are termed cytotoxic. They prevent mitosis by various mechanisms including damaging DNA and inhibition of the cellular machinery involved in cell division.[34][127] One theory as to why these drugs kill cancer cells is that they induce a programmed form of cell death known as apoptosis.[128]
As chemotherapy affects cell division, tumors with high growth rates (such as acute myelogenous leukemia and the aggressive lymphomas, including Hodgkin's disease) are more sensitive to chemotherapy, as a larger proportion of the targeted cells are undergoing cell division at any time.  Malignancies with slower growth rates, such as indolent lymphomas, tend to respond to chemotherapy much more modestly.[1] Heterogeneic tumours may also display varying sensitivities to chemotherapy agents, depending on the subclonal populations within the tumor.
Cells from the immune system also make crucial contributions to the antitumor effects of chemotherapy.[129] For example, the chemotherapeutic drugs oxaliplatin and cyclophosphamide can cause tumor cells to die in a way that is detectable by the immune system (called immunogenic cell death), which mobilizes immune cells with antitumor functions.[130] Chemotherapeutic drugs that cancer immunogenic tumor cell death can make unresponsive tumors sensitive to immune checkpoint therapy.[131]
Some chemotherapy drugs are used in diseases other than cancer, such as in autoimmune disorders,[132] and noncancerous plasma cell dyscrasia. In some cases they are often used at lower doses, which means that the side effects are minimized,[132] while in other cases doses similar to ones used to treat cancer are used. Methotrexate is used in the treatment of rheumatoid arthritis (RA),[133] psoriasis,[134] ankylosing spondylitis[135] and multiple sclerosis.[136][137] The anti-inflammatory response seen in RA is thought to be due to increases in adenosine, which causes immunosuppression; effects on immuno-regulatory cyclooxygenase-2 enzyme pathways; reduction in pro-inflammatory cytokines; and anti-proliferative properties.[133] Although methotrexate is used to treat both multiple sclerosis and ankylosing spondylitis, its efficacy in these diseases is still uncertain.[135][136][137] Cyclophosphamide is sometimes used to treat lupus nephritis, a common symptom of systemic lupus erythematosus.[138] Dexamethasone along with either bortezomib or melphalan is commonly used as a treatment for AL amyloidosis. Recently, bortezomid in combination with cyclophosphamide and dexamethasone has also shown promise as a treatment for AL amyloidosis. Other drugs used to treat myeloma such as lenalidomide have shown promise in treating AL amyloidosis.[139]
Chemotherapy drugs are also used in conditioning regimens prior to bone marow transplant (hematopoietic stem cell transplant). Conditioning regimens are used to suppress the recipient's immune system in order to allow a transplant to engraft. Cyclophosphamide is a common cytotoxic drug used in this manner, and is often used in conjunction with total body irradiation. Chemotherapeutic drugs may be used at high doses to permanently remove the recipient's bone marrow cells (myeloablative conditioning) or at lower doses that will prevent permanent bone marrow loss (non-myeloablative and reduced intensity conditioning).[140] When used in non-cancer setting, the treatment is still called "chemotherapy", and is often done in the same treatment centers used for people with cancer.
Healthcare workers exposed to antineoplastic agents take precautions to keep their exposure to a minimum. There is a limitation in cytotoxics dissolution in Australia and the United States to 20 dissolutions per pharmacist/nurse,[citation needed] since pharmacists who prepare these drugs or nurses who may prepare or administer them are the two occupational groups with the highest potential exposure to antineoplastic agents. In addition, physicians and operating room personnel may also be exposed as they treat people. Hospital staff, such as shipping and receiving personnel, custodial workers, laundry workers, and waste handlers, all have potential exposure to these drugs during the course of their work. The increased use of antineoplastic agents in veterinary oncology also puts these workers at risk for exposure to these drugs.[141][142] Routes of entry into the worker's body are skin absorption, inhalation, and ingestion via hand-to-mouth.[142] The long-term effects of exposure include chromosomal abnormalities and infertility.[4]:38
There is an extensive list of antineoplastic agents. Several classification schemes have been used to subdivide the medicines used for cancer into several different types.[citation needed]
The first use of small-molecule drugs to treat cancer was in the early 20th century, although the specific chemicals first used were not originally intended for that purpose. Mustard gas was used as a chemical warfare agent during World War I and was discovered to be a potent suppressor of hematopoiesis (blood production).[143] A similar family of compounds known as nitrogen mustards were studied further during World War II at the Yale School of Medicine.[144] It was reasoned that an agent that damaged the rapidly growing white blood cells might have a similar effect on cancer.[144] Therefore, in December 1942, several people with advanced lymphomas (cancers of the lymphatic system and lymph nodes) were given the drug by vein, rather than by breathing the irritating gas.[144] Their improvement, although temporary, was remarkable.[145] Concurrently, during a military operation in World War II, following a German air raid on the Italian harbour of Bari, several hundred people were accidentally exposed to mustard gas, which had been transported there by the Allied forces to prepare for possible retaliation in the event of German use of chemical warfare. The survivors were later found to have very low white blood cell counts.[146] After WWII was over and the reports declassified, the experiences converged and led researchers to look for other substances that might have similar effects against cancer. The first chemotherapy drug to be developed from this line of research was mustine. Since then, many other drugs have been developed to treat cancer, and drug development has exploded into a multibillion-dollar industry, although the principles and limitations of chemotherapy discovered by the early researchers still apply.[147]
The word chemotherapy without a modifier usually refers to cancer treatment, but its historical meaning was broader. The term was coined in the early 1900s by Paul Ehrlich as meaning any use of chemicals to treat any disease (chemo- + -therapy), such as the use of antibiotics (antibacterial chemotherapy).[148]  Ehrlich was not optimistic that effective chemotherapy drugs would be found for the treatment of cancer.[148]  The first modern chemotherapeutic agent was arsphenamine, an arsenic compound discovered in 1907 and used to treat syphilis.[149] This was later followed by sulfonamides (sulfa drugs) and penicillin. In today's usage, the sense "any treatment of disease with drugs" is often expressed with the word pharmacotherapy.
The top 10 best-selling (in terms of revenue) cancer drugs of 2013:[150]
Specially targeted delivery vehicles aim to increase effective levels of chemotherapy for tumor cells while reducing effective levels for other cells. This should result in an increased tumor kill or reduced toxicity or both.[151]
Antibody-drug conjugates (ADCs) comprise an antibody, drug and a linker between them. The antibody will be targeted at a preferentially expressed protein in the tumour cells (known as a tumor antigen) or on cells that the tumor can utilise, such as blood vessel endothelial cells. They bind to the tumor antigen and are internalised, where the linker releases the drug into the cell. These specially targeted delivery vehicles vary in their stability, selectivity, and choice of target, but, in essence, they all aim to increase the maximum effective dose that can be delivered to the tumor cells.[152] Reduced systemic toxicity means that they can also be used in people who are sicker, and that they can carry new chemotherapeutic agents that would have been far too toxic to deliver via traditional systemic approaches.[citation needed]
The first approved drug of this type was gemtuzumab ozogamicin (Mylotarg), released by Wyeth (now Pfizer). The drug was approved to treat acute myeloid leukemia, but has now been withdrawn from the market because the drug did not meet efficacy targets in further clinical trials.[153][154] Two other drugs, trastuzumab emtansine and brentuximab vedotin, are both in late clinical trials, and the latter has been granted accelerated approval for the treatment of refractory Hodgkin's lymphoma and systemic anaplastic large cell lymphoma.[152]
Nanoparticles are 1–1000 nanometer (nm) sized particles that can promote tumor selectivity and aid in delivering low-solubility drugs. Nanoparticles can be targeted passively or actively. Passive targeting exploits the difference between tumor blood vessels and normal blood vessels. Blood vessels in tumors are "leaky" because they have gaps from 200–2000 nm, which allow nanoparticles to escape into the tumor. Active targeting uses biological molecules (antibodies, proteins, DNA and receptor ligands) to preferentially target the nanoparticles to the tumor cells. There are many types of nanoparticle delivery systems, such as silica, polymers, liposomes and magnetic particles. Nanoparticles made of magnetic material can also be used to concentrate agents at tumor sites using an externally applied magnetic field.[151]  They have emerged as a useful vehicle in magnetic drug delivery for poorly soluble agents such as paclitaxel.[155]
Electrochemotherapy is the combined treatment in which injection of a chemotherapeutic drug is followed by application of high-voltage electric pulses locally to the tumor. The treatment enables the chemotherapeutic drugs, which otherwise cannot or hardly go through the membrane of cells (such as bleomycin and cisplatin), to enter the cancer cells. Hence, greater effectiveness of antitumor treatment is achieved.
Clinical electrochemotherapy has been successfully used for treatment of cutaneous and subcutaneous tumors irrespective of their histological origin.[156][157][158][159][160][161] The method has been reported as safe, simple and highly effective in all reports on clinical use of electrochemotherapy. According to the ESOPE project (European Standard Operating Procedures of Electrochemotherapy), the Standard Operating Procedures (SOP) for electrochemotherapy were prepared, based on the experience of the leading European cancer centres on electrochemotherapy.[158][162] Recently, new electrochemotherapy modalities have been developed for treatment of internal tumors using surgical procedures, endoscopic routes or percutaneous approaches to gain access to the treatment area.[163][164]
Hyperthermia therapy is heat treatment for cancer that can be a powerful tool when used in combination with chemotherapy (thermochemotherapy) or radiation for the control of a variety of cancers.  The heat can be applied locally to the tumor site, which will dilate blood vessels to the tumor, allowing more chemotherapeutic medication to enter the tumor.  Additionally, the tumor cell membrane will become more porous, further allowing more of the chemotherapeutic medicine to enter the tumor cell.
Hyperthermia has also been shown to help prevent or reverse "chemo-resistance."  Chemotherapy resistance sometimes develops over time as the tumors adapt and can overcome the toxicity of the chemo medication. "Overcoming chemoresistance has been extensively studied within the past, especially using CDDP-resistant cells. In regard to the potential benefit that drug-resistant cells can be recruited for effective therapy by combining chemotherapy with hyperthermia, it was important to show that chemoresistance against several anticancer drugs (e.g. mitomycin C, anthracyclines, BCNU, melphalan) including CDDP could be reversed at least partially by the addition of heat.[165]
Chemotherapy is used in veterinary medicine similar to how it is used in human medicine.[166]
RET inhibitors: Vandetanib (also VEGFR and EGFR). Entrectinib (ALK, ROS1, NTRK).
c-MET inhibitor: Cabozantinib (also VEGFR2).



Chemotherapy - Wikipedia

Chemotherapy (often abbreviated to chemo and sometimes CTX or CTx) is a type of cancer treatment that uses one or more anti-cancer drugs (chemotherapeutic agents) as part of a standardized chemotherapy regimen. Chemotherapy may be given with a curative intent (which almost always involves combinations of drugs), or it may aim to prolong life or to reduce symptoms (palliative chemotherapy). Chemotherapy is one of the major categories of the medical discipline specifically devoted to pharmacotherapy for cancer, which is called medical oncology.
The term chemotherapy has come to connote non-specific usage of intracellular poisons to inhibit mitosis, cell division. The connotation excludes more selective agents that block extracellular signals (signal transduction). The development of therapies with specific molecular or genetic targets, which inhibit growth-promoting signals from classic endocrine hormones (primarily estrogens for breast cancer and androgens for prostate cancer) are now called hormonal therapies. By contrast, other inhibitions of growth-signals like those associated with receptor tyrosine kinases are referred to as targeted therapy.
Importantly, the use of drugs (whether chemotherapy, hormonal therapy or targeted therapy) constitutes systemic therapy for cancer in that they are introduced into the blood stream and are therefore in principle able to address cancer at any anatomic location in the body. Systemic therapy is often used in conjunction with other modalities that constitute local therapy (i.e. treatments whose efficacy is confined to the anatomic area where they are applied) for cancer such as radiation therapy, surgery or hyperthermia therapy.
Traditional chemotherapeutic agents are cytotoxic by means of interfering with cell division (mitosis) but cancer cells vary widely in their susceptibility to these agents. To a large extent, chemotherapy can be thought of as a way to damage or stress cells, which may then lead to cell death if apoptosis is initiated. Many of the side effects of chemotherapy can be traced to damage to normal cells that divide rapidly and are thus sensitive to anti-mitotic drugs: cells in the bone marrow, digestive tract and hair follicles. This results in the most common side-effects of chemotherapy: myelosuppression (decreased production of blood cells, hence also immunosuppression), mucositis (inflammation of the lining of the digestive tract), and alopecia (hair loss). Because of the effect on immune cells (especially lymphocytes), chemotherapy drugs often find use in a host of diseases that result from harmful overactivity of the immune system against self (so-called autoimmunity). These include rheumatoid arthritis, systemic lupus erythematosus, multiple sclerosis, vasculitis and many others.
There are a number of strategies in the administration of chemotherapeutic drugs used today. Chemotherapy may be given with a curative intent or it may aim to prolong life or to palliate symptoms.
All chemotherapy regimens require that the recipient be capable of undergoing the treatment. Performance status is often used as a measure to determine whether a person can receive chemotherapy, or whether dose reduction is required.  Because only a fraction of the cells in a tumor die with each treatment (fractional kill), repeated doses must be administered to continue to reduce the size of the tumor.[7] Current chemotherapy regimens apply drug treatment in cycles, with the frequency and duration of treatments limited by toxicity.[8]
The efficacy of chemotherapy depends on the type of cancer and the stage. The overall effectiveness ranges from being curative for some cancers, such as some leukemias,[9][10] to being ineffective, such as in some brain tumors,[11] to being needless in others, like most non-melanoma skin cancers.[12]
Dosage of chemotherapy can be difficult: If the dose is too low, it will be ineffective against the tumor, whereas, at excessive doses, the toxicity (side-effects) will be intolerable to the person receiving it.[1] The standard method of determining chemotherapy dosage is based on calculated body surface area (BSA). The BSA is usually calculated with a mathematical formula or a nomogram, using the recipient's weight and height, rather than by direct measurement of body area. This formula was originally derived in a 1916 study and attempted to translate medicinal doses established with laboratory animals to equivalent doses for humans.[13] The study only included 9 human subjects.[14] When chemotherapy was introduced in the 1950s, the BSA formula was adopted as the official standard for chemotherapy dosing for lack of a better option.[15][16]
Recently, the validity of this method in calculating uniform doses has been questioned. The reason for this is that the formula only takes into account the individual's weight and height. Drug absorption and clearance are influenced by multiple factors, including age, gender, metabolism, disease state, organ function, drug-to-drug interactions, genetics, and obesity, which has a major impact on the actual concentration of the drug in the person's bloodstream.[15][17][18] As a result, there is high variability in the systemic chemotherapy drug concentration in people dosed by BSA, and this variability has been demonstrated to be more than 10-fold for many drugs.[14][19]   In other words, if two people receive the same dose of a given drug based on BSA, the concentration of that drug in the bloodstream of one person may be 10 times higher or lower compared to that of the other person.[19] This variability is typical with many chemotherapy drugs dosed by BSA, and, as shown below, was demonstrated in a study of 14 common chemotherapy drugs.[14]
The result of this pharmacokinetic variability among people, is that many people do not receive the right dose to achieve optimal treatment effectiveness with minimized toxic side effects. Some people are overdosed while others are underdosed.[15][17][18][20][21][22][23] For example, in a randomized clinical trial, investigators found 85% of metastatic colorectal cancer patients treated with 5-fluorouracil (5-FU) did not receive the optimal therapeutic dose when dosed by the BSA standard—68% were underdosed and 17% were overdosed.[20]
There has been controversy over the use of BSA to calculate chemotherapy doses for people who are obese.[24] Because of their higher BSA, clinicians often arbitrarily reduce the dose prescribed by the BSA formula for fear of overdosing.[24] In many cases, this can result in sub-optimal treatment.[24]
Several clinical studies have demonstrated that when chemotherapy dosing is individualized to achieve optimal systemic drug exposure, treatment outcomes are improved and toxic side effects are reduced.[20][22] In the 5-FU clinical study cited above, people whose dose was adjusted to achieve a pre-determined target exposure realized an 84% improvement in treatment response rate and a six-month improvement in overall survival (OS) compared with those dosed by BSA.[20]
In the same study, investigators compared the incidence of common 5-FU-associated grade 3/4 toxicities between the dose-adjusted people and people dosed per BSA.[20] The incidence of debilitating grades of diarrhea was reduced from 18% in the BSA-dosed group to 4% in the dose-adjusted group and serious hematologic side effects were eliminated.[20]  Because of the reduced toxicity, dose-adjusted patients were able to be treated for longer periods of time.[20] BSA-dosed people were treated for a total of 680 months while people in the dose-adjusted group were treated for a total of 791 months.[20] Completing the course of treatment is an important factor in achieving better treatment outcomes.
Similar results were found in a study involving people with colorectal cancer who were treated with the popular FOLFOX regimen.[22] The incidence of serious diarrhea was reduced from 12% in the BSA-dosed group of patients to 1.7% in the dose-adjusted group, and the incidence of severe mucositis was reduced from 15% to 0.8%.[22]
The FOLFOX study also demonstrated an improvement in treatment outcomes.[22] Positive response increased from 46% in the BSA-dosed group to 70% in the dose-adjusted group.  Median progression free survival (PFS) and overall survival (OS) both improved by six months in the dose adjusted group.[22]
One approach that can help clinicians individualize chemotherapy dosing is to measure the drug levels in blood plasma over time and adjust dose according to a formula or algorithm to achieve optimal exposure. With an established target exposure for optimized treatment effectiveness with minimized toxicities, dosing can be personalized to achieve target exposure and optimal results for each person.  Such an algorithm was used in the clinical trials cited above and resulted in significantly improved treatment outcomes.
Oncologists are already individualizing dosing of some cancer drugs based on exposure. Carboplatin[25]:4 and busulfan[26][27] dosing rely upon results from blood tests to calculate the optimal dose for each person. Simple blood tests are also available for dose optimization of methotrexate,[28] 5-FU, paclitaxel, and docetaxel.[29][30]
Alkylating agents are the oldest group of chemotherapeutics in use today. Originally derived from mustard gas used in World War I, there are now many types of alkylating agents in use.[1] They are so named because of their ability to alkylate many molecules, including proteins, RNA and DNA. This ability to bind covalently to DNA via their alkyl group is the primary cause for their anti-cancer effects.[32] DNA is made of two strands and the molecules may either bind twice to one strand of DNA (intrastrand crosslink) or may bind once to both strands (interstrand crosslink). If the cell tries to replicate crosslinked DNA during cell division, or tries to repair it, the DNA strands can break. This leads to a form of programmed cell death called apoptosis.[31][33] Alkylating agents will work at any point in the cell cycle and thus are known as cell cycle-independent drugs. For this reason the effect on the cell is dose dependent; the fraction of cells that die is directly proportional to the dose of drug.[34]
The subtypes of alkylating agents are the nitrogen mustards, nitrosoureas, tetrazines, aziridines,[35] cisplatins and derivatives, and non-classical alkylating agents. Nitrogen mustards include mechlorethamine, cyclophosphamide, melphalan, chlorambucil, ifosfamide and busulfan. Nitrosoureas include N-Nitroso-N-methylurea (MNU), carmustine (BCNU), lomustine (CCNU) and semustine (MeCCNU), fotemustine and streptozotocin. Tetrazines include dacarbazine, mitozolomide and temozolomide. Aziridines include thiotepa, mytomycin and diaziquone (AZQ). Cisplatin and derivatives include cisplatin, carboplatin and oxaliplatin.[32][33] They impair cell function by forming covalent bonds with the amino, carboxyl, sulfhydryl, and phosphate groups in biologically important molecules.[36] Non-classical alkylating agents include procarbazine and hexamethylmelamine.[32][33]
Anti-metabolites are a group of molecules that impede DNA and RNA synthesis. Many of them have a similar structure to the building blocks of DNA and RNA. The building blocks are nucleotides; a molecule comprising a nucleobase, a sugar and a phosphate group. The nucleobases are divided into purines (guanine and adenine) and pyrimidines (cytosine, thymine and uracil). Anti-metabolites resemble either nucleobases or nucleosides (a nucleotide without the phosphate group), but have altered chemical groups.[37] These drugs exert their effect by either blocking the enzymes required for DNA synthesis or becoming incorporated into DNA or RNA. By inhibiting the enzymes involved in DNA synthesis, they prevent mitosis because the DNA cannot duplicate itself. Also, after misincorporation of the molecules into DNA, DNA damage can occur and programmed cell death (apoptosis) is induced. Unlike alkylating agents, anti-metabolites are cell cycle dependent. This means that they only work during a specific part of the cell cycle, in this case S-phase (the DNA synthesis phase). For this reason, at a certain dose, the effect plateaus and proportionally no more cell death occurs with increased doses. Subtypes of the anti-metabolites are the anti-folates, fluoropyrimidines, deoxynucleoside analogues and thiopurines.[32][37]
The anti-folates include methotrexate and pemetrexed. Methotrexate inhibits dihydrofolate reductase (DHFR), an enzyme that regenerates tetrahydrofolate from dihydrofolate. When the enzyme is inhibited by methotrexate, the cellular levels of folate coenzymes diminish. These are required for thymidylate and purine production, which are both essential for DNA synthesis and cell division.[3]:55–59[4]:11 Pemetrexed is another anti-metabolite that affects purine and pyrimidine production, and therefore also inhibits DNA synthesis. It primarily inhibits the enzyme thymidylate synthase, but also has effects on DHFR, aminoimidazole carboxamide ribonucleotide formyltransferase and glycinamide ribonucleotide formyltransferase.[38] The fluoropyrimidines include fluorouracil and capecitabine. Fluorouracil is a nucleobase analogue that is metabolised in cells to form at least two active products; 5-fluourouridine monophosphate (FUMP) and 5-fluoro-2'-deoxyuridine 5'-phosphate (fdUMP). FUMP becomes incorporated into RNA and fdUMP inhibits the enzyme thymidylate synthase; both of which lead to cell death.[4]:11 Capecitabine is a prodrug of 5-fluorouracil that is broken down in cells to produce the active drug.[39] The deoxynucleoside analogues include cytarabine, gemcitabine, decitabine, azacitidine, fludarabine, nelarabine, cladribine, clofarabine, and pentostatin. The thiopurines include thioguanine and mercaptopurine.[32][37]
Anti-microtubule agents are plant-derived chemicals that block cell division by preventing microtubule function. Microtubules are an important cellular structure composed of two proteins; α-tubulin and β-tubulin. They are hollow rod shaped structures that are required for cell division, among other cellular functions.[40] Microtubules are dynamic structures, which means that they are permanently in a state of assembly and disassembly. Vinca alkaloids and taxanes are the two main groups of anti-microtubule agents, and although both of these groups of drugs cause microtubule dysfunction, their mechanisms of action are completely opposite. The vinca alkaloids prevent the formation of the microtubules, whereas the taxanes prevent the microtubule disassembly. By doing so, they prevent the cancer cells from completing mitosis. Following this, cell cycle arrest occurs, which induces programmed cell death (apoptosis).[32][41] Also, these drugs can affect blood vessel growth; an essential process that tumours utilise in order to grow and metastasise.[41]
Vinca alkaloids are derived from the Madagascar periwinkle, Catharanthus roseus[42][43] (formerly known as Vinca rosea). They bind to specific sites on tubulin, inhibiting the assembly of tubulin into microtubules. The original vinca alkaloids are natural products that include vincristine and vinblastine.[44][45][46][47] Following the success of these drugs, semi-synthetic vinca alkaloids were produced: vinorelbine (used in the treatment of non-small-cell lung cancer[46][48][49]), vindesine, and vinflunine.[41] These drugs are cell cycle-specific. They bind to the tubulin molecules in S-phase and prevent proper microtubule formation required for M-phase.[34]
Taxanes are natural and semi-synthetic drugs. The first drug of their class, paclitaxel, was originally extracted from the Pacific Yew tree, Taxus brevifolia.  Now this drug and another in this class, docetaxel, are produced semi-synthetically from a chemical found in the bark of another Yew tree; Taxus baccata. These drugs promote microtubule stability, preventing their disassembly. Paclitaxel prevents the cell cycle at the boundary of G2-M, whereas docetaxel exerts its effect during S-phase. Taxanes present difficulties in formulation as medicines because they are poorly soluble in water.[41]
Podophyllotoxin is an antineoplastic lignan obtained primarily from the American Mayapple (Podophyllum peltatum) and Himalayan Mayapple (Podophyllum hexandrum or Podophyllum emodi). It has anti-microtubule activity, and its mechanism is similar to that of vinca alkaloids in that they bind to tubulin, inhibiting microtubule formation. Podophyllotoxin is used to produce two other drugs with different mechanisms of action: etoposide and teniposide.[50][51]
Topoisomerase inhibitors are drugs that affect the activity of two enzymes: topoisomerase I and topoisomerase II. When the DNA double-strand helix is unwound, during DNA replication or transcription, for example, the adjacent unopened DNA winds tighter (supercoils), like opening the middle of a twisted rope. The stress caused by this effect is in part aided by the topoisomerase enzymes. They produce single- or double-strand breaks into DNA, reducing the tension in the DNA strand. This allows the normal unwinding of DNA to occur during replication or transcription. Inhibition of topoisomerase I or II interferes with both of these processes.[52][53]
Two topoisomerase I inhibitors, irinotecan and topotecan, are semi-synthetically derived from camptothecin, which is obtained from the Chinese ornamental tree Camptotheca acuminata.[34] Drugs that target topoisomerase II can be divided into two groups. The topoisomerase II poisons cause increased levels enzymes bound to DNA. This prevents DNA replication and transcription, causes DNA strand breaks, and leads to programmed cell death (apoptosis). These agents include etoposide, doxorubicin, mitoxantrone and teniposide. The second group, catalytic inhibitors, are drugs that block the activity of topoisomerase II, and therefore prevent DNA synthesis and translation because the DNA cannot unwind properly. This group includes novobiocin, merbarone, and aclarubicin, which also have other significant mechanisms of action.[54]
The cytotoxic antibiotics are a varied group of drugs that have various mechanisms of action. The common theme that they share in their chemotherapy indication is that they interrupt cell division. The most important subgroup is the anthracyclines and the bleomycins; other prominent examples include mitomycin C, mitoxantrone, and actinomycin.[55]
Among the anthracyclines, doxorubicin and daunorubicin were the first, and were obtained from the bacterium Streptomyces peucetius.[56]   Derivatives of these compounds include epirubicin and idarubicin. Other clinically used drugs in the anthracyline group are pirarubicin, aclarubicin, and mitoxantrone. The mechanisms of anthracyclines include DNA intercalation (molecules insert between the two strands of DNA), generation of highly reactive free radicals that damage intercellular molecules and topoisomerase inhibition.[57]
Actinomycin is a complex molecule that intercalates DNA and prevents RNA synthesis.[58]
Bleomycin, a glycopeptide isolated from Streptomyces verticillus, also intercalates DNA, but produces free radicals that damage DNA. This occurs when bleomycin binds to a metal ion, becomes chemically reduced and reacts with oxygen.[59][3]:87
Mitomycin is a cytotoxic antibiotic with the ability to alkylate DNA.[60]
Most chemotherapy is delivered intravenously, although a number of agents can be administered orally (e.g., melphalan, busulfan, capecitabine).
There are many intravenous methods of drug delivery, known as vascular access devices. These include the winged infusion device, peripheral venous catheter, midline catheter, peripherally inserted central catheter (PICC), central venous catheter and implantable port. The devices have different applications regarding duration of chemotherapy treatment, method of delivery and types of chemotherapeutic agent.[4]:94–95
Depending on the person, the cancer, the stage of cancer, the type of chemotherapy, and the dosage, intravenous chemotherapy may be given on either an inpatient or an outpatient basis. For continuous, frequent or prolonged intravenous chemotherapy administration, various systems may be surgically inserted into the vasculature to maintain access.[4]:113–118 Commonly used systems are the Hickman line, the Port-a-Cath, and the PICC line. These have a lower infection risk, are much less prone to phlebitis or extravasation, and eliminate the need for repeated insertion of peripheral cannulae.[citation needed]
Isolated limb perfusion (often used in melanoma),[61] or isolated infusion of chemotherapy into the liver[62] or the lung have been used to treat some tumors. The main purpose of these approaches is to deliver a very high dose of chemotherapy to tumor sites without causing overwhelming systemic damage.[63] These approaches can help control solitary or limited metastases, but they are by definition not systemic, and, therefore, do not treat distributed metastases or micrometastases.
Topical chemotherapies, such as 5-fluorouracil, are used to treat some cases of non-melanoma skin cancer.[64]
If the cancer has central nervous system involvement, or with meningeal disease, intrathecal chemotherapy may be administered.[1]
Chemotherapeutic techniques have a range of side-effects that depend on the type of medications used. The most common medications affect mainly the fast-dividing cells of the body, such as blood cells and the cells lining the mouth, stomach, and intestines. Chemotherapy-related toxicities can occur acutely after administration, within hours or days, or chronically, from weeks to years.[3]:265
Virtually all chemotherapeutic regimens can cause depression of the immune system, often by paralysing the bone marrow and leading to a decrease of white blood cells, red blood cells, and platelets.
Anemia and thrombocytopenia may require blood transfusion. Neutropenia (a decrease of the neutrophil granulocyte count below 0.5 x 109/litre) can be improved with synthetic G-CSF (granulocyte-colony-stimulating factor, e.g., filgrastim, lenograstim).
In very severe myelosuppression, which occurs in some regimens, almost all the bone marrow stem cells (cells that produce white and red blood cells) are destroyed, meaning allogenic or autologous bone marrow cell transplants are necessary. (In autologous BMTs, cells are removed from the person before the treatment, multiplied and then re-injected afterward; in allogenic BMTs, the source is a donor.) However, some people still develop diseases because of this interference with bone marrow.[citation needed]
Although people receiving chemotherapy are encouraged to wash their hands, avoid sick people, and take other infection-reducing steps, about 85% of infections are due to naturally occurring microorganisms in the person's own gastrointestinal tract (including oral cavity) and skin.[65]:130 This may manifest as systemic infections, such as sepsis, or as localized outbreaks, such as Herpes simplex, shingles, or other members of the Herpesviridea.[66] The risk of illness and death can be reduced by taking common antibiotics such as quinolones or trimethoprim/sulfamethoxazole before any fever or sign of infection appears.[67] Quinolones show effective prophylaxis mainly with hematological cancer.[67] However, in general, for every five people who are immunosuppressed following chemotherapy who take an antibiotic, one fever can be prevented; for every 34 who take an antibiotic, one death can be prevented.[67] Sometimes, chemotherapy treatments are postponed because the immune system is suppressed to a critically low level.
In Japan, the government has approved the use of some medicinal mushrooms like Trametes versicolor, to counteract depression of the immune system in people undergoing chemotherapy.[68]
Due to immune system suppression, neutropenic enterocolitis (typhlitis) is a "life-threatening gastrointestinal complication of chemotherapy."[69] Typhlitis is an intestinal infection which may manifest itself through symptoms including nausea, vomiting, diarrhea, a distended abdomen, fever, chills, or abdominal pain and tenderness.
Typhlitis is a medical emergency. It has a very poor prognosis and is often fatal unless promptly recognized and aggressively treated.[70] Successful treatment hinges on early diagnosis provided by a high index of suspicion and the use of CT scanning, nonoperative treatment for uncomplicated cases, and sometimes elective right hemicolectomy to prevent recurrence.[70]
Nausea, vomiting, anorexia, diarrhoea, abdominal cramps, and constipation are common side-effects of chemotherapeutic medications that kill fast-dividing cells.[71] Malnutrition and dehydration can result when the recipient does not eat or drink enough, or when the person vomits frequently, because of gastrointestinal damage. This can result in rapid weight loss, or occasionally in weight gain, if the person eats too much in an effort to allay nausea or heartburn. Weight gain can also be caused by some steroid medications. These side-effects can frequently be reduced or eliminated with antiemetic drugs. Self-care measures, such as eating frequent small meals and drinking clear liquids or ginger tea, are often recommended. In general, this is a temporary effect, and frequently resolves within a week of finishing treatment. However, a high index of suspicion is appropriate, since diarrhea and bloating are also symptoms of typhlitis, a very serious and potentially life-threatening medical emergency that requires immediate treatment.
Anemia can be a combined outcome caused by myelosuppressive chemotherapy, and possible cancer-related causes such as bleeding, blood cell destruction (hemolysis), hereditary disease, kidney dysfunction, nutritional
deficiencies or anemia of chronic disease. Treatments to mitigate anemia include hormones to boost blood production (erythropoietin), iron supplements, and blood transfusions.[72][73][74] Myelosuppressive therapy can cause a tendency to bleed easily, leading to anemia. Medications that kill rapidly dividing cells or blood cells can reduce the number of platelets in the blood, which can result in bruises and bleeding. Extremely low platelet counts may be temporarily boosted through platelet transfusions and new drugs to increase platelet counts during chemotherapy are being developed.[75][76] Sometimes, chemotherapy treatments are postponed to allow platelet counts to recover.
Fatigue may be a consequence of the cancer or its treatment, and can last for months to years after treatment. One physiological cause of fatigue is anemia, which can be caused by chemotherapy, surgery, radiotherapy, primary and metastatic disease or nutritional depletion.[77][78] Anaerobic exercise has been found to be beneficial in reducing fatigue in people with solid tumours.[79]
Nausea and vomiting are two of the most feared cancer treatment-related side-effects for people with cancer and their families. In 1983, Coates et al. found that people receiving chemotherapy ranked nausea and vomiting as the first and second most severe side-effects, respectively. Up to 20% of people receiving highly emetogenic agents in this era postponed, or even refused, potentially curative treatments.[80] Chemotherapy-induced nausea and vomiting (CINV) are common with many treatments and some forms of cancer. Since the 1990s, several novel classes of antiemetics have been developed and commercialized, becoming a nearly universal standard in chemotherapy regimens, and helping to successfully manage these symptoms in many people. Effective mediation of these unpleasant and sometimes-crippling symptoms results in increased quality of life for the recipient and more efficient treatment cycles, due to less stoppage of treatment due to better tolerance and better overall health.
Hair loss (alopecia) can be caused by chemotherapy that kills rapidly dividing cells; other medications may cause hair to thin.  These are most often temporary effects: hair usually starts to regrow a few weeks after the last treatment, but sometimes with a change in colour, texture, thickness or style. Sometimes hair has a tendency to curl after regrowth, resulting in "chemo curls." Severe hair loss occurs most often with drugs such as doxorubicin, daunorubicin, paclitaxel, docetaxel, cyclophosphamide, ifosfamide and etoposide. Permanent thinning or hair loss can result from some standard chemotherapy regimens.
Chemotherapy induced hair loss occurs by a non-androgenic mechanism, and can manifest as alopecia totalis, telogen effluvium, or less often alopecia areata.[81] It is usually associated with systemic treatment due to the high mitotic rate of hair follicles, and more reversible than androgenic hair loss,[82][83] although permanent cases can occur.[84] Chemotherapy induces hair loss in women more often than men.[85]
Scalp cooling offers a means of preventing both permanent and temporary hair loss; however, concerns about this method have been raised.[86][87]
Development of secondary neoplasia after successful chemotherapy or radiotherapy treatment can occur. The most common secondary neoplasm is secondary acute myeloid leukemia, which develops primarily after treatment with alkylating agents or topoisomerase inhibitors.[88] Survivors of childhood cancer are more than 13 times as likely to get a secondary neoplasm during the 30 years after treatment than the general population.[89] Not all of this increase can be attributed to chemotherapy.
Some types of chemotherapy are gonadotoxic and may cause infertility.[90] Chemotherapies with high risk include procarbazine and other alkylating drugs such as cyclophosphamide, ifosfamide, busulfan, melphalan, chlorambucil, and chlormethine.[90] Drugs with medium risk include doxorubicin and platinum analogs such as cisplatin and carboplatin.[90] On the other hand, therapies with low risk of gonadotoxicity include plant derivatives such as vincristine and vinblastine, antibiotics such as bleomycin and dactinomycin, and antimetabolites such as methotrexate, mercaptopurine, and 5-fluorouracil.[90]
Female infertility by chemotherapy appears to be secondary to premature ovarian failure by loss of primordial follicles.[91] This loss is not necessarily a direct effect of the chemotherapeutic agents, but could be due to an increased rate of growth initiation to replace damaged developing follicles.[91]
People may choose between several methods of fertility preservation prior to chemotherapy, including cryopreservation of semen, ovarian tissue, oocytes, or embryos.[92] As more than half of cancer patients are elderly, this adverse effect is only relevant for a minority of patients. A study in France between 1999 and 2011 came to the result that embryo freezing before administration of gonadotoxic agents to females caused a delay of treatment in 34% of cases, and a live birth in 27% of surviving cases who wanted to become pregnant, with the follow-up time varying between 1 and 13 years.[93]
Potential protective or attenuating agents include GnRH analogs, where several studies have shown a protective effect in vivo in humans, but some studies show no such effect. Sphingosine-1-phosphate (S1P) has shown similar effect, but its mechanism of inhibiting the sphingomyelin apoptotic pathway may also interfere with the apoptosis action of chemotherapy drugs.[94]
In chemotherapy as a conditioning regimen in hematopoietic stem cell transplantation, a study of people conditioned with cyclophosphamide alone for severe aplastic anemia came to the result that ovarian recovery occurred in all women younger than 26 years at time of transplantation, but only in five of 16 women older than 26 years.[95]
Chemotherapy is teratogenic during pregnancy, especially during the first trimester, to the extent that abortion usually is recommended if pregnancy in this period is found during chemotherapy.[96] Second- and third-trimester exposure does not usually increase the teratogenic risk and adverse effects on cognitive development, but it may increase the risk of various complications of pregnancy and fetal myelosuppression.[96]
In males previously having undergone chemotherapy or radiotherapy, there appears to be no increase in genetic defects or congenital malformations in their children conceived after therapy.[96] The use of assisted reproductive technologies and micromanipulation techniques might increase this risk.[96] In females previously having undergone chemotherapy, miscarriage and congenital malformations are not increased in subsequent conceptions.[96] However, when in vitro fertilization and embryo cryopreservationis practised between or shortly after treatment, possible genetic risks to the growing oocytes exist, and hence it has been recommended that the babies be screened.[96]
Between 30 and 40 percent of people undergoing chemotherapy experience chemotherapy-induced peripheral neuropathy (CIPN), a progressive, enduring, and often irreversible condition, causing pain, tingling, numbness and sensitivity to cold, beginning in the hands and feet and sometimes progressing to the arms and legs.[97] Chemotherapy drugs associated with CIPN include thalidomide, epothilones, vinca alkaloids, taxanes, proteasome inhibitors, and the platinum-based drugs.[97][98] Whether CIPN arises, and to what degree, is determined by the choice of drug, duration of use, the total amount consumed and whether the person already has peripheral neuropathy. Though the symptoms are mainly sensory, in some cases motor nerves and the autonomic nervous system are affected.[99] CIPN often follows the first chemotherapy dose and increases in severity as treatment continues, but this progression usually levels off at completion of treatment. The platinum-based drugs are the exception; with these drugs, sensation may continue to deteriorate for several months after the end of treatment.[100] Some CIPN appears to be irreversible.[100] Pain can often be managed with drug or other treatment but the numbness is usually resistant to treatment.[101]
Some people receiving chemotherapy report fatigue or non-specific neurocognitive problems, such as an inability to concentrate; this is sometimes called post-chemotherapy cognitive impairment, referred to as "chemo brain" in popular and social media.[102]
In particularly large tumors and cancers with high white cell counts, such as lymphomas, teratomas, and some leukemias, some people develop tumor lysis syndrome. The rapid breakdown of cancer cells causes the release of chemicals from the inside of the cells. Following this, high levels of uric acid, potassium and phosphate are found in the blood. High levels of phosphate induce secondary hypoparathyroidism, resulting in low levels of calcium in the blood.  This causes kidney damage and the high levels of potassium can cause cardiac arrhythmia. Although prophylaxis is available and is often initiated in people with large tumors, this is a dangerous side-effect that can lead to death if left untreated.[4]:202
Cardiotoxicity (heart damage) is especially prominent with the use of anthracycline drugs (doxorubicin, epirubicin, idarubicin, and liposomal doxorubicin). The cause of this is most likely due to the production of free radicals in the cell and subsequent DNA damage. Other chemotherapeutic agents that cause cardiotoxicity, but at a lower incidence, are cyclophosphamide, docetaxel and clofarabine.[103]
Hepatotoxicity (liver damage) can be caused by many cytotoxic drugs. The susceptibility of an individual to liver damage can be altered by other factors such as the cancer itself, viral hepatitis, immunosuppression and nutritional deficiency. The liver damage can consist of damage to liver cells, hepatic sinusoidal syndrome (obstruction of the veins in the liver), cholestasis (where bile does not flow from the liver to the intestine) and liver fibrosis.[104][105]
Nephrotoxicity (kidney damage) can be caused by tumor lysis syndrome and also due direct effects of drug clearance by the kidneys. Different drugs will affect different parts of the kidney and the toxicity may be asymptomatic (only seen on blood or urine tests) or may cause acute renal failure.[106][107]
Ototoxicity (damage to the inner ear) is a common side effect of platinum based drugs that can produce symptoms such as dizziness and vertigo.[108][109]
Less common side-effects include red skin (erythema), dry skin, damaged fingernails, a dry mouth (xerostomia), water retention, and sexual impotence. Some medications can trigger allergic or pseudoallergic reactions.
Specific chemotherapeutic agents are associated with organ-specific toxicities, including cardiovascular disease (e.g., doxorubicin), interstitial lung disease (e.g., bleomycin) and occasionally secondary neoplasm (e.g., MOPP therapy for Hodgkin's disease).
Hand-foot syndrome is another side effect to cytotoxic chemotherapy.
Chemotherapy does not always work, and even when it is useful, it may not completely destroy the cancer. People frequently fail to understand its limitations. In one study of people who had been newly diagnosed with incurable, stage 4 cancer, more than two-thirds of people with lung cancer and more than four-fifths of people with colorectal cancer still believed that chemotherapy was likely to cure their cancer.[110]
The blood–brain barrier poses an obstacle to delivery of chemotherapy to the brain. This is because the brain has an extensive system in place to protect it from harmful chemicals. Drug transporters can pump out drugs from the brain and brain's blood vessel cells into the cerebrospinal fluid and blood circulation. These transporters pump out most chemotherapy drugs, which reduces their efficacy for treatment of brain tumors. Only small lipophilic alkylating agents such as lomustine or temozolomide are able to cross this blood–brain barrier.[111][112][113]
Blood vessels in tumors are very different from those seen in normal tissues. As a tumor grows, tumor cells furthest away from the blood vessels become low in oxygen (hypoxic). To counteract this they then signal for new blood vessels to grow. The newly formed tumor vasculature is poorly formed and does not deliver an adequate blood supply to all areas of the tumor. This leads to issues with drug delivery because many drugs will be delivered to the tumor by the circulatory system.[114]
Resistance is a major cause of treatment failure in chemotherapeutic drugs. There are a few possible causes of resistance in cancer, one of which is the presence of small pumps on the surface of cancer cells that actively move chemotherapy from inside the cell to the outside. Cancer cells produce high amounts of these pumps, known as p-glycoprotein, in order to protect themselves from chemotherapeutics. Research on p-glycoprotein and other such chemotherapy efflux pumps is currently ongoing. Medications to inhibit the function of p-glycoprotein are undergoing investigation, but due to toxicities and interactions with anti-cancer drugs their development has been difficult.[115][116] Another mechanism of resistance is gene amplification, a process in which multiple copies of a gene are produced by cancer cells. This overcomes the effect of drugs that reduce the expression of genes involved in replication. With more copies of the gene, the drug can not prevent all expression of the gene and therefore the cell can restore its proliferative ability. Cancer cells can also cause defects in the cellular pathways of apoptosis (programmed cell death). As most chemotherapy drugs kill cancer cells in this manner, defective apoptosis allows survival of these cells, making them resistant. Many chemotherapy drugs also cause DNA damage, which can be repaired by enzymes in the cell that carry out DNA repair. Upregulation of these genes can overcome the DNA damage and prevent the induction of apoptosis. Mutations in genes that produce drug target proteins, such as tubulin, can occur which prevent the drugs from binding to the protein, leading to resistance to these types of drugs.[117] Drugs used in chemotherapy can induce cell stress, which can kill a cancer cell; however, under certain conditions, cells stress can induce changes in gene expression that enables resistance to several types of drugs.[118]
Targeted therapies are a relatively new class of cancer drugs that can overcome many of the issues seen with the use of cytotoxics. They are divided into two groups: small molecule and antibodies. The massive toxicity seen with the use of cytotoxics is due to the lack of cell specificity of the drugs. They will kill any rapidly dividing cell, tumor or normal. Targeted therapies are designed to affect cellular proteins or processes that are utilised by the cancer cells. This allows a high dose to cancer tissues with a relatively low dose to other tissues. Although the side effects are often less severe than that seen of cytotoxic chemotherapeutics, life-threatening effects can occur. Initially, the targeted therapeutics were supposed to be solely selective for one protein. Now it is clear that there is often a range of protein targets that the drug can bind. An example target for targeted therapy is the protein produced by the Philadelphia chromosome, a genetic lesion found commonly in chronic myelomonocytic leukemia. This fusion protein has enzyme activity that can be inhibited by imatinib, a small molecule drug.[119][120][121][122]
Cancer is the uncontrolled growth of cells coupled with malignant behaviour: invasion and metastasis (among other features).[123] It is caused by the interaction between genetic susceptibility and environmental factors.[124][125] These factors lead to accumulations of genetic mutations in oncogenes (genes that control the growth rate of cells) and tumor suppressor genes (genes that help to prevent cancer), which gives cancer cells their malignant characteristics, such as uncontrolled growth.[126]:93–94
In the broad sense, most chemotherapeutic drugs work by impairing mitosis (cell division), effectively targeting fast-dividing cells. As these drugs cause damage to cells, they are termed cytotoxic. They prevent mitosis by various mechanisms including damaging DNA and inhibition of the cellular machinery involved in cell division.[34][127] One theory as to why these drugs kill cancer cells is that they induce a programmed form of cell death known as apoptosis.[128]
As chemotherapy affects cell division, tumors with high growth rates (such as acute myelogenous leukemia and the aggressive lymphomas, including Hodgkin's disease) are more sensitive to chemotherapy, as a larger proportion of the targeted cells are undergoing cell division at any time.  Malignancies with slower growth rates, such as indolent lymphomas, tend to respond to chemotherapy much more modestly.[1] Heterogeneic tumours may also display varying sensitivities to chemotherapy agents, depending on the subclonal populations within the tumor.
Cells from the immune system also make crucial contributions to the antitumor effects of chemotherapy.[129] For example, the chemotherapeutic drugs oxaliplatin and cyclophosphamide can cause tumor cells to die in a way that is detectable by the immune system (called immunogenic cell death), which mobilizes immune cells with antitumor functions.[130] Chemotherapeutic drugs that cancer immunogenic tumor cell death can make unresponsive tumors sensitive to immune checkpoint therapy.[131]
Some chemotherapy drugs are used in diseases other than cancer, such as in autoimmune disorders,[132] and noncancerous plasma cell dyscrasia. In some cases they are often used at lower doses, which means that the side effects are minimized,[132] while in other cases doses similar to ones used to treat cancer are used. Methotrexate is used in the treatment of rheumatoid arthritis (RA),[133] psoriasis,[134] ankylosing spondylitis[135] and multiple sclerosis.[136][137] The anti-inflammatory response seen in RA is thought to be due to increases in adenosine, which causes immunosuppression; effects on immuno-regulatory cyclooxygenase-2 enzyme pathways; reduction in pro-inflammatory cytokines; and anti-proliferative properties.[133] Although methotrexate is used to treat both multiple sclerosis and ankylosing spondylitis, its efficacy in these diseases is still uncertain.[135][136][137] Cyclophosphamide is sometimes used to treat lupus nephritis, a common symptom of systemic lupus erythematosus.[138] Dexamethasone along with either bortezomib or melphalan is commonly used as a treatment for AL amyloidosis. Recently, bortezomid in combination with cyclophosphamide and dexamethasone has also shown promise as a treatment for AL amyloidosis. Other drugs used to treat myeloma such as lenalidomide have shown promise in treating AL amyloidosis.[139]
Chemotherapy drugs are also used in conditioning regimens prior to bone marow transplant (hematopoietic stem cell transplant). Conditioning regimens are used to suppress the recipient's immune system in order to allow a transplant to engraft. Cyclophosphamide is a common cytotoxic drug used in this manner, and is often used in conjunction with total body irradiation. Chemotherapeutic drugs may be used at high doses to permanently remove the recipient's bone marrow cells (myeloablative conditioning) or at lower doses that will prevent permanent bone marrow loss (non-myeloablative and reduced intensity conditioning).[140] When used in non-cancer setting, the treatment is still called "chemotherapy", and is often done in the same treatment centers used for people with cancer.
Healthcare workers exposed to antineoplastic agents take precautions to keep their exposure to a minimum. There is a limitation in cytotoxics dissolution in Australia and the United States to 20 dissolutions per pharmacist/nurse,[citation needed] since pharmacists who prepare these drugs or nurses who may prepare or administer them are the two occupational groups with the highest potential exposure to antineoplastic agents. In addition, physicians and operating room personnel may also be exposed as they treat people. Hospital staff, such as shipping and receiving personnel, custodial workers, laundry workers, and waste handlers, all have potential exposure to these drugs during the course of their work. The increased use of antineoplastic agents in veterinary oncology also puts these workers at risk for exposure to these drugs.[141][142] Routes of entry into the worker's body are skin absorption, inhalation, and ingestion via hand-to-mouth.[142] The long-term effects of exposure include chromosomal abnormalities and infertility.[4]:38
There is an extensive list of antineoplastic agents. Several classification schemes have been used to subdivide the medicines used for cancer into several different types.[citation needed]
The first use of small-molecule drugs to treat cancer was in the early 20th century, although the specific chemicals first used were not originally intended for that purpose. Mustard gas was used as a chemical warfare agent during World War I and was discovered to be a potent suppressor of hematopoiesis (blood production).[143] A similar family of compounds known as nitrogen mustards were studied further during World War II at the Yale School of Medicine.[144] It was reasoned that an agent that damaged the rapidly growing white blood cells might have a similar effect on cancer.[144] Therefore, in December 1942, several people with advanced lymphomas (cancers of the lymphatic system and lymph nodes) were given the drug by vein, rather than by breathing the irritating gas.[144] Their improvement, although temporary, was remarkable.[145] Concurrently, during a military operation in World War II, following a German air raid on the Italian harbour of Bari, several hundred people were accidentally exposed to mustard gas, which had been transported there by the Allied forces to prepare for possible retaliation in the event of German use of chemical warfare. The survivors were later found to have very low white blood cell counts.[146] After WWII was over and the reports declassified, the experiences converged and led researchers to look for other substances that might have similar effects against cancer. The first chemotherapy drug to be developed from this line of research was mustine. Since then, many other drugs have been developed to treat cancer, and drug development has exploded into a multibillion-dollar industry, although the principles and limitations of chemotherapy discovered by the early researchers still apply.[147]
The word chemotherapy without a modifier usually refers to cancer treatment, but its historical meaning was broader. The term was coined in the early 1900s by Paul Ehrlich as meaning any use of chemicals to treat any disease (chemo- + -therapy), such as the use of antibiotics (antibacterial chemotherapy).[148]  Ehrlich was not optimistic that effective chemotherapy drugs would be found for the treatment of cancer.[148]  The first modern chemotherapeutic agent was arsphenamine, an arsenic compound discovered in 1907 and used to treat syphilis.[149] This was later followed by sulfonamides (sulfa drugs) and penicillin. In today's usage, the sense "any treatment of disease with drugs" is often expressed with the word pharmacotherapy.
The top 10 best-selling (in terms of revenue) cancer drugs of 2013:[150]
Specially targeted delivery vehicles aim to increase effective levels of chemotherapy for tumor cells while reducing effective levels for other cells. This should result in an increased tumor kill or reduced toxicity or both.[151]
Antibody-drug conjugates (ADCs) comprise an antibody, drug and a linker between them. The antibody will be targeted at a preferentially expressed protein in the tumour cells (known as a tumor antigen) or on cells that the tumor can utilise, such as blood vessel endothelial cells. They bind to the tumor antigen and are internalised, where the linker releases the drug into the cell. These specially targeted delivery vehicles vary in their stability, selectivity, and choice of target, but, in essence, they all aim to increase the maximum effective dose that can be delivered to the tumor cells.[152] Reduced systemic toxicity means that they can also be used in people who are sicker, and that they can carry new chemotherapeutic agents that would have been far too toxic to deliver via traditional systemic approaches.[citation needed]
The first approved drug of this type was gemtuzumab ozogamicin (Mylotarg), released by Wyeth (now Pfizer). The drug was approved to treat acute myeloid leukemia, but has now been withdrawn from the market because the drug did not meet efficacy targets in further clinical trials.[153][154] Two other drugs, trastuzumab emtansine and brentuximab vedotin, are both in late clinical trials, and the latter has been granted accelerated approval for the treatment of refractory Hodgkin's lymphoma and systemic anaplastic large cell lymphoma.[152]
Nanoparticles are 1–1000 nanometer (nm) sized particles that can promote tumor selectivity and aid in delivering low-solubility drugs. Nanoparticles can be targeted passively or actively. Passive targeting exploits the difference between tumor blood vessels and normal blood vessels. Blood vessels in tumors are "leaky" because they have gaps from 200–2000 nm, which allow nanoparticles to escape into the tumor. Active targeting uses biological molecules (antibodies, proteins, DNA and receptor ligands) to preferentially target the nanoparticles to the tumor cells. There are many types of nanoparticle delivery systems, such as silica, polymers, liposomes and magnetic particles. Nanoparticles made of magnetic material can also be used to concentrate agents at tumor sites using an externally applied magnetic field.[151]  They have emerged as a useful vehicle in magnetic drug delivery for poorly soluble agents such as paclitaxel.[155]
Electrochemotherapy is the combined treatment in which injection of a chemotherapeutic drug is followed by application of high-voltage electric pulses locally to the tumor. The treatment enables the chemotherapeutic drugs, which otherwise cannot or hardly go through the membrane of cells (such as bleomycin and cisplatin), to enter the cancer cells. Hence, greater effectiveness of antitumor treatment is achieved.
Clinical electrochemotherapy has been successfully used for treatment of cutaneous and subcutaneous tumors irrespective of their histological origin.[156][157][158][159][160][161] The method has been reported as safe, simple and highly effective in all reports on clinical use of electrochemotherapy. According to the ESOPE project (European Standard Operating Procedures of Electrochemotherapy), the Standard Operating Procedures (SOP) for electrochemotherapy were prepared, based on the experience of the leading European cancer centres on electrochemotherapy.[158][162] Recently, new electrochemotherapy modalities have been developed for treatment of internal tumors using surgical procedures, endoscopic routes or percutaneous approaches to gain access to the treatment area.[163][164]
Hyperthermia therapy is heat treatment for cancer that can be a powerful tool when used in combination with chemotherapy (thermochemotherapy) or radiation for the control of a variety of cancers.  The heat can be applied locally to the tumor site, which will dilate blood vessels to the tumor, allowing more chemotherapeutic medication to enter the tumor.  Additionally, the tumor cell membrane will become more porous, further allowing more of the chemotherapeutic medicine to enter the tumor cell.
Hyperthermia has also been shown to help prevent or reverse "chemo-resistance."  Chemotherapy resistance sometimes develops over time as the tumors adapt and can overcome the toxicity of the chemo medication. "Overcoming chemoresistance has been extensively studied within the past, especially using CDDP-resistant cells. In regard to the potential benefit that drug-resistant cells can be recruited for effective therapy by combining chemotherapy with hyperthermia, it was important to show that chemoresistance against several anticancer drugs (e.g. mitomycin C, anthracyclines, BCNU, melphalan) including CDDP could be reversed at least partially by the addition of heat.[165]
Chemotherapy is used in veterinary medicine similar to how it is used in human medicine.[166]
RET inhibitors: Vandetanib (also VEGFR and EGFR). Entrectinib (ALK, ROS1, NTRK).
c-MET inhibitor: Cabozantinib (also VEGFR2).



Targeted therapy - Wikipedia
Targeted therapy or molecularly targeted therapy is one of the major modalities of medical treatment (pharmacotherapy) for cancer, others being hormonal therapy and cytotoxic chemotherapy. As a form of molecular medicine, targeted therapy blocks the growth of cancer cells by interfering with specific targeted molecules needed for carcinogenesis and tumor growth,[1] rather than by simply interfering with all rapidly dividing cells (e.g. with traditional chemotherapy). Because most agents for targeted therapy are biopharmaceuticals, the term biologic therapy is sometimes synonymous with targeted therapy when used in the context of cancer therapy (and thus distinguished from chemotherapy, that is, cytotoxic therapy). However, the modalities can be combined; antibody-drug conjugates combine biologic and cytotoxic mechanisms into one targeted therapy.
Another form of targeted therapy involves use of nanoengineered enzymes to bind to a tumor cell such that the body's natural cell degradation process can digest the cell, effectively eliminating it from the body.  The basic biological mechanism behind such research techniques are under investigation in a limited form with drugs derived from medicinal cannabis (drug) today in the United States.  One example includes reduction and elimination of brain tumors with intake of small amounts of oil derived from engineered strains of medicinal cannabis.
Targeted cancer therapies are expected to be more effective than older forms of treatments and less harmful to normal cells. Many targeted therapies are examples of immunotherapy (using immune mechanisms for therapeutic goals) developed by the field of cancer immunology. Thus, as immunomodulators, they are one type of biological response modifiers.
The most successful targeted therapies are chemical entities that target or preferentially target a protein or enzyme that carries a mutation or other genetic alteration that is specific to cancer cells and not found in normal host tissue. One of the most successful molecular targeted therapeutic is Gleevec, which is a kinase inhibitor with exceptional affinity for the oncofusion protein BCR-Abl which is a strong driver of tumorigenesis in chronic myelogenous leukemia. Although employed in other indications, Gleevec is most effective targeting BCR-Abl. Other examples of molecular targeted therapeutics targeting mutated oncogenes, include PLX27892 which targets mutant B-raf in melanoma.
There are targeted therapies for colorectal cancer, head and neck cancer, breast cancer, multiple myeloma, lymphoma, prostate cancer, melanoma and other cancers.[2]
Biomarkers are usually required to aid the selection of patients who will likely respond to a given targeted therapy.[3]
The definitive experiments that showed that targeted therapy would reverse the malignant phenotype of tumor cells involved treating Her2/neu transformed cells with monoclonal antibodies in vitro and in vivo by Mark Greene’s laboratory and reported from 1985.[4]
Some have challenged the use of the term, stating that drugs usually associated with the term are insufficiently selective.[5] The phrase occasionally appears in scare quotes: "targeted therapy".[6]  Targeted therapies may also be described as "chemotherapy" or "non-cytotoxic chemotherapy", as "chemotherapy" strictly means only "treatment by chemicals". But in typical medical and general usage "chemotherapy" is now mostly used specifically for "traditional" cytotoxic chemotherapy.
The main categories of targeted therapy are currently small molecules and monoclonal antibodies.
Many are tyrosine-kinase inhibitors.
Several are in development and a few have been licensed by the FDA and the European Commission. Examples of licensed monoclonal antibodies include:
Many antibody-drug conjugates (ADCs) are being developed. See also ADEPT (antibody-directed enzyme prodrug therapy).
In the U.S., the National Cancer Institute's  Molecular Targets Development Program (MTDP) aims to identify and evaluate molecular targets that may be candidates for drug development.
RET inhibitors: Vandetanib (also VEGFR and EGFR). Entrectinib (ALK, ROS1, NTRK).
c-MET inhibitor: Cabozantinib (also VEGFR2).



Pancreatic cancer - Wikipedia

Pancreatic cancer arises when cells in the pancreas, a glandular organ behind the stomach, begin to multiply out of control and form a mass. These cancerous cells have the ability to invade other parts of the body.[10] There are a number of types of pancreatic cancer.[6] The most common, pancreatic adenocarcinoma, accounts for about 85% of cases, and the term "pancreatic cancer" is sometimes used to refer only to that type.[6] These adenocarcinomas start within the part of the pancreas which makes digestive enzymes.[6] Several other types of cancer, which collectively represent the majority of the non-adenocarcinomas, can also arise from these cells.[6] One to two percent of cases of pancreatic cancer are neuroendocrine tumors, which arise from the hormone-producing cells of the pancreas.[6] These are generally less aggressive than pancreatic adenocarcinoma.[6]
Signs and symptoms of the most common form of pancreatic cancer may include yellow skin, abdominal or back pain, unexplained weight loss, light-colored stools, dark urine and loss of appetite.[1] There are usually no symptoms in the disease's early stages, and symptoms that are specific enough to suggest pancreatic cancer typically do not develop until the disease has reached an advanced stage.[1][2] By the time of diagnosis, pancreatic cancer has often spread to other parts of the body.[6][11]
Pancreatic cancer rarely occurs before the age of 40, and more than half of cases of pancreatic adenocarcinoma occur in those over 70.[2] Risk factors for pancreatic cancer include tobacco smoking, obesity, diabetes, and certain rare genetic conditions.[2] About 25% of cases are linked to smoking,[3] and 5–10% are linked to inherited genes.[2] Pancreatic cancer is usually diagnosed by a combination of medical imaging techniques such as ultrasound or computed tomography, blood tests, and examination of tissue samples (biopsy).[3][4] The disease is divided into stages, from early (stage I) to late (stage IV).[11] Screening the general population has not been found to be effective.[12]
The risk of developing pancreatic cancer is lower among non-smokers, and people who maintain a healthy weight and limit their consumption of red or processed meat.[5] A smoker's chance of developing the disease decreases if they stop smoking, and almost returns to that of the rest of the population after 20 years.[6] Pancreatic cancer can be treated with surgery, radiotherapy, chemotherapy, palliative care, or a combination of these.[1] Treatment options are partly based on the cancer stage.[1] Surgery is the only treatment that can cure pancreatic adenocarcinoma,[11] and may also be done to improve quality of life without the potential for cure.[1][11] Pain management and medications to improve digestion are sometimes needed.[11] Early palliative care is recommended even for those receiving treatment that aims for a cure.[13][14]
In 2015, pancreatic cancers of all types resulted in 411,600 deaths globally.[9] Pancreatic cancer is the fifth most common cause of death from cancer in the United Kingdom,[15] and the fourth most common in the United States.[16][17] The disease occurs most often in the developed world, where about 70% of the new cases in 2012 originated.[6] Pancreatic adenocarcinoma typically has a very poor prognosis: after diagnosis, 25% of people survive one year and 5% live for five years.[6][7] For cancers diagnosed early, the five-year survival rate rises to about 20%.[18] Neuroendocrine cancers have better outcomes; at five years from diagnosis, 65% of those diagnosed are living, though survival varies considerably depending on the type of tumor.[6]
The many types of pancreatic cancer can be divided into two general groups. The vast majority of cases (about 95%) occur in the part of the pancreas which produces digestive enzymes, known as the exocrine component. There are several sub-types of exocrine pancreatic cancers, but their diagnosis and treatment have much in common.  The small minority of cancers that arise in the hormone-producing (endocrine) tissue of the pancreas have different clinical characteristics and are called pancreatic neuroendocrine tumors, sometimes abbreviated as "PanNETs". Both groups occur mainly (but not exclusively) in people over 40, and are slightly more common in men, but some rare sub-types mainly occur in women or children.[19][20]
The exocrine group is dominated by pancreatic adenocarcinoma (variations of this name may add "invasive" and "ductal"), which is by far the most common type, representing about 85% of all pancreatic cancers.[2] Nearly all these start in the ducts of the pancreas, as pancreatic ductal adenocarcinoma (PDAC).[21] This is despite the fact that the tissue from which it arises – the pancreatic ductal epithelium – represents less than 10% of the pancreas by cell volume, because it constitutes only the ducts (an extensive but capillary-like duct-system fanning out) within the pancreas.[22]  This cancer originates in the ducts that carry secretions (such as enzymes and bicarbonate) away from the pancreas. About 60–70% of adenocarcinomas occur in the head of the pancreas.[2]
The next most common type, acinar cell carcinoma of the pancreas, arises in the clusters of cells that produce these enzymes, and represents 5% of exocrine pancreas cancers.[23] Like the 'functioning' endocrine cancers described below, acinar cell carcinomas may cause over-production of certain molecules, in this case digestive enzymes, which may cause symptoms such as skin rashes and joint pain.
Cystadenocarcinomas account for 1% of pancreatic cancers, and they have a better prognosis than the other exocrine types.[23]
Pancreatoblastoma is a rare form, mostly occurring in childhood, and with a relatively good prognosis.  Other exocrine cancers include adenosquamous carcinomas, signet ring cell carcinomas, hepatoid carcinomas, colloid carcinomas, undifferentiated carcinomas, and undifferentiated carcinomas with osteoclast-like giant cells.  Solid pseudopapillary tumor is a rare low-grade neoplasm that mainly affects younger women, and generally has a very good prognosis.[2][24]
Pancreatic mucinous cystic neoplasms are a broad group of pancreas tumors that have varying malignant potential. They are being detected at a greatly increased rate as CT scans become more powerful and common, and discussion continues as how best to assess and treat them, given that many are benign.[25]
The small minority of tumors that arise elsewhere in the pancreas are mainly pancreatic neuroendocrine tumors (PanNETs).[26] Neuroendocrine tumors (NETs) are a diverse group of benign or malignant tumors that arise from the body's neuroendocrine cells, which are responsible for integrating the nervous and endocrine systems. NETs can start in most organs of the body, including the pancreas, where the various malignant types are all considered to be rare.  PanNETs are grouped into 'functioning' and 'non-functioning' types, depending on the degree to which they produce hormones. The functioning types secrete hormones such as insulin, gastrin, and glucagon into the bloodstream, often in large quantities, giving rise to serious symptoms such as low blood sugar, but also favoring relatively early detection. The most common functioning PanNETs are insulinomas and gastrinomas, named after the hormones they secrete. The non-functioning types do not secrete hormones in a sufficient quantity to give rise to overt clinical symptoms. For this reason, non-functioning PanNETs are often diagnosed only after the cancer has spread to other parts of the body.[27]
As with other neuroendocrine tumors, the history of the terminology and classification of PanNETs is complex.[26] PanNETs are sometimes called "islet cell cancers",[28] even though it is now known that they do not actually arise from islet cells as previously thought.[27]
Since pancreatic cancer usually does not cause recognizable symptoms in its early stages, the disease is typically not diagnosed until it has spread beyond the pancreas itself.[4]  This is one of the main reasons for the generally poor survival rates. Exceptions to this are the functioning PanNETs, where over-production of various active hormones can give rise to symptoms (which depend on the type of hormone).[29]
Bearing in mind that the disease is rarely diagnosed before the age of 40, common symptoms of pancreatic adenocarcinoma occurring before diagnosis include:
Other common manifestations of the disease include: weakness and tiring easily; dry mouth; sleep problems; and a palpable abdominal mass.[31]
The spread of pancreatic cancer to other organs (metastasis) may also cause symptoms. Typically, pancreatic adenocarcinoma first spreads to nearby lymph nodes, and later to the liver or to the peritoneal cavity, large intestine or lungs.[3] It is uncommon for it to spread to the bones or brain.[33]
Cancers in the pancreas may also be secondary cancers that have spread from other parts of the body.  This is uncommon, found in only about 2% of cases of pancreatic cancer. Kidney cancer is by far the most common cancer to spread to the pancreas, followed by colorectal cancer, and then cancers of the skin, breast, and lung.  Surgery may be performed on the pancreas in such cases, whether in hope of a cure or to alleviate symptoms.[34]
Risk factors for pancreatic adenocarcinoma include:[2][6][11][35]
Drinking alcohol excessively is a major cause of chronic pancreatitis, which in turn predisposes to pancreatic cancer.  However, considerable research has failed to firmly establish alcohol consumption as a direct risk factor for pancreatic cancer. Overall, the association is consistently weak and the majority of studies have found no association, with smoking a strong confounding factor. The evidence is stronger for a link with heavy drinking, of at least six drinks per day.[3][42]
Exocrine cancers are thought to arise from several types of precancerous lesions within the pancreas. But these lesions do not always progress to cancer, and the increased numbers detected as a by-product of the increasing use of CT scans for other reasons are not all treated.[3]  Apart from pancreatic serous cystadenomas (SCNs), which are almost always benign, four types of precancerous lesion are recognized.
The first is pancreatic intraepithelial neoplasia. These lesions are microscopic abnormalities in the pancreas and are often found in autopsies of people with no diagnosed cancer. These lesions may progress from low to high grade and then to a tumor. More than 90% of cases at all grades carry a faulty KRAS gene, while in grades 2 and 3 damage to three further genes – CDKN2A (p16), p53 and SMAD4 – are increasingly often found.[2]
A second type are the intraductal papillary mucinous neoplasms (IPMNs). These are macroscopic lesions, which are found in about 2% of all adults. This rate rises to ~10% by age 70. These lesions have about a 25% risk of developing into invasive cancer. They may have KRAS gene mutations (~40–65% of cases) and in the GNAS Gs alpha subunit and RNF43, affecting the Wnt signaling pathway.[2] Even if removed surgically, there remains a considerably increased risk of pancreatic cancer developing subsequently.[3]
The third type, pancreatic mucinous cystic neoplasms (MCNs) mainly occur in women, and may remain benign or progress to cancer.[43] If these lesions become large, cause symptoms, or have suspicious features, they can usually be successfully removed by surgery.[3]
A fourth type of cancer that arises in the pancreas is the intraductal tubulopapillary neoplasm. This type was recognised by the WHO in 2010 and constitutes about 1–3% of all pancreatic neoplasms.  Mean age at diagnosis is 61 years (range 35–78 years). About 50% of these lesions become invasive. Diagnosis depends on histology as these lesions are very difficult to differentiate from other lesions on either clinical or radiological grounds.[44]
The genetic events found in ductal adenocarcinoma have been well characterized, and complete exome sequencing has been done for the common types of tumor. Four genes have each been found to be mutated in the majority of adenocarcinomas: KRAS (in 95% of cases), CDKN2A (also in 95%), TP53 (75%), and SMAD4 (55%).  The last of these are especially associated with a poor prognosis.[3] SWI/SNF mutations/deletions occur in about 10–15% of the adenocarcinomas.[2]  The genetic alterations in several other types of pancreatic cancer and precancerous lesions have also been researched.[3] Transcriptomics analyses and mRNA sequencing for the common forms of pancreatic cancer have found that 75% of human genes are expressed in the tumors, with some 200 genes more specifically expressed in pancreatic cancer as compared to other tumor types.[45][46]
The genes often found mutated in PanNETs are different from those in exocrine pancreatic cancer.[47] For example, KRAS mutation is normally absent. Instead, hereditary MEN1 gene mutations give rise to MEN1 syndrome, in which primary tumors occur in two or more endocrine glands. About 40–70% of people born with a MEN1 mutation eventually develop a PanNet.[48] Other genes that are frequently mutated include DAXX, mTOR and ATRX.[27]
The symptoms of pancreatic adenocarcinoma do not usually appear in the disease's early stages, and are individually not distinctive to the disease.[3][11][30]  The symptoms at diagnosis vary according to the location of the cancer in the pancreas, which anatomists divide (from left to right on most diagrams) into the thick head, the neck, and the tapering body, ending in the tail.
Regardless of a tumor's location, the most common symptom is unexplained weight loss, which may be considerable. A large minority (between 35% and 47%) of people diagnosed with the disease will have had nausea, vomiting or a feeling of weakness.  Tumors in the head of the pancreas typically also cause jaundice, pain, loss of appetite, dark urine, and light-colored stools. Tumors in the body and tail typically also cause pain.[30]
People sometimes have recent onset of atypical type 2 diabetes that is difficult to control, a history of recent but unexplained blood vessel inflammation caused by blood clots (thrombophlebitis) known as Trousseau sign, or a previous attack of pancreatitis.[30] A doctor may suspect pancreatic cancer when the onset of diabetes in someone over 50 years old is accompanied  by typical symptoms such as unexplained weight loss, persistent abdominal or back pain, indigestion, vomiting, or fatty feces.[11] Jaundice accompanied by a painlessly swollen gallbladder (known as Courvoisier's sign) may also raise suspicion, and can help differentiate pancreatic cancer from gallstones.[49]
Medical imaging techniques, such as computed tomography (CT scan) and endoscopic ultrasound (EUS) are used both to confirm the diagnosis and to help decide whether the tumor can be surgically removed (its "resectability").[11] On contrast CT scan, pancreatic cancer typically shows a gradually increasing radiocontrast uptake, rather than a fast washout as seen in a normal pancreas or a delayed washout as seen in chronic pancreatitis.[50] Magnetic resonance imaging and positron emission tomography may also be used,[2] and magnetic resonance cholangiopancreatography may be useful in some cases.[30] Abdominal ultrasound is less sensitive and will miss small tumors, but can identify cancers that have spread to the liver and build-up of fluid in the peritoneal cavity (ascites).[11]  It may be used for a quick and cheap first examination before other techniques.[51]
A biopsy by fine needle aspiration, often guided by endoscopic ultrasound, may be used where there is uncertainty over the diagnosis, but a histologic diagnosis is not usually required for removal of the tumor by surgery to go ahead.[11]
Liver function tests can show a combination of results indicative of bile duct obstruction (raised conjugated bilirubin, γ-glutamyl transpeptidase and alkaline phosphatase levels). CA19-9 (carbohydrate antigen 19.9) is a tumor marker that is frequently elevated in pancreatic cancer. However, it lacks sensitivity and specificity, not least because 5% of people lack the Lewis (a) antigen and cannot produce CA19-9. It has a sensitivity of 80% and specificity of 73% in detecting pancreatic adenocarcinoma, and is used for following known cases rather than diagnosis.[2][11]
The most common form of pancreatic cancer (adenocarcinoma) is typically characterized by moderately to poorly differentiated glandular structures on microscopic examination. There is typically considerable desmoplasia or formation of a dense fibrous stroma or structural tissue consisting of a range of cell types (including myofibroblasts, macrophages, lymphocytes and mast cells) and deposited material (such as type I collagen and hyaluronic acid).  This creates a tumor microenvironment that is short of blood vessels (hypovascular) and so of oxygen (tumor hypoxia).[2]  It is thought that this prevents many chemotherapy drugs from reaching the tumor, as one factor making the cancer especially hard to treat.[2][3]
Pancreatic cancer is usually staged following a CT scan.[30]  The most widely used cancer staging system for pancreatic cancer is the one formulated by the American Joint Committee on Cancer (AJCC) together with the Union for International Cancer Control (UICC). The AJCC-UICC staging system designates four main overall stages, ranging from early to advanced disease, based on TNM classification of Tumor size, spread to lymph Nodes, and Metastasis.[52]
To help decide treatment, the tumors are also divided into three broader categories based on whether surgical removal seems possible: in this way, tumors are judged  to be "resectable", "borderline resectable", or "unresectable".[53] When the disease is still in an early stage (AJCC-UICC stages I and II), without spread to large blood vessels or distant organs such as the liver or lungs, surgical resection of the tumor can normally be performed, if the patient is willing to undergo this major operation and is thought to be sufficiently fit.[11] The AJCC-UICC staging system allows distinction between stage III tumors that are judged to be "borderline resectable" (where surgery is technically feasible because the celiac axis and superior mesenteric artery are still free) and those that are "unresectable" (due to more locally advanced disease); in terms of the more detailed TNM classification, these two groups correspond to T3 and T4 respectively.[3]
Stage T1 pancreatic cancer
Stage T2 pancreatic cancer
Stage T3 pancreatic cancer
Stage T4 pancreatic cancer
Pancreatic cancer in nearby lymph nodes – Stage N1
Locally advanced adenocarcinomas have spread into neighboring organs, which may be any of the following (in roughly decreasing order of frequency): the duodenum, stomach, transverse colon, spleen, adrenal gland, or kidney. Very often they also spread to the important blood or lymphatic vessels and nerves that run close to the pancreas, making surgery far more difficult. Typical sites for metastatic spread (stage IV disease) are the liver, peritoneal cavity and lungs, all  of which occur in 50% or more of fully advanced cases.[54]
The 2010 WHO classification of tumors of the digestive system grades all the pancreatic neuroendocrine tumors (PanNETs) into three categories, based on their degree of cellular differentiation (from "NET G1" through to the poorly differentiated "NET G3").[20] The U.S. National Comprehensive Cancer Network recommends use of the same AJCC-UICC staging system as pancreatic adenocarcinoma.[55]:52 Using this scheme, the stage-by-stage outcomes for PanNETs are dissimilar to those of the exocrine cancers.[56] A different TNM system for PanNETs has been proposed by the European Neuroendocrine Tumor Society.[20]
Apart from not smoking, the American Cancer Society recommends keeping a healthy weight, and increasing consumption of fruits, vegetables, and whole grains, while decreasing consumption of red and processed meat, although there is no consistent evidence this will prevent or reduce pancreatic cancer specifically.[57] A 2014 review of research concluded that there was evidence that consumption of citrus fruits and curcumin reduced risk of pancreatic cancer, while there was possibly a beneficial effect from whole grains, folate, selenium, and non-fried fish.[42]
In the general population, screening of large groups is not currently considered effective, although newer techniques, and the screening of tightly targeted groups, are being evaluated.[58][59] Nevertheless, regular screening with endoscopic ultrasound and MRI/CT imaging is recommended for those at high risk from inherited genetics.[4][51][59][60]
A key assessment that is made after diagnosis is whether surgical removal of the tumor is possible (see Staging), as this is the only cure for this cancer. Whether or not surgical resection can be offered depends on how much the cancer has spread. The exact location of the tumor is also a significant factor, and CT can show how it relates to the major blood vessels passing close to the pancreas. The general health of the person must also be assessed, though age in itself is not an obstacle to surgery.[3]
Chemotherapy and, to a lesser extent, radiotherapy are likely to be offered to most people, whether or not surgery is possible. Specialists advise that the management of pancreatic cancer should be in the hands of a multidisciplinary team including specialists in several aspects of oncology, and is, therefore, best conducted in larger centers.[2][3]
Surgery with the intention of a cure is only possible in around one-fifth (20%) of new cases.[11]  Although CT scans help, in practice it can be difficult to determine whether the tumor can be fully removed (its "resectability"), and it may only become apparent during surgery that it is not possible to successfully remove the tumor without damaging other vital tissues. Whether or not surgical resection can be offered depends on various factors, including the precise extent of local anatomical adjacency to, or involvement of, the venous or arterial blood vessels,[2] as well as surgical expertise and a careful consideration of projected post-operative recovery.[61][62] The age of the person is not in itself a reason not to operate, but their general performance status needs to be adequate for a major operation.[11]
One particular feature that is evaluated is the encouraging presence, or discouraging absence, of a clear layer or plane of fat creating a barrier between the tumor and the vessels.[3]  Traditionally, an assessment is made of the tumor's proximity to major venous or arterial vessels, in terms of "abutment" (defined as the tumor touching no more than half a blood vessel's circumference without any fat to separate it), "encasement" (when the tumor encloses most of the vessel's circumference), or full vessel involvement.[63]:22 A resection that includes encased sections of blood vessels may be possible in some cases,[64][65] particularly if preliminary neoadjuvant therapy is feasible,[66][67][68] using chemotherapy[62][63]:36[69] and/or radiotherapy.[63]:29–30
Even when the operation appears to have been successful, cancerous cells are often found around the edges ("margins") of the removed tissue,  when a pathologist  examines them microscopically (this will always be done), indicating the cancer has not been entirely removed.[2] Furthermore, cancer stem cells are usually not evident microscopically, and if they are present they may continue to develop and spread.[70][71] An exploratory laparoscopy (a small, camera-guided surgical procedure) may therefore be performed to gain a clearer idea of the outcome of a full operation.[72]
For cancers involving the head of the pancreas, the Whipple procedure is the most commonly attempted curative surgical treatment. This is a major operation which involves removing the pancreatic head and the curve of the duodenum together ("pancreato-duodenectomy"), making a bypass for food from the stomach to the jejunum ("gastro-jejunostomy") and attaching a loop of jejunum to the cystic duct to drain bile ("cholecysto-jejunostomy"). It can be performed only if the person is likely to survive major surgery and if the cancer is localized without invading local structures or metastasizing. It can, therefore, be performed only in a minority of cases.  Cancers of the tail of the pancreas can be resected using a procedure known as a distal pancreatectomy, which often also entails removal of the spleen.[2][3] Nowadays, this can often be done using minimally invasive surgery.[2][3]
Although curative surgery no longer entails the very high death rates that occurred until the 1980s, a high proportion of people (about 30–45%) still have to be treated for a post-operative sickness that is not caused by the cancer itself.  The most common complication of surgery is difficulty in emptying the stomach.[3]  Certain more limited surgical procedures may also be used to ease symptoms (see Palliative care): for instance, if the cancer is invading or compressing the duodenum or colon. In such cases, bypass surgery might overcome the obstruction and improve quality of life but is not intended as a cure.[11]
After surgery, adjuvant chemotherapy with gemcitabine or 5-FU can be offered if the person is sufficiently fit, after a recovery period of one to two months.[4][51]  In people not suitable for curative surgery, chemotherapy may be used to extend life or improve its quality.[3] Before surgery, neoadjuvant chemotherapy or chemoradiotherapy  may be used in cases that are considered to be "borderline resectable" (see Staging) in order to reduce the cancer to a level where surgery could be beneficial. In other cases neoadjuvant therapy remains controversial, because it delays surgery.[3][4][73]
Gemcitabine was approved by the United States Food and Drug Administration (FDA) in 1997, after a clinical trial reported improvements in quality of life and a 5-week improvement in median survival duration in people with advanced pancreatic cancer.[74] This was the first chemotherapy drug approved by the FDA primarily for a nonsurvival clinical trial endpoint.[75]  Chemotherapy using gemcitabine alone was the standard for about a decade, as a number of trials testing it in combination with other drugs failed to demonstrate significantly better outcomes. However, the combination of gemcitabine with erlotinib was found to increase survival modestly, and erlotinib was licensed by the FDA for use in pancreatic cancer in 2005.[76]
The FOLFIRINOX chemotherapy regimen using four drugs was found more effective than gemcitabine, but with substantial side effects, and is thus only suitable for people with good performance status.  This is also true of protein-bound paclitaxel (nab-paclitaxel), which was licensed by the FDA in 2013 for use with gemcitabine in pancreas cancer.[77] By the end of 2013, both FOLFIRINOX and nab-paclitaxel with gemcitabine were regarded as good choices for those able to tolerate the side-effects, and gemcitabine remained an effective option for those who were not.  A head-to-head trial between the two new options is awaited, and trials investigating other variations continue.  However, the changes of the last few years have only increased survival times by a few months.[74]  Clinical trials are often conducted for novel adjuvant therapies.[4]
The role of radiotherapy as an auxiliary (adjuvant) treatment after potentially curative surgery has been controversial since the 1980s.[3] The European Society for Medical Oncology recommends that adjuvant radiotherapy should only be used for people enrolled in clinical trials.[51]  However, there is a continuing tendency for clinicians in the US to be more ready to use adjuvant radiotherapy than those in Europe.  Many clinical trials have tested a variety of treatment combinations since the 1980s, but have failed to settle the matter conclusively.[3][4]
Radiotherapy may form part of treatment to attempt to shrink a tumor to a resectable state, but its use on unresectable tumors remains controversial as there are conflicting results from clinical trials.  The preliminary results of one trial, presented in 2013, "markedly reduced enthusiasm" for its use on locally advanced tumors.[2]
Treatment of PanNETs, including the less common malignant types, may include a number of approaches.[55][78][79][80]  Some small tumors of less than 1 cm. that are identified incidentally, for example on a CT scan performed for other purposes, may be followed by watchful waiting.[55] This depends on the assessed risk of surgery which is influenced by the site of the tumor and the presence of other medical problems.[55] Tumors within the pancreas only (localized tumors), or with limited metastases, for example to the liver, may be removed by surgery.  The type of surgery depends on the tumor location, and the degree of spread to lymph nodes.[20]
For localized tumors, the surgical procedure may be much less extensive than the types of surgery used to treat pancreatic adenocarcinoma described above, but otherwise surgical procedures are similar to those for exocrine tumors.  The range of possible outcomes varies greatly; some types have a very high survival rate after surgery while others have a poor outlook.  As all this group are rare, guidelines emphasize that treatment should be undertaken in a specialized center.[20][27] Use of liver transplantation may be considered in certain cases of liver metastasis.[81]
For functioning tumors, the somatostatin analog class of medications, such as octreotide, can reduce the excessive production of hormones.[20] Lanreotide can slow tumor growth.[82] If the tumor is not amenable to surgical removal and is causing symptoms, targeted therapy with everolimus or sunitinib can reduce symptoms and slow progression of the disease.[27][83][84] Standard cytotoxic chemotherapy is generally not very effective for PanNETs, but may be used when other drug treatments fail to prevent the disease from progressing,[27][85] or in poorly differentiated PanNET cancers.[86]
Radiation therapy is occasionally used if there is pain due to anatomic extension, such as metastasis to bone.  Some  PanNETs absorb specific peptides or hormones, and these PanNETs may respond to nuclear medicine therapy with radiolabeled peptides or hormones such as iobenguane (iodine-131-MIBG).[87][88][89][90] Radiofrequency ablation (RFA), cryoablation, and hepatic artery embolization may also be used.[91][92]
Palliative care is medical care which focuses on treatment of symptoms from serious illness, such as cancer, and improving quality of life.[93]  Because pancreatic adenocarcinoma is usually diagnosed after it has progressed to an advanced stage, palliative care as a treatment of symptoms is often the only treatment possible.[94]
Palliative care focuses not on treating the underlying cancer, but on treating symptoms such as pain or nausea, and can assist in decision-making, including when or if hospice care will be beneficial.[95]  Pain can be managed with medications such as opioids or through procedural intervention, by a nerve block on the celiac plexus (CPB).  This alters or, depending on the technique used, destroys the nerves that transmit pain from the abdomen.  CPB is a safe and effective way to reduce the pain, which generally reduces the need to use opioid painkillers, which have significant negative side effects.[3][96]
Other symptoms or complications that can be treated with palliative surgery are obstruction by the tumor of the intestines or bile ducts.  For the latter, which occurs in well over half of cases, a small metal tube called a stent may be inserted by endoscope to keep the ducts draining.[30]  Palliative care can also help treat depression that often comes with the diagnosis of pancreatic cancer.[3]
Both surgery and advanced inoperable tumors often lead to digestive system disorders from a lack of the exocrine products of the pancreas (exocrine insufficiency).  These can be treated by taking pancreatin which contains manufactured pancreatic enzymes, and is best taken with food.[11]  Difficulty in emptying the stomach (delayed gastric emptying) is common and can be a serious problem, involving hospitalization. Treatment may involve a variety of approaches, including draining the stomach by nasogastric aspiration and drugs called proton-pump inhibitors or H2 antagonists, which both reduce production of gastric acid.[11] Medications like metoclopramide can also be used to clear stomach contents.
Pancreatic adenocarcinoma and the other less common exocrine cancers have a very poor prognosis, as they are normally diagnosed at a late stage when the cancer is already locally advanced or has spread to other parts of the body.[2]  Outcomes are much better for PanNETs: many are benign and completely without clinical symptoms, and even those cases not treatable with surgery have an average five-year survival rate of 16%,[53] although the outlook varies considerably according to the type.[29]
For locally advanced and metastatic pancreatic adenocarcinomas, which together represent over 80% of cases, numerous recent trials comparing chemotherapy regimes have shown increased survival times, but not to more than one year.[2][74] Overall five-year survival for pancreatic cancer in the US has improved from 2% in cases diagnosed in 1975–77, and 4% in 1987–89 diagnoses, to 6% in 2003–09.[97] In the less than 20% of cases of pancreatic adenocarcinoma with a diagnosis of a localized and small cancerous growth (less than 2 cm in Stage T1), about 20% of Americans survive to five years.[18]
About 1500 genes are linked to outcomes in pancreatic adenocarcinoma. These include both unfavorable genes, where high expression is related to poor outcome, for example C-Met and MUC-1, and favorable genes where high expression is associated with better survival, for example the transcription factor PELP1.[45][46]
As of 2012, pancreatic cancer resulted in 330,000 deaths globally,[6] up from 310,000 in 2010 and 200,000 in 1990.[98] In 2014, an estimated 46,000 people in the US are expected to be diagnosed with pancreatic cancer and 40,000 to die of it.[2]  Although it accounts for only 2.5% of new cases, pancreatic cancer is responsible for 6% of cancer deaths each year.[99]  It is the seventh highest cause of death from cancer worldwide.[6]
Globally pancreatic cancer is the 11th most common cancer in women and the 12th most common in men.[6] The majority of recorded cases occur in developed countries.[6] People from the United States have an average lifetime risk of about 1 in 67 (or 1.5%) of developing the disease,[100] slightly higher than the figure for the UK.[101] The disease is more common in men than women,[6][2] though the difference in rates has narrowed over recent decades, probably reflecting earlier increases in female smoking.  In the United States the risk for African Americans is over 50% greater than for whites, but the rates in Africa and East Asia are much lower than those in North America or Europe. The United States, Central and eastern Europe, and Argentina and Uruguay all have high rates.[6]
Pancreatic cancer is the 10th most common cancer in the UK (around 8,800 people were diagnosed with the disease in 2011), and it is the 5th most common cause of cancer death (around 8,700 people died in 2012).[102]
The annual incidence of clinically recognized PanNETs is low (about 5 per one million person-years) and is dominated by the non-functioning types.[24] Somewhere between 45% and 90% of PanNETs are thought to be of the non-functioning types.[20][27] Studies of autopsies have uncovered small PanNETs rather frequently, suggesting that the prevalence of tumors that remain inert and asymptomatic may be relatively high.[27] Overall PanNETs are thought to account for about 1 to 2% of all pancreatic tumors.[24] The definition and classification of PanNETs has changed over time, affecting what is known about their epidemiology and clinical relevance.[47]
The earliest recognition of pancreatic cancer has been attributed to the 18th-century Italian scientist Giovanni Battista Morgagni, the historical father of modern-day anatomic pathology, who claimed to have traced several cases of cancer in the pancreas. Many 18th and 19th-century physicians were skeptical about the existence of the disease, given the similar appearance of pancreatitis. Some case reports were published in the 1820s and 1830s, and a genuine histopathologic diagnosis was eventually recorded by the American clinician Jacob Mendes Da Costa, who also doubted the reliability of Morgagni's interpretations. By the start of the 20th century, cancer of the head of the pancreas had become a well-established diagnosis.[103]
Regarding the recognition of PanNETs, the possibility of cancer of the islet cells was initially suggested in 1888. The first case of hyperinsulinism due to a tumor of this type was reported in 1927. Recognition of a non-insulin-secreting type of PanNET is generally ascribed to the American surgeons,  R. M. Zollinger and E. H. Ellison, who gave their names to Zollinger–Ellison syndrome, after postulating the existence of a gastrin-secreting pancreatic tumor in a report of two cases of unusually severe peptic ulcers published in 1955.[103] In 2010, the WHO recommended that PanNETs be referred to as "neuroendocrine" rather than "endocrine" tumors.[26]
The first reported partial pancreaticoduodenectomy was performed by the Italian surgeon Alessandro Codivilla in 1898, but the patient only survived 18 days before succumbing to complications. Early operations were compromised partly because of mistaken beliefs that people would die if their duodenum were removed, and also, at first, if the flow of pancreatic juices stopped.  Later it was thought, also mistakenly, that the pancreatic duct could simply be tied up without serious adverse effects; in fact, it will very often leak later on.  In 1907–08, after some more unsuccessful operations by other surgeons, experimental procedures were tried on corpses by French surgeons.[104]
In 1912 the German surgeon Walther Kausch was the first to remove large parts of the duodenum and pancreas together (en bloc). This was in Breslau, now Wrocław in Poland. In 1918 it was demonstrated in operations on dogs that total removal of the duodenum is compatible with life, but this was not reported in human surgery until 1935, when the American surgeon Allen Oldfather Whipple published the results of a series of three operations at Columbia Presbyterian Hospital in New York. Only one of the patients had the duodenum totally removed, but he survived for two years before dying of metastasis to the liver.  The first operation was unplanned, as cancer was only discovered in the operating theater.  Whipple's success showed the way for the future, but the operation remained a difficult and dangerous one until recent decades.  He published several refinements to his procedure, including the first total removal of the duodenum in 1940, but he only performed a total of 37 operations.[104]
The discovery in the late 1930s that vitamin K prevented bleeding with jaundice, and the development of blood transfusion as an everyday process, both improved post-operative survival,[104] but about 25% of people never left hospital alive as late as the 1970s.[105] In the 1970s a group of American surgeons wrote urging that the procedure was too dangerous and should be abandoned.  Since then outcomes in larger centers have improved considerably, and mortality from the operation is often less than 4%.[22]  In 2006 a report was published of a series of 1,000 consecutive pancreaticoduodenectomies performed by a single surgeon from Johns Hopkins Hospital between 1969 and 2003. The rate of these operations had increased steadily over this period, with only three of them before 1980, and the median operating time reduced from 8.8 hours in the 1970s to 5.5 hours in the 2000s, and mortality within 30 days or in hospital was only 1%.[104][105]  Another series of 2,050 operations at the Massachusetts General Hospital between 1941 and 2011 showed a similar picture of improvement.[106]
Small precancerous neoplasms for many pancreatic cancers are being detected at greatly increased rates by modern medical imaging. One type, the intraductal papillary mucinous neoplasm (IPMN) was first described by Japanese researchers in 1982. It was noted in 2010 that: "For the next decade, little attention was paid to this report; however, over the subsequent 15 years, there has been a virtual explosion in the recognition of this tumor."[54]
Worldwide efforts on many levels are underway to understand pancreatic cancer, but progress has been slow, particularly into understanding the disease's causes.[107] There are several fundamental unanswered questions.[108][109]  The nature of the changes that lead to the disease are being intensely investigated, such as the roles played by genes such as KRAS and p53.[38][110][111]  A key question is the timing of events as the disease develops and progresses – particularly the role of diabetes,[112] and how and when the disease spreads.[113]
Research on early detection is ongoing.[58][59] For instance, the European Registry of Hereditary Pancreatitis and Familial Pancreatic Cancer (EUROPAC) trial is aiming to determine whether regular screening is appropriate for people with a family history of the disease, or who have hereditary pancreatitis.[114] The knowledge that new onset of diabetes can be an early sign of the disease could facilitate timely diagnosis and prevention if a workable screening strategy can be developed.[112][115][116]
Another area of interest is in assessing whether keyhole surgery (laparoscopy) would be better than Whipple's procedure in treating the disease surgically, particularly in terms of recovery time.[117] Irreversible electroporation is a relatively novel ablation technique that has shown promise in downstaging and prolonging survival in persons with locally advanced disease. It is especially suitable for treatment of tumors that are in proximity to peri-pancreatic vessels without risk of vascular trauma.[118][119] The limited success of outcomes after surgery has led to a number of trials that were running in 2014 to test outcomes using chemotherapy or radiochemotherapy before surgery. This had previously not been found to be helpful, but is being trialed again, using drug combinations which have emerged from the many trials of post-operative therapies, such as FOLFIRINOX.[2]
Efforts are underway to develop new drugs.[38][120] Some of these involve targeted therapies against the cancer cells' molecular mechanisms.[121][122][123] Others aim to target the highly resistant cancer stem cells.[71][124] Still others aim to affect the non-neoplastic stroma and microenvironment of the tumor, which is known to influence cell proliferation and metastasis.[123][124][125] A further approach involves the use of immunotherapy, such as oncolytic viruses.[126]



Radiation therapy - Wikipedia
Radiation therapy or radiotherapy, often abbreviated RT, RTx, or XRT, is therapy using ionizing radiation, generally as part of cancer treatment to control or kill malignant cells and normally delivered by a linear accelerator. Radiation therapy may be curative in a number of types of cancer if they are localized to one area of the body. It may also be used as part of adjuvant therapy, to prevent tumor recurrence after surgery to remove a primary malignant tumor (for example, early stages of breast cancer). Radiation therapy is synergistic with chemotherapy, and has been used before, during, and after chemotherapy in susceptible cancers. The subspecialty of oncology concerned with radiotherapy is called radiation oncology.
Radiation therapy is commonly applied to the cancerous tumor because of its ability to control cell growth. Ionizing radiation works by damaging the DNA of cancerous tissue leading to cellular death.  To spare normal tissues (such as skin or organs which radiation must pass through to treat the tumor), shaped radiation beams are aimed from several angles of exposure to intersect at the tumor, providing a much larger absorbed dose there than in the surrounding, healthy tissue. Besides the tumour itself, the radiation fields may also include the draining lymph nodes if they are clinically or radiologically involved with tumor, or if there is thought to be a risk of subclinical malignant spread. It is necessary to include a margin of normal tissue around the tumor to allow for uncertainties in daily set-up and internal tumor motion. These uncertainties can be caused by internal movement (for example, respiration and bladder filling) and movement of external skin marks relative to the tumor position.
Radiation oncology is the medical specialty concerned with prescribing radiation, and is distinct from radiology, the use of radiation in medical imaging and diagnosis. Radiation may be prescribed by a radiation oncologist with intent to cure ("curative") or for adjuvant therapy. It may also be used as palliative treatment (where cure is not possible and the aim is for local disease control or symptomatic relief) or as therapeutic treatment (where the therapy has survival benefit and it can be curative). It is also common to combine radiation therapy with surgery, chemotherapy, hormone therapy, immunotherapy or some mixture of the four. Most common cancer types can be treated with radiation therapy in some way.
The precise treatment intent (curative, adjuvant, neoadjuvant therapeutic, or palliative) will depend on the tumor type, location, and stage, as well as the general health of the patient. Total body irradiation (TBI) is a radiation therapy technique used to prepare the body to receive a bone marrow transplant. Brachytherapy, in which a radioactive source is placed inside or next to the area requiring treatment, is another form of radiation therapy that minimizes exposure to healthy tissue during procedures to treat cancers of the breast, prostate and other organs. Radiation therapy has several applications in non-malignant conditions, such as the treatment of trigeminal neuralgia, acoustic neuromas, severe thyroid eye disease, pterygium, pigmented villonodular synovitis, and prevention of keloid scar growth, vascular restenosis, and heterotopic ossification. The use of radiation therapy in non-malignant conditions is limited partly by worries about the risk of radiation-induced cancers.
Different cancers respond to radiation therapy in different ways.[1][2][3]
The response of a cancer to radiation is described by its radiosensitivity.
Highly radiosensitive cancer cells are rapidly killed by modest doses of radiation. These include leukemias, most lymphomas and germ cell tumors.
The majority of epithelial cancers are only moderately radiosensitive, and require a significantly higher dose of radiation (60-70 Gy) to achieve a radical cure.
Some types of cancer are notably radioresistant, that is, much higher doses are required to produce a radical cure than may be safe in clinical practice. Renal cell cancer and melanoma are generally considered to be radioresistant but radiation therapy is still a palliative option for many patients with metastatic melanoma. Combining radiation therapy with immunotherapy is an active area of investigation and has shown some promise for melanoma and other cancers.[4]
It is important to distinguish the radiosensitivity of a particular tumor, which to some extent is a laboratory measure, from the radiation "curability" of a cancer in actual clinical practice. For example, leukemias are not generally curable with radiation therapy, because they are disseminated through the body. Lymphoma may be radically curable if it is localised to one area of the body. Similarly, many of the common, moderately radioresponsive tumors are routinely treated with curative doses of radiation therapy if they are at an early stage. For example: non-melanoma skin cancer, head and neck cancer, breast cancer, non-small cell lung cancer, cervical cancer, anal cancer, and prostate cancer. Metastatic cancers are generally incurable with radiation therapy because it is not possible to treat the whole body.
Before treatment, a CT scan is often performed to identify the tumor and surrounding normal structures. The patient receives small skin marks to guide the placement of treatment fields.[5] Patient positioning is crucial at this stage as the patient will have to be set-up in the identical position during treatment.  Many patient positioning devices have been developed for this purpose, including masks and cushions which can be molded to the patient.
The response of a tumor to radiation therapy is also related to its size. Due to complex radiobiology, very large tumors respond less well to radiation than smaller tumors or microscopic disease. Various strategies are used to overcome this effect. The most common technique is surgical resection prior to radiation therapy. This is most commonly seen in the treatment of breast cancer with wide local excision or mastectomy followed by adjuvant radiation therapy. Another method is to shrink the tumor with neoadjuvant chemotherapy prior to radical radiation therapy. A third technique is to enhance the radiosensitivity of the cancer by giving certain drugs during a course of radiation therapy. Examples of radiosensitizing drugs include: Cisplatin, Nimorazole, and Cetuximab.
The effect of radiotherapy on control of cancer has been shown to be limited to the first five years after surgery, particularly for breast cancer. The difference between breast cancer recurrence in patients who receive radiotherapy vs. those who don't is seen mostly in the first 2–3 years and no difference is seen after 5 years.[6]
Radiation therapy is in itself painless. Many low-dose palliative treatments (for example, radiation therapy to bony metastases) cause minimal or no side effects, although short-term pain flare-up can be experienced in the days following treatment due to oedema compressing nerves in the treated area. Higher doses can cause varying side effects during treatment (acute side effects), in the months or years following treatment (long-term side effects), or after re-treatment (cumulative side effects). The nature, severity, and longevity of side effects depends on the organs that receive the radiation, the treatment itself (type of radiation, dose, fractionation, concurrent chemotherapy), and the patient.
Most side effects are predictable and expected. Side effects from radiation are usually limited to the area of the patient's body that is under treatment. Side effects are dose- dependent; for example higher doses of head and neck radiation can be associated with cardiovascular complications, thyroid dysfunction, and pituitary axis dysfunction.[7] Modern radiation therapy aims to reduce side effects to a minimum and to help the patient understand and deal with side effects that are unavoidable.
The main side effects reported are fatigue and skin irritation, like a mild to moderate sun burn. The fatigue often sets in during the middle of a course of treatment and can last for weeks after treatment ends. The irritated skin will heal, but  may not be as elastic as it was before.[8]
Late side effects occur months to years after treatment and are generally limited to the area that has been treated. They are often due to damage of blood vessels and connective tissue cells. Many late effects are reduced by fractionating treatment into smaller parts.
Cumulative effects from this process should not be confused with long-term effects—when short-term effects have disappeared and long-term effects are subclinical, reirradiation can still be problematic.[26] These doses are calculated by the radiation oncologist and many factors are taken into account before the subsequent radiation takes place.
During the first two weeks after fertilization, radiation therapy is lethal but not teratogenic.[27] High doses of radiation during pregnancy induce anomalies, impaired growth and intellectual disability, and there may be an increased risk of childhood leukemia and other tumours in the offspring.[27]
In males previously having undergone radiotherapy, there appears to be no increase in genetic defects or congenital malformations in their children conceived after therapy.[27] However, the use of assisted reproductive technologies and micromanipulation techniques might increase this risk.[27]
Hypopituitarism commonly develops after radiation therapy for sellar and parasellar neoplasms, extrasellar brain tumours, head and neck tumours, and following whole body irradiation for systemic malignancies.[28] Radiation-induced hypopituitarism mainly affects growth hormone and gonadal hormones.[28] In contrast, adrenocorticotrophic hormone (ACTH) and thyroid stimulating hormone (TSH) deficiencies are the least common among people with radiation-induced hypopituitarism.[28] Changes in prolactin-secretion is usually mild, and vasopressin deficiency appears to be very rare as a consequence of radiation.[28]
There are rigorous procedures in place to minimise the risk of accidental overexposure of radiation therapy to patients. However, mistakes do occasionally occur; for example, the radiation therapy machine Therac-25 was responsible for at least six accidents between 1985 and 1987, where patients were given up to one hundred times the intended dose; two people were killed directly by the radiation overdoses. From 2005 to 2010, a hospital in Missouri overexposed 76 patients (most with brain cancer) during a five-year period because new radiation equipment had been set up incorrectly.[29] Although medical errors are exceptionally rare, radiation oncologists, medical physicists and other members of the radiation therapy treatment team are working to eliminate them. ASTRO has launched a safety initiative called  Target Safely that, among other things, aims to record errors nationwide so that doctors can learn from each and every mistake and prevent them from happening. ASTRO also publishes a list of questions for patients to ask their doctors about radiation safety to ensure every treatment is as safe as possible.[30]
Radiation therapy is used to treat early stage Dupuytren's disease and Ledderhose disease. When Dupuytren's disease is at the nodules and cords stage or fingers are at a minimal deformation stage of less than 10 degrees, then radiation therapy is used to prevent further progress of the disease. Radiation therapy is also used post surgery in some cases to prevent the disease continuing to progress. Low doses of radiation are used typically three gray of radiation for five days, with a break of three months followed by another phase of three gray of radiation for five days.[31]
Radiation therapy works by damaging the DNA of cancerous cells. This DNA damage is caused by one of two types of energy, photon or charged particle. This damage is either direct or indirect ionization of the atoms which make up the DNA chain. Indirect ionization happens as a result of the ionization of water, forming free radicals, notably hydroxyl radicals, which then damage the DNA.
In photon therapy, most of the radiation effect is through free radicals. Cells have mechanisms for repairing single-strand DNA damage and double-stranded DNA damage. However, double-stranded DNA breaks are much more difficult to repair, and can lead to dramatic chromosomal abnormalities and genetic deletions. Targeting double-stranded breaks increases the probability that cells will undergo cell death. Cancer cells are generally less differentiated and more stem cell-like; they reproduce more than most healthy differentiated cells, and have a diminished ability to repair sub-lethal damage. Single-strand DNA damage is then passed on through cell division; damage to the cancer cells' DNA accumulates, causing them to die or reproduce more slowly.
One of the major limitations of photon radiation therapy is that the cells of solid tumors become deficient in oxygen. Solid tumors can outgrow their blood supply, causing a low-oxygen state known as hypoxia. Oxygen is a potent radiosensitizer, increasing the effectiveness of a given dose of radiation by forming DNA-damaging free radicals. Tumor cells in a hypoxic environment may be as much as 2 to 3 times more resistant to radiation damage than those in a normal oxygen environment.[32]
Much research has been devoted to overcoming hypoxia including the use of high pressure oxygen tanks, hyperthermia therapy (heat therapy which dilates blood vessels to the tumor site), blood substitutes that carry increased oxygen, hypoxic cell radiosensitizer drugs such as misonidazole and metronidazole, and hypoxic cytotoxins (tissue poisons), such as tirapazamine.  Newer research approaches are currently being studied, including preclinical and clinical investigations into the use of an oxygen diffusion-enhancing compound such as trans sodium crocetinate (TSC) as a  radiosensitizer.[33]
Charged particles such as protons and boron, carbon, and neon ions can cause direct damage to cancer cell DNA through high-LET (linear energy transfer) and have an antitumor effect independent of tumor oxygen supply because these particles act mostly via direct energy transfer usually causing double-stranded DNA breaks. Due to their relatively large mass, protons and other charged particles have little lateral side scatter in the tissue—the beam does not broaden much, stays focused on the tumor shape, and delivers small dose side-effects to surrounding tissue. They also more precisely target the tumor using the Bragg peak effect. See proton therapy for a good example of the different effects of intensity-modulated radiation therapy (IMRT) vs. charged particle therapy. This procedure reduces damage to healthy tissue between the charged particle radiation source and the tumor and sets a finite range for tissue damage after the tumor has been reached. In contrast, IMRT's use of uncharged particles causes its energy to damage healthy cells when it exits the body. This exiting damage is not therapeutic, can increase treatment side effects, and increases the probability of secondary cancer induction.[34] This difference is very important in cases where the close proximity of other organs makes any stray ionization very damaging (example: head and neck cancers).
This x-ray exposure is especially bad for children, due to their growing bodies, and they have a 30% chance of a second malignancy after 5 years post initial RT.[35]
The amount of radiation used in photon radiation therapy is measured in grays (Gy), and varies depending on the type and stage of cancer being treated. For curative cases, the typical dose for a solid epithelial tumor ranges from 60 to 80 Gy, while lymphomas are treated with 20 to 40 Gy.
Preventive (adjuvant) doses are typically around 45–60 Gy in 1.8–2 Gy fractions (for breast, head, and neck cancers.) Many other factors are considered by radiation oncologists when selecting a dose, including whether the patient is receiving chemotherapy, patient comorbidities, whether radiation therapy is being administered before or after surgery, and the degree of success of surgery.
Delivery parameters of a prescribed dose are determined during treatment planning (part of dosimetry). Treatment planning is generally performed on dedicated computers using specialized treatment planning software. Depending on the radiation delivery method, several angles or sources may be used to sum to the total necessary dose. The planner will try to design a plan that delivers a uniform prescription dose to the tumor and minimizes dose to surrounding healthy tissues.
In radiation therapy, three-dimensional dose distributions may be evaluated using the dosimetry technique known as gel dosimetry.[36]
The total dose is fractionated (spread out over time) for several important reasons. Fractionation allows normal cells time to recover, while tumor cells are generally less efficient in repair between fractions. Fractionation also allows tumor cells that were in a relatively radio-resistant phase of the cell cycle during one treatment to cycle into a sensitive phase of the cycle before the next fraction is given. Similarly, tumor cells that were chronically or acutely hypoxic (and therefore more radioresistant) may reoxygenate between fractions, improving the tumor cell kill.[37]
Fractionation regimens are individualised between different radiation therapy centers and even between individual doctors. In North America, Australia, and Europe, the typical fractionation schedule for adults is 1.8 to 2 Gy per day, five days a week. In some cancer types, prolongation of the fraction schedule over too long can allow for the tumor to begin repopulating, and for these tumor types, including head-and-neck and cervical squamous cell cancers, radiation treatment is preferably completed within a certain amount of time. For children, a typical fraction size may be 1.5 to 1.8 Gy per day, as smaller fraction sizes are associated with reduced incidence and severity of late-onset side effects in normal tissues.
In some cases, two fractions per day are used near the end of a course of treatment. This schedule, known as a concomitant boost regimen or hyperfractionation, is used on tumors that regenerate more quickly when they are smaller. In particular, tumors in the head-and-neck demonstrate this behavior.
Patients receiving palliative radiation to treat uncomplicated painful bone metastasis should not receive more than a single fraction of radiation.[38] A single treatment gives comparable pain relief and morbidity outcomes to multiple-fraction treatments, and for patients with limited life expectancy, a single treatment is best to improve patient comfort.[38]
One fractionation schedule that is increasingly being used and continues to be studied is hypofractionation. This is a radiation treatment in which the total dose of radiation is divided into large doses. Typical doses vary significantly by cancer type, from 2.2 Gy/fraction to 20 Gy/fraction, the latter being typical of stereotactic treatments (stereotactic ablative body radiotherapy, or SABR – also known as SBRT, or stereotactic body radiotherapy) for subcranial lesions, or SRS (stereotactic radiosurgery) for intracranial lesions. The rationale of hypofractionation is to reduce the probability of local recurrence by denying clonogenic cells the time they require to reproduce and also to exploit the radiosensitivity of some tumors.[39] In particular, stereotactic treatments are intended to destroy clonogenic cells by a process of ablation – i.e. the delivery of a dose intended to destroy clonogenic cells directly, rather than to interrupt the process of clonogenic cell division repeatedly (apoptosis), as in routine radiotherapy.
Different cancer types have different radiation sensitivity. However, predicting the sensitivity based on genomic or proteomic analyses of biopsy samples has proved difficult.[40][41] An alternative approach to genomics and proteomics was offered by the discovery that radiation protection in microbes is offered by non-enzymatic complexes of manganese and small organic metabolites.[42] The content and variation of manganese (measurable by electron paramagnetic resonance) were found to be good predictors of radiosensitivity, and this finding extends also to human cells.[43] An association was confirmed between total cellular manganese contents and their variation, and clinically-inferred radioresponsiveness in different tumor cells, a finding that may be useful for more precise radiodosages and improved treatment of cancer patients.[44]
Historically, the three main divisions of radiation therapy are :
The differences relate to the position of the radiation source; external is outside the body, brachytherapy uses sealed radioactive sources placed precisely in the area under treatment, and systemic radioisotopes are given by infusion or oral ingestion. Brachytherapy can use temporary or permanent placement of radioactive sources. The temporary sources are usually placed by a technique called afterloading. In afterloading a hollow tube or applicator is placed surgically in the organ to be treated, and the sources are loaded into the applicator after the applicator is implanted. This minimizes radiation exposure to health care personnel.
Particle therapy is a special case of external beam radiation therapy where the particles are protons or heavier ions.
The following three sections refer to treatment using x-rays.
Conventional external beam radiation therapy (2DXRT) is delivered via two-dimensional beams using kilovoltage therapy x-ray units or medical linear accelerators which generate high energy x-rays.[45][46] 2DXRT mainly consists of a single beam of radiation delivered to the patient from several directions: often front or back, and both sides. Conventional refers to the way the treatment is planned or simulated on a specially calibrated diagnostic x-ray machine known as a simulator because it recreates the linear accelerator actions (or sometimes by eye), and to the usually well-established arrangements of the radiation beams to achieve a desired plan. The aim of simulation is to accurately target or localize the volume which is to be treated. This technique is well established and is generally quick and reliable. The worry is that some high-dose treatments may be limited by the radiation toxicity capacity of healthy tissues which lie close to the target tumor volume. An example of this problem is seen in radiation of the prostate gland, where the sensitivity of the adjacent rectum limited the dose which could be safely prescribed using 2DXRT planning to such an extent that tumor control may not be easily achievable. Prior to the invention of the CT, physicians and physicists had limited knowledge about the true radiation dosage delivered to both cancerous and healthy tissue. For this reason, 3-dimensional conformal radiation therapy is becoming the standard treatment for a number of tumor sites. More recently other forms of imaging are used including MRI, PET, SPECT and Ultrasound.[47]
Stereotactic radiation is a specialized type of external beam radiation therapy. It uses focused radiation beams targeting a well-defined tumor using extremely detailed imaging scans. Radiation oncologists perform stereotactic treatments, often with the help of a neurosurgeon for tumors in the brain or spine.
There are two types of stereotactic radiation. Stereotactic radiosurgery (SRS) is when doctors use a single or several stereotactic radiation treatments of the brain or spine. Stereotactic body radiation therapy (SBRT) refers to one or several stereotactic radiation treatments with the body, such as the lungs.[48]
Some doctors say an advantage to stereotactic treatments is that they deliver the right amount of radiation to the cancer in a shorter amount of time than traditional treatments, which can often take 6 to 11 weeks. Plus treatments are given with extreme accuracy, which should limit the effect of the radiation on healthy tissues. One problem with stereotactic treatments is that they are only suitable for certain small tumors.
Stereotactic treatments can be confusing because many hospitals call the treatments by the name of the manufacturer rather than calling it SRS or SBRT. Brand names for these treatments include Axesse, Cyberknife, Gamma Knife, Novalis, Primatom, Synergy, X-Knife, TomoTherapy, Trilogy and Truebeam.[49] This list changes as equipment manufacturers continue to develop new, specialized technologies to treat cancers.
The planning of radiation therapy treatment has been revolutionized by the ability to delineate tumors and adjacent normal structures in three dimensions using specialized CT and/or MRI scanners and planning software.[50]
Virtual simulation, the most basic form of planning, allows more accurate placement of radiation beams than is possible using conventional X-rays, where soft-tissue structures are often difficult to assess and normal tissues difficult to protect.
An enhancement of virtual simulation is 3-dimensional conformal radiation therapy (3DCRT), in which the profile of each radiation beam is shaped to fit the profile of the target from a beam's eye view (BEV) using a multileaf collimator (MLC) and a variable number of beams. When the treatment volume conforms to the shape of the tumor, the relative toxicity of radiation to the surrounding normal tissues is reduced, allowing a higher dose of radiation to be delivered to the tumor than conventional techniques would allow.[5]
Intensity-modulated radiation therapy (IMRT) is an advanced type of high-precision radiation that is the next generation of 3DCRT.[51] IMRT also improves the ability to conform the treatment volume to concave tumor shapes,[5] for example when the tumor is wrapped around a vulnerable structure such as the spinal cord or a major organ or blood vessel.[52] Computer-controlled x-ray accelerators distribute precise radiation doses to malignant tumors or specific areas within the tumor. The pattern of radiation delivery is determined using highly tailored computing applications to perform optimization and treatment simulation (Treatment Planning). The radiation dose is consistent with the 3-D shape of the tumor by controlling, or modulating, the radiation beam’s intensity. The radiation dose intensity is elevated near the gross tumor volume while radiation among the neighboring normal tissues is decreased or avoided completely. This results in better tumor targeting, lessened side effects, and improved treatment outcomes than even 3DCRT.
3DCRT is still used extensively for many body sites but the use of IMRT is growing in more complicated body sites such as CNS, head and neck, prostate, breast, and lung. Unfortunately, IMRT is limited by its need for additional time from experienced medical personnel. This is because physicians must manually delineate the tumors one CT image at a time through the entire disease site which can take much longer than 3DCRT preparation. Then, medical physicists and dosimetrists must be engaged to create a viable treatment plan. Also, the IMRT technology has only been used commercially since the late 1990s even at the most advanced cancer centers, so radiation oncologists who did not learn it as part of their residency programs must find additional sources of education before implementing IMRT.
Proof of improved survival benefit from either of these two techniques over conventional radiation therapy (2DXRT) is growing for many tumor sites, but the ability to reduce toxicity is generally accepted. This is particularly the case for head and neck cancers in a series of pivotal trials performed by Professor Christopher Nutting of the Royal Marsden Hospital. Both techniques enable dose escalation, potentially increasing usefulness. There has been some concern, particularly with IMRT,[53] about increased exposure of normal tissue to radiation and the consequent potential for secondary malignancy. Overconfidence in the accuracy of imaging may increase the chance of missing lesions that are invisible on the planning scans (and therefore not included in the treatment plan) or that move between or during a treatment (for example, due to respiration or inadequate patient immobilization). New techniques are being developed to better control this uncertainty—for example, real-time imaging combined with real-time adjustment of the therapeutic beams. This new technology is called image-guided radiation therapy (IGRT) or four-dimensional radiation therapy.
Another technique is the real-time tracking and localization of one or more small implantable electric devices implanted inside or close to the tumor. There are various types of medical implantable devices that are used for this purpose. It can be a magnetic transponder which senses the magnetic field generated by several transmitting coils, and then transmits the measurements back to the positioning system to determine the location.[54] The implantable device can also be a small wireless transmitter sending out an RF signal which then will be received by a sensor array and used for localization and real-time tracking of the tumor position.[55][56]
Volumetric modulated arc therapy (VMAT) is a radiation technique introduced in 2007[57] which can achieve highly conformal dose distributions on target volume coverage and sparing of normal tissues. The specificity of this technique is to modify three parameters during the treatment. VMAT delivers radiation by rotating gantry (usually 360° rotating fields with one or more arcs), changing speed and shape of the beam with a multileaf collimator (MLC) ("sliding window" system of moving) and fluence output rate (dose rate) of the medical linear accelerator. VMAT has an advantage in patient treatment, compared with conventional static field intensity modulated radiotherapy (IMRT), of reduced radiation delivery times.[58][59] Comparisons between VMAT and conventional IMRT for their sparing of healthy tissues and Organs at Risk (OAR) depends upon the cancer type. In the treatment of nasopharyngeal, oropharyngeal and hypopharyngeal carcinomas VMAT provides equivalent or better OAR protection.[57][58][59] In the treatment of prostate cancer the OAR protection result is mixed[57] with some studies favoring VMAT, others favoring IMRT.[60]
In particle therapy (proton therapy being one example), energetic ionizing particles (protons or carbon ions) are directed at the target tumor.[61] The dose increases while the particle penetrates the tissue, up to a maximum (the Bragg peak) that occurs near the end of the particle's range, and it then drops to (almost) zero. The advantage of this energy deposition profile is that less energy is deposited into the healthy tissue surrounding the target tissue.
Auger therapy (AT) makes use of a very high dose[62] of ionizing radiation in situ that provides molecular modifications at an atomic scale. AT differs from conventional radiation therapy in several aspects; it neither relies upon radioactive nuclei to cause cellular radiation damage at a cellular dimension, nor engages multiple external pencil-beams from different directions to zero-in to deliver a dose to the targeted area with reduced dose outside the targeted tissue/organ locations. Instead, the in situ delivery of a very high dose at the molecular level using AT aims for in situ molecular modifications involving molecular breakages and molecular re-arrangements such as a change of stacking structures as well as cellular metabolic functions related to the said molecule structures.
Contact x-ray brachytherapy (also called "CXB", "electronic brachytherapy" or the "Papillon Technique") is a type of radiation therapy using kilovoltage X-rays applied close to the tumour to treat rectal cancer. The process involves inserting the x-ray tube through the anus into the rectum and placing it against the cancerous tissue, then high doses of X-rays are emitted directly into the tumor at two weekly intervals.  It is typically used for treating early rectal cancer in patients who may not be candidates for surgery.[63][64][65] A 2015 NICE review found the main side effect to be bleeding that occurred in about 38% of cases, and radiation-induced ulcer which occurred in 27% of cases.[63]
Brachytherapy is delivered by placing radiation source(s) inside or next to the area requiring treatment. Brachytherapy is commonly used as an effective treatment for cervical,[66] prostate,[67] breast,[68] and skin cancer[69] and can also be used to treat tumours in many other body sites.[70]
In brachytherapy, radiation sources are precisely placed directly at the site of the cancerous tumour. This means that the irradiation only affects a very localized area – exposure to radiation of healthy tissues further away from the sources is reduced. These characteristics of brachytherapy provide advantages over external beam radiation therapy – the tumour can be treated with very high doses of localized radiation, whilst reducing the probability of unnecessary damage to surrounding healthy tissues.[70][71] A course of brachytherapy can often be completed in less time than other radiation therapy techniques. This can help reduce the chance of surviving cancer cells dividing and growing in the intervals between each radiation therapy dose.[71]
As one example of the localized nature of breast brachytherapy, the SAVI device delivers the radiation dose through multiple catheters, each of which can be individually controlled. This approach decreases the exposure of healthy tissue and resulting side effects, compared both to external beam radiation therapy and older methods of breast brachytherapy.[72]
Systemic radioisotope therapy (RIT) is a form of targeted therapy. Targeting can be due to the chemical properties of the isotope such as radioiodine which is specifically absorbed by the thyroid gland a thousandfold better than other bodily organs. Targeting can also be achieved by attaching the radioisotope to another molecule or antibody to guide it to the target tissue. The radioisotopes are delivered through infusion (into the bloodstream) or ingestion. Examples are the infusion of metaiodobenzylguanidine (MIBG) to treat neuroblastoma, of oral iodine-131 to treat thyroid cancer or thyrotoxicosis, and of hormone-bound lutetium-177 and yttrium-90 to treat neuroendocrine tumors (peptide receptor radionuclide therapy).
Another example is the injection of yttrium-90 radioactive glass or resin microspheres into the hepatic artery to radioembolize liver tumors or liver metastases.  These microspheres are used for the treatment approach known as selective internal radiation therapy. The microspheres are approximately 30 µm in diameter (about one-third of a human hair) and are delivered directly into the artery supplying blood to the tumors. These treatments begin by guiding a catheter up through the femoral artery in the leg, navigating to the desired target site and administering treatment. The blood feeding the tumor will carry the microspheres directly to the tumor enabling a more selective approach than traditional systemic chemotherapy. There are currently two different kinds of microspheres: SIR-Spheres and TheraSphere.
A major use of systemic radioisotope therapy is in the treatment of bone metastasis from cancer. The radioisotopes travel selectively to areas of damaged bone, and spare normal undamaged bone. Isotopes commonly used in the treatment of bone metastasis are radium-223,[73] strontium-89 and samarium (153Sm) lexidronam.[74]
In 2002, the United States Food and Drug Administration (FDA) approved ibritumomab tiuxetan (Zevalin), which is an anti-CD20 monoclonal antibody conjugated to yttrium-90.[75]
In 2003, the FDA approved the tositumomab/iodine (131I) tositumomab regimen (Bexxar), which is a combination of an iodine-131 labelled and an unlabelled anti-CD20 monoclonal antibody.[76]
These medications were the first agents of what is known as radioimmunotherapy, and they were approved for the treatment of refractory non-Hodgkins lymphoma.
Intraoperative radiation therapy (IORT) is applying therapeutic levels of radiation to a target area, such as a cancer tumor, while the area is exposed during surgery.[77]
The rationale for IORT is to deliver a high dose of radiation precisely to the targeted area with minimal exposure of surrounding tissues which are displaced or shielded during the IORT. Conventional radiation techniques such as external beam radiotherapy (EBRT) following surgical removal of the tumor have several drawbacks: The tumor bed where the highest dose should be applied is frequently missed due to the complex localization of the wound cavity even when modern radiotherapy planning is used. Additionally, the usual delay between the surgical removal of the tumor and EBRT may allow a repopulation of the tumor cells. These potentially harmful effects can be avoided by delivering the radiation more precisely to the targeted tissues leading to immediate sterilization of residual tumor cells. Another aspect is that wound fluid has a stimulating effect on tumor cells. IORT was found to inhibit the stimulating effects of wound fluid.[78]
Deep inspiration breath-hold (DIBH) is a method of delivering radiotherapy while limiting radiation exposure to the heart and lungs.[79] It is used primarily for treating left-sided breast cancer. The technique involves a patient holding their breath during treatment. There are two basic methods of performing DIBH: free-breathing breath-hold and spirometry-monitored deep inspiration breath hold.[80]
Medicine has used radiation therapy as a treatment for cancer for more than 100 years, with its earliest roots traced from the discovery of x-rays in 1895 by Wilhelm Röntgen.[81] Emil Grubbe of Chicago was possibly the first American physician to use x-rays to treat cancer, beginning in 1896.[82]
The field of radiation therapy began to grow in the early 1900s largely due to the groundbreaking work of Nobel Prize–winning scientist Marie Curie (1867–1934), who discovered the radioactive elements polonium and radium in 1898. This began a new era in medical treatment and research.[81]  Through the 1920s the hazards of radiation exposure were not understood, and little protection was used. Radium was believed to have wide curative powers and radiotherapy was applied to many diseases.
Prior to World War 2, the only practical sources of radiation for radiotherapy were radium and its "emanation", radon gas, and the x-ray tube.  External beam radiotherapy (teletherapy) began at the turn of the century with relatively low voltage (<150 kV) x-ray machines.  It was found that while superficial tumors could be treated with low voltage x-rays, more penetrating, higher energy beams were required to reach tumors inside the body, requiring higher voltages.  Orthovoltage X-rays, which used tube voltages of 200-500 kV, began to be used during the 1920s.  To reach the most deeply buried tumors without exposing intervening skin and tissue to dangerous radiation doses required rays with energies of 1 MV or above, called "megavolt" radiation.  Producing megavolt x-rays required voltages on the x-ray tube of 3 to 5 million volts, which required huge expensive installations.  Megavoltage x-ray units were first built in the late 1930s but because of cost were limited to a few institutions.    One of the first, installed at St. Bartholomew's hospital, London in 1937 and used until 1960, used a 30 foot long x-ray tube and weighed 10 tons.    Radium produced megavolt gamma rays, but was extremely rare and expensive due to its low occurrence in ores.  In 1937 the entire world supply of radium for radiotherapy was 50 grams, valued at £800,000, or $50 million in 2005 dollars.
The invention of the nuclear reactor in the Manhattan Project during World War 2 made possible the production of artificial radioisotopes for radiotherapy.  Cobalt therapy, teletherapy machines using megavolt gamma rays emitted by cobalt-60, a radioisotope produced by irradiating ordinary cobalt metal in a reactor, revolutionized the field between the 1950s and the early 1980s.  Cobalt machines were relatively cheap, robust and simple to use, although due to its 5.27 year half-life the cobalt had to be replaced about every 5 years.
Medical linear particle accelerators, developed since the 1940s, began replacing x-ray and cobalt units in the 1980s and these older therapies are now declining. The first medical linear accelerator was used at the Hammersmith Hospital in London in 1953.[83] Linear accelerators can produce higher energies, have more collimated beams, and do not produce radioactive waste with its attendant disposal problems like radioisotope therapies.
With Godfrey Hounsfield’s invention of computed tomography (CT) in 1971, three-dimensional planning became a possibility and created a shift from 2-D to 3-D radiation delivery. CT-based planning allows physicians to more accurately determine the dose distribution using axial tomographic images of the patient's anatomy.   The advent of new imaging technologies, including magnetic resonance imaging (MRI) in the 1970s and positron emission tomography (PET) in the 1980s, has moved radiation therapy from 3-D conformal to intensity-modulated radiation therapy (IMRT) and to image-guided radiation therapy (IGRT) tomotherapy. These advances allowed radiation oncologists to better see and target tumors, which have resulted in better treatment outcomes, more organ preservation and fewer side effects.[84]
While access to radiotherapy is improving globally, more than half of patients in low and middle income countries still do not have available access to the therapy as of 2017.[85]



Brachytherapy - Wikipedia
Brachytherapy is a form of radiotherapy where a sealed radiation source is placed inside or next to the area requiring treatment.  Brachytherapy is commonly used as an effective treatment for cervical, prostate, breast, and skin cancer and can also be used to treat tumours in many other body sites.[1] Treatment results have demonstrated that the cancer-cure rates of brachytherapy are either comparable to surgery and external beam radiotherapy (EBRT) or are improved when used in combination with these techniques.[2][3][4] Brachytherapy can be used alone or in combination with other therapies such as surgery, EBRT and chemotherapy.
Brachytherapy contrasts with unsealed source radiotherapy, in which a therapeutic radionuclide (radioisotope) is injected into the body to chemically localize to the tissue requiring destruction. It also contrasts to EBRT, in which high-energy x-rays (or occasionally gamma-rays from a radioisotope like cobalt-60) are directed at the tumour from outside the body. Brachytherapy instead involves the precise placement of short-range radiation-sources (radioisotopes, Iodine-125 for instance) directly at the site of the cancerous tumour. These are enclosed in a protective capsule or wire, which allows the ionizing radiation to escape to treat and kill surrounding tissue but prevents the charge of radioisotope from moving or dissolving in body fluids. The capsule may be removed later, or (with some radioisotopes) it may be allowed to remain in place.[1]:Ch. 1[5]
A feature of brachytherapy is that the irradiation affects only a very localized area around the radiation sources. Exposure to radiation of healthy tissues farther away from the sources is therefore reduced. In addition, if the patient moves or if there is any movement of the tumour within the body during treatment, the radiation sources retain their correct position in relation to the tumour. These characteristics of brachytherapy provide advantages over EBRT – the tumour can be treated with very high doses of localised radiation whilst reducing the probability of unnecessary damage to surrounding healthy tissues.[1]:Ch. 1[5]
A course of brachytherapy can be completed in less time than other radiotherapy techniques. This can help reduce the chance for surviving cancer-cells to divide and grow in the intervals between each radiotherapy dose.[5] Patients typically have to make fewer visits to the radiotherapy clinic compared with EBRT, and may receive the treatment as outpatients. This makes treatment accessible and convenient for many patients.[6][7] These features of brachytherapy mean that most patients are able to tolerate the brachytherapy procedure very well.
The global market for brachytherapy reached US$680 million in 2013, of which the high-dose rate (HDR) and LDR segments accounted for 70%. Microspheres and electronic brachytherapy comprised the remaining 30%.[8] One analysis predicts that the brachytherapy market may reach over US$2.4 billion in 2030, growing by 8% annually, mainly driven by the microspheres market as well as electronic brachytherapy, which is gaining significant interest worldwide as a user-friendly technology.[9] The word brachy is Greek for short distance.
Brachytherapy is commonly used to treat cancers of the cervix, prostate, breast, and skin.[1]
Brachytherapy can also be used in the treatment of tumours of the brain, eye, head and neck region (lip, floor of mouth, tongue, nasopharynx and oropharynx),[10] respiratory tract (trachea and bronchi), digestive tract (oesophagus, gall bladder, bile-ducts, rectum, anus),[11] urinary tract (bladder, urethra, penis), female reproductive tract (uterus, vagina, vulva), and soft tissues.[1]
As the radiation sources can be precisely positioned at the tumour treatment site, brachytherapy enables a high dose of radiation to be applied to a small area. Furthermore, because the radiation sources are placed in or next to the target tumour, the sources maintain their position in relation to the tumour when the patient moves or if there is any movement of the tumour within the body. Therefore, the radiation sources remain accurately targeted. This enables clinicians to achieve a high level of dose conformity – i.e. ensuring the whole of the tumour receives an optimal level of radiation. It also reduces the risk of damage to healthy tissue, organs or structures around the tumour,[12] thus enhancing the chance of cure and preservation of organ function.
The use of HDR brachytherapy enables overall treatment times to be reduced compared with EBRT.[13][14]
Patients receiving brachytherapy generally have to make fewer visits for radiotherapy compared with EBRT, and overall radiotherapy treatment plans can be completed in less time.[15]
Many brachytherapy procedures are performed on an outpatient basis. This convenience may be particularly relevant for patients who have to work, older patients, or patients who live some distance from treatment centres, to ensure that they have access to radiotherapy treatment and adhere to treatment plans. Shorter treatment times and outpatient procedures can also help improve the efficiency of radiotherapy clinics.[16][17]
Brachytherapy can be used with the aim of curing the cancer in cases of small or locally advanced tumours, provided the cancer has not metastasized (spread to other parts of the body). In appropriately selected cases, brachytherapy for primary tumours often represents a comparable approach to surgery, achieving the same probability of cure and with similar side effects.[18][19]
However, in locally advanced tumours, surgery may not routinely provide the best chance of cure and is often not technically feasible to perform. In these cases radiotherapy, including brachytherapy, offers the only chance of cure.[20][21]
In more advanced disease stages, brachytherapy can be used as palliative treatment for symptom relief from pain and bleeding.
In cases where the tumour is not easily accessible or is too large to ensure an optimal distribution of irradiation to the treatment area, brachytherapy can be combined with other treatments, such as EBRT and/or surgery.[1]:Ch. 1 Combination therapy of brachytherapy exclusively with chemotherapy is rare.[22]
Brachytherapy is commonly used in the treatment of early or locally confined cervical cancer and is a standard of care in many countries.[1]:Ch. 14[23][24][25][26]
Cervical cancer can be treated with either LDR, PDR or HDR brachytherapy.[25][27][28]
Used in combination with EBRT, brachytherapy can provide better outcomes than EBRT alone.[2]
The precision of brachytherapy enables a high dose of targeted radiation to be delivered to the cervix, while minimising radiation exposure to adjacent tissues and organs.[24][25][29][30]
The chances of staying free of disease (disease-free survival) and of staying alive (overall survival) are similar for LDR, PDR and HDR treatments.[21][31]
However, a key advantage of HDR treatment is that each dose can be delivered on an outpatient basis with a short administration time[2] providing greater convenience for many patients.
Brachytherapy to treat prostate cancer can be given either as permanent LDR seed implantation or as temporary HDR brachytherapy.[1]:Ch. 20[32][33]
Permanent seed implantation is suitable for patients with a localised tumour and good prognosis[32][34][35][36] and has been shown to be a highly effective treatment to prevent the cancer from returning.[34][37] The survival rate is similar to that found with EBRT or surgery (radical prostatectomy), but with fewer side effects such as impotence and incontinence.[38] The procedure can be completed quickly and patients are usually able to go home on the same day of treatment and return to normal activities after 1 to 2 days.[6]
Permanent seed implantation is often a less invasive treatment option compared to the surgical removal of the prostate.[6]
Temporary HDR brachytherapy is a newer approach to treating prostate cancer, but is currently less common than seed implantation. It is predominately used as to provide an extra dose in addition to EBRT (known as "boost" therapy) as it offers an alternative method to deliver a high dose of radiation therapy that conforms to the shape of the tumour within the prostate, while sparing radiation exposure to surrounding tissues.[4][33][35][39][40][41]
HDR brachytherapy as a boost for prostate cancer also means that the EBRT course can be shorter than when EBRT is used alone.[20][39][40][41]
Radiation therapy is standard of care for women who have undergone lumpectomy or mastectomy surgery, and is an integral component of breast-conserving therapy.[1]:Ch. 18[42]
Brachytherapy can be used after surgery, before chemotherapy or palliatively in the case of advanced disease.[43] Brachytherapy to treat breast cancer is usually performed with HDR temporary brachytherapy. Post surgery, breast brachytherapy can be used as a "boost" following whole breast irradiation (WBI) using EBRT.[42][44]
More recently, brachytherapy alone is used to deliver APBI (accelerated partial breast irradiation), involving delivery of radiation to only the immediate region surrounding the original tumour.[42][44][45]
The main benefit of breast brachytherapy compared to whole breast irradiation is that a high dose of radiation can be precisely applied to the tumour while sparing radiation to healthy breast tissues and underlying structures such as the ribs and lungs.[43] APBI can typically be completed over the course of a week.[45] The option of brachytherapy may be particularly important in ensuring that working women, the elderly or women without easy access to a treatment centre, are able to benefit from breast-conserving therapy due to the short treatment course compared with WBI (which often requires more visits over the course of 1–2 months).[7]
There are five methods that can be used to deliver breast brachytherapy: Interstitial breast brachytherapy, Intracavitary breast brachytherapy, Intraoperative radiation therapy, Permanent Breast Seed Implantation and non-invasive breast brachytherapy using mammography for target localization and an HDR source.
Interstitial breast brachytherapy involves the temporary placement of several flexible plastic catheters in the breast tissue. These are carefully positioned to allow optimal targeting of radiation to the treatment area while sparing the surrounding breast tissue.[7] The catheters are connected to an afterloader, which delivers the planned radiation dose to the treatment area. Interstitial breast brachytherapy can be used as "boost" after EBRT, or as APBI.[44]
Intraoperative radiation therapy (IORT) delivers radiation at the same time as the surgery to remove the tumour (lumpectomy).[46] An applicator is placed in the cavity left after tumour removal and a mobile electronic device generates radiation (either x-rays[46] or electrons[47]) and delivers it via the applicator. Radiation is delivered all at once and the applicator removed before closing the incision.
Intracavitary breast brachytherapy (also known as "balloon brachytherapy") involves the placement of a single catheter into the breast cavity left after the removal of the tumour (lumpectomy).[7] The catheter can be placed at the time of the lumpectomy or postoperatively.[7] Via the catheter, a balloon is then inflated in the cavity. The catheter is then connected to an afterloader, which delivers the radiation dose through the catheter and into the balloon. Currently, intracavitary breast brachytherapy is only routinely used for APBI.[48]
There are also devices that combine the features of interstitial and intracavitary breast brachytherapy (e.g. SAVI). These devices use multiple catheters but are inserted through a single-entry point in the breast. Studies suggest the use of multiple catheters enables physicians to target the radiation more precisely.[49][50]
Permanent breast seed implantation (PBSI) implants many radioactive "seeds" (small pellets) into the breast in the area surrounding the site of the tumour, similar to permanent seed prostate brachytherapy.[51] The seeds are implanted in a single 1-2 hour procedure and deliver radiation over the following months as the radioactive material inside them decays. Risk of radiation from the implants to others (e.g. partner/spouse) has been studied and found to be safe.[51]
HDR brachytherapy for nonmelanomatous skin cancer, such as basal cell carcinoma and squamous cell carcinoma, provides an alternative treatment option to surgery. This is especially relevant for cancers on the nose, ears, eyelids or lips, where surgery may cause disfigurement or require extensive reconstruction.[1]:Ch. 28 Various applicators can be used to ensure close contact between the radiation source(s) and the skin, which conform to the curvature of the skin and help ensure precision delivery of the optimal irradiation dose.[1]:Ch. 28
Brachytherapy for skin cancer provides good cosmetic results and clinical efficacy; studies with up to 5 years follow-up have shown that brachytherapy is highly effective in terms local control, and is comparable to EBRT.[52][53][54] Treatment times are typically short, providing convenience for patients.[55]
It has been suggested that brachytherapy may become a standard of treatment for skin cancer in the near future.[55]
Brachytherapy can be used in the treatment of coronary in-stent restenosis, in which a catheter is placed inside blood vessels, through which sources are inserted and removed.[56]
In treating In-stent restenosis (ISR) Drug Eluting stents (DES) have been found to be superior to Intracoronary Brachytherapy (ICBT).
However, there is continued interest in vascular brachytherapy for persistent restenosis in failed stents and vein grafts.
The therapy has also been investigated for use in the treatment of peripheral vasculature stenosis[57] and considered for the treatment of atrial fibrillation.[58]
The likelihood and nature of potential acute, sub-acute or long-term side-effects associated with brachytherapy depends on the location of the tumour being treated and the type of brachytherapy being used.
Acute side effects associated with brachytherapy include localised bruising, swelling, bleeding, discharge or discomfort within the implanted region. These usually resolve within a few days following completion of treatment.[59]
Patients may also feel fatigued for a short period following treatment.[59][60]
Brachytherapy treatment for cervical or prostate cancer can cause acute and transient urinary symptoms such as urinary retention, urinary incontinence or painful urination (dysuria).[38][61][62]
Transient increased bowel frequency, diarrhoea, constipation or minor rectal bleeding, may also occur.[38][61][62] Acute and subacute side effects usually resolve over a matter of days or a few weeks. In the case of permanent (seed) brachytherapy for prostate cancer, there is a small chance that some seeds may migrate out of the treatment region into the bladder or urethra and be passed in the urine.
Brachytherapy for skin cancer may result in a shedding of the outer layers of skin (desquamation) around the area of treatment in the weeks following therapy, which typically heals in 5–8 weeks.[1]:Ch. 28 If the cancer is located on the lip, ulceration may occur as a result of brachytherapy, but usually resolves after 4–6 weeks.[63]
Most of the acute side effects associated with brachytherapy can be treated with medication or through dietary changes, and usually disappear over time (typically a matter of weeks), once the treatment is completed. The acute side effects of HDR brachytherapy are broadly similar to EBRT.[60]
In a small number of people, brachytherapy may cause long-term side effects due to damage or disruption of adjacent tissues or organs. Long-term side effects are usually mild or moderate in nature. For example, urinary and digestive problems may persist as a result of brachytherapy for cervical or prostate cancer, and may require ongoing management.[38][61][62]
Brachytherapy for prostate cancer may cause erectile dysfunction in approximately 15-30% of patients.[1]:Ch. 20[64] However, the risk of erectile dysfunction is related to age (older men are at a greater risk than younger men) and also the level of erectile function prior to receiving brachytherapy. In patients who do experience erectile dysfunction, the majority of cases can successfully be treated with drugs such as Viagra.[1]:Ch. 20 Importantly, the risk of erectile dysfunction after brachytherapy is less than after radical prostatectomy.[18][61]
Brachytherapy for breast or skin cancer may cause scar tissue to form around the treatment area. In the case of breast brachytherapy, fat necrosis may occur as a result of fatty acids entering the breast tissues. This can cause the breast tissue to become swollen and tender. Fat necrosis is a benign condition and typically occurs 4–12 months after treatment and affects about 2% of patients.[65][66]
Patients often ask if they need to have special safety precautions around family and friends after receiving brachytherapy. If temporary brachytherapy is used, no radioactive sources remain in the body after treatment. Therefore, there is no radiation risk to friends or family from being in close proximity with them.[67]
If permanent brachytherapy is used, low dose radioactive sources (seeds) are left in the body after treatment – the radiation levels are very low and decrease over time. In addition, the irradiation only affects tissues within a few millimeters of the radioactive sources (i.e. the tumour being treated). As a precaution, some people receiving permanent brachytherapy may be advised to not hold any small children or be too close to pregnant women for a short time after treatment. Radiation oncologists or nurses can provide specific instructions to patients and advise for how long they need to be careful.[67]
Different types of brachytherapy can be defined according to (1) the placement of the radiation sources in the target treatment area, (2) the rate or ‘intensity’ of the irradiation dose delivered to the tumour, and (3) the duration of dose delivery.
The two main types of brachytherapy treatment in terms of the placement of the radioactive source are interstitial and contact.
In the case of interstitial brachytherapy, the sources are placed directly in the target tissue of the affected site, such as the prostate or breast.[1]:Ch. 1
Contact brachytherapy involves placement of the radiation source in a space next to the target tissue.[1]:Ch. 1 This space may be a body cavity (intracavitary brachytherapy) such as the cervix, uterus or vagina; a body lumen (intraluminal brachytherapy) such as the trachea or oesophagus; or externally (surface brachytherapy) such as the skin.[1]:Ch. 1 A radiation source can also be placed in blood vessels (intravascular brachytherapy) for the treatment of coronary in-stent restenosis.[68]
The dose rate of brachytherapy refers to the level or ‘intensity’ with which the radiation is delivered to the surrounding medium and is expressed in Grays per hour (Gy/h).
Low-dose rate (LDR) brachytherapy involves implanting radiation sources that emit radiation at a rate of up to 2 Gy·h−1.[69] LDR brachytherapy is commonly used for cancers of the oral cavity,[10] oropharynx,[10] sarcomas[1]:Ch. 27 and prostate cancer[1]:Ch. 20[70]
Medium-dose rate (MDR) brachytherapy is characterized by a medium rate of dose delivery, ranging between 2 Gy·h−1 to 12 Gy·h−1.[69]
High-dose rate (HDR) brachytherapy is when the rate of dose delivery exceeds 12 Gy·h−1.[69] The most common applications of HDR brachytherapy are in tumours of the cervix, esophagus, lungs, breasts and prostate.[1] Most HDR treatments are performed on an outpatient basis, but this is dependent on the treatment site.[12]
Pulsed-dose rate (PDR) brachytherapy involves short pulses of radiation, typically once an hour, to simulate the overall rate and effectiveness of LDR treatment. Typical tumour sites treated by PDR brachytherapy are gynaecological[1]:Ch. 14 and head and neck cancers.[10]
The placement of radiation sources in the target area can be temporary or permanent.
Temporary brachytherapy involves placement of radiation sources for a set duration (usually a number of minutes or hours) before being withdrawn.[1]:Ch. 1 The specific treatment duration will depend on many different factors, including the required rate of dose delivery and the type, size and location of the cancer. In LDR and PDR brachytherapy, the source typically stays in place up to 24 hours before being removed, while in HDR brachytherapy this time is typically a few minutes.[71]
Permanent brachytherapy, also known as seed implantation, involves placing small LDR radioactive seeds or pellets (about the size of a grain of rice) in the tumour or treatment site and leaving them there permanently to gradually decay. Over a period of weeks or months, the level of radiation emitted by the sources will decline to almost zero. The inactive seeds then remain in the treatment site with no lasting effect.[64] Permanent brachytherapy is most commonly used in the treatment of prostate cancer.[70]
To accurately plan the brachytherapy procedure, a thorough clinical examination is performed to understand the characteristics of the tumour. In addition, a range of imaging modalities can be used to visualise the shape and size of the tumour and its relation to surrounding tissues and organs. These include x-ray radiography, ultrasound, computed axial tomography (CT or CAT) scans and magnetic resonance imaging (MRI).[1]:Ch. 5 The data from many of these sources can be used to create a 3D visualisation of the tumour and the surrounding tissues.[1]:Ch. 5
Using this information, a plan of the optimal distribution of the radiation sources can be developed. This includes consideration of how the source carriers (applicators), which are used to deliver the radiation to the treatment site, should be placed and positioned.[1]:Ch. 5 Applicators are non-radioactive and are typically needles or plastic catheters. The specific type of applicator used will depend on the type of cancer being treated and the characteristics of the target tumour.[1]:Ch. 5
This initial planning helps to ensure that ‘cold spots’ (too little irradiation) and ‘hot spots’ (too much irradiation) are avoided during treatment, as these can respectively result in treatment failure and side-effects.[29]
Before radioactive sources can be delivered to the tumour site, the applicators have to be inserted and correctly positioned in line with the initial planning.
Imaging techniques, such as x-ray, fluoroscopy and ultrasound are typically used to help guide the placement of the applicators to their correct positions and to further refine the treatment plan.[1]:Ch. 5 CAT scans and MRI can also be used.[1]:Ch. 5 Once the applicators are inserted, they are held in place against the skin using sutures or adhesive tape to prevent them from moving. Once the applicators are confirmed as being in the correct position, further imaging can be performed to guide detailed treatment planning.[1]:Ch. 5
The images of the patient with the applicators in situ are imported into treatment planning software and the patient is brought into a dedicated shielded room for treatment. The treatment planning software enables multiple 2D images of the treatment site to be translated into a 3D ‘virtual patient’, within which the position of the applicators can be defined.[1]:Ch. 5 The spatial relationships between the applicators, the treatment site and the surrounding healthy tissues within this ‘virtual patient’ are a copy of the relationships in the actual patient.
To identify the optimal spatial and temporal distribution of radiation sources within the applicators of the implanted tissue or cavity, the treatment planning software allows virtual radiation sources to be placed within the virtual patient. The software shows a graphical representation of the distribution of the irradiation. This serves as a guide for the brachytherapy team to refine the distribution of the sources and provide a treatment plan that is optimally tailored to the anatomy of each patient before actual delivery of the irradiation begins.[72] This approach is sometimes called ‘dose-painting’.
The radiation sources used for brachytherapy are always enclosed within a non-radioactive capsule. The sources can be delivered manually, but are more commonly delivered through a technique known as ‘afterloading’.
Manual delivery of brachytherapy is limited to a few LDR applications, due to risk of radiation exposure to clinical staff.[71]
In contrast, afterloading involves the accurate positioning of non-radioactive applicators in the treatment site, which are subsequently loaded with the radiation sources. In manual afterloading, the source is delivered into the applicator by the operator.
Remote afterloading systems provide protection from radiation exposure to healthcare professionals by securing the radiation source in a shielded safe. Once the applicators are correctly positioned in the patient, they are connected to an ‘afterloader’ machine (containing the radioactive sources) through a series of connecting guide tubes. The treatment plan is sent to the afterloader, which then controls the delivery of the sources along the guide tubes into the pre-specified positions within the applicator. This process is only engaged once staff are removed from the treatment room. The sources remain in place for a pre-specified length of time, again following the treatment plan, following which they are returned along the tubes to the afterloader.
On completion of delivery of the radioactive sources, the applicators are carefully removed from the body. Patients typically recover quickly from the brachytherapy procedure, enabling it to often be performed on an outpatient basis.[12]
Between 2003 and 2012  in United States community hospitals, the rate of hospital stays with brachytherapy (internal radiation therapy) had a 24.4 percent average annual decrease among adults aged 45–64 years and a 27.3 percent average annual decrease among adults aged 65–84 years. Brachytherapy was the OR procedure with the greatest change in occurrence among hospital stays paid by Medicare and private insurance.[73]
Commonly used radiation sources (radionuclides) for brachytherapy.[74]
Brachytherapy dates back to 1901 (shortly after the discovery of radioactivity by Henri Becquerel in 1896) when Pierre Curie suggested to Henri-Alexandre Danlos that a radioactive source could be inserted into a tumour.[75][76]
It was found that the radiation caused the tumour to shrink.[76] Independently, Alexander Graham Bell also suggested the use of radiation in this way.[76] In the early twentieth century, techniques for the application of brachytherapy were pioneered at the Curie institute in Paris by Danlos and at St Luke's and Memorial Hospital in New York by Robert Abbe.[1]:Ch. 1[76]
Interstitial radium therapy was common in the 1930s.[1]:Ch. 1 Gold seeds filled with radon were used as early as 1942[77] until at least 1958.[78] Gold shells were selected by Gino Failla around 1920 to shield beta rays while passing gamma rays.[79] Cobalt needles were also used briefly after World War II.[1]:Ch. 1 Radon and cobalt were replaced by radioactive tantalum and gold, before iridium rose in prominence.[1]:Ch. 1 First used in 1958, iridium is the most commonly used artificial source for brachytherapy today.[1]:Ch. 1
Following initial interest in brachytherapy in Europe and the US, its use declined in the middle of the twentieth century due to the problem of radiation exposure to operators from the manual application of the radioactive sources.[76][80] However, the development of remote afterloading systems, which allow the radiation to be delivered from a shielded safe, and the use of new radioactive sources in the 1950s and 1960s, reduced the risk of unnecessary radiation exposure to the operator and patients.[75] This, together with more recent advancements in three-dimensional imaging modalities, computerised treatment planning systems and delivery equipment has made brachytherapy a safe and effective treatment for many types of cancer today.[1]:Ch. 1
The word "brachytherapy" comes from the Greek word βραχύς brachys, meaning "short-distance" or "short".
Due to the small size of brachytherapy sources and low control in early decades, there is a risk that some of these have escaped into the environment to become orphaned sources. A radium needle was found in a Prague playground in 2011, radiating 500 µSv/h from one metre away.[81][82][83]




Lymph node - Wikipedia
A lymph node or lymph gland is an ovoid or kidney-shaped organ of the lymphatic system, and of the adaptive immune system, that is widely present throughout the body. They are linked by the lymphatic vessels as a part of the circulatory system. Lymph nodes are major sites of B and T lymphocytes, and other white blood cells. Lymph nodes are important for the proper functioning of the immune system, acting as filters for foreign particles and cancer cells. Lymph nodes do not have a detoxification function, which is primarily dealt with by the liver and kidneys.
In the lymphatic system the lymph node is a secondary lymphoid organ.[3] A lymph node is enclosed in a fibrous capsule and is made up of an outer cortex and an inner medulla.[3]
Lymph nodes also have clinical significance. They become inflamed or enlarged in various diseases which may range from trivial throat infections, to life-threatening cancers. The condition of the lymph nodes is very important in cancer staging, which decides the treatment to be used, and determines the prognosis. When swollen, inflamed or enlarged, lymph nodes can be hard, firm or tender.[4]
Lymph nodes are kidney or oval shaped and range in size from a few millimeters to about 1–2 cm long.[5] Each lymph node is surrounded by a fibrous capsule, which extends inside the lymph node to form trabeculae. The substance of the lymph node is divided into the outer cortex and the inner medulla. The cortex is continuous around the medulla except where the medulla comes into direct contact with the hilum.[5]
Thin reticular fibers of reticular connective tissue, and elastin form a supporting meshwork called a reticulin inside the node. B cells are mainly found in the outer (superficial) cortex where they are clustered  together as follicular B cells in lymphoid follicles and the T cells are mainly in the paracortex.[6] The lymph node is divided into compartments called lymph nodules (or lobules) each consisting of a cortical region of combined follicle B cells, a paracortical region of T cells, and a basal part of the nodule in the medulla.[7]
The number and composition of follicles can change especially when challenged by an antigen, when they develop a germinal center.[5] Elsewhere in the node, there are only occasional leukocytes. As part of the reticular network there are follicular dendritic cells in the B cell follicle and fibroblastic reticular cells in the T cell cortex. The reticular network not only provides the structural support, but also the surface for adhesion of the dendritic cells, macrophages and lymphocytes. It allows exchange of material with blood through the high endothelial venules and provides the growth and regulatory factors necessary for activation and maturation of immune cells.[8]
Lymph enters the convex side of the lymph node through multiple afferent lymphatic vessels, and flows through spaces called sinuses. A lymph sinus which includes the subcapsular sinus, is a channel within the node, lined by endothelial cells along with fibroblastic reticular cells and this allows for the smooth flow of lymph through them.The endothelium of the subcapsular sinus is continuous with that of the afferent lymph vessel and is also with that of the similar sinuses flanking the trabeculae and within the cortex. All of these sinuses drain the filtered lymphatic fluid into the medullary sinuses, from where the lymph flows into the efferent lymph vessels to exit the node at the hilum on the concave side.[5] These vessels are smaller and don't allow the passage of the macrophages so that they remain contained to function within the lymph node. In the course of the lymph, lymphocytes may be activated as part of the adaptive immune response.
The lymph node capsule is composed of dense irregular connective tissue with some plain collagenous fibers, and from its internal surface are given off a number of membranous processes or trabeculae. They pass inward, radiating toward the center of the node, for about one-third or one-fourth of the space between the circumference and the center of the node. In some animals they are sufficiently well-marked to divide the peripheral or cortical portion of the node into a number of compartments (nodules), but in humans this arrangement is not obvious. The larger trabeculae springing from the capsule break up into finer bands, and these interlace to form a mesh-work in the central or medullary portion of the node. In these trabecular spaces formed by the interlacing trabeculae is contained the proper lymph node substance or lymphoid tissue. The node pulp does not, however, completely fill the spaces, but leaves, between its outer margin and the enclosing trabeculae, a channel or space of uniform width throughout. This is termed the subcapsular sinus (lymph path or lymph sinus). Running across it are a number of finer trabeculae of reticular connective tissue, the fibers of which are, for the most part, covered by ramifying cells.
The subcapsular sinus (lymph path, lymph sinus, marginal sinus) is the space between the capsule and the cortex which allows the free movement of lymphatic fluid and so contains few lymphocytes.[5] It is continuous with the similar lymph sinuses that flank the trabeculae.[5]
The lymph node contains lymphoid tissue, i.e., a meshwork or fibers called reticulum with white blood cells enmeshed in it. The regions where there are few cells within the meshwork are known as lymph sinus. It is lined by reticular cells, fibroblasts and fixed macrophages.[5]
The subcapsular sinus has clinical importance as it is the most likely location where the earliest manifestations of a metastatic carcinoma in a lymph node would be found.
The cortex of the lymph node is the outer portion of the node, underneath the capsule and the subcapsular sinus.[7] It has an outer superficial part and a deeper part known as the paracortex.[7] The subcapsular sinus drains to the trabecular sinuses, and then the lymph flows into the medullary sinuses.
The outer cortex consists mainly of the B cells arranged as follicles, which may develop a germinal center when challenged with an antigen, and the deeper paracortex mainly consists of the T cells. Here the T-cells mainly interact with dendritic cells, and the reticular network is dense.[9]
The medulla contains large blood vessels, sinuses and medullary cords that contain antibody-secreting plasma cells.
The medullary cords are cords of lymphatic tissue, and include plasma cells, macrophages, and B cells. The medullary sinuses (or sinusoids) are vessel-like spaces separating the medullary cords. Lymph flows into the medullary sinuses from cortical sinuses, and into the efferent lymphatic vessel. There is usually only one efferent vessel though sometimes there may be two.[10] Medullary sinuses contain histiocytes (immobile macrophages) and reticular cells.
Lymph nodes are present throughout the body, are more concentrated near and within the trunk, and are divided in the study of anatomy into groups. Some lymph nodes can be felt when enlarged (and occasionally when not), such as the axillary lymph nodes under the arm, the cervical lymph nodes of the head and neck and the inguinal lymph nodes near the groin crease. Some lymph nodes can be seen, such as the tonsils. Most lymph nodes however lie within the trunk adjacent to other major structures in the body - such as the paraaortic lymph nodes and the tracheobronchial lymph nodes.
There are no lymph nodes in the central nervous system, which is separated from the body by the blood-brain barrier.
The primary function of lymph nodes is the filtering of lymph to identify and fight infection. In order to do this, lymph nodes contain lymphocytes, a type of white blood cell, which includes B cells and T cells. These circulate through the bloodstream and enter and reside in lymph nodes.[11] B cells produce antibodies. Each antibody has a single predetermined target, an antigen, that it can bind to. These circulate throughout the bloodstream and if they find this target, the antibodies bind to it and stimulate an immune response. Each B cell produces different antibodies, and this process is driven in lymph nodes. B cells enter the bloodstream as "naive" cells produced in bone marrow. After entering a lymph node, they then enter a lymphoid follicle, where they multiply and divide, each producing a different antibody. If a cell is stimulated, it will go on to produce more antibodies (a plasma cell) or act as a memory cell to help the body fight future infection.[12] If a cell is not stimulated, it will undergo apoptosis and die.[12]
Antigens are molecules found on bacterial cell walls, chemical substances secreted from bacteria, or sometimes even molecules present in body tissue itself. These are taken up by cells throughout the body called antigen-presenting cells, such as dendritic cells.[13] These antigen presenting cells enter the lymph system and then lymph nodes. They present the antigen to T cells and, if there is a T cell with the appropriate T cell receptor, it will be activated.[12]
B cells acquire antigen directly from the afferent lymph.  If a B cell binds its cognate antigen it will be activated.  Some B cells will immediately develop into antibody secreting plasma cells, and secrete IgM.  Other B cells will internalize the antigen and present it to Follicular helper T cells on the B and T cell zone interface.  If a cognate FTh cell is found it will upregulate CD40L and promote somatic hypermutation and isotype class switching of the B cell, increasing its antigen binding affinity and changing its effector function.  Proliferation of cells within the lymph node will make the node expand.
Lymph is present throughout the body, and circulates through lymphatic vessels. These drain into and from lymph nodes – afferent vessels drain into nodes, and efferent vessels from nodes. When lymph fluid enters a node, it drains into the node just beneath the capsule in a space called the subcapsular sinus. The subcapsular sinus drains into trabecular sinuses and finally into medullary sinuses. The sinus space is criss-crossed by the pseudopods of macrophages, which act to trap foreign particles and filter the lymph. The medullary sinuses converge at the hilum and lymph then leaves the lymph node via the efferent lymphatic vessel towards either a more central lymph node or ultimately for drainage into a central venous subclavian blood vessel.
The spleen and tonsils are the larger secondary lymphoid organs that serve similar functions to lymph nodes, though the spleen filters blood cells rather than lymph.
Lymph nodes may become enlarged due to an infection, tumor, autoimmune disease, drug reactions, or to leukemia.[14] Swollen lymph nodes (or the disease causing them) are referred to as lymphadenopathy.[15] Swollen lymph nodes may be seen, as in enlarged tonsils, or seen as gross enlargement of nodes due to lymphoma. They may be felt, or seen on a scan. Swollen lymph nodes may be painful or cause other symptoms such as a difficulty in swallowing or in breathing. When very large they may compress on a blood vessel.  Swelling can occur in one node, in a localised area, or be widespread.
The taking of a medical history and exam by a medical practitioner can help point to the cause of the swelling, whether it be a localised infection, or a systemic disorder. Many symptoms or signs may point to the cause of swelling - for example, a sore throat and a cough may point to an upper respiratory tract infection as the cause of tonsil swelling. Changes in the appearance of a breast or a mass that has been felt may explain underarm pain and axillary lymph node swelling. Ongoing fevers or night sweats may suggest a systemic infection or a lymphoma as the cause of swelling. Depending on these findings, a wide variety of medical tests that include blood tests and scans may be needed to further examine the cause. A biopsy of a lymph node may also be needed.
Lymphedema is another and fairly widespread condition that results in fluid retention and tissue swelling. It can be congenital as a result usually of undeveloped or absent lymph nodes, and is known as primary lymphedema. Secondary lymphedema usually results from the removal of lymph nodes during breast cancer surgery or from other damaging treatments such as radiation. It can also be caused by some parasitic infections. Affected tissues are at a great risk of infection.
Lymph nodes can be affected by both primary cancers of lymph tissue, and secondary cancers affecting other part of the body. Primary cancers of lymph tissue are called lymphomas and include Hodgkin lymphoma and non-Hodgkin lymphoma. Cancer of lymph nodes can cause a wide range of symptoms from painless long-term slowly growing swelling to sudden, rapid enlargement over days or weeks. Lymphoma is managed by haematologists and oncologists
Local cancer in many parts of the body can cause lymph nodes to enlarge, usually because of tumours cells that have metastasised into the node. Lymph node involvement is often a key part in the diagnosis and treatment of cancer, acting as "sentinels" of local disease, incorporated into TNM staging and other cancer staging systems. As part of the investigations or workup for cancer, lymph nodes may be imaged or even surgically removed. Whether lymph nodes are affected will affect the stage of the cancer and overall treatment and prognosis.
Lymphatic system
Section of small lymph node of rabbit.
Lymphatics of the arm
Lymphatics of the axillary region
Lymph node histology




Palliative care - Wikipedia
Palliative care[1] is a multidisciplinary approach to specialized medical and nursing care for people with life-limiting illnesses. It focuses on providing relief from the symptoms, pain, physical stress, and mental stress at any stage of illness. The goal is to improve quality of life for both the person and their family.[2][3] Evidence as of 2016 supports palliative care's efficacy in the improvement of a patient's quality of life.[4]
Palliative care is provided by a team of physicians, nurses, physiotherapists, occupational therapists and other health professionals who work together with the primary care physician and referred specialists and other hospital or hospice staff to provide additional support. It is appropriate at any age and at any stage in a serious illness and can be provided as the main goal of care or along with curative treatment. Although it is an important part of end-of-life care, it is not limited to that stage. Palliative care can be provided across multiple settings including in hospitals, at home, as part of community palliative care programs, and in skilled nursing facilities. Interdisciplinary palliative care teams work with people and their families to clarify goals of care and provide symptom management, psycho-social, and spiritual support.
Physicians sometimes use the term palliative care in a sense meaning palliative therapies without curative intent, when no cure can be expected (as often happens in late-stage cancers). For example, tumor debulking can continue to reduce pain from mass effect even when it is no longer curative. A clearer usage is palliative, noncurative therapy when that is what is meant, because palliative care can be used along with curative or aggressive therapies.
Medications and treatments are said to have a palliative effect if they relieve symptoms without having a curative effect on the underlying disease or cause. This can include treating nausea related to chemotherapy or something as simple as morphine to treat the pain of broken leg or ibuprofen to treat pain related to an influenza infection.
Palliative care is given to people who have any serious illness and who have physical, psychological, social, or spiritual distress as a result of the treatment they are seeking or receiving.[5] Palliative care increases comfort by lessening pain, controlling symptoms, and lessening stress for the patient and family, and should not be delayed when it is indicated.[5] Evidence shows that end-of-life communication interventions decrease utilization (such as length of stay), particularly in the intensive care unit setting,[6] and that palliative care interventions (mostly in the outpatient setting) are effective for improving patient and caregiver perceptions of care.[7]
Palliative care is not reserved for people in end-of-life care and can improve quality of life, decrease depressive symptoms, and increase survival time.[5] If palliative care is indicated for a person in an emergency department, then that care should begin in the emergency department immediately and with referral to additional palliative care services.[8] Emergency care physicians often are the first medical professionals to open the discussion about palliative care and hospice services with people needing care and their families.[8][9]
In some cases, medical specialty professional organizations recommend that sick people and physicians respond to an illness only with palliative care and not with a therapy directed at the disease. The following items are indications named by the American Society of Clinical Oncology as characteristics of a person who should receive palliative care but not any cancer-directed therapy.[10][11]
These characteristics may be generally applicable to other disease conditions besides cancer.[citation needed]
Palliative care is a term derived from Latin palliare, "to cloak." It refers to specialised medical care for people with serious illnesses. It is focused on providing people with relief from the symptoms, pain and stress of a serious illness — whatever the prognosis.[12] The goal is to improve quality of life for both the sick person and the family as they are the central system for care.
A World Health Organization statement[13] describes palliative care as "an approach that improves the quality of life of patients and their families facing the problems associated with life-threatening illness, through the prevention and relief of suffering by means of early identification and impeccable assessment and treatment of pain and other problems, physical, psychosocial and spiritual." More generally, however, the term "palliative care" may refer to any care that alleviates symptoms, whether or not there is hope of a cure by other means; thus, palliative treatments may be used to alleviate the side effects of curative treatments, such as relieving the nausea associated with chemotherapy.
The term "palliative care" is increasingly used with regard to diseases other than cancer such as chronic, progressive pulmonary disorders, renal disease, chronic heart failure, HIV/AIDS and progressive neurological conditions. In addition, the rapidly growing field of pediatric palliative care has clearly shown the need for services geared specifically for children with serious illness.
While palliative care may seem to offer a broad range of services, the goals of palliative treatment are concrete: relief from suffering, treatment of pain and other distressing symptoms, psychological and spiritual care, a support system to help the individual live as actively as possible and a support system to sustain and rehabilitate the individual's family.[14]
Starting in 2006 in the United States, palliative medicine is now a board certified sub-speciality of internal medicine with specialised fellowships for physicians who are interested in the field.[15]
In the United States, a distinction should be made between palliative care and hospice care. Hospice services and palliative care programs share similar goals of providing symptom relief and pain management.[16] Palliative care services can be appropriate for anyone with a serious, complex illness, whether they are expected to recover fully, to live with chronic illness for an extended time, or to experience disease progression.
Hospice care focuses on five topics: communication, collaboration, compassionate caring, comfort, and cultural (spiritual) care.  The end of life treatment in hospice differs from that in hospitals because the medical and support staff are specialized in treating only the terminally ill.  This specialization allows for the staff to handle the legal and ethical matters surrounding death more thoroughly and efficiently with survivors of the patient. Hospice comfort care also differentiates because patients are admitted to continue managing discomfort relief treatments while the terminally ill receiving comfort care in a hospital are admitted because end-of-life symptoms are poorly controlled or because current outpatient symptom relief efforts are ineffective.
Hospice is a type of care involving palliation without curative intent. Usually, it is used for people with no further options for curing their disease or in people who have decided not to pursue further options that are arduous, likely to cause more symptoms, and not likely to succeed. Hospice care under the Medicare Hospice Benefit requires that two physicians certify that a person has less than six months to live if the disease follows its usual course. This does not mean, though, that if a person is still living after six months in hospice he or she will be discharged from the service.
The philosophy and multi-disciplinary team approach are similar with hospice and palliative care, and indeed the training programs and many organizations provide both. The biggest difference between hospice and palliative care is the type of illness people have, where they are in their illness especially related to prognosis, and their goals/wishes regarding curative treatment.
Outside the United States there is generally no such division of terminology or funding, and all such care with a primarily palliative focus, whether or not for people with a terminal illness, is usually referred to as palliative care.
Outside the United States the term hospice usually refers to a building or institution which specializes in palliative care, rather than to a particular stage of care progression.  Such institutions may predominantly specialize in providing care in an end-of-life setting; but they may also be available for people with other specific palliative care needs.
Despite the fact that many individuals are now dying either at home or in a care facility, as of 2010, 29% of all deaths in the United States occurred in a hospital setting, these statistics increased in 2016 to about 60% of all deaths occurred in the hospital which is a substantial increase from 2010. which is still a rather substantial percentage.[17][citation needed] Comfort care can require meticulous techniques to alleviate distress caused by severe health troubles near the end of life.[18] Doctors, nurses, nurses aides, social workers, chaplains, and other hospital support staff work systematically together to carry out end of life care and comfort in the hospital setting. Hospitals are able to accommodate the demand for acute medical attention as well as education and supportive therapies for the families of their loved ones. Within hospital settings, there is an increasing shortage of board-certified palliative care specialists. This shortage results in the responsibility of comfort care falling on the shoulders of other individuals.[17]
Comfort care in hospitals differs from comfort care in hospices because patients’ end-of-life symptoms are poorly controlled prior to checking in.  The average time between death and the admission of a terminally ill patient is 7.9 days.[18] Patients receiving end of life care in a hospice setting typically have a longer time between their admission and death; 60% of hospice patients passed within approximately 30 days of being admitted. The average length of stay at a hospice house from admission to death is about 48 hours.[19]

A method for the assessment of symptoms in people admitted to palliative care is the Edmonton Symptoms Assessment Scale (ESAS), in which there are eight visual analog scales (VAS) of 0 to 10, indicating the levels of pain, activity, nausea, depression, anxiety, drowsiness, appetite and sensation of well-being,[20] sometimes with the addition of shortness of breath.[21] On the scales, 0 means that the symptom is absent and 10 that it is of worst possible severity.[21] It is completed either by the person in need of care alone, by the person with a nurse's assistance, or by the nurses or relatives.[20]
Medications used in palliative care are used differently from standard medications, based on established practices with varying degrees of evidence.[22] Examples include the use of antipsychotic medications to treat nausea, anticonvulsants to treat pain, and morphine to treat dyspnea. Routes of administration may differ from acute or chronic care, as many people in palliative care lose the ability to swallow. A common alternative route of administration is subcutaneous, as it is less traumatic and less difficult to maintain than intravenous medications. Other routes of administration include sublingual, intramuscular and transdermal. Medications are often managed at home by family or nursing support.[23]
Palliative care interventions in care homes may contribute to lower discomfort for residents with dementia, and to improve family member's views of the quality of care.[24] However, higher quality research is needed to support the benefits of these interventions for older people dying in these facilities.[24]
For many, knowing that the end of life is approaching induces various forms of emotional and psychological distress. The key to effective palliative care is to provide a safe way for the individual to address their distresses, that is to say their total suffering, a concept first thought up by Cicely Saunders, and now widely used, for instance by authors like Twycross or Woodruff.[25] Dealing with total suffering involves a broad range of concerns, starting with treating physical symptoms such as pain, nausea and breathlessness with various medications. Usually, the sick person's concerns are pain, fears about the future, loss of independence, worries about their family and feeling like a burden. The interdisciplinary team also often includes a licensed mental health professional, a licensed social worker, or a counselor, as well as spiritual support such as a chaplain, who can play roles in helping people and their families cope. There are five principal methods for addressing patient anxiety in palliative care settings. They are counseling, visualisation, cognitive methods, drug therapy and relaxation therapy. Palliative pets can play a role in this last category.[26]
To take care of a patient’s pain that is at the End of Life, one has to understand that it is of the utmost importance to take care of the Total Body Pain. This Total Body Pain is the sum of all of the physical, psychosocial, and spiritual pain they can be enduring at this stressful time.[18] When someone is at the end of their life and they are seeking comfort care, the majority of the time they are in excruciating pain. This pain can be a physical manifestation to where their body is beginning to fight back on itself causing a multitude of physical symptoms. The pain can be in a psychosocial manifestation and can be dealt with by the medical team having open communication about how to cope with and prepare for death. The last aspect of pain that is included in Total Body Pain is the spiritual pain manifestation; if patients spiritual needs are met, then studies show that they will be more likely to get hospice care. Addressing the needs of the Total Body Pain can lead to a better quality of life overall for the patients.[18]
The Physical pain can be managed in a way that uses adequate pain medications as long as they will not put the patient at further risk for developing or increasing medical diagnoses such as heart problems or difficulty breathing.[18] Patients at the end of life can exhibit many physical symptoms that can cause extreme pain such as dyspnea (or difficulty breathing), Coughing, Xerostomia (Dry Mouth), Nausea and Vomiting, Constipation, Fever, Delirium, Excessive Oral and Pharyngeal Secretions (“Death Rattle”) and many more painful symptoms can be seen that they are hoping to get some pain relief from.[27]
Once the immediate physical pain has been dealt with, it is important to remember to be a compassionate and empathetic caregiver that is there to listen and be there for their patients. Being able to identify the distressing factors in their life other than the pain can help them be more comfortable.[28] When a patient has their needs met then they are more likely to be open to the idea of hospice or treatments outside of comfort care. Having a Psychosocial assessment allows the medical team to help facilitate a healthy patient-family understanding of adjustment, coping and support. This communication between the medical team and the patients and family can also help facilitate discussions on the process of maintaining and enhancing relationships, finding meaning in the dying process, and achieving a sense of control while confronting and preparing for death.[18]
When a patient is at the end of life, one of the most important things that a lot of them want to talk to their physicians about is their spirituality. Regardless of this desire, less than 50% of physicians believe that it is their job to address these religious concerns, and only a minority of patients have been recorded to have had their spiritual needs met.[28] Most of the time these patients are referred to Chaplain services if they are available or they rely on the medical staff available and any family and friends that may be there as well. Chaplain services are one of the best services available for meeting this spiritual need. That being said, there are not enough Chaplains available at any one time and the majority of them are not qualified to be giving services to Comfort Care patients whom often have the most serious illnesses. According to a multiple site cohort study involving 343 advanced cancer patients, it was found that those who had their religious needs met were more likely than those who didn’t have their religious needs met to go through with more hospice care and to not get unnecessary treatments at the end of life, as well as the study showed that they ended up having higher quality of life scores than those who did not have their spiritual needs met.[28]
Palliative care for children and young people is an active and total approach to care, from the point of diagnosis, throughout the child’s life, death and beyond. It embraces physical, emotional, social and spiritual elements and focuses on the enhancement of quality of life for the child or young person, and support for the whole family. It includes the management of distressing symptoms, provision of short breaks, end of life care and bereavement support.[29]
Palliative care can be introduced at any point throughout a child’s life; it is completely individual. Some children may require palliative care from birth, others only as their condition deteriorates. Families may also vary as to whether they wish to pursue treatments aimed to cure or significantly prolong life. In practice, palliative care should be offered from diagnosis of a life-limiting condition or recognition that curative treatment for a life-threatening condition is not an option; however, each situation is different and care should be tailored to the child.
There are an estimated 49,000 children and young people in the UK living with a life-threatening or life-limiting condition that may require palliative care services.[31][32] A 2015 survey from the Royal College of Nursing (RCN) found that nearly a third of children's nurses said they don't have the resources to deliver adequate care in the home setting.[33]
Palliative care began in the hospice movement and is now widely used outside of traditional hospice care. Hospices were originally places of rest for travellers in the 4th century. In the 19th century a religious order established hospices for the dying in Ireland and London. The modern hospice is a relatively recent concept that originated and gained momentum in the United Kingdom after the founding of St. Christopher's Hospice in 1967. It was founded by Dame Cicely Saunders, widely regarded as the founder of the modern hospice movement. Dame Cicely Saunders, went to St.Thomas’ Hospital in 1944 to become a nurse. After working with the terminally ill she went and became a doctor in 1957 so that she could start her own hospice.[34]  Dr. Cicely Saunders then opened her own hospice after she saw all of the terminally ill patients that she nursed in excruciating pain because their pain was not being managed like it should have been.[citation needed]
In the UK in 2005 there were just under 1,700 hospice services consisting of 220 inpatient units for adults with 3,156 beds, 33 inpatient units for children with 255 beds, 358 home care services, 104 hospice at home services, 263 day care services and 293 hospital teams. These services together helped over 250,000 people in 2003 and 2004.[citation needed]
Hospice in the United States has grown from a volunteer-led movement to a significant part of the health care system. In 2005 around 1.2 million persons and their families received hospice care. Hospice is the only Medicare benefit that includes pharmaceuticals, medical equipment, twenty-four-hour/seven-day-a-week access to care and support for loved ones following a death. Most hospice care is delivered at home. Hospice care is also available to people in home-like hospice residences, nursing homes, assisted living facilities, veterans' facilities, hospitals, and prisons.[citation needed]
The first United States hospital-based palliative care consult service was developed by the Wayne State University School of Medicine in 1985 at Detroit Receiving Hospital.[35] The first palliative medicine program in the United States was started in 1987 by Declan Walsh, MD at the Cleveland Clinic Cancer Center in Cleveland, Ohio.[36] This is a comprehensive integrated program, responsible for several notable innovations in US palliative medicine;[37] the first clinical and research fellowship (1991),[38] acute care palliative medicine inpatient unit (1994), and Chair in Palliative Medicine (1994). The program evolved into The Harry R. Horvitz Center for Palliative Medicine which was designated as a World Health Organization international demonstration project and accredited by the European Society for Medical Oncology as an Integrated Center of Oncology and Palliative Care.[39] Other programs followed: most notable the Palliative Care Program at the Medical College of Wisconsin (1993); Pain and Palliative Care Service, Memorial Sloan-Kettering Cancer Center (1996); and The Lilian and Benjamin Hertzberg Palliative Care Institute, Mount Sinai School of Medicine (1997). Since then there has been a dramatic increase in hospital-based palliative care programs, now numbering more than 1,400. Eighty percent of US hospitals with more than 300 beds have a program.[40]
A widely cited report in 2007 of a randomized controlled trial with 298 patients found that palliative care delivered to patients and their caregivers at home improved satisfaction with care while decreasing medical service use and the cost of care.[41]
A 2009 study regarding the availability of palliative care in 120 US cancer center hospitals reported the following: only 23% of the centers have beds that are dedicated to palliative care; 37% offer inpatient hospice; 75% have a median time of referral to palliative care to the time of death of 30 to 120 days; research programs, palliative care fellowships, and mandatory rotations for oncology fellows were uncommon.[42]
The results of a 2010 study in The New England Journal of Medicine showed that people with lung cancer who received early palliative care in addition to standard oncologic care experienced less depression, increased quality of life and survived 2.7 months longer than those receiving standard oncologic care.[43]
In 2011, The Joint Commission (an independent, not-for-profit organization that accredits and certifies thousands of health care organizations and programs in the United States) began an Advanced Certification Program for Palliative Care that recognizes hospital inpatient programs. In order to obtain this certification, a hospital must show superior care and enhancement of the quality of life for people with serious illness.[44]
The first pan-European center devoted to improving palliative care and end-of-life care was established in Trondheim, Norway in 2009. The center is based at NTNU's Faculty of Medicine and at St. Olav's Hospital/Trondheim University Hospital and coordinates efforts between groups and individual researchers across Europe, specifically Scotland, England, Italy, Denmark, Germany and Switzerland, along with the United States, Canada and Australia.[citation needed]
Families of persons who get a referral to palliative care during a hospitalization incur less costs than people with similar conditions who do not get a palliative care referral.[45]
Funding for hospice and palliative care services varies. In Great Britain and many other countries all palliative care is offered free, either through the National Health Service (as in the UK) or through charities working in partnership with the local health services. Palliative care services in the US are paid by philanthropy, fee-for service mechanisms, or from direct hospital support while hospice care is provided as Medicare benefit; similar hospice benefits are offered by Medicaid and most private health insurers. Under the Medicare Hospice Benefit (MHB) a person signs off their Medicare Part B (acute hospital payment) and enrols in the MHB through Medicare Part B with direct care provided by a Medicare certified hospice agency. Under terms of the MHB the Hospice agency is responsible for the Plan of Care and may not bill the person for services. The hospice agency, together with the person's primary physician, is responsible for determining the Plan of Care. All costs related to the terminal illness are paid from a per diem rate (~US $126/day) that the hospice agency receives from Medicare – this includes all drugs and equipment, nursing, social service, chaplain visits and other services deemed appropriate by the hospice agency; Medicare does not pay for custodial care. People may elect to withdraw from the MHB and return to Medicare Part A and later re-enrol in hospice.
In most countries hospice care and palliative care is provided by an interdisciplinary team consisting of physicians, pharmacists, registered nurses, nursing assistants, social workers, hospice chaplains, physiotherapists, occupational therapists, complementary therapists, volunteers, and, most importantly, the family. The team's focus is to optimize the person's comfort. In some countries, additional members of the team may include certified nursing assistants and home healthcare aides, as well as volunteers from the community (largely untrained but some being skilled medical personnel), and housekeepers. In the United States, the physician subspeciality of hospice and palliative medicine was established in 2006[46] to provide expertise in the care of people with life-limiting, advanced disease, and catastrophic injury; the relief of distressing symptoms; the coordination of interdisciplinary care in diverse settings; the use of specialized care systems including hospice; the management of the imminently dying patient; and legal and ethical decision making in end-of-life care.[47]
Caregivers, both family and volunteers, are crucial to the palliative care system. Caregivers and people being treated often form lasting friendships over the course of care. As a consequence caregivers may find themselves under severe emotional and physical strain. Opportunities for caregiver respite are some of the services hospices provide to promote caregiver well-being. Respite may last a few hours up to several days (the latter being done by placing the primary person being cared for in a nursing home or inpatient hospice unit for several days).[48]
In the US, board certification for physicians in palliative care was through the American Board of Hospice and Palliative Medicine; recently this was changed to be done through any of 11 different speciality boards through an American Board of Medical Specialties-approved procedure. Additionally, board certification is available to osteopathic physicians (D.O.) in the United States through four medical speciality boards through an American Osteopathic Association Bureau of Osteopathic Specialists-approved procedure.[49] More than 50 fellowship programs provide one to two years of speciality training following a primary residency. In Britain palliative care has been a full speciality of medicine since 1989 and training is governed by the same regulations through the Royal College of Physicians as with any other medical speciality.[50] Nurses, in the United States and internationally, can receive continuing education credits through Palliative Care specific trainings, such as those offered by End-of-Life Nursing Education Consortium (ELNEC) [51]
In India Tata Memorial Centre, Mumbai has started a physician course in palliative medicine for the first time in the country since 2012.
In the United States, hospice and palliative care represent two different aspects of care with similar philosophy, but with different payment systems and location of services. Palliative care services are most often provided in acute care hospitals organized around an interdisciplinary consultation service, with or without an acute inpatient palliative care unit. Palliative care may also be provided in the dying person's home as a "bridge" program between traditional US home care services and hospice care or provided in long-term care facilities.[52] In contrast over 80% of hospice care in the US is provided at home with the remainder provided to people in long-term care facilities or in free standing hospice residential facilities. In the UK hospice is seen as one part of the speciality of palliative care and no differentiation is made between 'hospice' and 'palliative care'.
In the UK palliative care services offer inpatient care, home care, day care and outpatient services, and work in close partnership with mainstream services. Hospices often house a full range of services and professionals for children and adults. In 2015 the UK's palliative care was ranked as the best in the world "due to comprehensive national policies, the extensive integration of palliative care into the National Health Service, a strong hospice movement, and deep community engagement on the issue."[53]
The focus on a person's quality of life has increased greatly since the 1990s. In the United States today, 55% of hospitals with more than 100 beds offer a palliative-care program,[40] and nearly one-fifth of community hospitals have palliative-care programs.[54] A relatively recent development is the palliative-care team, a dedicated health care team that is entirely geared toward palliative treatment.
Physicians practicing palliative care do not always receive support from the people they are treating, family members, healthcare professionals or their social peers. More than half of physicians in one survey reported that they have had at least one experience where a patient's family members, another physician or another health care professional had characterised their work as being "euthanasia, murder or killing" during the last five years. A quarter of them had received similar comments from their own friends or family member, or from a patient.[55]



Specialty (medicine) - Wikipedia
A specialty, or speciality, in medicine is a branch of medical practice. After completing medical school, physicians or surgeons usually further their medical education in a specific specialty of medicine by completing a multiple year residency to become a medical specialist.[1]
To a certain extent, medical practitioners have always been specialized. According to Galen, specialization was common among Roman physicians. The particular system of modern medical specialities evolved gradually during the 19th century. Informal social recognition of medical specialization evolved before the formal legal system. The particular subdivision of the practice of medicine into various specialities varies from country to country, and is somewhat arbitrary.[2]
Medical specialties can be classified along several axes. These are:
Throughout history, the most important has been the division into surgical and internal medicine specialties. The surgical specialties are the specialties in which an important part of diagnosis and treatment is achieved through major surgical techniques. The internal medicine specialties are the specialties in which the main diagnosis and treatment is never major surgery. In some countries Anesthesiology is classified as a surgical discipline, since it is vital in the surgical process, though anesthesiologists never perform major surgery themselves.
Many specialties are organ-based. Many symptoms and diseases come from a particular organ. Others are based mainly around a set of techniques, such as radiology, which was originally based around X-rays.
The age range of patients seen by any given specialist can be quite variable. Paediatricians handle most complaints and diseases in children that do not require surgery, and there are several subspecialties (formally or informally) in paediatrics that mimic the organ-based specialties in adults. Paediatric surgery may or may not be a separate specialty that handles some kinds of surgical complaints in children.
A further subdivision is the diagnostic versus therapeutic specialties. While the diagnostic process is of great importance in all specialties, some specialists perform mainly or only diagnostic examinations, such as pathology, clinical neurophysiology, and radiology. This line is becoming somewhat blurred with interventional radiology, an evolving field that uses image expertise to perform minimally invasive procedures.
The European Union publishes a list of specialties recognized in the European Union, and by extension, the European Economic Area.[3] Note that there is substantial overlap between some of the specialties and it is likely that for example "Clinical radiology" and "Radiology" refer to a large degree to the same pattern of practice across Europe.
In this table, as in many healthcare arenas, medical specialties are organized into the following groups:
The mean annual salary of a medical specialist is $175,011[6][citation needed] in the US, and $272,000[6]October 2014}} for surgeons.
Unfortunately, as in many professional fields, male physicians earn more than female doctors. Male primary care doctors earned $225,000 a year compared to female primary care doctors, who earned $192,000.Jul 25, 2016
Surgeon Salary and Physician Salary: How Much Doctors Make | </ref> time.com › orthopedic-surgeon-salary
The table below details the average range of salaries for physicians of selected specialties as of July 2010. Also given in the average number of hours worked per week for full-time physicians (numbers are from 2003).
work/week[9]
According to a 2010 study, physician and surgeon median annual income was $166,400.[11]
Specialty training in Australia and New Zealand is overseen by the specialty colleges:
Specialty training in Canada is overseen by the Royal College of Physicians and Surgeons of Canada, the College of Family Physicians of Canada, and by Collège des médecins du Québec.
In Germany these doctors use the term Facharzt.
Specialty training in India is overseen by the Medical Council of India, which is responsible for recognition of post graduate training and by the National Board of Examinations. And education of Ayurveda in overseen by Central Council of Indian Medicine (CCIM), the council conducts u.g and p.g courses all over India, while Central Council of Homoeopathy does the same in the field of Homeopathy.
There are three agencies or organizations in the United States that collectively oversee physician board certification of MD and DO physicians in the United States in the 26 approved medical specialties recognized in the country. These organizations are the American Board of Medical Specialties (ABMS) and the American Medical Association (AMA); the American Osteopathic Association Bureau of Osteopathic Specialists (AOABOS) and the American Osteopathic Association; the American Board of Physician Specialties (ABPS) and the American Association of Physician Specialists (AAPS). Each of these agencies and their associated national medical organization functions as its various specialty academies, colleges and societies.
All boards of certification now require that medical practitioners demonstrate, by examination, continuing mastery of the core knowledge and skills for a chosen specialty. Recertification varies by particular specialty between every seven and every ten years.
In the United States there are hierarchies of medical specialties in the cities of a region. Small towns and cities have primary care, middle sized cities offer secondary care, and metropolitan cities have tertiary care. Income, size of population, population demographics, distance to the doctor, all influence the numbers and kinds of specialists and physicians located in a city.[12]
Economic demand influences the location of particular specialties. For example, more orthopedic surgeons are found in ski areas, obstetricians in the suburbs, and boutique specialties such as hypnosis, plastic surgery, psychiatry are more likely to practice in high income areas. Small populations can usually only support primary care. A large population is needed to support specialists who treat rare diseases. Some specialties need to cooperate and thus locate near each other, such as hematology, oncology, and pathology, or cardiology, thoracic surgery and pulmonology.
A population's income level determines whether sufficient physicians can practice in an area and whether public subsidy is needed to maintain the health of the population. Developing countries and poor areas usually have shortages of physicians and specialties, and those in practice usually locate in larger cities. For some underlying theory regarding physician location, see central place theory.[12]
In the U.S. Army, the term "medical specialist" refers to occupational therapists, physical therapists, dietitians and physician assistants, also known as allied health professionals. Also included in the term "medical specialist", but not in the term "allied health professional" are EMT/combat medics.[citation needed]
In Sweden, a medical license is required before commencing specialty training. Those graduating from Swedish medical schools are first required to do a rotational internship of about 1.5 to 2 years in various specialties before attaining a medical license. The specialist training lasts 5 years.[13]
In the United States, graduates from medical schools can start specialty training directly in the form of residency. The medical license is attained during the course of the residency.
A survey of physicians in the United States came to the result that dermatologists are most satisfied with their choice of specialty followed by radiologists, oncologists, plastic surgeons, and gastroenterologists.[14] In contrast, primary care physicians were the least satisfied, followed by nephrologists, obstetricians/gynecologists, and pulmonologists.[14] Surveys have also revealed high levels of depression among medical students (25 - 30%) as well as among physicians in training (22 - 43%), which for many specialties, continue into regular practice.[15][16]




Hospice - Wikipedia
Hospice care is a type of care and philosophy of care that focuses on the palliation of a chronically ill, terminally ill or seriously ill patient's pain and symptoms, and attending to their emotional and spiritual needs. In Western society, the concept of hospice has been evolving in Europe since the 11th century. Then, and for centuries thereafter in Roman Catholic tradition, hospices were places of hospitality for the sick, wounded, or dying, as well as those for travelers and pilgrims. The modern concept of hospice includes palliative care for the incurably ill given in such institutions as hospitals or nursing homes, but also care provided to those who would rather spend their last months and days of life in their own homes. The first modern hospice care was created by Cicely Saunders in 1967.
In the United States the term is largely defined by the practices of the Medicare system and other health insurance providers, which make hospice care available, either in an inpatient facility or at the patient's home, to patients with a terminal prognosis who are medically certified at hospice onset to have less than six months to live.  According to the NHPCO [National Hospice and Palliative Care Organization] 2012 report on facts and figures of Hospice care, 66.4% received care in their place of residence and 26.1% in a Hospice inpatient facility.[1][2]  In the late 1970s the U.S. government began to view hospice care as a humane care option for the terminally ill. In 1982 Congress initiated the creation of the Medicare Hospice Benefit which became permanent in 1986. In 1993, President Clinton installed hospice as a guaranteed benefit and an accepted component of health care provisions.[3]  Outside the United States, the term hospice tends to be primarily associated with the particular buildings or institutions that specialize in such care (although so-called "hospice at home" services may also be available).[citation needed] Outside the United States such institutions may similarly provide care mostly in an end-of-life setting, but they may also be available for patients with other specific palliative care needs. Hospice care also involves assistance for patients’ families to help them cope with what is happening and provide care and support to keep the patient at home.[4] Although the movement has met with some resistance, hospice has rapidly expanded through the United Kingdom, the United States and elsewhere.
Etymologically, the word "hospice" derives from the Latin hospes, a word which served double duty in referring both to guests and hosts.[5] Historians believe the first hospices originated[where?] in the 11th century, around 1065. The rise of the Crusading movement[where?] in the 1090s saw the incurably ill permitted into places dedicated to treatment by Crusaders.[5][6] In the early 14th century, the order of the Knights Hospitaller of St. John of Jerusalem opened the first hospice in Rhodes, meant to provide refuge for travelers and care for the ill and dying.[7] Hospices flourished in the Middle Ages, but languished as religious orders became dispersed.[5] They were revived in the 17th century in France by the Daughters of Charity of Saint Vincent de Paul.[7] France continued to see development in the hospice field; the hospice of L'Association des Dames du Calvaire, founded by Jeanne Garnier, opened in 1843.[8] Six other hospices followed before 1900.[8]
Meanwhile, hospices also developed in other areas. In the United Kingdom, attention was drawn to the needs of the terminally ill in the middle of the 19th century, with Lancet and the British Medical Journal publishing articles pointing to the need of the impoverished terminally ill for good care and sanitary conditions.[9] Steps were taken to remedy inadequate facilities with the opening of the Friedenheim in London, which by 1892 offered 35 beds to patients dying of tuberculosis.[9] Four more hospices were established in London by 1905.[9] Australia, too, saw active hospice development, with notable hospices including the Home for Incurables in Adelaide (1879), the Home of Peace (1902) and the Anglican House of Peace for the Dying in Sydney (1907).[10] In 1899, New York City saw the opening of St. Rose's Hospice by the Servants for Relief of Incurable Cancer, who soon expanded with six locations in other cities.[8]
The more influential early developers of Hospice included the Irish Religious Sisters of Charity, who opened Our Lady's Hospice in Harold's Cross, Dublin, Ireland in 1879.[8] It became very busy, with as many as 20,000 people—primarily suffering tuberculosis and cancer—coming to the hospice to die between 1845 and 1945.[8] The Sisters of Charity expanded internationally, opening the Sacred Heart Hospice for the Dying in Sydney in 1890, with hospices in Melbourne and New South Wales following in the 1930s.[11] In 1905, they opened St Joseph's Hospice in London.[7][12] There in the 1950s Cicely Saunders developed many of the foundational principles of modern hospice care.[7] Over the years these centers became more prompt and in the 1970s till now this is where they place people to live out their final day (DeSpelder, 2014).
Dame Cicely Saunders was a British registered nurse whose chronic health problems had forced her to pursue a career in medical social work. The relationship she developed with a dying Polish refugee helped solidify her ideas that terminally ill patients needed compassionate care to help address their fears and concerns as well as palliative comfort for physical symptoms.[13] After the refugee's death, Saunders began volunteering at St Luke's Home for the Dying Poor, where a physician told her that she could best influence the treatment of the terminally ill as a physician.[13] Saunders entered medical school while continuing her volunteer work at St. Joseph's. When she achieved her degree in 1957, she took a position there.[13]
Saunders emphasized focusing on the patient rather than the disease and introduced the notion of 'total pain',[14] which included psychological and spiritual as well as the physical aspects. She experimented with a wide range of opioids for controlling physical pain but included also the needs of the patient's family.
She disseminated her philosophy internationally in a series of tours of the United States that began in 1963.[15][16] In 1967, Saunders opened St Christopher's Hospice. Florence Wald, the dean of Yale School of Nursing who had heard Saunders speak in America, spent a month working with Saunders there in 1969 before bringing the principles of modern hospice care back to the United States, establishing Hospice, Inc. in 1971.[7][17] Another early hospice program in the United States, Alive Hospice, was founded in Nashville, Tennessee, on November 14, 1975.[18] By 1977 the National Hospice Organization had been formed, and by 1979, a president, Ann G. Blues, had been elected at the national meeting in Washington DC and principles of hospice care had been addressed.[19] 
At about the same time that Saunders was disseminating her theories and developing her hospice, in 1965, Swiss psychiatrist Elisabeth Kübler Ross also began to consider the social responses to terminal illness, which she found inadequate at the Chicago hospital where her American physician husband was employed.[20] Her 1969 best-seller, On Death and Dying, was influential on how the medical profession responded to the terminally ill,[20] and along with Saunders and other thanatology pioneers helped to focus attention on the types of care available to them.[15]
Hospice has faced resistance springing from various factors, including professional or cultural taboos against open communication about death among physicians or the wider population, discomfort with unfamiliar medical techniques, and professional callousness towards the terminally ill.[21] Nevertheless, the movement has, with national differences in focus and application, spread throughout the world.[22]
In 1984, Dr. Josefina Magno, who had been instrumental in forming the American Academy of Hospice and Palliative Medicine and sat as first executive director of the US National Hospice Organization, founded the International Hospice Institute, which in 1996 became the International Hospice Institute and College and later the International Association for Hospice and Palliative Care (IAHPC).[23][24] The IAHPC, with a board of directors as of 2008 from such diverse countries as Scotland, Argentina, China and Uganda,[25] works from the philosophy that each country should develop a palliative care model based on its own resources and conditions, evaluating hospice experiences in other countries but adapting to their own needs.[26] Dr. Derek Doyle, who was a founding member of IAHPC, told the British Medical Journal in 2003 that through her work the Philippine-born Magno had seen "more than 8000 hospice and palliative services established in more than 100 countries."[24] Standards for Palliative and Hospice Care have been developed in a number of countries around the world, including Australia, Canada, Hungary, Italy, Japan, Moldova, Norway, Poland, Romania, Spain, Switzerland, the United Kingdom and the United States.[27]
In 2006, the United States-based National Hospice and Palliative Care Organization (NHPCO) and the United Kingdom's Help the Hospices jointly commissioned an independent, international study of worldwide palliative care practices. Their survey found that 15% of the world's countries offered widespread palliative care services with integration into major health care institutions, while an additional 35% offered some form of palliative care services, though these might be localized or very limited.[28] As of 2009, there were an estimated 10,000 programs internationally intended to provide palliative care, although the term hospice is not always employed to describe such services.[29]
In hospice care the main guardians are the family care giver and a hospice nurse who makes periodic stops.  Hospice can be administered in a nursing home, hospice building, or sometimes a hospital; however, it is most commonly practiced in the home.[30] In order to be considered for hospice care, one has to be terminally ill or expected to die within six months.
A hospice was opened in 1980 in Harare (Salisbury, at the time), Zimbabwe, the first in Sub-Saharan Africa.[31] In spite of skepticism in the medical community,[21] the hospice movement spread, and in 1987 the Hospice Palliative Care Association of South Africa formed.[32] In 1990, Nairobi Hospice opened in Nairobi, Kenya.[32] As of 2006, Kenya, South Africa and Uganda were among the 35 countries of the world offering widespread, well-integrated palliative care.[32] Programs there are based on the United Kingdom model, but focus less on in-patient care, emphasizing home-based assistance.[33]
Since the foundation of hospice in Kenya in the early 1990s, palliative care has spread through the country. Representatives of Nairobi Hospice sit on the committee to develop a Health Sector Strategic Plan for the Ministry of Health and are working with the Ministry of Health to help develop specific palliative care guidelines for cervical cancer.[32] The Government of Kenya has supported hospice by donating land to Nairobi Hospice and providing funding to several of its nurses.[32]
In South Africa, hospice services are widespread, focusing on diverse communities (including orphans and homeless) and offered in diverse settings (including in-patient, day care and home care).[32] Over half of hospice patients in South Africa in the 2003–2004 year were diagnosed with AIDS, with the majority of the remaining having been diagnosed with cancer.[32] Palliative care in South Africa is supported by the Hospice Palliative Care Association of South Africa and by national programmes partly funded by the President's Emergency Plan for AIDS Relief.[32]
Hospice Africa Uganda (HAU) founded by Anne Merriman, began offering services in 1993 in a two-bedroom house loaned for the purpose by Nsambya Hospital.[32] HAU has since expanded to a base of operations at Makindye, Kampala, with hospice services also offered at roadside clinics by Mobile Hospice Mbarara since January 1998. That same year saw the opening of Little Hospice Hoima in June. Hospice care in Uganda is supported by community volunteers and professionals, as Makerere University offers a distance diploma in palliative care.[34] The government of Uganda has a strategic plan for palliative care and permits nurses and clinical officers from HAU to prescribe morphine.
Canadian physician Balfour Mount, who first coined the term "palliative care", was a pioneer in the Canadian hospice movement, which focuses primarily on palliative care in a hospital setting.[35][36] Having read the work of Kubler-Ross, Mount set out to study the experiences of the terminally ill at Royal Victoria Hospital, Montreal; the "abysmal inadequacy", as he termed it, that he found prompted him to spend a week with Saunders at St. Christopher's.[37] Inspired, Mount decided to adapt Saunders' model for Canada. Given differences in medical funding in Canada, he determined that a hospital-based approach would be more affordable, creating a specialized ward at Royal Victoria in January, 1975.[36][37] For Canada, whose official languages include English and French, Mount felt the term "palliative care ward" would be more appropriate, as the word hospice was already used in France to refer to nursing homes.[36][37] Hundreds of palliative care programs followed throughout Canada through the 1970s and 1980s.[38]
However, as of 2004, according to the Canadian Hospice Palliative Care Association (CHPCA), hospice palliative care was only available to 5-15% of Canadians, with available services having decreased with reduced government funding.[39] At that time, Canadians were increasingly expressing a desire to die at home, but only two of Canada's ten provinces were provided medication cost coverage for care provided at home.[39] Only four of the ten identified palliative care as a core health service.[39] At that time, palliative care was not widely taught at nursing schools or universally certified at medical colleges; there were only 175 specialized palliative care physicians in all of Canada.[39]
Hospice in the United States has grown from a volunteer-led movement to improve care for people dying alone, isolated, or in hospitals, to a significant part of the health care system. In 2010, an estimated 1.581 million patients received services from hospice. Hospice is the only Medicare benefit that includes pharmaceuticals, medical equipment, twenty-four-hour/seven-day-a-week access to care, and support for loved ones following a death. Hospice care is also covered by Medicaid and most private insurance plans.  Most hospice care is delivered at home. Hospice care is also available to people in home-like hospice residences, nursing homes, assisted living facilities, veterans' facilities, hospitals, and prisons. The first hospice in the US was the Connecticut Hospice, located in Branford, Connecticut.[40]
The first United States hospital-based palliative care programs began in the late 1980s by committed volunteers across the country.  The first hospital-based palliative care consult service developed in the United States was the Wayne State University School of Medicine in 1985 at Detroit Receiving Hospital.[41] The first United States-based palliative medicine and hospice service program was started in 1987 by Declan Walsh, MD at the Cleveland Clinic Cancer Center in Cleveland, Ohio.[42] The program evolved into The Harry R. Horvitz Center for Palliative Medicine which was designated as a World Health Organization international demonstration project and accredited by the European Society of Medical Oncology as an Integrated Center of Oncology and Palliative Care.   Other programs followed: most notable the Palliative Care Program at the Medical College of Wisconsin (1993); Pain and Palliative Care Service, Memorial Sloan-Kettering Cancer Center (1996); and The Lilian and Benjamin Hertzberg Palliative Care Institute, Mount Sinai School of Medicine (1997). By 1995, hospices were a $2.8 billion industry in the United States, with $1.9 billion from Medicare alone funding patients in 1,857 hospice programs with Medicare certification.[43] In that year, 72% of hospice providers were non-profit.[43] By 1998, there were 3,200 hospices either in operation or under development throughout the United States and Puerto Rico, according to the NHPCO.[43] According to 2007's Last Rights: Rescuing the End of Life from the Medical System, hospice sites are expanding at a national rate of about 3.5% per year.[44] As of 2008, approximately 900,000 people in the United States were using hospice every year,[45] with more than one-third of dying Americans using the service.[46]
Hospice plays an important role in reducing Medicare costs. Over the past 20–30 years 27-30% of Medicare's total budget was spent on individuals in their last year of life.[47] Hospice care reduces ER visits and inpatient hospitalization which are costly and emotionally traumatizing for both the patient and their loved ones.
Hospice care may involve not treating illnesses. Patients and family members should understand the care or lack of care that is planned. If one has pneumonia, it may (or may not) be treated.[48] If not treated, this might actually increase suffering.  If the illness of the patient is not related to the terminal illness covered under the clinical determination of eligibility, the patient may seek standard treatment to address the cause of the suffering if they request such treatment.  Any Medicare services received by a hospice patient are covered under original Medicare including those hospice patients who have a Medicare Advantage plan and also services provided by a primary care physician for unrelated hospice treatments.[49]
The hospice movement has grown dramatically in the United Kingdom since Dame Cicely Saunders opened St Christopher's Hospice in 1967, widely considered the first modern hospice. According to the UK's Help the Hospices, in 2011 UK hospice services consisted of 220 inpatient units for adults with 3,175 beds, 42 inpatient units for children with 334 beds, 288 home care services, 127 hospice at home services, 272 day care services, and 343 hospital support services.[50] These services together helped over 250,000 patients in 2003 and 2004. Funding varies from 100% funding by the National Health Service to almost 100% funding by charities, but the service is always free to patients. The UK's palliative care has been ranked as the best in the world "due to comprehensive national policies, the extensive integration of palliative care into the National Health Service, a strong hospice movement, and deep community engagement on the issue."[51]
As of 2006 about 4% of all deaths in England and Wales occurred in a hospice setting (about 20,000 patients);[52] a further number of patients spent time in a hospice, or were helped by hospice-based support services, but died elsewhere.
Hospices also provide volunteering opportunities for over 100,000 people in the UK, whose economic value to the hospice movement has been estimated at over £112 million.[53]
Hospice Care in Australia predates the opening of St Christophers in London by 79 years. The establishment by the Irish Sisters of Charity of hospices in Sydney (1889) and in Melbourne (1938). The first hospice in New Zealand opened in 1979.[54]
Hospice care entered Poland in the middle of the 1970s.[55] Japan opened its first hospice in 1981, officially hosting 160 by July 2006.[56] The first hospice unit in Israel was opened in 1983.[57] India's first hospice, Shanti Avedna Ashram, opened in Bombay in 1986.[58][59][60][61] First hospice in the Nordics has been operating in Tampere, Finland since 1988. [62] The first modern free-standing hospice in China opened in Shanghai in 1988.[63] The first hospice unit in Taiwan, where the term for hospice translates "peaceful care", was opened in 1990.[21][64] The first free-standing hospice in Hong Kong, where the term for hospice translates "well-ending service", opened in 1992.[21][65] The first hospice in Russia was established in 1997.[66]
Since 2006 the World Hospice and Palliative Care Day is organised by a committee of the Worldwide Palliative Care Alliance, a network of hospice and palliative care national and regional organisations that support the development of hospice and palliative care worldwide. The event takes place on the second Saturday of October every year.[67]
 Media related to Hospices at Wikimedia Commons



Medical guideline - Wikipedia
A medical guideline (also called a clinical guideline or clinical practice line) is a document with the aim of guiding decisions and criteria regarding diagnosis, management, and treatment in specific areas of healthcare. Such documents have been in use for thousands of years during the entire history of medicine. However, in contrast to previous approaches, which were often based on tradition or authority, modern medical guidelines are based on an examination of current evidence within the paradigm of evidence-based medicine.[1][2][3] They usually include summarized consensus statements on best practice in healthcare. A healthcare provider is obliged to know the medical guidelines of his or her profession, and has to decide whether to follow the recommendations of a guideline for an individual treatment.[4]
Modern clinical guidelines identify, summarize and evaluate the highest quality evidence and most current data about prevention, diagnosis, prognosis, therapy including dosage of medications, risk/benefit and cost-effectiveness. Then they define the most important questions related to clinical practice and identify all possible decision options and their outcomes. Some guidelines contain decision or computation algorithms to be followed. Thus, they integrate the identified decision points and respective courses of action with the clinical judgment and experience of practitioners. Many guidelines place the treatment alternatives into classes to help providers in deciding which treatment to use.
Additional objectives of clinical guidelines are to standardize medical care, to raise quality of care, to reduce several kinds of risk (to the patient, to the healthcare provider, to medical insurers and health plans) and to achieve the best balance between cost and medical parameters such as effectiveness, specificity, sensitivity, resolutiveness, etc. It has been demonstrated repeatedly that the use of guidelines by healthcare providers such as hospitals is an effective way of achieving the objectives listed above, although they are not the only ones.
Guidelines are usually produced at national or international levels by medical associations or governmental bodies, such as the United States Agency for Healthcare Research and Quality. Local healthcare providers may produce their own set of guidelines or adapt them from existing top-level guidelines.
Special computer software packages known as guideline execution engines have been developed to facilitate the use of medical guidelines in concert with an electronic medical record system. 
The Guideline Interchange Format (GLIF) is a computer representation format for clinical guidelines that can be used with such engines.[5]
The USA and other countries maintain medical guideline clearinghouses. In the USA, the National Guideline Clearinghouse maintains a catalog of high-quality guidelines published by various health and medical associations. In the United Kingdom, clinical practice guidelines are published primarily by the National Institute for Health and Care Excellence (NICE). In The Netherlands, two bodies—the Institute for Healthcare Improvement (CBO) and College of General Practitioners (NHG)—have published specialist and primary care guidelines, respectively. In Germany, the German Agency for Quality in Medicine (ÄZQ) coordinates a national program for disease management guidelines. All these organisations are now members of the Guidelines International Network (G-I-N), an international network of organisations and individuals involved in clinical practice guidelines.
Checklists have been used  in medical practice to attempt to ensure that clinical practice guidelines are followed. An example is the Surgical Safety Checklist developed for the World Health Organization by Dr. Atul Gawande.[6] According to a meta-analysis after introduction of the checklist mortality dropped by 23% and all complications by 40%, but further high-quality studies are required to make the meta-analysis more robust.[7]  In the UK, a study on the implementation of a checklist for provision of medical care to elderly patients admitting to hospital found that the checklist highlighted limitations with frailty assessment in acute care and motivated teams to review routine practices, but that work is needed to understand whether and how checklists can be embedded in complex multidisciplinary care.[8]
Guidelines may lose their clinical relevance as they age and newer research emerges.[9] Even 20% of strong recommendations, especially when based on opinion rather than trials, from practice guidelines may be retracted.[10]
The New York Times reported in 2004 that some simple clinical practice guidelines are not routinely followed to the extent they might be.[11] It has been found that providing a nurse or other medical assistant with a checklist of recommended procedures can result in the attending physician being reminded in a timely manner regarding procedures that might have been overlooked.
Guidelines may have both methodological problems and conflict of interest.[12] As such, the quality of guidelines may vary substantially, especially for guidelines that are published on-line and have not had to follow methodological reporting standards often required by reputable clearinghouses.[13]
Guidelines may make recommendations that are stronger than the supporting evidence.[14]
In response to many of these problems with traditional guidelines, the BMJ created a new series of trustworthy guidelines focused on the most pressing medical issues called BMJ Rapid Recommendations.[15]



Cancer immunotherapy - Wikipedia
Cancer immunotherapy (sometimes called immuno-oncology) is the artificial stimulation of the immune system to treat cancer, improving on the system's natural ability to fight cancer. It is an application of the fundamental research of cancer immunology and a growing subspecialty of oncology. It exploits the fact that cancer cells often have molecules on their surface that can be detected by the immune system, known as tumor antigens; they are often proteins or other macromolecules (e.g. carbohydrates). Immunotherapies can be categorized as active, passive or hybrid (active and passive). Active immunotherapy directs the immune system to attack tumor cells by targeting tumor antigens. Passive immunotherapies enhance existing anti-tumor responses and include the use of monoclonal antibodies, lymphocytes and cytokines.
Among these, multiple antibody therapies are approved in various jurisdictions to treat a wide range of cancers.[1] Antibodies are proteins produced by the immune system that bind to a target antigen on the cell surface. The immune system normally uses them to fight pathogens. Each antibody is specific to one or a few proteins. Those that bind to tumor antigens treat cancer. Cell surface receptors are common targets for antibody therapies and include CD20, CD274 and CD279. Once bound to a cancer antigen, antibodies can induce antibody-dependent cell-mediated cytotoxicity, activate the complement system, or prevent a receptor from interacting with its ligand, all of which can lead to cell death. Approved antibodies include alemtuzumab, ipilimumab, nivolumab, ofatumumab and rituximab.
Active cellular therapies usually involve the removal of immune cells from the blood or from a tumor. Those specific for the tumor are grown in culture and returned to the patient where they attack the tumor; alternatively, immune cells can be genetically engineered to express a tumor-specific receptor, cultured and returned to the patient. Cell types that can be used in this way are natural killer (NK) cells, lymphokine-activated killer cells, cytotoxic T cells and dendritic cells.
Dendritic cell therapy provokes anti-tumor responses by causing dendritic cells to present tumor antigens to lymphocytes, which activates them, priming them to kill other cells that present the antigen. Dendritic cells are antigen presenting cells (APCs) in the mammalian immune system.[2] In cancer treatment they aid cancer antigen targeting.[3] The only approved cellular cancer therapy based on dendritic cells is sipuleucel-T.
One method of inducing dendritic cells to present tumor antigens is by vaccination with autologous tumor lysates[4] or short peptides (small parts of protein that correspond to the protein antigens on cancer cells). These peptides are often given in combination with adjuvants (highly immunogenic substances) to increase the immune and anti-tumor responses. Other adjuvants include proteins or other chemicals that attract and/or activate dendritic cells, such as granulocyte macrophage colony-stimulating factor (GM-CSF).
Dendritic cells can also be activated in vivo by making tumor cells express GM-CSF. This can be achieved by either genetically engineering tumor cells to produce GM-CSF or by infecting tumor cells with an oncolytic virus that expresses GM-CSF.
Another strategy is to remove dendritic cells from the blood of a patient and activate them outside the body. The dendritic cells are activated in the presence of tumor antigens, which may be a single tumor-specific peptide/protein or a tumor cell lysate (a solution of broken down tumor cells). These cells (with optional adjuvants) are infused and provoke an immune response.
Dendritic cell therapies include the use of antibodies that bind to receptors on the surface of dendritic cells. Antigens can be added to the antibody and can induce the dendritic cells to mature and provide immunity to the tumor. Dendritic cell receptors such as TLR3, TLR7, TLR8 or CD40 have been used as antibody targets.[3]
Sipuleucel-T (Provenge) was approved for treatment of asymptomatic or minimally symptomatic metastatic castration-resistant prostate cancer in 2010. The treatment consists of removal of antigen presenting cells from blood by leukapheresis and growing them with the fusion protein PA2024 made from GM-CSF and prostate-specific prostatic acid phosphatase (PAP) and reinfused. This process is repeated three times.[5][6][7][8]
The premise of CAR-T immunotherapy is to modify T cells to recognize cancer cells in order to more effectively target and destroy them. Scientists harvest T cells from people, genetically alter them to add a chimeric antigen receptor (CAR) that specifically recognizes cancer cells, then infuse the resulting CAR-T cells into patients to attack their tumors.
Tisagenlecleucel (Kymriah), a chimeric antigen receptor (CAR-T) therapy, was approved by FDA in 2017 to treat acute lymphoblastic leukemia (ALL).[9] This treatment removes CD19 positive cells (B-cells) from the body (including the diseased cells, but also normal antibody producing cells).
Axicabtagene ciloleucel (Yescarta) is another CAR-T therapeutic, approved in 2017 for treatment of diffuse large B-cell lymphoma (DLBCL).[10]
Antibodies are a key component of the adaptive immune response, playing a central role in both recognizing foreign antigens and stimulating an immune response. Antibodies are Y-shaped proteins produced by some B cells and are composed of two regions: an antigen-binding fragment (Fab), which binds to antigens, and a Fragment crystallizable (Fc) region, which interacts with so-called Fc receptors that are expressed on the surface of different immune cell types including macrophages, neutrophils and NK cells. Many immunotherapeutic regimens involve antibodies. Monoclonal antibody technology engineers and generates antibodies against specific antigens, such as those present on tumor surfaces. These antibodies that are specific to the antigens of the tumor, can then be injected into a tumor
Two types are used in cancer treatments:[11]
Fc’s ability to bind Fc receptors is important because it allows antibodies to activate the immune system. Fc regions are varied: they exist in numerous subtypes and can be further modified, for example with the addition of sugars in a process called glycosylation. Changes in the Fc region can alter an antibody’s ability to engage Fc receptors and, by extension, will determine the type of immune response that the antibody triggers.[13] Many cancer immunotherapy drugs, including PD-1 and PD-L1 inhibitors, are antibodies. For example, immune checkpoint blockers targeting PD-1 are antibodies designed to bind PD-1 expressed by T cells and reactivate these cells to eliminate tumors.[14] Anti-PD-1 drugs contain not only an Fab region that binds PD-1 but also an Fc region. Experimental work indicates that the Fc portion of cancer immunotherapy drugs can affect the outcome of treatment. For example, anti-PD-1 drugs with Fc regions that bind inhibitory Fc receptors can have decreased therapeutic efficacy.[15] Imaging studies have further shown that the Fc region of anti-PD-1 drugs can bind Fc receptors expressed by tumor-associated macrophages. This process removes the drugs from their intended targets (i.e. PD-1 molecules expressed on the surface of T cells) and limits therapeutic efficacy.[16] Furthermore, antibodies targeting the co-stimulatory protein CD40 require engagement with selective Fc receptors for optimal therapeutic efficacy.[17] Together, these studies underscore the importance of Fc status in antibody-based immune checkpoint targeting strategies.
Antibodies are also referred to as murine, chimeric, humanized and human. Murine antibodies are from a different species  and carry a risk of immune reaction. Chimeric antibodies attempt to reduce murine antibodies' immunogenicity by replacing part of the antibody with the corresponding human counterpart, known as the constant region. Humanized antibodies are almost completely human; only the complementarity determining regions of the variable regions are derived from murine sources. Human antibodies have been produced using unmodified human DNA.[12]
Antibody-dependent cell-mediated cytotoxicity (ADCC) requires antibodies to bind to target cell surfaces. Antibodies are formed of a binding region (Fab) and the Fc region that can be detected by immune system cells via their Fc surface receptors. Fc receptors are found on many immune system cells, including NK cells. When NK cells encounter antibody-coated cells, the latter's Fc regions interact with their Fc receptors, releasing perforin and granzyme B to kill the tumor cell. Examples include Rituximab, Ofatumumab and Alemtuzumab. Antibodies under development have altered Fc regions that have higher affinity for a specific type of Fc receptor, FcγRIIIA, which can dramatically increase effectiveness.[18][19]
The complement system includes blood proteins that can cause cell death after an antibody binds to the cell surface (the classical complement pathway, among the ways of complement activation). Generally the system deals with foreign pathogens, but can be activated with therapeutic antibodies in cancer. The system can be triggered if the antibody is chimeric, humanized or human; as long as it contains the IgG1 Fc region. Complement can lead to cell death by activation of the membrane attack complex, known as complement-dependent cytotoxicity; enhancement of antibody-dependent cell-mediated cytotoxicity; and CR3-dependent cellular cytotoxicity. Complement-dependent cytotoxicity occurs when antibodies bind to the cancer cell surface, the C1 complex binds to these antibodies and subsequently protein pores are formed in the cancer cell membrane.[20]
Alemtuzumab (Campath-1H) is an anti-CD52 humanized IgG1 monoclonal antibody indicated for the treatment of fludarabine-refractory chronic lymphocytic leukemia (CLL), cutaneous T-cell lymphoma, peripheral T-cell lymphoma and T-cell prolymphocytic leukemia. CD52 is found on >95% of peripheral blood lymphocytes (both T-cells and B-cells) and monocytes, but its function in lymphocytes is unknown. It binds to CD52 and initiates its cytotoxic effect by complement fixation and ADCC mechanisms. Due to the antibody target (cells of the immune system) common complications of alemtuzumab therapy are infection, toxicity and myelosuppression.[32][33][34]
Durvalumab
Durvalumab (Imfinzi) is a human immunoglobulin G1 kappa (IgG1κ) monoclonal antibody that blocks the interaction of programmed cell death ligand 1 (PD-L1) with the PD-1 and CD80 (B7.1) molecules. Durvalumab is approved for the treatment of patients with locally advanced or metastatic urothelial carcinoma who:
Ipilimumab (Yervoy) is a human IgG1 antibody that binds the surface protein CTLA4. In normal physiology T-cells are activated by two signals: the T-cell receptor binding to an antigen-MHC complex and T-cell surface receptor CD28 binding to CD80 or CD86 proteins. CTLA4 binds to CD80 or CD86, preventing the binding of CD28 to these surface proteins and therefore negatively regulates the activation of T-cells.[35][36][37][38]
Active cytotoxic T-cells are required for the immune system to attack melanoma cells. Normally inhibited active melanoma-specific cytotoxic T-cells can produce an effective anti-tumor response. Ipilumumab can cause a shift in the ratio of regulatory T-cells to cytotoxic T-cells to increase the anti-tumor response. Regulatory T-cells inhibit other T-cells, which may benefit the tumor.[35][36][37][38]
Ofatumumab is a second generation human IgG1 antibody that binds to CD20. It is used in the treatment of chronic lymphocytic leukemia (CLL) because the cancerous cells of CLL are usually CD20-expressing B-cells. Unlike rituximab, which binds to a large loop of the CD20 protein, ofatumumab binds to a separate, small loop. This may explain their different characteristics. Compared to rituximab, ofatumumab induces complement-dependent cytotoxicity at a lower dose with less immunogenicity.[39][40]
As of 2017, pembrolizumab, which blocks PD-1, programmed cell death protein 1, has been used via intravenous infusion to treat inoperable or metastatic melanoma, metastatic non-small cell lung cancer (NSCLC) in certain situations, as a second-line treatment for head and neck squamous cell carcinoma (HNSCC), after platinum-based chemotherapy, and for the treatment of adult and pediatric patients with refractory classic Hodgkin's lymphoma (cHL).[41][42]
Rituximab is a chimeric monoclonal IgG1 antibody specific for CD20, developed from its parent antibody Ibritumomab. As with ibritumomab, rituximab targets CD20, making it effective in treating certain B-cell malignancies. These include aggressive and indolent lymphomas such as diffuse large B-cell lymphoma and follicular lymphoma and leukemias such as B-cell chronic lymphocytic leukemia. Although the function of CD20 is relatively unknown, CD20 may be a calcium channel involved in B-cell activation. The antibody's mode of action is primarily through the induction of ADCC and complement-mediated cytotoxicity. Other mechanisms include apoptosis[clarification needed] and cellular growth arrest. Rituximab also increases the sensitivity of cancerous B-cells to chemotherapy.[43][44][44][45][46][47]
Cytokines are proteins produced by many types of cells present within a tumor. They can modulate immune responses. The tumor often employs them to allow it to grow and reduce the immune response. These immune-modulating effects allow them to be used as drugs to provoke an immune response. Two commonly used cytokines are interferons and interleukins.[48]
Interleukin-2 and interferon-α are cytokines, proteins that regulate and coordinate the behavior of the immune system. They have the ability to enhance anti-tumor activity and thus can be used as passive cancer treatments. Interferon-α is used in the treatment of hairy-cell leukaemia, AIDS-related Kaposi's sarcoma, follicular lymphoma, chronic myeloid leukaemia and malignant melanoma. Interleukin-2 is used in the treatment of malignant melanoma and renal cell carcinoma.
Interferons are produced by the immune system. They are usually involved in anti-viral response, but also have use for cancer. They fall in three groups: type I (IFNα and IFNβ), type II (IFNγ) and type III (IFNλ). IFNα has been approved for use in hairy-cell leukaemia, AIDS-related Kaposi's sarcoma, follicular lymphoma, chronic myeloid leukaemia and melanoma. Type I and II IFNs have been researched extensively and although both types promote anti-tumor immune system effects, only type I IFNs have been shown to be clinically effective. IFNλ shows promise for its anti-tumor effects in animal models.[49][50]
Unlike type I IFNs, Interferon gamma is not approved yet for the treatment of any cancer.However, improved survival was observed when Interferon gamma was administrated to patients with bladder carcinoma and melanoma cancers. The most promising result was achieved in patients with stage 2 and 3 of ovarian carcinoma.The in vitro study of IFN-gamma in cancer cells is more extensive and results indicate anti-proliferative activity of IFN-gamma leading to the growth inhibition or cell death, generally induced by apoptosis but sometimes by autophagy.[51]
Interleukins have an array of immune system effects. Interleukin-2 is used in the treatment of malignant melanoma and renal cell carcinoma. In normal physiology it promotes both effector T cells and T-regulatory cells, but its exact mechanism of action is unknown.[48][52]
Combining various immunotherapies such as PD1 and CTLA4 inhibitors can enhance anti-tumor response leading to durable responses.[53][54]
Combining ablation therapy of tumors with immunotherapy enhances the immunostimulating response and has synergistic effects for curative metastatic cancer treatment.[55]
Combining checkpoint immunotherapies with pharmaceutical agents has the potential to improve response, and such combination therapies are a highly investigated area of clinical investigation.[56] Immunostimulatory drugs such as CSF-1R inhibitors and TLR agonists have been particularly effective in this setting.[57][58]
Japan's Ministry of Health, Labour and Welfare approved the use of polysaccharide-K extracted from the mushroom, Coriolus versicolor, in the 1980s, to stimulate the immune systems of patients undergoing chemotherapy.  It is a dietary supplement in the US and other jurisdictions.[59]
Adoptive T cell therapy is a form of passive immunization by the transfusion of T-cells (adoptive cell transfer). They are found in blood and tissue and usually activate when they find foreign pathogens. Specifically they activate when the T-cell's surface receptors encounter cells that display parts of foreign proteins on their surface antigens. These can be either infected cells, or antigen presenting cells (APCs). They are found in normal tissue and in tumor tissue, where they are known as tumor infiltrating lymphocytes (TILs). They are activated by the presence of APCs such as dendritic cells that present tumor antigens. Although these cells can attack the tumor, the environment within the tumor is highly immunosuppressive, preventing immune-mediated tumour death.[60]
Multiple ways of producing and obtaining tumour targeted T-cells have been developed. T-cells specific to a tumor antigen can be removed from a tumor sample (TILs) or filtered from blood. Subsequent activation and culturing is performed ex vivo, with the results reinfused. Activation can take place through gene therapy, or by exposing the T cells to tumor antigens.
As of 2014, multiple ACT clinical trials were underway.[61][62][63][64][65] Importantly, one study from 2018 showed that clinical responses can be obtained in patients with metastatic melanoma resistant to multiple previous immunotherapies.[66]
The first 2 adoptive T-cell therapies, tisagenlecleucel and axicabtagene ciloleucel, were approved by the FDA in 2017.[67][10]
Another approach is adoptive transfer of haploidentical γδ T cells or  NK cells from a healthy donor. The major advantage of this approach is that these cells do not cause GVHD. The disadvantage  is frequently impaired function of the transferred cells.[68]
Many tumor cells overexpress CD47 to escape immunosurveilance of host immune system. CD47 binds to its receptor signal regulatory protein alpha (SIRPα) and downregulate phagocytosis of tumor cell.[69] Therefore, anti-CD47 therapy aims to restore clearance of tumor cells. Additionally, growing evidence supports the employment of tumor antigen-specific T cell response in response to anti-CD47 therapy.[70][71] A number of therapeutics is being developed, including anti-CD47 antibodies, engineered decoy receptors, anti-SIRPα antibodies and bispecific agents.[70] As of 2017, wide range of solid and hematologic malignancies were being clinically tested.[70][72]
Carbohydrate antigens on the surface of cells can be used as targets for immunotherapy. GD2 is a ganglioside found on the surface of many types of cancer cell including neuroblastoma, retinoblastoma, melanoma, small cell lung cancer, brain tumors, osteosarcoma, rhabdomyosarcoma, Ewing’s sarcoma, liposarcoma, fibrosarcoma, leiomyosarcoma and other soft tissue sarcomas. It is not usually expressed on the surface of normal tissues, making it a good target for immunotherapy. As of 2014, clinical trials were underway.[73]
Immune checkpoints affect immune system function. Immune checkpoints can be stimulatory or inhibitory. Tumors can use these checkpoints to protect themselves from immune system attacks. Currently approved checkpoint therapies block inhibitory checkpoint receptors. Blockade of negative feedback signaling to immune cells thus results in an enhanced immune response against tumors.[74]
One ligand-receptor interaction under investigation is the interaction between the transmembrane programmed cell death 1 protein (PDCD1, PD-1; also known as CD279) and its ligand, PD-1 ligand 1 (PD-L1, CD274). PD-L1 on the cell surface binds to PD1 on an immune cell surface, which inhibits immune cell activity. Among PD-L1 functions is a key regulatory role on T cell activities. It appears that (cancer-mediated) upregulation of PD-L1 on the cell surface may inhibit T cells that might otherwise attack. PD-L1 on cancer cells also inhibits FAS- and interferon-dependent apoptosis, protecting cells from cytotoxic molecules produced by T cells. Antibodies that bind to either PD-1 or PD-L1 and therefore block the interaction may allow the T-cells to attack the tumor.[75]
The first checkpoint antibody approved by the FDA was ipilimumab, approved in 2011 for treatment of melanoma.[76]  It blocks the immune checkpoint molecule CTLA-4. Clinical trials have also shown some benefits of anti-CTLA-4 therapy on lung cancer or pancreatic cancer, specifically in combination with other drugs.[77][78] In on-going trials the combination of CTLA-4 blockade with PD-1 or PD-L1 inhibitors is tested on different types of cancer.[79]
However, patients treated with check-point blockade (specifically CTLA-4 blocking antibodies), or a combination of check-point blocking antibodies, are at high risk of suffering from immune-related adverse events such as dermatologic, gastrointestinal, endocrine, or hepatic autoimmune reactions.[80] These are most likely due to the breadth of the induced T-cell activation when anti-CTLA-4 antibodies are administered by injection in the blood stream.
Using a mouse model of bladder cancer, researchers have found that a local injection of a low dose anti-CTLA-4 in the tumour area had the same tumour inhibiting capacity as when the antibody was delivered in the blood.[81] At the same time the levels of circulating antibodies were lower, suggesting that local administration of the anti-CTLA-4 therapy might result in fewer adverse events.[81]
Initial clinical trial results with IgG4 PD1 antibody Nivolumab were published in 2010.[74] It was approved in 2014. Nivolumab is approved to treat melanoma, lung cancer, kidney cancer, bladder cancer, head and neck cancer, and Hodgkin's lymphoma.[82] A 2016 clinical trial for non-small cell lung cancer failed to meet its primary endpoint for treatment in the first line setting, but is FDA approved in subsequent lines of therapy.[83]
Pembrolizumab is another PD1 inhibitor that was approved by the FDA in 2014. 
Keytruda (Pembrolizumab) is approved to treat melanoma and lung cancer.[82]
Antibody BGB-A317 is a PD-1 inhibitor (designed to not bind Fc gamma receptor I) in early clinical trials.[84]
In May 2016, PD-L1 inhibitor atezolizumab[85] was approved  for treating bladder cancer.
Anti-PD-L1 antibodies currently in development include  avelumab[86] and durvalumab,[87] in addition to an affimer biotherapeutic.[88]
Other modes of enhancing [adoptive] immuno-therapy include targeting so-called intrinsic checkpoint blockades e.g. CISH. A number of cancer patients do not respond to immune checkpoint blockade. Response rate may be improved by combining immune checkpoint blockade with additional rationally selected anticancer therapies (out of which some may stimulate T cell infiltration into tumors). For example, targeted therapies such, radiotherapy, vasculature targeting agents, and immunogenic chemotherapy[89] can improve immune checkpoint blockade response in animal models of cancer.
An oncolytic virus is a virus that preferentially infects and kills cancer cells. As the infected cancer cells are destroyed by oncolysis, they release new infectious virus particles or virions to help destroy the remaining tumour. Oncolytic viruses are thought not only to cause direct destruction of the tumour cells, but also to stimulate host anti-tumour immune responses for long-term immunotherapy.[90][91][92]
The potential of viruses as anti-cancer agents was first realized in the early twentieth century, although coordinated research efforts did not begin until the 1960s. A number of viruses including adenovirus, reovirus, measles, herpes simplex, Newcastle disease virus and vaccinia have now been clinically tested as oncolytic agents. T-Vec is the first FDA-approved oncolytic virus for the treatment of melanoma.  A number of other oncolytic viruses are in Phase II-III development.[citation needed]
Certain compounds found in mushrooms, primarily polysaccharides, can up-regulate the immune system and may have anti-cancer properties. For example, beta-glucans such as lentinan have been shown in laboratory studies to stimulate macrophage, NK cells, T cells and immune system cytokines and have been investigated in clinical trials as immunologic adjuvants.[93]
Many tumors express mutations. These mutations potentially create new targetable antigens (neoantigens) for use in T cell immunotherapy. The presence of CD8+ T cells in cancer lesions, as identified using RNA sequencing data, is higher in tumors with a high mutational burden. The level of transcripts associated with cytolytic activity of natural killer cells and T cells positively correlates with mutational load in many human tumors. In non–small cell lung cancer patients treated with lambrolizumab, mutational load shows a strong correlation with clinical response. In melanoma patients treated with ipilimumab, long-term benefit is also associated with a higher mutational load, although less significantly. The predicted MHC binding neoantigens in patients with a long-term clinical benefit were enriched for a series of tetrapeptide motifs that were not found in tumors of patients with no or minimal clinical benefit.[94] However, human neoantigens identified in other studies do not show the bias toward tetrapeptide signatures.[95]



Laser - Wikipedia

A laser is a device that emits light through a process of optical amplification based on the stimulated emission of electromagnetic radiation. The term "laser" originated as an acronym for "light amplification by stimulated emission of radiation".[1][2]  The first laser was built in 1960 by Theodore H. Maiman at Hughes Research Laboratories, based on theoretical work by Charles Hard Townes and Arthur Leonard Schawlow.
A laser differs from other sources of light in that it emits light coherently, spatially and temporally. Spatial coherence allows a laser to be focused to a tight spot, enabling applications such as laser cutting and lithography. Spatial coherence also allows a laser beam to stay narrow over great distances (collimation), enabling applications such as laser pointers. Lasers can also have high temporal coherence, which allows them to emit light with a very narrow spectrum, i.e., they can emit a single color of light. Temporal coherence can be used to produce pulses of light as short as a femtosecond.
Among their many applications, lasers are used in optical disk drives, laser printers, and barcode scanners; DNA sequencing instruments, fiber-optic and free-space optical communication; laser surgery and skin treatments; cutting and welding materials; military and law enforcement devices for marking targets and measuring range and speed; and laser lighting displays in entertainment.
Lasers are distinguished from other light sources by their coherence. Spatial coherence is typically expressed through the output being a narrow beam, which is diffraction-limited. Laser beams can be focused to very tiny spots, achieving a very high irradiance, or they can have very low divergence in order to concentrate their power at a great distance.
Temporal (or longitudinal) coherence implies a polarized wave at a single frequency whose phase is correlated over a relatively great distance (the coherence length) along the beam.[4] A beam produced by a thermal or other incoherent light source has an instantaneous amplitude and phase that vary randomly with respect to time and position,  thus having a short coherence length.
Lasers are characterized according to their wavelength in a vacuum.  Most "single wavelength" lasers actually produce radiation in several modes having slightly differing frequencies (wavelengths), often not in a single polarization. Although temporal coherence implies monochromaticity, there are lasers that emit a broad spectrum of light or emit different wavelengths of light simultaneously. There are some lasers that are not single spatial mode and consequently have light beams that diverge more than is required by the diffraction limit. However, all such devices are classified as "lasers" based on their method of producing light, i.e., stimulated emission. Lasers are employed in applications where light of the required spatial or temporal coherence could not be produced using simpler technologies.
The word laser started as an acronym for "light amplification by stimulated emission of radiation". In this usage, the term "light" includes electromagnetic radiation of any frequency, not only visible light, hence the terms infrared laser, ultraviolet laser, X-ray laser, gamma-ray laser, and so on. Because the microwave predecessor of the laser, the maser, was developed first, devices of this sort operating at microwave and radio frequencies are referred to as "masers" rather than "microwave lasers" or "radio lasers". In the early technical literature, especially at Bell Telephone Laboratories, the laser was called an optical maser; this term is now obsolete.[5]
A laser that produces light by itself is technically an optical oscillator rather than an optical amplifier as suggested by the acronym. It has been humorously noted that the acronym LOSER, for "light oscillation by stimulated emission of radiation", would have been more correct.[6] With the widespread use of the original acronym as a common noun,  optical amplifiers have come to be referred to as "laser amplifiers", notwithstanding the apparent redundancy in that designation.
The back-formed verb to lase is frequently used in the field, meaning "to produce laser light,"[7] especially in reference to the gain medium of a laser; when a laser is operating it is said to be "lasing." Further use of the words laser and maser in an extended sense, not referring to laser technology or devices, can be seen in usages such as astrophysical maser and atom laser.
A laser consists of a gain medium, a mechanism to energize it, and something to provide optical feedback.[8] The gain medium is a material with properties that allow it to amplify light by way of stimulated emission. Light of a specific wavelength that passes through the gain medium is amplified (increases in power).
For the gain medium to amplify light, it needs to be supplied with energy in a process called pumping. The energy is typically supplied as an electric current or as light at a different wavelength. Pump light may be provided by a flash lamp or by another laser.
The most common type of laser uses feedback from an optical cavity—a pair of mirrors on either end of the gain medium. Light bounces back and forth between the mirrors, passing through the gain medium and being amplified each time. Typically one of the two mirrors, the output coupler, is partially transparent. Some of the light escapes through this mirror. Depending on the design of the cavity (whether the mirrors are flat or curved), the light coming out of the laser may spread out or form a narrow beam. In analogy to electronic oscillators, this device is sometimes called a laser oscillator.
Most practical lasers contain additional elements that affect properties of the emitted light, such as the polarization,  wavelength, and shape of the beam.
Electrons and how they interact with electromagnetic fields are important in our understanding of chemistry and physics.
In the classical view, the energy of an electron orbiting an atomic nucleus is larger for orbits further from the nucleus of an atom. However, quantum mechanical effects force electrons to take on discrete positions in orbitals. Thus, electrons are found in specific energy levels of an atom, two of which are shown below:
When an electron absorbs energy either from light (photons) or heat (phonons), it receives that incident quantum of energy. But transitions are only allowed in between discrete energy levels such as the two shown above.
This leads to emission lines and absorption lines.
When an electron is excited from a lower to a higher energy level, it will not stay that way forever.
An electron in an excited state may decay to a lower energy state which is not occupied, according to a particular time constant characterizing that transition. When such an electron decays without external influence, emitting a photon, that is called "spontaneous emission". The phase associated with the photon that is emitted is random. A material with many atoms in such an excited state may thus result in radiation which is very spectrally limited (centered around one wavelength of light), but the individual photons would have no common phase relationship and would emanate in random directions. This is the mechanism of fluorescence and thermal emission.
An external electromagnetic field at a frequency associated with a transition can affect the quantum mechanical state of the atom. As the electron in the atom makes a transition between two stationary states (neither of which shows a dipole field), it enters a transition state which does have a dipole field, and which acts like a small electric dipole, and this dipole oscillates at a characteristic frequency. In response to the external electric field at this frequency, the probability of the atom entering this transition state is greatly increased. Thus, the rate of transitions between two stationary states is enhanced beyond that due to spontaneous emission. Such a transition to the higher state is called absorption, and it destroys an incident photon (the photon's energy goes into powering the increased energy of the higher state). A transition from the higher to a lower energy state, however, produces an additional photon; this is the process of stimulated emission.
The gain medium is put into an excited state by an external source of energy. In most lasers this medium consists of a population of atoms which have been excited into such a state by means of an outside light source, or an electrical field which supplies energy for atoms to absorb and be transformed into their excited states.
The gain medium of a laser is normally a material of controlled purity, size, concentration, and shape, which amplifies the beam by the process of stimulated emission described above. This material can be of any state: gas, liquid, solid, or plasma. The gain medium absorbs pump energy, which raises some electrons into higher-energy ("excited") quantum states. Particles can interact with light by either absorbing or emitting photons. Emission can be spontaneous or stimulated. In the latter case, the photon is emitted in the same direction as the light that is passing by. When the number of particles in one excited state exceeds the number of particles in some lower-energy state, population inversion is achieved and the amount of stimulated emission due to light that passes through is larger than the amount of absorption. Hence, the light is amplified. By itself, this makes an optical amplifier. When an optical amplifier is placed inside a resonant optical cavity, one obtains a laser oscillator.[9]
In a few situations it is possible to obtain lasing with only a single pass of EM radiation through the gain medium, and this produces a laser beam without any need for a resonant or reflective cavity (see for example nitrogen laser).[10] Thus, reflection in a resonant cavity is usually required for a laser, but is not absolutely necessary.
The optical resonator is sometimes referred to as an "optical cavity", but this is a misnomer: lasers use open resonators as opposed to the literal cavity that would be employed at microwave frequencies in a maser.
The resonator typically consists of two mirrors between which a coherent beam of light travels in both directions, reflecting back on itself so that an average photon will pass through the gain medium repeatedly before it is emitted from the output aperture or lost to diffraction or absorption.
If the gain (amplification) in the medium is larger than the resonator losses, then the power of the recirculating light can rise exponentially. But each stimulated emission event returns an atom from its excited state to the ground state, reducing the gain of the medium. With increasing beam power the net gain (gain minus loss) reduces to unity and the gain medium is said to be saturated. In a continuous wave (CW) laser, the balance of pump power against gain saturation and cavity losses produces an equilibrium value of the laser power inside the cavity; this equilibrium determines the operating point of the laser. If the applied pump power is too small, the gain will never be sufficient to overcome the cavity losses, and laser light will not be produced. The minimum pump power needed to begin laser action is called the lasing threshold. The gain medium will amplify any photons passing through it, regardless of direction; but only the photons in a spatial mode supported by the resonator will pass more than once through the medium and receive substantial amplification.
In most lasers, lasing begins with stimulated emission amplifying random spontaneously emitted photons present in the gain medium. Stimulated emission produces light that matches the input signal in wavelength, phase, and polarization. This, combined with the filtering effect of the optical resonator gives laser light its characteristic coherence, and may give it uniform polarization and monochromaticity, depending on the resonator's design.  Some lasers use a separate injection seeder to start the process off with a beam that is already highly coherent. This can produce beams with a narrower spectrum than would otherwise be possible.
Many lasers produce a beam that can be approximated as a Gaussian beam; such beams have the minimum divergence possible for a given beam diameter. Some lasers, particularly high-power ones, produce multimode beams, with the transverse modes often approximated using Hermite–Gaussian or Laguerre-Gaussian functions. Some high power lasers use a flat-topped profile known as a "tophat beam". Unstable laser resonators (not used in most lasers) produce fractal-shaped beams.[11] Specialized optical systems can produce more complex beam geometries, such as Bessel beams and optical vortexes.
Near the "waist" (or focal region) of a laser beam, it is highly collimated: the wavefronts are planar, normal to the direction of propagation, with no beam divergence at that point. However, due to diffraction, that can only remain true well within the Rayleigh range. The beam of a single transverse mode (gaussian beam) laser eventually diverges at an angle which varies inversely with the beam diameter, as required by diffraction theory. Thus, the "pencil beam" directly generated by a common helium–neon laser would spread out to a size of perhaps 500 kilometers when shone on the Moon (from the distance of the earth). On the other hand, the light from a semiconductor laser typically exits the tiny crystal with a large divergence: up to 50°. However even such a divergent beam can be transformed into a similarly collimated beam by means of a lens system, as is always included, for instance, in a laser pointer whose light originates from a laser diode. That is possible due to the light being of a single spatial mode. This unique property of laser light, spatial coherence, cannot be replicated using standard light sources (except by discarding most of the light) as can be appreciated by comparing the beam from a flashlight (torch) or spotlight to that of almost any laser.
A laser beam profiler is used to measure the intensity profile, width, and divergence of laser beams.
Diffuse reflection of a laser beam from a matte surface produces a speckle pattern with interesting properties.
The mechanism of producing radiation in a laser relies on stimulated emission, where energy is extracted from a transition in an atom or molecule. This is a quantum phenomenon discovered by Einstein who derived the relationship between the A coefficient describing spontaneous emission and the B coefficient which applies to absorption and stimulated emission. However, in the case of the free electron laser, atomic energy levels are not involved; it appears that the operation of this rather exotic device can be explained without reference to quantum mechanics.
A laser can be classified as operating in either continuous or pulsed mode, depending on whether the power output is essentially continuous over time or whether its output takes the form of pulses of light on one or another time scale. Of course even a laser whose output is normally continuous can be intentionally turned on and off at some rate in order to create pulses of light. When the modulation rate is on time scales much slower than the cavity lifetime and the time period over which energy can be stored in the lasing medium or pumping mechanism, then it is still classified as a "modulated" or "pulsed" continuous wave laser. Most laser diodes used in communication systems fall in that category.
Some applications of lasers depend on a beam whose output power is constant over time. Such a laser is known as continuous wave (CW). Many types of lasers can be made to operate in continuous wave mode to satisfy such an application. Many of these lasers actually lase in several longitudinal modes at the same time, and beats between the slightly different optical frequencies of those oscillations will in fact produce amplitude variations on time scales shorter than the round-trip time (the reciprocal of the frequency spacing between modes), typically a few nanoseconds or less. In most cases these lasers are still termed "continuous wave" as their output power is steady when averaged over any longer time periods, with the very high frequency power variations having little or no impact in the intended application. (However the term is not applied to mode-locked lasers, where the intention is to create very short pulses at the rate of the round-trip time).
For continuous wave operation, it is required for the population inversion of the gain medium to be continually replenished by a steady pump source. In some lasing media this is impossible. In some other lasers it would require pumping the laser at a very high continuous power level which would be impractical or destroy the laser by producing excessive heat. Such lasers cannot be run in CW mode.
Pulsed operation of lasers refers to any laser not classified as continuous wave, so that the optical power appears in pulses of some duration at some repetition rate. This encompasses a wide range of technologies addressing a number of different motivations. Some lasers are pulsed simply because they cannot be run in continuous mode.
In other cases, the application requires the production of pulses having as large an energy as possible. Since the pulse energy is equal to the average power divided by the repetition rate, this goal can sometimes be satisfied by lowering the rate of pulses so that more energy can be built up in between pulses. In laser ablation, for example, a small volume of material at the surface of a work piece can be evaporated if it is heated in a very short time, while supplying the energy gradually would allow for the heat to be absorbed into the bulk of the piece, never attaining a sufficiently high temperature at a particular point.
Other applications rely on the peak pulse power (rather than the energy in the pulse), especially in order to obtain nonlinear optical effects. For a given pulse energy, this requires creating pulses of the shortest possible duration utilizing techniques such as Q-switching.
The optical bandwidth of a pulse cannot be narrower than the reciprocal of the pulse width. In the case of extremely short pulses, that implies lasing over a considerable bandwidth, quite contrary to the very narrow bandwidths typical of CW lasers. The lasing medium in some dye lasers and vibronic solid-state lasers produces optical gain over a wide bandwidth, making a laser possible which can thus generate pulses of light as short as a few femtoseconds (10−15 s).
In a Q-switched laser, the population inversion is allowed to build up by introducing loss inside the resonator which exceeds the gain of the medium; this can also be described as a reduction of the quality factor or 'Q' of the cavity. Then, after the pump energy stored in the laser medium has approached the maximum possible level, the introduced loss mechanism (often an electro- or acousto-optical element) is rapidly removed (or that occurs by itself in a passive device), allowing lasing to begin which rapidly obtains the stored energy in the gain medium. This results in a short pulse incorporating that energy, and thus a high peak power.
A mode-locked laser is capable of emitting extremely short pulses on the order of tens of picoseconds down to less than 10 femtoseconds. These pulses will repeat at the round trip time, that is, the time that it takes light to complete one round trip between the mirrors comprising the resonator. Due to the Fourier limit (also known as energy-time uncertainty), a pulse of such short temporal length has a spectrum spread over a considerable bandwidth. Thus such a gain medium must have a gain bandwidth sufficiently broad to amplify those frequencies. An example of a suitable material is titanium-doped, artificially grown sapphire (Ti:sapphire) which has a very wide gain bandwidth and can thus produce pulses of only a few femtoseconds duration.
Such mode-locked lasers are a most versatile tool for researching processes occurring on extremely short time scales (known as femtosecond physics, femtosecond chemistry and ultrafast science), for maximizing the effect of nonlinearity in optical materials (e.g. in second-harmonic generation, parametric down-conversion, optical parametric oscillators and the like). Due to the large peak power and the ability to generate phase-stabilized trains of ultrafast laser pulses, mode-locking ultrafast lasers underpin precision metrology and spectroscopy applications.[12]
Another method of achieving pulsed laser operation is to pump the laser material with a source that is itself pulsed, either through electronic charging in the case of flash lamps, or another laser which is already pulsed. Pulsed pumping was historically used with dye lasers where the inverted population lifetime of a dye molecule was so short that a high energy, fast pump was needed. The way to overcome this problem was to charge up large capacitors which are then switched to discharge through flashlamps, producing an intense flash. Pulsed pumping is also required for three-level lasers in which the lower energy level rapidly becomes highly populated preventing further lasing until those atoms relax to the ground state. These lasers, such as the excimer laser and the copper vapor laser, can never be operated in CW mode.
In 1917, Albert Einstein established the theoretical foundations for the laser and the maser in the paper Zur Quantentheorie der Strahlung (On the Quantum Theory of Radiation) via a re-derivation of Max Planck's law of radiation, conceptually based upon probability coefficients (Einstein coefficients) for the absorption, spontaneous emission, and stimulated emission of electromagnetic radiation.[13] In 1928, Rudolf W. Ladenburg confirmed the existence of the phenomena of stimulated emission and negative absorption.[14] In 1939, Valentin A. Fabrikant predicted the use of stimulated emission to amplify "short" waves.[15] In 1947, Willis E. Lamb and R. C. Retherford found apparent stimulated emission in hydrogen spectra and effected the first demonstration of stimulated emission.[14] In 1950, Alfred Kastler (Nobel Prize for Physics 1966) proposed the method of optical pumping, experimentally confirmed, two years later, by Brossel, Kastler, and Winter.[16]
In 1951, Joseph Weber submitted a paper on using stimulated emissions to make a microwave amplifier to the June 1952 Institute of Radio Engineers Vacuum Tube Research Conference at Ottawa, Ontario, Canada.[17] After this presentation, RCA asked Weber to give a seminar on this idea, and Charles Hard Townes asked him for a copy of the paper.[18]
In 1953, Charles Hard Townes and graduate students James P. Gordon and Herbert J. Zeiger produced the first microwave amplifier, a device operating on similar principles to the laser, but amplifying microwave radiation rather than infrared or visible radiation. Townes's maser was incapable of continuous output.[citation needed] Meanwhile, in the Soviet Union, Nikolay Basov and Aleksandr Prokhorov were independently working on the quantum oscillator and solved the problem of continuous-output systems by using more than two energy levels. These gain media could release stimulated emissions between an excited state and a lower excited state, not the ground state, facilitating the maintenance of a population inversion. In 1955, Prokhorov and Basov suggested optical pumping of a multi-level system as a method for obtaining the population inversion, later a main method of laser pumping.
Townes reports that several eminent physicists—among them Niels Bohr, John von Neumann, and Llewellyn Thomas—argued the maser violated Heisenberg's uncertainty principle and hence could not work. Others such as Isidor Rabi and Polykarp Kusch expected that it would be impractical and not worth the effort.[19] In 1964 Charles H. Townes, Nikolay Basov, and Aleksandr Prokhorov shared the Nobel Prize in Physics, "for fundamental work in the field of quantum electronics, which has led to the construction of oscillators and amplifiers based on the maser–laser principle".
In 1957, Charles Hard Townes and Arthur Leonard Schawlow, then at Bell Labs, began a serious study of the infrared laser. As ideas developed, they abandoned infrared radiation to instead concentrate upon visible light. The concept originally was called an "optical maser". In 1958, Bell Labs filed a patent application for their proposed optical maser; and Schawlow and Townes submitted a manuscript of their theoretical calculations to the Physical Review, published that year in Volume 112, Issue No. 6.
Simultaneously, at Columbia University, graduate student Gordon Gould was working on a doctoral thesis about the energy levels of excited thallium. When Gould and Townes met, they spoke of radiation emission, as a general subject; afterwards, in November 1957, Gould noted his ideas for a "laser", including using an open resonator (later an essential laser-device component). Moreover, in 1958, Prokhorov independently proposed using an open resonator, the first published appearance (in the USSR) of this idea. Elsewhere, in the U.S., Schawlow and Townes had agreed to an open-resonator laser design – apparently unaware of Prokhorov's publications and Gould's unpublished laser work.
At a conference in 1959, Gordon Gould published the term LASER in the paper The LASER, Light Amplification by Stimulated Emission of Radiation.[1][6] Gould's linguistic intention was using the "-aser" word particle as a suffix – to accurately denote the spectrum of the light emitted by the LASER device; thus x-rays: xaser, ultraviolet: uvaser, et cetera; none established itself as a discrete term, although "raser" was briefly popular for denoting radio-frequency-emitting devices.
Gould's notes included possible applications for a laser, such as spectrometry, interferometry, radar, and nuclear fusion. He continued developing the idea, and filed a patent application in April 1959. The U.S. Patent Office denied his application, and awarded a patent to Bell Labs, in 1960. That provoked a twenty-eight-year lawsuit, featuring scientific prestige and money as the stakes. Gould won his first minor patent in 1977, yet it was not until 1987 that he won the first significant patent lawsuit victory, when a Federal judge ordered the U.S. Patent Office to issue patents to Gould for the optically pumped and the gas discharge laser devices. The question of just how to assign credit for inventing the laser remains unresolved by historians.[20]
On May 16, 1960, Theodore H. Maiman operated the first functioning laser [21][22] at Hughes Research Laboratories, Malibu, California, ahead of several research teams, including those of Townes, at Columbia University, Arthur Schawlow, at Bell Labs,[23] and Gould, at the TRG (Technical Research Group) company. Maiman's functional laser used a solid-state flashlamp-pumped synthetic ruby crystal to produce red laser light, at 694 nanometers wavelength; however, the device only was capable of pulsed operation, because of its three-level pumping design scheme. Later that year, the Iranian physicist Ali Javan, and William R. Bennett, and Donald Herriott, constructed the first gas laser, using helium and neon that was capable of continuous operation in the infrared (U.S. Patent 3,149,290); later, Javan received the Albert Einstein Award in 1993. Basov and Javan proposed the semiconductor laser diode concept. In 1962, Robert N. Hall demonstrated the first laser diode device, made of gallium arsenide and emitting at 850 nm in the near-infrared band of the spectrum. Later that year, Nick Holonyak, Jr. demonstrated the first semiconductor laser with a visible emission. This first semiconductor laser could only be used in pulsed-beam operation, and when cooled to liquid nitrogen temperatures (77 K). In 1970, Zhores Alferov, in the USSR, and Izuo Hayashi and Morton Panish of Bell Telephone Laboratories also independently developed room-temperature, continual-operation diode lasers, using the heterojunction structure.
Since the early period of laser history, laser research has produced a variety of improved and specialized laser types, optimized for different performance goals, including:
and this research continues to this day.
In 2017, researchers at TU Delft demonstrated an AC Josephson junction microwave laser.[24] Since the laser operates in the superconducting regime, it is more stable than other semiconductor-based lasers. The device has potential for applications in quantum computing.[25] In 2017, researchers at TU Munich demonstrated the smallest mode locking laser capable of emitting pairs of phase-locked picosecond laser pulses with a repetition frequency up to 200 GHz.[12]
In 2017, researchers from the Physikalisch-Technische Bundesanstalt (PTB), together with US researchers from JILA, a joint institute of the National Institute of Standards and Technology (NIST) and the University of Colorado Boulder, established a new world record by developing an erbium-doped fiber laser with a linewidth of only 10 millihertz.[26][27]
Following the invention of the HeNe gas laser, many other gas discharges have been found to amplify light coherently.
Gas lasers using many different gases have been built and used for many purposes. The helium–neon laser (HeNe) is able to operate at a number of different wavelengths, however the vast majority are engineered to lase at 633 nm; these relatively low cost but highly coherent lasers are extremely common in optical research and educational laboratories. Commercial carbon dioxide (CO2) lasers can emit many hundreds of watts in a single spatial mode which can be concentrated into a tiny spot. This emission is in the thermal infrared at 10.6 µm; such lasers are regularly used in industry for cutting and welding. The efficiency of a CO2 laser is unusually high: over 30%.[28] Argon-ion lasers can operate at a number of lasing transitions between 351 and 528.7 nm. Depending on the optical design one or more of these transitions can be lasing simultaneously; the most commonly used lines are 458 nm, 488 nm and 514.5 nm. A nitrogen transverse electrical discharge in gas at atmospheric pressure (TEA) laser is an inexpensive gas laser, often home-built by hobbyists, which produces rather incoherent UV light at 337.1 nm.[29] Metal ion lasers are gas lasers that generate deep ultraviolet wavelengths. Helium-silver (HeAg) 224 nm and neon-copper (NeCu) 248 nm are two examples. Like all low-pressure gas lasers, the gain media of these lasers have quite narrow oscillation linewidths, less than 3 GHz (0.5 picometers),[30] making them candidates for use in fluorescence suppressed Raman spectroscopy.
Chemical lasers are powered by a chemical reaction permitting a large amount of energy to be released quickly. Such very high power lasers are especially of interest to the military, however continuous wave chemical lasers at very high power levels, fed by streams of gasses, have been developed and have some industrial applications. As examples, in the hydrogen fluoride laser (2700–2900 nm) and the deuterium fluoride laser (3800 nm) the reaction is the combination of hydrogen or deuterium gas with combustion products of ethylene in nitrogen trifluoride.
Excimer lasers are a special sort of gas laser powered by an electric discharge in which the lasing medium is an excimer, or more precisely an exciplex in existing designs. These are molecules which can only exist with one atom in an excited electronic state. Once the molecule transfers its excitation energy to a photon, therefore, its atoms are no longer bound to each other and the molecule disintegrates. This drastically reduces the population of the lower energy state thus greatly facilitating a population inversion. Excimers currently used are all noble gas compounds; noble gasses are chemically inert and can only form compounds while in an excited state. Excimer lasers typically operate at ultraviolet wavelengths with major applications including semiconductor photolithography and LASIK eye surgery. Commonly used excimer molecules include ArF (emission at 193 nm), KrCl (222 nm), KrF (248 nm), XeCl (308 nm), and XeF (351 nm).[31]
The molecular fluorine laser,  emitting at 157 nm in the vacuum ultraviolet is sometimes referred to as an excimer laser, however this appears to be a misnomer inasmuch as F2 is a stable compound.
Solid-state lasers use a crystalline or glass rod which is "doped" with ions that provide the required energy states. For example, the first working laser was a ruby laser, made from ruby (chromium-doped corundum). The population inversion is actually maintained in the dopant. These materials are pumped optically using a shorter wavelength than the lasing wavelength, often from a flashtube or from another laser. The usage of the term "solid-state" in laser physics is narrower than in typical use. Semiconductor lasers (laser diodes) are typically not referred to as solid-state lasers.
Neodymium is a common dopant in various solid-state laser crystals, including yttrium orthovanadate (Nd:YVO4), yttrium lithium fluoride (Nd:YLF) and yttrium aluminium garnet (Nd:YAG). All these lasers can produce high powers in the infrared spectrum at 1064 nm. They are used for cutting, welding and marking of metals and other materials, and also in spectroscopy and for pumping dye lasers. These lasers are also commonly frequency doubled, tripled or quadrupled to produce 532 nm (green, visible), 355 nm and 266 nm (UV) beams, respectively. Frequency-doubled diode-pumped solid-state (DPSS) lasers are used to make bright green laser pointers.
Ytterbium, holmium, thulium, and erbium are other common "dopants" in solid-state lasers.[32] Ytterbium is used in crystals such as Yb:YAG, Yb:KGW, Yb:KYW, Yb:SYS, Yb:BOYS, Yb:CaF2, typically operating around 1020–1050 nm. They are potentially very efficient and high powered due to a small quantum defect. Extremely high powers in ultrashort pulses can be achieved with Yb:YAG. Holmium-doped YAG crystals emit at 2097 nm and form an efficient laser operating at infrared wavelengths strongly absorbed by water-bearing tissues. The Ho-YAG is usually operated in a pulsed mode, and passed through optical fiber surgical devices to resurface joints, remove rot from teeth, vaporize cancers, and pulverize kidney and gall stones.
Titanium-doped sapphire (Ti:sapphire) produces a highly tunable infrared laser, commonly used for spectroscopy. It is also notable for use as a mode-locked laser producing ultrashort pulses of extremely high peak power.
Thermal limitations in solid-state lasers arise from unconverted pump power that heats the medium. This heat, when coupled with a high thermo-optic coefficient (dn/dT) can cause thermal lensing and reduce the quantum efficiency. Diode-pumped thin disk lasers overcome these issues by having a gain medium that is much thinner than the diameter of the pump beam. This allows for a more uniform temperature in the material. Thin disk lasers have been shown to produce beams of up to one kilowatt.[33]
Solid-state lasers or laser amplifiers where the light is guided due to the total internal reflection in a single mode optical fiber are instead called fiber lasers. Guiding of light allows extremely long gain regions providing good cooling conditions; fibers have high surface area to volume ratio which allows efficient cooling. In addition, the fiber's waveguiding properties tend to reduce thermal distortion of the beam. Erbium and ytterbium ions are common active species in such lasers.
Quite often, the fiber laser is designed as a double-clad fiber. This type of fiber consists of a fiber core, an inner cladding and an outer cladding. The index of the three concentric layers is chosen so that the fiber core acts as a single-mode fiber for the laser emission while the outer cladding acts as a highly multimode core for the pump laser. This lets the pump propagate a large amount of power into and through the active inner core region, while still having a high numerical aperture (NA) to have easy launching conditions.
Pump light can be used more efficiently by creating a fiber disk laser, or a stack of such lasers.
Fiber lasers have a fundamental limit in that the intensity of the light in the fiber cannot be so high that optical nonlinearities induced by the local electric field strength can become dominant and prevent laser operation and/or lead to the material destruction of the fiber. This effect is called photodarkening. In bulk laser materials, the cooling is not so efficient, and it is difficult to separate the effects of photodarkening from the thermal effects, but the experiments in fibers show that the photodarkening can be attributed to the formation of long-living color centers.[citation needed]
Photonic crystal lasers are lasers based on nano-structures that provide the mode confinement and the density of optical states (DOS) structure required for the feedback to take place.[clarification needed] They are typical micrometer-sized[dubious  – discuss] and tunable on the bands of the photonic crystals.[34][clarification needed]
Semiconductor lasers are diodes which are electrically pumped. Recombination of electrons and holes created by the applied current introduces optical gain. Reflection from the ends of the crystal form an optical resonator, although the resonator can be external to the semiconductor in some designs.
Commercial laser diodes emit at wavelengths from 375 nm to 3500 nm.[35] Low to medium power laser diodes are used in laser pointers, laser printers and CD/DVD players. Laser diodes are also frequently used to optically pump other lasers with high efficiency. The highest power industrial laser diodes, with power up to 20 kW, are used in industry for cutting and welding.[36] External-cavity semiconductor lasers have a semiconductor active medium in a larger cavity. These devices can generate high power outputs with good beam quality, wavelength-tunable narrow-linewidth radiation, or ultrashort laser pulses.
In 2012, Nichia and OSRAM developed and manufactured commercial high-power green laser diodes (515/520 nm), which compete with traditional diode-pumped solid-state lasers.[37][38]
Vertical cavity surface-emitting lasers (VCSELs) are semiconductor lasers whose emission direction is perpendicular to the surface of the wafer. VCSEL devices typically have a more circular output beam than conventional laser diodes. As of 2005, only 850 nm VCSELs are widely available, with 1300 nm VCSELs beginning to be commercialized,[39] and 1550 nm devices an area of research. VECSELs are external-cavity VCSELs. Quantum cascade lasers are semiconductor lasers that have an active transition between energy sub-bands of an electron in a structure containing several quantum wells.
The development of a silicon laser is important in the field of optical computing. Silicon is the material of choice for integrated circuits, and so electronic and silicon photonic components (such as optical interconnects) could be fabricated on the same chip. Unfortunately, silicon is a difficult lasing material to deal with, since it has certain properties which block lasing. However, recently teams have produced silicon lasers through methods such as fabricating the lasing material from silicon and other semiconductor materials, such as indium(III) phosphide or gallium(III) arsenide, materials which allow coherent light to be produced from silicon. These are called hybrid silicon laser. Recent developments have also shown the use of monolithically integrated nanowire lasers directly on silicon for optical interconnects, paving the way for chip level applications.[40] These heterostructure nanowire lasers capable of optical interconnects in silicon are also capable of emitting pairs of phase-locked picosecond pulses with a repetition frequency up to 200 GHz, allowing for on-chip optical signal processing.[12] Another type is a Raman laser, which takes advantage of Raman scattering to produce a laser from materials such as silicon.
Lasing without maintaining the medium excited into a population inversion was demonstrated in 1992 in sodium gas and again in 1995 in rubidium gas by various international teams.[41][42] This was accomplished by using an external maser to induce "optical transparency" in the medium by introducing and destructively interfering the ground electron transitions between two paths, so that the likelihood for the ground electrons to absorb any energy has been cancelled.
Dye lasers use an organic dye as the gain medium. The wide gain spectrum of available dyes, or mixtures of dyes, allows these lasers to be highly tunable, or to produce very short-duration pulses (on the order of a few femtoseconds). Although these tunable lasers are mainly known in their liquid form, researchers have also demonstrated narrow-linewidth tunable emission in dispersive oscillator configurations incorporating solid-state dye gain media.[43] In their most prevalent form these solid state dye lasers use dye-doped polymers as laser media.
Free-electron lasers, or FELs, generate coherent, high power radiation that is widely tunable, currently ranging in wavelength from microwaves through terahertz radiation and infrared to the visible spectrum, to soft X-rays. They have the widest frequency range of any laser type. While FEL beams share the same optical traits as other lasers, such as coherent radiation, FEL operation is quite different. Unlike gas, liquid, or solid-state lasers, which rely on bound atomic or molecular states, FELs use a relativistic electron beam as the lasing medium, hence the term free-electron.
The pursuit of a high-quantum-energy laser using transitions between isomeric states of an atomic nucleus has been the subject of wide-ranging academic research since the early 1970s.  Much of this is summarized in three review articles.[44][45][46] This research has been international in scope, but mainly based in the former Soviet Union and the United States.  While many scientists remain optimistic that a breakthrough is near, an operational gamma-ray laser is yet to be realized.[47]
Some of the early studies were directed toward short pulses of neutrons exciting the upper isomer state in a solid so the gamma-ray transition could benefit from the line-narrowing of Mössbauer effect.[48][49] In conjunction, several advantages were expected from two-stage pumping of a three-level system.[50] It was conjectured that the nucleus of an atom, embedded in the near field of a laser-driven coherently-oscillating electron cloud would experience a larger dipole field than that of the driving laser.[51][52]  Furthermore, nonlinearity of the oscillating cloud would produce both spatial and temporal harmonics, so nuclear transitions of higher multipolarity could also be driven at multiples of the laser frequency.[53][54][55][56][57][58][59]
In September 2007, the BBC News reported that there was speculation about the possibility of using positronium annihilation to drive a very powerful gamma ray laser.[60] Dr. David Cassidy of the University of California, Riverside proposed that a single such laser could be used to ignite a nuclear fusion reaction, replacing the banks of hundreds of lasers currently employed in inertial confinement fusion experiments.[60]
Space-based X-ray lasers pumped by a nuclear explosion have also been proposed as antimissile weapons.[61][62] Such devices would be one-shot weapons.
Living cells have been used to produce laser light.[63][64] The cells were genetically engineered to produce green fluorescent protein (GFP). The GFP is used as the laser's "gain medium", where light amplification takes place. The cells were then placed between two tiny mirrors, just 20 millionths of a meter across, which acted as the "laser cavity" in which light could bounce many times through the cell. Upon bathing the cell with blue light, it could be seen to emit directed and intense green laser light.
When lasers were invented in 1960, they were called "a solution looking for a problem".[65] Since then, they have become ubiquitous, finding utility in thousands of highly varied applications in every section of modern society, including consumer electronics, information technology, science, medicine, industry, law enforcement, entertainment, and the military. Fiber-optic communication using lasers is a key technology in modern communications, allowing services such as the Internet.
The first use of lasers in the daily lives of the general population was the supermarket barcode scanner, introduced in 1974. The laserdisc player, introduced in 1978, was the first successful consumer product to include a laser but the compact disc player was the first laser-equipped device to become common, beginning in 1982 followed shortly by laser printers.
Some other uses are:
In 2004, excluding diode lasers, approximately 131,000 lasers were sold with a value of US$2.19 billion.[68] In the same year, approximately 733 million diode lasers, valued at $3.20 billion, were sold.[69]
Lasers have many uses in medicine, including laser surgery (particularly eye surgery), laser healing, kidney stone treatment, ophthalmoscopy, and cosmetic skin treatments such as acne treatment, cellulite and striae reduction, and hair removal.
Lasers are used to treat cancer by shrinking or destroying tumors or precancerous growths. They are most commonly used to treat superficial cancers that are on the surface of the body or the lining of internal organs. They are used to treat basal cell skin cancer and the very early stages of others like cervical, penile, vaginal, vulvar, and non-small cell lung cancer. Laser therapy is often combined with other treatments, such as surgery, chemotherapy, or radiation therapy. Laser-induced interstitial thermotherapy (LITT), or interstitial laser photocoagulation, uses lasers to treat some cancers using hyperthermia, which uses heat to shrink tumors by damaging or killing cancer cells. Lasers are more precise than traditional surgery methods and cause less damage, pain, bleeding, swelling, and scarring. A disadvantage is that surgeons must have specialized training. It may be more expensive than other treatments.[70][71]
Many types of laser can potentially be used as incapacitating weapons, through their ability to produce temporary or permanent vision loss when aimed at the eyes. The degree, character, and duration of vision impairment caused by eye exposure to laser light varies with the power of the laser, the wavelength(s), the collimation of the beam, the exact orientation of the beam, and the duration of exposure. Lasers of even a fraction of a watt in power can produce immediate, permanent vision loss under certain conditions, making such lasers potential non-lethal but incapacitating weapons. The extreme handicap that laser-induced blindness represents makes the use of lasers even as non-lethal weapons morally controversial, and weapons designed to cause permanent blindness have been banned by the Protocol on Blinding Laser Weapons. Weapons designed to cause temporary blindness, known as dazzlers, are used by military and sometimes law enforcement organizations. Incidents of pilots being exposed to lasers while flying have prompted aviation authorities to implement special procedures to deal with such hazards.[72] See Lasers and aviation safety for more on this topic.
Laser weapons capable of directly damaging or destroying a target in combat are still in the experimental stage. The general idea of laser-beam weaponry is to hit a target with a train of brief pulses of light. The rapid evaporation and expansion of the surface causes shockwaves that damage the target.[citation needed] The power needed to project a high-powered laser beam of this kind is beyond the limit of current mobile power technology, thus favoring chemically powered gas dynamic lasers. Example experimental systems include MIRACL and the Tactical High Energy Laser.
Throughout the 2000s, the United States Air Force worked on the Boeing YAL-1, an airborne laser mounted in a Boeing 747. It was intended to be used to shoot down incoming ballistic missiles over enemy territory. In March 2009, Northrop Grumman claimed that its engineers in Redondo Beach had successfully built and tested an electrically powered solid state laser capable of producing a 100-kilowatt beam, powerful enough to destroy an airplane. According to Brian Strickland, manager for the United States Army's Joint High Power Solid State Laser program, an electrically powered laser is capable of being mounted in an aircraft, ship, or other vehicle because it requires much less space for its supporting equipment than a chemical laser.[73] However, the source of such a large electrical power in a mobile application remained unclear. Ultimately, the project was deemed to be infeasible,[74][75][76] and was cancelled in December 2011,[77] with the Boeing YAL-1 prototype being stored and eventually dismantled.
The United States Navy is developing a laser weapon referred to as the Laser Weapon System or LaWS.[78]
In recent years, some hobbyists have taken interests in lasers. Lasers used by hobbyists are generally of class IIIa or IIIb (see Safety), although some have made their own class IV types.[79] However, compared to other hobbyists, laser hobbyists are far less common, due to the cost and potential dangers involved. Due to the cost of lasers, some hobbyists use inexpensive means to obtain lasers, such as salvaging laser diodes from broken DVD players (red), Blu-ray players (violet), or even higher power laser diodes from CD or DVD burners.[80]
Hobbyists also have been taking surplus pulsed lasers from retired military applications and modifying them for pulsed holography. Pulsed Ruby and pulsed YAG lasers have been used.
Different applications need lasers with different output powers. Lasers that produce a continuous beam or a series of short pulses can be compared on the basis of their average power. Lasers that produce pulses can also be characterized based on the peak power of each pulse. The peak power of a pulsed laser is many orders of magnitude greater than its average power. The average output power is always less than the power consumed.
Examples of pulsed systems with high peak power:
Even the first laser was recognized as being potentially dangerous. Theodore Maiman characterized the first laser as having a power of one "Gillette" as it could burn through one Gillette razor blade. Today, it is accepted that even low-power lasers with only a few milliwatts of output power can be hazardous to human eyesight when the beam hits the eye directly or after reflection from a shiny surface. At wavelengths which the cornea and the lens can focus well, the coherence and low divergence of laser light means that it can be focused by the eye into an extremely small spot on the retina, resulting in localized burning and permanent damage in seconds or even less time.
Lasers are usually labeled with a safety class number, which identifies how dangerous the laser is:
The indicated powers are for visible-light, continuous-wave lasers. For pulsed lasers and invisible wavelengths, other power limits apply. People working with class 3B and class 4 lasers can protect their eyes with safety goggles which are designed to absorb light of a particular wavelength.
Infrared lasers with wavelengths longer than about 1.4 micrometers are often referred to as "eye-safe", because the cornea tends to absorb light at these wavelengths, protecting the retina from damage. The label "eye-safe" can be misleading, however, as it applies only to relatively low power continuous wave beams; a high power or Q-switched laser at these wavelengths can burn the cornea, causing severe eye damage, and even moderate power lasers can injure the eye.
Lasers can be a hazard to both civil and miliatary aviation, due to the potential to temporarily distract or blind pilots. See Lasers and aviation safety for more on this topic.



Alternative cancer treatments - Wikipedia

Alternative cancer treatments are alternative or complementary treatments for cancer that have not been approved by the government agencies responsible for the regulation of therapeutic goods.  They include diet and exercise, chemicals, herbs, devices, and manual procedures. The treatments are not supported by evidence, either because no proper testing has been conducted, or because testing did not demonstrate statistically significant efficacy.  Concerns have been raised about the safety of some of them. Some treatments that have been proposed in the past have been found in clinical trials to be useless or unsafe.  Some of these obsolete or disproven treatments continue to be promoted, sold, and used. Promoting or marketing such treatments is illegal in most of the developed world including the United States and European Union.
A distinction is typically made between complementary treatments which do not disrupt conventional medical treatment, and alternative treatments which may replace conventional treatment. 
Alternative cancer treatments are typically contrasted with experimental cancer treatments – which are treatments for which experimental testing is underway – and with complementary treatments, which are non-invasive practices used alongside other treatment. All approved chemotherapeutic cancer treatments were considered experimental cancer treatments before their safety and efficacy testing was completed.
Since the 1940s, medical science has developed chemotherapy, radiation therapy, adjuvant therapy and the newer targeted therapies, as well as refined surgical techniques for removing cancer.  Before the development of these modern, evidence-based treatments, 90% of cancer patients died within five years.[2]  With modern mainstream treatments, only 34% of cancer patients die within five years.[3]  However, while mainstream forms of cancer treatment generally prolong life or permanently cure cancer, most treatments also have side effects ranging from unpleasant to fatal, such as pain, blood clots, fatigue, and infection.[4] These side effects and the lack of a guarantee that treatment will be successful create appeal for alternative treatments for cancer, which purport to cause fewer side effects or to increase survival rates despite evidence to suggest a 2–5 fold increase in death with alternative medicines.[5]
Alternative cancer treatments have typically not undergone properly conducted, well-designed clinical trials, or the results have not been published due to publication bias (a refusal to publish results of a treatment outside that journal's focus area, guidelines or approach).  Among those that have been published, the methodology is often poor.  A 2006 systematic review of 214 articles covering 198 clinical trials of alternative cancer treatments concluded that almost none conducted dose-ranging studies, which are necessary to ensure that the patients are being given a useful amount of the treatment.[6]  These kinds of treatments appear and vanish frequently, and have throughout history.[7]
Complementary and alternative cancer treatments are often grouped together, in part because of the adoption of the phrase "complementary and alternative medicine" by the United States Congress.[8] However, according to Barrie R. Cassileth, in cancer treatment the distinction between complementary and alternative therapies is "crucial".[7]
Complementary treatments are used in conjunction with proven mainstream treatments. They tend to be pleasant for the patient, not involve substances with any pharmacological effects, inexpensive, and intended to treat side effects rather than to kill cancer cells.[9] Medical massage and self-hypnosis to treat pain are examples of complementary treatments.
About half the practitioners who dispense complementary treatments are physicians, although they tend to be generalists rather than oncologists. As many as 60% of American physicians have referred their patients to a complementary practitioner for some purpose.[7]
Alternative treatments, by contrast, are used in place of mainstream treatments. The most popular alternative cancer therapies include restrictive diets, mind-body interventions, bioelectromagnetics, nutritional supplements, and herbs.[7] The popularity and prevalence of different treatments varies widely by region.[10]  While conventional physicians should always be kept aware of any complementary treatments used by a patient, many physicians in the United Kingdom are at least tolerant of their use, and some might recommend them.[11]
Survey data about how many cancer patients use alternative or complementary therapies vary from nation to nation as well as from region to region. A 2000 study published by the European Journal of Cancer evaluated a sample of 1023 women from a British cancer registry suffering from breast cancer and found that 22.4% had consulted with a practitioner of complementary therapies in the previous twelve months. The study concluded that the patients had spent many thousands of pounds on such measures and that use "of practitioners of complementary therapies following diagnosis is a significant and possibly growing phenomenon".[12]
In Australia, one study reported that 46% of children suffering from cancer have been treated with at least one non-traditional therapy. Further 40% of those of any age receiving palliative care had tried at least one such therapy. Some of the most popular alternative cancer treatments were found to be dietary therapies, antioxidants, high dose vitamins, and herbal therapies.[13]
Use of unconventional cancer treatments in the United States has been influenced by the U.S. federal government's National Center for Complementary and Alternative Medicine (NCCAM), initially known as the Office of Alternative Medicine (OAM), which was established in 1992 as a National Institutes of Health (NIH) adjunct by the U.S. Congress. Over thirty American medical schools have offered general courses in alternative medicine, including the Georgetown, Columbia, and Harvard university systems, among others.[7]
People who choose alternative treatments tend to believe that evidence-based medicine is extremely invasive or ineffective, while still believing that their own health could be improved.[14]  They are loyal to their alternative healthcare providers and believe that "treatment should concentrate on the whole person".[14]
Cancer patients who choose alternative treatments instead of conventional treatments believe themselves less likely to die than patients who choose only conventional treatments.[15]  They feel a greater sense of control over their destinies, and report less anxiety and depression.[15]  They are more likely to engage in benefit finding, which is the psychological process of adapting to a traumatic situation and deciding that the trauma was valuable, usually because of perceived personal and spiritual growth during the crisis.[16]
However, patients who use alternative treatments have a poorer survival time, even after controlling for type and stage of disease.[17] In 2017, researchers at Yale School of Medicine published a paper which suggested that people who choose alternative medicine over conventional cancer treatments were more than twice as likely to die within five years of diagnosis. And specifically, in those with breast cancer, people choosing alternative medicine were 5.68 times more likely to die within five years.[18]
The reason that patients using alternative treatments die sooner may be because patients who accurately perceive that they are likely to survive do not attempt unproven remedies, and patients who accurately perceive that they are unlikely to survive are attracted to unproven remedies.[17]  Among patients who believe their condition to be untreatable by evidence-based medicine, "desperation drives them into the hands of anyone with a promise and a smile."[19] Con artists have long exploited patients' perceived lack of options to extract payments for ineffectual and even harmful treatments.[19]
In a survey of American cancer patients, Baby Boomers were more likely to support complementary and alternative treatments than people from an older generation.[20]  White, female, college-educated patients who had been diagnosed more than a year ago were more likely than others to report a favorable impression of at least some complementary and alternative benefits.[20]
Many therapies have been (and continue to be) promoted to treat or prevent cancer in humans but lack good scientific and medical evidence of effectiveness.  In many cases, there is good scientific evidence that the alleged treatments do not work. Unlike accepted cancer treatments, unproven and disproven treatments are generally ignored or avoided by the medical community, and are often pseudoscientific.[21]
Despite this, many of these therapies have continued to be promoted as effective, particularly by promoters of alternative medicine. Scientists consider this practice quackery,[22][23] and some of those engaged in it have been investigated and prosecuted by public health regulators such as the US Federal Trade Commission,[24] the Mexican Secretariat of Health[25] and the Canadian Competition Bureau.[26] In the United Kingdom, the Cancer Act makes the unauthorized promotion of cancer treatments a criminal offense.[27][28]
Most studies of complementary and alternative medicine in the treatment of cancer pain are of low quality in terms of scientific evidence. Studies of massage therapy have produced mixed results, but overall show some temporary benefit for reducing pain, anxiety, and depression and a very low risk of harm, unless the patient is at risk for bleeding disorders.[34][35]  There is weak evidence for a modest benefit from hypnosis, supportive psychotherapy and cognitive therapy.  Results about Reiki and touch therapy were inconclusive.  The most studied such treatment, acupuncture, has demonstrated no benefit as an adjunct analgesic in cancer pain. The evidence for music therapy is equivocal, and some herbal interventions such as PC-SPES, mistletoe, and saw palmetto are known to be toxic to some cancer patients. The most promising evidence, though still weak, is for mind–body interventions such as biofeedback and relaxation techniques.[36]
As stated in the scientific literature, the measures listed below are defined as 'complementary' because they are applied in conjunction with mainstream anti-cancer measures such as chemotherapy, in contrast to the ineffective therapies viewed as 'alternative' since they are offered as substitutes for mainstream measures.[7]
Some alternative cancer treatments are based on unproven or disproven theories of how cancer begins or is sustained in the body.  Some common concepts are:
Government agencies around the world routinely investigate purported alternative cancer treatments in an effort to protect their citizens from fraud and abuse.
In 2008, the United States Federal Trade Commission acted against companies that made unsupported claims that their products, some of which included highly toxic chemicals, could cure cancer.[46] Targets included Omega Supply, Native Essence Herb Company,  Daniel Chapter One, Gemtronics, Inc., Herbs for Cancer, Nu-Gen Nutrition, Inc., Westberry Enterprises, Inc., Jim Clark's All Natural Cancer Therapy, Bioque Technologies, Inc., Cleansing Time Pro, and Premium-essiac-tea-4less.



Carcinoma in situ - Wikipedia
Carcinoma in situ (CIS), also known as in situ neoplasm, is a group of abnormal cells.[1][2] While they are a form of neoplasm[3], there is disagreement over whether CIS should be classified as cancer. This controversy also depends on the exact CIS in question (i.e. cervical, skin, breast). Some authors do not classify them as cancer, however, recognizing that they can potentially become cancer.[1] Others classify certain types as a non-invasive form of cancer.[4][5] The term "pre-cancer" has also been used.
These abnormal cells grow in their normal place, thus "in situ" (from Latin for "in its place"). For example, carcinoma in situ of the skin, also called Bowen's disease, is the accumulation of dysplastic epidermal cells within the epidermis only, that has failed to penetrate into the deeper dermis. For this reason, CIS will usually not form a tumor. Rather, the lesion is flat (in the skin, cervix, etc.) or follows the existing architecture of the organ (in the breast, lung, etc.). Exceptions include CIS of the colon (polyps), the bladder (preinvasive papillary cancer), or the breast (ductal carcinoma in situ or lobular carcinoma in situ).
Many forms of CIS have a high probability of progression into cancer,[6] and therefore removal may be recommended; however, progression of CIS is known to be highly variable and not all CIS becomes invasive cancer.
In the TNM classification, carcinoma in situ is reported as TisN0M0 (stage 0).[7]
These terms are related since they represent the steps of the progression toward cancer:
Carcinoma in situ is, by definition, a localized phenomenon, with no potential for metastasis unless it progresses into cancer. Therefore, its removal eliminates the risk of subsequent progression into a life-threatening condition.
Some forms of CIS (e.g., colon polyps and polypoid tumours of the bladder) can be removed using an endoscope, without conventional surgical resection. Dysplasia of the uterine cervix is removed by excision (cutting it out) or by burning with a laser. Bowen's disease of the skin is removed by excision. Other forms require major surgery, the best known being intraductal carcinoma of the breast (also treated with radiotherapy). One of the most dangerous forms of CIS is the "pneumonic form" of BAC of the lung, which can require extensive surgical removal of large parts of the lung. When too large, it often cannot be completely removed, with eventual disease progression and death of the patient.



Developing country - Wikipedia

A developing country (or a low and middle income country (LMIC), less developed country, less economically developed country (LEDC), or underdeveloped country) is a country with a less developed industrial base and a low Human Development Index (HDI) relative to other countries.[2] However, this definition is not universally agreed upon. There is also no clear agreement on which countries fit this category.[3] A nation's GDP per capita compared with other nations can also be a reference point.
The term "developing" describes a currently observed situation and not a changing dynamic or expected direction of progress. Since the late 1990s, developing countries tended to demonstrate higher growth rates than developed countries.[4] Developing countries include, in decreasing order of economic growth or size of the capital market: newly industrialized countries, emerging markets, frontier markets, least developed countries. Therefore, the least developed countries are the poorest of the developing countries.
Developing countries tend to have some characteristics in common. For example, with regards to health risks, they commonly have: low levels of access to safe drinking water, sanitation and hygiene; energy poverty; high levels of pollution (e.g. air pollution, indoor air pollution, water pollution); high proportion of people with tropical and infectious diseases (neglected tropical diseases); high number of road traffic accidents. Often, there is also widespread poverty, low education levels, inadequate access to family planning services, corruption at all government levels and a lack of so-called good governance. Effects of global warming (climate change) are expected to impact developing countries more than wealthier countries, as most of them have a high "climate vulnerability".[5]
The Sustainable Development Goals, by the United Nations, were set up to help overcome many of these problems. Development aid or development cooperation is financial aid given by governments and other agencies to support the economic, environmental, social and political development of developing countries.

The UN acknowledges that it has "no established convention for the designation of "developed" and "developing" countries or areas".[6][3] According to its so-called M49 standards, published in 1999:The designations "developed" and "developing" are intended for statistical convenience and do not necessarily express a judgement about the stage reached by a particular country or area in the development process.[7][8]The UN implies that developing countries are those not on a tightly defined list of developed countries:
There is no established convention for the designation of "developed" and "developing" countries or areas in the United Nations system. In common practice, Japan in Asia, Canada and the United States in northern America, Australia and New Zealand in Oceania, and Europe are considered "developed" regions or areas. In international trade statistics, the Southern African Customs Union is also treated as a developed region and Israel as a developed country; countries emerging from the former Yugoslavia are treated as developing countries; and countries of eastern Europe and of the Commonwealth of Independent States [the former Soviet Union] in Europe are not included under either developed or developing regions.[3]
However, under other criteria, some countries are at an intermediate stage of development, or, as the International Monetary Fund (IMF) put it, following the fall of the Soviet Union, "countries in transition": all those of Central and Eastern Europe (including Central European countries that still belonged to the "Eastern Europe Group" in the UN institutions); the former Soviet Union (USSR) countries in Central Asia (Kazakhstan, Uzbekistan, Kyrgyzstan, Tajikistan and Turkmenistan); and Mongolia. By 2009, the IMF's World Economic Outlook classified countries as advanced, emerging, or developing, depending on "(1) per capita income level, (2) export diversification—so oil exporters that have high per capita GDP would not make the advanced classification because around 70% of its exports are oil, and (3) degree of integration into the global financial system"[9]
Along with the current level of development, countries can also be classified by how much their level of development has changed over a specific period of time.[10]
In the 2016 edition of its World Development Indicators, the World Bank made a decision to no longer distinguish between “developed” and “developing” countries in the presentation of its data, considering the two-category distinction outdated.[11] Instead, the World Bank classifies countries into four groups, based on Gross National Income per capita, re-set each year on July 1.  In 2016, the four categories in US dollars were:[11]
Kofi Annan, former Secretary General of the United Nations, defined a developed country as "one that allows all its citizens to enjoy a free and healthy life in a safe environment".[12]
Development can be measured by economic or human factors. Developing countries are, in general, countries that have not achieved a significant degree of industrialization relative to their populations, and have, in most cases, a medium to low standard of living. There is an association between low income and high population growth.[13] The development of a country is measured with statistical indexes such as income per capita (per person), gross domestic product per capita, life expectancy, the rate of literacy, freedom index and others. The UN has developed the Human Development Index (HDI), a compound indicator of some of the above statistics, to gauge the level of human development for countries where data is available. The UN had set Millennium Development Goals from a blueprint developed by all of the world's countries and leading development institutions, in order to evaluate growth.[14] These goals ended in 2015, to be superseded by the Sustainable Development Goals.
The concept of the developing nation is found, under one term or another, in numerous theoretical systems having diverse orientations — for example, theories of decolonization, liberation theology, Marxism, anti-imperialism, modernization, social change and political economy.
Another important indicator is the sectoral changes that have occurred since the stage of development of the country. On an average, countries with a 50% contribution from the secondary sector (manufacturing) have grown substantially. Similarly countries with a tertiary sector stronghold also see a greater rate of economic development.
There are several terms used to classify countries into rough levels of development.  Classification of any given country differs across sources, and sometimes these classifications or the specific terminology used is considered disparaging.  Use of the term "market" instead of "country" usually indicates specific focus on the characteristics of the countries' capital markets as opposed to the overall economy.
Developing countries can also be categorized by geography:
Other classifications include:
There is criticism for using the term "developing country". The term could imply inferiority of this kind of country compared with a developed country. It could assume a desire to develop along the traditional Western model of economic development which a few countries, such as Cuba and Bhutan, choose not to follow.[22] Alternative measurements such as gross national happiness have been suggested as important indicators.
The classification of countries as "developing" implies that other countries are developed. This bipartite division is contentious.[citation needed]
To moderate the euphemistic aspect of the word "developing", international organizations have started to use the term less economically developed country for the poorest nations—which can, in no sense, be regarded as developing. This highlights that the standard of living across the entire developing world varies greatly. Other terms sometimes used are less developed countries, underdeveloped nations, and non-industrialized nations. Conversely, developed countries, most economically developed countries, industrialized nations are the opposite end of the spectrum.
Over the past few decades since the fall of the Soviet Union and the end of the Cold War, the term Third World has been used interchangeably with developing countries, but the concept has become outdated in recent years as it no longer represents the current political or economic state of the world. The three-world model arose during the Cold War to define countries aligned with NATO (the First World), the Communist Bloc (the Second World, although this term was less used), or neither (the Third World). Strictly speaking, "Third World" was a political, rather than an economic, grouping.
The term "Global South" began to be used more widely since about 2004.[23][24] It can also include poorer "southern" regions of wealthy "northern" countries.[25] The Global South refers to these countries' "interconnected histories of colonialism, neo-imperialism, and differential economic and social change through which large inequalities in living standards, life expectancy, and access to resources are maintained".[26]
Most developing countries have these criteria in common:[27][28]
According to UN-Habitat, around 33% of the urban population in the developing world in 2012, or about 863 million people, lived in slums.[29] In 2012, the proportion of urban population living in slums was highest in Sub-Saharan Africa (62%), followed by South Asia (35%), Southeast Asia (31%) and East Asia (28%).[29]:127
The UN-Habitat reports that 43% of urban population in developing countries and 78% of those in the least developed countries are slum dwellers.[30]
Slums form and grow in different parts of the world for many different reasons. Causes include rapid rural-to-urban migration, economic stagnation and depression, high unemployment, poverty, informal economy, forced or manipulated ghettoization, poor planning, politics, natural disasters and social conflicts.[31][32][33] For example, as populations expand in poorer countries, rural people are moving to cities in an extensive urban migration that is resulting in the creation of slums.[34]
In some cities, especially in countries in Southern Asia and sub-Saharan, slums are not just marginalized neighborhoods holding a small population; slums are widespread, and are home to a large part of urban population. These are sometimes called "slum cities".[35]
Several forms of violence against women are more prevalent in developing countries than in other parts of the world. For example, dowry violence and bride burning is associated with India, Bangladesh and Nepal. Acid throwing is also associated with these countries, as well as in Southeast Asia, including Cambodia. Honor killing is associated with the Middle East and South Asia. Marriage by abduction is found in Ethiopia, Central Asia and the Caucasus. Abuse related to payment of bride price (such as violence, trafficking and forced marriage) is linked to parts of Sub-Saharan Africa and Oceania.[36][37]
Female genital mutilation is another form of violence against women which is still occurring in many developing countries. It is found mostly in Africa, and to a lesser extent in the Middle East and some other parts of Asia. Developing countries with the highest rate of women who have been cut are Somalia (with 98 per cent of women affected), Guinea (96 per cent), Djibouti (93 per cent), Egypt (91 per cent), Eritrea (89 per cent), Mali (89 per cent), Sierra Leone (88 per cent), Sudan (88 per cent), Gambia (76 per cent), Burkina Faso (76 per cent), and Ethiopia (74 per cent).[38] Due to globalization and immigration, FGM is spreading beyond the borders of Africa and Middle East, to countries such as Australia, Belgium, Canada, France, New Zealand, the U.S., and UK.[39]
The Istanbul Convention prohibits female genital mutilation (Article 38).[40] As of 2016, FGM has been legally banned in many African countries.[41]
People in developing countries usually have a lower life expectancy than people in developed countries.
Undernutrition is more common in developing countries.[42] Certain groups have higher rates of undernutrition, including women—in particular while pregnant or breastfeeding—children under five years of age, and the elderly. Malnutrition in children and stunted growth of children is the cause for more than 200 million children under five years of age in developing countries not reaching their developmental potential.[43]  About 165 million children were estimated to have stunted growth from malnutrition in 2013.[44] In some developing countries, overnutrition in the form of obesity is beginning to present within the same communities as undernutrition.[45]
The following list shows the further significant environmentally-related causes or conditions, as well as certain diseases with a strong environmental component:[46]
Access to water, sanitation and hygiene (WASH) services is at very low levels in many developing countries. In 2015 the World Health Organization (WHO) estimated that "1 in 3 people, or 2.4 billion, are still without sanitation facilities" while 663 million people still lack access to safe and clean drinking water.[48][49] The estimate in 2017 by JMP states that 4.5 billion people currently do not have safely managed sanitation.[50] The majority of these people live in developing countries.
About 892 million people, or 12 per cent of the global population, practiced open defecation instead of using toilets in 2016.[50] Seventy-six per cent (678 million) of the 892 million people practicing open defecation in the world live in just seven countries. India is the country with the highest number of people practicing open defecation.[50] Further countries with a high number of people openly defecating are Nigeria (47 million), followed by Indonesia (31 million), Ethiopia (27 million), Pakistan (23 million),[51] Niger (14 million) and Sudan (11 million).[50][52]
Sustainable Development Goal 6 is one of 17 Sustainable Development Goals established by the UN in 2015. It calls for clean water and sanitation for all people. This is particularly relevant for people in developing countries.
In 2009, about 1.4 billion of people in the world lived without electricity, and 2.7 billion relied on wood, charcoal, and dung (dry animal dung fuel) for home energy requirements. This lack of access to modern energy technology limits income generation, blunts efforts to escape poverty, affects people's health, and contributes to global deforestation and climate change. Small-scale renewable energy technologies and distributed energy options, such as onsite solar power and improved cookstoves, offer rural households modern energy services.[53]
Renewable energy can be particularly suitable for developing countries. In rural and remote areas, transmission and distribution of energy generated from fossil fuels can be difficult and expensive. Producing renewable energy locally can offer a viable alternative.[54]
Renewable energy can directly contribute to poverty alleviation by providing the energy needed for creating businesses and employment. Renewable energy technologies can also make indirect contributions to alleviating poverty by providing energy for cooking, space heating, and lighting.[55]
Kenya is the world leader in the number of solar power systems installed per capita.[56]
Indoor air pollution in developing nations is a major health hazard.[57] A major source of indoor air pollution in developing countries is the burning of biomass. Three billion people in developing countries across the globe rely on biomass in the form of wood, charcoal, dung, and crop residue, as their domestic cooking fuel.[58] Because much of the cooking is carried out indoors in environments that lack proper ventilation, millions of people, primarily poor women and children face serious health risks.
Globally, 4.3 million deaths were attributed to exposure to IAP in developing countries in 2012, almost all in low and middle income countries.  The South East Asian and Western Pacific regions bear most of the burden with 1.69 and 1.62 million deaths, respectively. Almost 600,000 deaths occur in Africa.[59] An earlier estimate from 2000 but the death toll between 1.5 million and 2 million deaths.[60]
Finding an affordable solution to address the many effects of indoor air pollution is complex. Strategies include improving combustion, reducing smoke exposure, improving safety and reducing labor, reducing fuel costs, and addressing sustainability.[61]
Water pollution is a major problem in many developing countries. It requires ongoing evaluation and revision of water resource policy at all levels (international down to individual aquifers and wells). It has been suggested that water pollution is the leading worldwide cause of death and diseases,[62][63] and that it accounts for the deaths of more than 14,000 people daily.[63]
India and China are two countries with high levels of water pollution: An estimated 580 people in India die of water pollution related illness (including waterborne diseases) every day.[64] About 90 per cent of the water in the cities of China is polluted.[65] As of 2007, half a billion Chinese had no access to safe drinking water.[66]
Further details of water pollution in several countries, including many developing countries:
The effects of global warming such as extreme weather events, droughts, floods, biodiversity loss, disease and sea level rise are dangerous for humans and the environment.[67] Developing countries are the least able to adapt to climate change (and are therefore called "highly climate vulnerable") due to their relatively low levels of wealth, technology, education, infrastructure and access to resources. This applies to many countries in Sub-Saharan Africa or Small Island Developing States. Some of those island states are likely to face total inundation.[68] Fragile states or failed states like Afghanistan, Haiti, Myanmar, Sierra Leone, and Somalia are among the worst affected.
Climate vulnerability has been quantified in the Climate Vulnerability Monitor reports of 2010 and 2012. Climate vulnerability in developing countries occurs in four impact areas: health, extreme weather, habitat loss, and economic stress.[67][5] A report by the Climate Vulnerability Monitor in 2012 estimated that climate change causes 400,000 deaths on average each year, mainly due to hunger and communicable diseases in developing countries.[69]:17 These effects are most severe for the world’s poorest countries.
A changing climate also results in economic burdens. The economies in Least Developed Countries have lost an average of 7% of their gross domestic product for the year 2010, mainly due to reduced labor productivity.[69]:14 Rising sea levels cost 1% of GDP to the least developed countries in 2010 – 4% in the Pacific  – with 65 billion dollars annually lost from the world economy.[67] Another example is the impact on fisheries: approximately 40 countries are acutely vulnerable to the impact of greenhouse gas emissions on fisheries. Developing countries with large fisheries sectors are particularly affected.[69]:279
In many cases, developing countries produce only small quantities of greenhouse gas emissions per capita but are very vulnerable to the negative effects of global warming.[68] Such countries include Comoros, The Gambia, Guinea-Bissau, São Tomé and Príncipe, Solomon Islands and Vanuatu - they have been called "forced riders" as opposed to the "free riders".[5] Internationally there is recognition of this issue, which is known under the term "climate justice". It has been a key topic at the United Nations Climate Change Conferences (COP).
During the Cancún COP16 in 2010, donor countries promised an annual $100 billion by 2020 through the Green Climate Fund for developing countries to adapt to climate change. However, concrete pledges by developed countries have not been forthcoming.[70][71] Emmanuel Macron (President of France) said at the 2017 United Nations Climate Change Conference in Bonn (COP 23): "Climate change adds further injustice to an already unfair world".[72]
Climate stress is likely to add to existing migration patterns in developing countries and beyond but is not expected to generate entirely new flows of people.[73]:110 A report by World Bank in 2018 estimated that around 143 million people in three regions (Sub-Saharan Africa, South Asia, and Latin America) could be forced to move within their own countries to escape the slow-onset impacts of climate change. They will migrate from less viable areas with lower water availability and crop productivity and from areas affected by rising sea level and storm surges.[74]
Economic development and climate are inextricably linked, particularly around poverty, gender equality, and energy.[75] Tackling climate change will only be possible if the Sustainable Development Goals (SDGs) are met (goal number 13 is on climate action).[75]
Over the last few decades, global population growth has largely been driven by developing countries, which often have higher birth rates (higher fertility rate) than developed countries. According to the United Nations, family planning can help to slow population growth and decrease poverty in these countries.[76]
The following are considered developing economies according to the International Monetary Fund's World Economic Outlook Database, April 2018.[81][82]
Countries not listed by IMF
The following, including the Four Asian Tigers and new Eurozone European countries, were considered developing countries and regions until the '90s, and are now listed as advanced economies (developed countries and regions) by the IMF. Time in brackets is the time to be listed as advanced economies.
Three economies lack data before being listed as advanced economies. Because of the lack of data, it is difficult to judge whether they were advanced economies or developing economies before being listed as advanced economies.
Five countries belong to the "emerging markets" groups and are together called the BRICS countries:  



Venous thrombosis - Wikipedia
A venous thrombus (PL thrombi) is  a blood clot (thrombus) that forms within a vein. Thrombosis is a term for a blood clot occurring inside a blood vessel. A common type of venous thrombosis is a deep vein thrombosis (DVT), which is a blood clot in the deep veins of the leg. If the thrombus breaks off (embolizes) and flows towards the lungs, it can become a pulmonary embolism (PE), a blood clot in the lungs.
An inflammatory reaction is usually present, mainly in the superficial veins and, for this reason this pathology is called most of the time thrombophlebitis. The inflammatory reaction and the white blood cells play a role in the resolution of venous clots.[1]
The initial treatment for venous thromboembolism is typically with either low molecular weight heparin (LMWH) or unfractionated heparin. LMWH appears to have lower rates of side effects; however, both result in similar rates of survival.[2]
Superficial venous thromboses cause discomfort but generally not serious consequences, as do the deep vein thromboses (DVTs) that form in the deep veins of the legs or in the pelvic veins. Nevertheless, they can progress to the deep veins through the perforator veins  or, they can be responsible for a lung embolism mainly if the head of the clot is poorly attached to the vein wall and is situated near the sapheno-femoral junction.
When a blood clot breaks loose and travels in the blood, this is called a venous thromboembolism (VTE). The abbreviation DVT/PE refers to a VTE where a deep vein thrombosis (DVT) has moved to the lungs (PE or pulmonary embolism).[3]
Since the veins return blood to the heart, if a piece of a blood clot formed in a vein breaks off it can be transported to the right side of the heart, and from there into the lungs. A piece of thrombus that is transported in this way is an embolus: the process of forming a thrombus that becomes embolic is called a thromboembolism. An embolism that lodges in the lungs is a pulmonary embolism (PE). A pulmonary embolism is a very serious condition that can be fatal depending on the dimensions of the embolus. Venous thromboembolism (VTE) refers to both DVTs and PEs.
Systemic embolisms of venous origin can occur in patients with an atrial or ventricular septal defect, through which an embolus may pass into the arterial system. Such an event is termed a paradoxical embolism.
Venous thrombi are caused mainly by a combination of venous stasis and hypercoagulability—but to a lesser extent endothelial damage and activation.[4] The three factors of stasis, hypercoaguability, and alterations in the blood vessel wall represent Virchow's triad, and changes to the vessel wall are the least understood.[5] Various risk factors increase the likelihood of any one individual developing a thrombosis.
The overall absolute risk of venous thrombosis per 100,000 woman years in current use of combined oral contraceptives is approximately 60, compared to 30 in non-users.[19] The risk of thromboembolism varies with different types of birth control pills; Compared with combined oral contraceptives containing levonorgestrel (LNG), and with the same dose of estrogen and duration of use, the rate ratio of deep vein thrombosis for combined oral contraceptives with norethisterone is 0.98, with norgestimate 1.19, with desogestrel (DSG) 1.82, with gestodene 1.86, with drospirenone (DRSP) 1.64, and with cyproterone acetate 1.88.[19] Venous thromboembolism occurs in 100–200 per 100,000 pregnant women every year.[19]
Regarding family history, age has substantial effect modification. For individuals with two or more affected siblings, the highest incidence rates is found among those ≥70 years of age (390 per 100,000 in male and 370 per 100,000 in female individuals), whereas the highest incidence ratios compared to those without affected siblings occurred at much younger ages (ratio of 4.3 among male individuals 20 to 29 years of age and 5.5 among female individuals 10 to 19 years of age).[20]
In contrast to the understanding for how arterial thromboses occur, as with heart attacks, venous thrombosis formation is not well understood.[21] With arterial thrombosis, blood vessel wall damage is required for thrombosis formation, as it initiates coagulation,[21] but the majority of venous thrombi form without any injured epithelium.[4]
Red blood cells and fibrin are the main components of venous thrombi,[4] and the thrombi appear to attach to the blood vessel wall endothelium, normally a non-thrombogenic surface, with fibrin.[21] Platelets in venous thrombi attach to downstream fibrin, while in arterial thrombi, they compose the core.[21] As a whole, platelets constitute less of venous thrombi when compared to arterial ones.[4] The process is thought to be initiated by tissue factor-affected thrombin production, which leads to fibrin deposition.[5]
The valves of veins are a recognized site of VT initiation. Due to the blood flow pattern, the base of the valve sinus is particularly deprived of oxygen (hypoxic). Stasis excacerbates hypoxia, and this state is linked to the activation of white blood cells (leukocytes) and the endothelium. Specifically, the two pathways of hypoxia-inducible factor-1 (HIF-1) and early growth response 1 (EGR-1) are activated by hypoxia, and they contribute to monocyte and endothelial activation. Hypoxia also causes reactive oxygen species (ROS) production that can activate HIF-1, EGR-1, and nuclear factor-κB (NF-κB), which regulates HIF-1 transcription.[5]
HIF-1 and EGR-1 pathways lead to monocyte association with endothelial proteins, such as P-selectin, prompting monocytes to release tissue factor filled microvesicles, which presumably initiate fibrin deposition (via thrombin) after binding the endothelial surface.[5]
Evidence supports the use of heparin in people following surgery who have a high risk of thrombosis to reduce the risk of DVTs; however, the effect on PEs or overall mortality is not known.[22] In hospitalized non-surgical patients, mortality does not appear to change.[23][24] It does not appear however to decrease the rate of symptomatic DVTs.[23] Using both heparin and compression stockings appears better than either one alone in reducing the rate of DVT.[25]
In hospitalized people who have had a stroke and not had surgery, mechanical measures (compression stockings) resulted in skin damage and no clinical improvement.[23] Data on the effectiveness of compression stockings among hospitalized non-surgical patients without stroke is scarce.[23]
The American College of Physicians (ACP) gave three strong recommendations with moderate quality evidence on VTE prevention in non-surgical patients: that hospitalized patients be assessed for their risk of thromboembolism and bleeding before prophylaxis (prevention); that heparin or a related drug is used if potential benefits are thought to outweigh potential harms; and that graduated compression stockings not be used.[26] As an ACP policy implication, the guideline stated a lack of support for any performance measures that incentivize physicians to apply universal prophylaxis without regard to the risks.[26] Goldhaber recommends that people should be assessed at their hospital discharge for persistent high-risk of venous thrombosis, and that people who adopt a heart-healthy lifestyle might lower their risk of venous thrombosis.[27]
People who have cancer have a higher risk of VTE and may respond differently to anticoagulant preventative treatments and prevention measures.[28] For people undergoing chemotherapy for cancer who are able to walk (ambulatory), low molecular weight heparins treatment (LMWH) decreases the risk of VTE.[29] Due to potential concerns of bleeding its routine use is not recommended.[29] For people who are having surgery for cancer, it is recommended that they receive anticoagulation therapy (preferably LMWH) in order to prevent a VTE.[30] LMWH is recommended for at least 7–10 days following cancer surgery, and for one month following surgery for people who have a high risk of VTEs.[31][30]
In adults who have had their lower leg casted, braced, or otherwise immobilized for more than a week, LMWH may decrease the risk and severity of deep vein thrombosis, but does not have any effect on the incidence of pulmonary embolism .[32] LMWH is recommended for adults not in hospital with an above-knee cast and a below-knee cast, and is safe for this indication.[33][needs update]
Following the completion of warfarin in those with prior VTE, long term aspirin is beneficial.[34]
Evidence-based clinical guidelines were published in 2016 for the treatment of VTE.[35]
Recommendations for those without cancer include anticoagulation (stopping further blood clots from forming) with dabigatran, rivaroxaban, apixaban, or edoxaban rather than warfarin or low molecular weight heparin (LMWH).[35] For those with cancer LMWH is recommended.[35] For long-term treatment in people with cancer, LMWH is probably more effective at reducing VTEs when compared to vitamin K agonists.[28] For initial treatment of VTE, fixed doses with LMWH may be more effective than adjusted doses of unfractionated heparin (UFH) in reducing blood clots.[36]  No differences in mortality, prevention of major bleeding, or preventing VTEs from recurring were observed between LMWH and UFH.[37] No differences have been detected in the route of administration of UFH (subcutaneous or intravenous).[36] LMWH is usually administered by a subcutaneous injection, and a persons blood clotting factors do not have to be monitored as closely as with UFH.[36] People with cancer have a higher risk of experiencing reoccurring VTE episodes ("recurrent VTE"), despite taking preventative anticoagulation medication. These people should be given therapeutic doses of LMWH medication, either by switching from another anticoagulant or by taking a higher dose of LMWH.[38] For long-term treatment of VTEs in people with cancer,  
For those with a small pulmonary embolism and few risk factors, no anticoagulation is needed.[35] Anticoagulation is; however, recommended in those who do have risk factors.[35] Thrombolysis is recommended  in those with PEs that are causing low blood pressure.[35]
Inferior vena cava filters (IVCFs) are not recommended in those who are on anticoagulants.[35] IVCFs may be used in clinical situations where a person has a high risk of experiencing a pulmonary embolism, but cannot be on anticoagulants due to a high risk of bleeding, or they have active bleeding.[38][39] Retrievable IVCFs are recommended if IVCFs must be used, and a plan should be created to remove the filter when it is no longer needed.[38]
Trials suggest that fondaparinux, a factor Xa inhibitor, reduces extension and recurrence of superficial venous thrombosis as well as progression to symptomatic embolism.[40]



Diagnosis - Wikipedia
Diagnosis is the identification of the nature and cause of a certain phenomenon. Diagnosis is used in many different disciplines with variations in the use of logic, analytics, and experience to determine "cause and effect". In systems engineering and computer science, it is typically used to determine the causes of symptoms, mitigations, and solutions.[1]



Autopsy - Wikipedia
An autopsy (post-mortem examination, obduction, necropsy, or autopsia cadaverum) is a surgical procedure that consists of a thorough examination of a corpse by dissection to determine the cause and manner of death or to evaluate any disease or injury that may be present for research or educational purposes. (The term "necropsy" is generally reserved for non-human animals; see below). Autopsies  usually performed by a specialized medical doctor called a pathologist.  In most cases, a medical examiner or coroner can determine cause of death and only a small portion of deaths require an autopsy.  
Autopsies are performed for either legal or medical purposes. For example, a forensic autopsy is carried out when the cause of death may be a criminal matter, while a clinical or academic autopsy is performed to find the medical cause of death and is used in cases of unknown or uncertain death, or for research purposes. Autopsies can be further classified into cases where external examination suffices, and those where the body is dissected and internal examination is conducted. Permission from next of kin may be required for internal autopsy in some cases. Once an internal autopsy is complete the body is reconstituted by sewing it back together.
The term "autopsy" derives from the Ancient Greek αὐτοψία autopsia, "to see for oneself", derived from αὐτός (autos, "oneself") and ὄψις (opsis, "sight, view").[1] The word “autopsy” has been used since around the 17th century, it refers to the examination of inside the dead human body to discover diseases and cause of death.[2]
The principal aims of an autopsy is to determine the cause of death, the state of health of the person before he or she died, and whether any medical diagnosis and treatment before death was appropriate.
In most Western countries the number of autopsies performed in hospitals has been decreasing every year since 1955. Critics, including pathologist and former JAMA editor George D. Lundberg, have charged that the reduction in autopsies is negatively affecting the care delivered in hospitals, because when mistakes result in death, they are often not investigated and lessons therefore remain unlearned.
When a person has given permission in advance of their death, autopsies may also be carried out for the purposes of teaching or medical research.
An autopsy is frequently performed in cases of sudden death, where a doctor is not able to write a death certificate, or when death is believed to result from an unnatural cause. These examinations are performed under a legal authority (Medical Examiner or Coroner or Procurator Fiscal) and do not require the consent of relatives of the deceased. The most extreme example is the examination of murder victims, especially when medical examiners are looking for signs of death or the murder method, such as bullet wounds and exit points, signs of strangulation, or traces of poison.
Some religions including Judaism and Islam usually discourage the performing of autopsies on their adherents.[3] Organizations such as ZAKA in Israel and Misaskim in the United States generally guide families how to ensure that an unnecessary autopsy is not made. Also in the Filipino culture autopsies are taboo. 
Autopsies are used in clinical medicine to identify medical error, or a previously unnoticed condition that may endanger the living, such as infectious diseases or exposure to hazardous materials[4].
A study that focused on myocardial infarction (heart attack) as a cause of death found significant errors of omission and commission,[5] i.e. a sizable number cases ascribed to myocardial infarctions (MIs) were not MIs and a significant number of non-MIs were actually MIs.
A systematic review of studies of the autopsy calculated that in about 25% of autopsies a major diagnostic error will be revealed.[6] However, this rate has decreased over time and the study projects that in a contemporary US institution, 8.4% to 24.4% of autopsies will detect major diagnostic errors.
A large meta-analysis suggested that approximately one-third of death certificates are incorrect and that half of the autopsies performed produced findings that were not suspected before the person died.[7]  Also, it is thought that over one fifth of unexpected findings can only be diagnosed histologically, i.e., by biopsy or autopsy, and that approximately one quarter of unexpected findings, or 5% of all findings, are major and can similarly only be diagnosed from tissue.
One study found that (out of 694 diagnoses) "Autopsies revealed 171 missed diagnoses, including 21 cancers, 12 strokes, 11  myocardial infarctions, 10 pulmonary emboli, and 9 endocarditis, among others".[8]
Focusing on intubated patients, one study found "abdominal pathologic conditions — abscesses, bowel perforations, or  infarction — were as frequent as pulmonary emboli as a cause of class I errors.  While patients with abdominal pathologic conditions generally complained of  abdominal pain, results of examination of the abdomen were considered unremarkable in most patients, and the symptom was not pursued".[9]
There are four main types of autopsies:[10]
A forensic autopsy is used to determine the cause and manner of death. Forensic science involves the application of the sciences to answer questions of interest to the legal system.
Medical examiners attempt to determine the time of death, the exact cause of death, and what, if anything, preceded the death, such as a struggle.  A forensic autopsy may include obtaining biological specimens from the deceased for toxicological testing, including stomach contents.  Toxicology tests may reveal the presence of one or more chemical "poisons" (all chemicals, in sufficient quantities, can be classified as a poison) and their quantity. Because post-mortem deterioration of the body, together with the gravitational pooling of bodily fluids, will necessarily alter the bodily environment, toxicology tests may overestimate, rather than underestimate, the quantity of the suspected chemical.[12]
Following an in-depth examination of all the evidence, a medical examiner or coroner will assign one of the manners of death provided for in the fact-finder's jurisdiction and will detail the evidence on the mechanism of the death.
In most[which?] United States jurisdictions[citation needed], each death is categorized as taking place in one of five "manners of death":
Most states require the state medical examiner to complete an autopsy report, and many mandate that the autopsy be videotaped.[citation needed]
Inquests may declare a death is caused by, among other verdicts:
Some jurisdictions[which?] place deaths in absentia, such as deaths at sea and missing persons declared dead in a court of law, in the "Undetermined" category on the grounds that due to the fact-finder's lack of ability to examine the body, the examiner has no personal knowledge of the manner of (assumed) death; others[which?] classify such deaths in an additional category "Other," reserving "Undetermined" for deaths in which the fact-finder has access to the body, but the information provided by the body and examination of it is insufficient to provide sufficient grounds for a determination.
Clinical autopsies serve two major purposes. They are performed to gain more insight into pathological processes and determine what factors contributed to a patient's death. Autopsies are also performed to ensure the standard of care at hospitals. Autopsies can yield insight into how patient deaths can be prevented in the future.
Within the United Kingdom, clinical autopsies can be carried out only with the consent of the family of the deceased person, as opposed to a medico-legal autopsy instructed by a Coroner (England & Wales) or Procurator Fiscal (Scotland) to which the family cannot object.[citation needed]
Over time, autopsies have not only been able to determine the cause of death, but also lead to discoveries of various diseases such as fetal alcohol syndrome, Legionnaire's disease, and even viral hepatitis.[13]
In 2004 in England and Wales, there were 514,000 deaths of which 225,500 were referred to the coroner. Of those, 115,800 (22.5% of all deaths) resulted in post-mortem examinations and there were 28,300 inquests, 570 with a jury.[14]
The rate of consented (hospital) autopsy in the UK and worldwide has declined rapidly over the past 50 years. In the UK in 2013 only 0.7% of inpatient adult deaths were followed by consented autopsy.[15]
In the United States, autopsy rates fell from 17% in 1980[16] to 14% in 1985[16] and 11.5% in 1989,[17] although the figures vary notably from county to county.[18]
The body is received at a medical examiner's office, municipal mortuary, or hospital in a body bag or evidence sheet. A new body bag is used for each body to ensure that only evidence from that body is contained within the bag. Evidence sheets are an alternative way to transport the body. An evidence sheet is a sterile sheet that covers the body when it is moved. If it is believed there may be any significant evidence on the hands, for example, gunshot residue or skin under the fingernails, a separate paper sack is put around each hand and taped shut around the wrist.
There are two parts to the physical examination of the body: the external and internal examination. Toxicology, biochemical tests or genetic testing/molecular autopsy often supplement these and frequently assist the pathologist in assigning the cause or causes of death.
At many institutions the person responsible for handling, cleaning, and moving the body is called a diener, the German word for servant. In the UK this role is performed by an Anatomical Pathology Technician (APT), who will also assist the pathologist in eviscerating the body and reconstruction after the autopsy. After the body is received, it is first photographed. The examiner then notes the kind of clothes and their position on the body before they are removed. Next, any evidence such as residue, flakes of paint or other material is collected from the external surfaces of the body. Ultraviolet light may also be used to search body surfaces for any evidence not easily visible to the naked eye. Samples of hair, nails and the like are taken, and the body may also be radiographically imaged. Once the external evidence is collected, the body is removed from the bag, undressed, and any wounds present are examined. The body is then cleaned, weighed, and measured in preparation for the internal examination.
A general description of the body as regards ethnic group, sex, age, hair colour and length, eye colour and other distinguishing features (birthmarks, old scar tissue, moles, tattoos, etc.) is then made. A voice recorder or a standard examination form is normally used to record this information.
In some countries[citation needed], e.g., Scotland, France, Germany, and Canada, an autopsy may comprise an external examination only. This concept is sometimes termed a "view and grant". The principle behind this is that the medical records, history of the deceased and circumstances of death have all indicated as to the cause and manner of death without the need for an internal examination.[19]
If not already in place, a plastic or rubber brick called a "head block" is placed under the shoulders of the deceased, hyperflexing the neck making the spine arch backward while stretching and pushing the chest upward to make it easier to incise. This gives the APT, or pathologist, maximum exposure to the trunk. After this is done, the internal examination begins. The internal examination consists of inspecting the internal organs of the body by dissection for evidence of trauma or other indications of the cause of death. For the internal examination there are a number of different approaches available:
There is no need for any incision to be made, which will be visible after completion of the examination when the deceased is dressed in a shroud.
In all of the above cases the incision then extends all the way down to the pubic bone (making a deviation to either side of the navel) and avoiding, where possible; transsecting any scars which may be present.
Bleeding from the cuts is minimal, or non-existent, because the pull of gravity is producing the only blood pressure at this point, related directly to the complete lack of cardiac functionality. However, in certain cases there is anecdotal evidence that bleeding can be quite profuse, especially in cases of drowning.
At this point, shears are used to open the chest cavity.  It is also possible to utilise a simple scalpel blade. The prosector uses the tool to cut through the ribs on the costal cartilage, to allow the sternum to be removed; this is done so that the heart and lungs can be seen in situ and that the heart, in particular the pericardial sac is not damaged or disturbed from opening. A PM 40 knife is used to remove the sternum from the soft tissue which attaches it to the mediastinum. Now the lungs and the heart are exposed. The sternum is set aside and will be eventually replaced at the end of the autopsy.
At this stage the organs are exposed. Usually, the organs are removed in a systematic fashion.  Making a decision as to what order the organs are to be removed will depend highly on the case in question. Organs can be removed in several ways: The first is the en masse technique of Letulle whereby all the organs are removed as one large mass. The second is the en bloc method of Ghon. The most popular in the UK is a modified version of this method, which is divided into four groups of organs. Although these are the two predominant evisceration techniques, in the UK variations on these are widespread.
One method is described here: The pericardial sac is opened to view the heart.  Blood for chemical analysis may be removed from the inferior vena cava or the pulmonary veins.  Before removing the heart, the pulmonary artery is opened in order to search for a blood clot.  The heart can then be removed by cutting the inferior vena cava, the pulmonary veins, the aorta and pulmonary artery, and the superior vena cava.  This method leaves the aortic arch intact, which will make things easier for the embalmer.  The left lung is then easily accessible and can be removed by cutting the bronchus, artery, and vein at the hilum.  The right lung can then be similarly removed.  The abdominal organs can be removed one by one after first examining their relationships and vessels.
Most pathologists, however, prefer the organs to be removed all in one "block".  Using dissection of the fascia, blunt dissection; using the fingers or hands and traction; the organs are dissected out in one piece for further inspection and sampling. During autopsies of infants, this method is used almost all of the time. The various organs are examined, weighed and tissue samples in the form of slices are taken. Even major blood vessels are cut open and inspected at this stage. Next the stomach and intestinal contents are examined and weighed. This could be useful to find the cause and time of death, due to the natural passage of food through the bowel during digestion. The more area empty, the longer the deceased had gone without a meal before death.
The body block that was used earlier to elevate the chest cavity is now used to elevate the head. To examine the brain, an incision is made from behind one ear, over the crown of the head, to a point behind the other ear. When the autopsy is completed, the incision can be neatly sewn up and is not noticed when the head is resting on a pillow in an open casket funeral. The scalp is pulled away from the skull in two flaps with the front flap going over the face and the rear flap over the back of the neck. The skull is then cut with a circular (or semicircular) bladed reciprocating saw to create a "cap" that can be pulled off, exposing the brain. The brain is then observed in situ. Then the brain's connection to the cranial nerves and spinal cord are severed, and the brain is lifted out of the skull for further examination. If the brain needs to be preserved before being inspected, it is contained in a large container of formalin (15 percent solution of formaldehyde gas in buffered water) for at least two, but preferably four weeks. This not only preserves the brain, but also makes it firmer, allowing easier handling without corrupting the tissue.
An important component of the autopsy is the reconstitution of the body such that it can be viewed, if desired, by relatives of the deceased following the procedure. After the examination, the body has an open and empty thoracic cavity with chest flaps open on both sides, the top of the skull is missing, and the skull flaps are pulled over the face and neck. It is unusual to examine the face, arms, hands or legs internally.
In the UK, following the Human Tissue Act 2004 all organs and tissue must be returned to the body unless permission is given by the family to retain any tissue for further investigation. Normally the internal body cavity is lined with cotton, wool, or a similar material, and the organs are then placed into a plastic bag to prevent leakage and are returned to the body cavity. The chest flaps are then closed and sewn back together and the skull cap is sewed back in place. Then the body may be wrapped in a shroud, and it is common for relatives to not be able to tell the procedure has been done when the body is viewed in a funeral parlor after embalming.
Around 3000 BC ancient Egyptians were one of the first civilizations to practice the removal and examination of the internal organs of humans in the religious practice of mummification.[1][20]
Autopsies that opened the body to determine the cause of death were attested at least in the early third millennium BC, although they were opposed in many ancient societies where it was believed that the outward disfigurement of dead persons prevented them from entering the afterlife[21] (as with the Egyptians, who removed the organs through tiny slits in the body).[1] Notable Greek autopsists were Galen (AD 129- c. 200/ 216),[22] Erasistratus and
Herophilus of Chalcedon, who lived in 3rd century BC Alexandria, but in general, autopsies were rare in ancient Greece.[21] In 44 BC, Julius Caesar was the subject of an official autopsy after his murder by rival senators, the physician's report noting that the second stab wound Caesar received was the fatal one.[21] Julius Caesar had been stabbed a total of 23 times.[23] By around 150 BC, ancient Roman legal practice had established clear parameters for autopsies.[1]
The dissection of human remains for medical or scientific reasons continued to be practiced irregularly after the Romans, for instance by the Arab physicians Avenzoar and Ibn al-Nafis. In Europe they were done with enough regularity to become skilled, as early as 1200, and successful efforts to preserve the body, by filling the veins with wax and metals.[22] Until the 20th century,[22] it was thought that the modern autopsy process derived from the anatomists of the Renaissance. Giovanni Battista Morgagni (1682–1771), celebrated as the father of anatomical pathology,[24] wrote the first exhaustive work on pathology, De Sedibus et Causis Morborum per Anatomen Indagatis (The Seats and Causes of Diseases Investigated by Anatomy, 1769).[1]
In the mid-1800s, Carl von Rokitansky and colleagues at the Second Vienna Medical School began to undertake dissections as a means to improve diagnostic medicine.[23]
In 1543 Andreas Vesalius conducted a public dissection of the body of a former criminal. He asserted and articulated the bones, this became the worlds oldest surviving anatomical preparation. It is still displayed at the Anatomical museum at the University of Basel.[25]
The 19th-century medical researcher Rudolf Virchow, in response to a lack of standardization of autopsy procedures, established and published specific autopsy protocols (one such protocol still bears his name). He also developed the concept of pathological processes.
Post-mortem examination, or necropsy, is far more common in veterinary medicine than in human medicine. For many species that exhibit few external symptoms (sheep), or that are not suited to detailed clinical examination (poultry, cage birds, zoo animals), it is a common method used by veterinary physicians to come to a diagnosis. A necropsy is mostly used like an autopsy to determine cause of death. The entire body is examined at the gross visual level, and samples are collected for additional analyses.[26]



Childhood cancer - Wikipedia
Childhood cancer (also known as pediatric cancer) is cancer in a child. In the United States, an arbitrarily adopted standard of the ages used are 0–14 years inclusive, that is, up to 14 years 11.9 months of age.[2][3] However, the definition of childhood cancer sometimes includes adolescents between 15–19 years old.[3] Pediatric oncology is the branch of medicine concerned with the diagnosis and treatment of cancer in children.
Worldwide, it is estimated that childhood cancer has an incidence of more than 175,000 per year, and a mortality rate of approximately 96,000 per year.[4]  In developed countries, childhood cancer has a mortality of approximately 20% of cases.[5] In low resource settings, on the other hand, mortality is approximately 80%, or even 90% in the world's poorest countries.[5]  In many developed countries the incidence is slowly increasing, as rates of childhood cancer increased by 0.6% per year between 1975 and 2002 in the United States[6] and by 1.1% per year between 1978 and 1997 in Europe.[7]
Children with cancer are at risk for developing various cognitive or learning problems.[8] These difficulties may be related to brain injury stemming from the cancer itself, such as a brain tumor or central nervous system metastasis or from side effects of cancer treatments such as chemotherapy and radiation therapy.  Studies have shown that chemo and radiation therapies may damage brain white matter and disrupt brain activity.
The most common cancers in children are (childhood) leukemia (32%), brain tumors (18%), and lymphomas (11%).[7][9] In 2005, 4.1 of every 100,000 young people under 20 years of age in the U.S. were diagnosed with leukemia, and 0.8 per 100,000 died from it.[10] The number of new cases was highest among the 1–4 age group, but the number of deaths was highest among the 10–14 age group.[10]
In 2005, 2.9 of every 100,000 people 0–19 years of age were found to have cancer of the brain or central nervous system, and 0.7 per 100,000 died from it.[10] These cancers were found most often in children between 1 and 4 years of age, but the most deaths occurred among those aged 5–9.[10] The main subtypes of brain and central nervous system tumors in children are: astrocytoma, brain stem glioma, craniopharyngioma, desmoplastic infantile ganglioglioma, ependymoma, high-grade glioma, medulloblastoma and atypical teratoid rhabdoid tumor.[11]
Other, less common childhood cancer types are:[11][9]
Adult survivors of childhood cancer have some physical, psychological, and social difficulties.
Premature heart disease is a major long-term complication in adult survivors of childhood cancer.[12] Adult survivors are eight times more likely to die of heart disease than other people, and more than half of children treated for cancer develop some type of cardiac abnormality, although this may be asymptomatic or too mild to qualify for a clinical diagnosis of heart disease.[12]
Familial and genetic factors are identified in 5-15% of childhood cancer cases. In <5-10% of cases, there are known environmental exposures and exogenous factors, such as prenatal exposure to tobacco, X-rays, or certain medications.[13] For the remaining 75-90% of cases, however, the individual causes remain unknown.[13] In most cases, as in carcinogenesis in general, the cancers are assumed to involve multiple risk factors and variables.[14]
Aspects that make the risk factors of childhood cancer different from those seen in adult cancers include:[15]
Also, a longer life expectancy in children avails for a longer time to manifest cancer processes with long latency periods, increasing the risk of developing some cancer types later in life.[15]
There are preventable causes of childhood malignancy, such as delivery overuse and misuse of ionizing radiation through computed tomography scans when the test is not indicated or when adult protocols are used.[16][17]
Internationally, the greatest variation in childhood cancer incidence occurs when comparing high-income countries to low-income ones.[18] This may result from differences in being able to diagnose cancer, differences in risk among different ethnic or racial population subgroups, as well as differences in risk factors.[18] An example of differing risk factors is in cases of pediatric Burkitt lymphoma, a form of non-Hodgkin lymphoma that sickens 6 to 7 children out of every 100,000 annually in parts of sub-Saharan Africa, where it is associated with a history of infection by both Epstein-Barr virus and malaria.[18][19][20] In industrialized countries, Burkitt lymphoma is not associated with these infectious conditions.[18]
In the United States, cancer is the second most common cause of death among children between the ages of 1 and 14 years, exceeded only by accidents.[10] More than 16 out of every 100,000 children and teens in the U.S. were diagnosed with cancer, and nearly 3 of every 100,000 died from the disease.[10] In the United States in 2012, it was estimated that there was an incidence of 12,000 new cases, and 1,300 deaths, from cancer among children 0 to 14 years of age.[21] Statistics from the 2014 American Cancer Society report:
Ages birth to 14[22]
Ages 15 to 19[22]
Note: Incidence and mortality rates are per 1,000,000 and age-adjusted to the 2000 US standard population. Observed survival percentage is based on data from 2003-2009.
Cancer in children is rare in the UK, with an average of 1,800 diagnosis every year but contributing to less than 1% of all cancer related deaths [23]. Age is not a confounding factor in mortality from the disease in the UK. From 2014-2016, approximately 230 children died from cancer, with Brain/CNS cancers being the most common culprit.
Currently, there are various organizations whose sole focus is fighting childhood cancer. Organizations focused on childhood cancer through cancer research and/or support programs include: Childhood Cancer Canada, CLIC Sargent and the Children's Cancer and Leukaemia Group (in the United Kingdom), Child Cancer Foundation (in New Zealand), Children's Cancer Recovery Foundation (in United States)[24], American Childhood Cancer Organization(in the United states),[25] Childhood Cancer Support (Australia) and the Hayim Association (in Israel).[26] Alex's Lemonade Stand Foundation allows people across the US to raise money for pediatric cancer research by organizing lemonade stands.[27] The National Pediatric Cancer Foundation focuses on finding less toxic and more effective treatments for pediatric cancers. This foundation works with 24 different hospitals across the US in search of treatments effective in practice.[28] Childhood Cancer International is the largest global pediatric cancer foundation. It focuses on early access to care for childhood cancers, focusing on patient support and patient advocacy.[29]
According to estimates by experts in the field of pediatric cancer, by 2020, cancer will cost $158 million annually for both research and treatment which marks a 27% increase since 2010.[30] Ways in which the foundations are helped by people include writing checks, collecting spare coins, bake/lemonade sales, donating potions of purchases from stores or restaurants, or Paid Time Off donations[31] as well as auctions, bike rides, dance-a-thons. Additionally, many of the major foundations have donation buttons.
In addition to advancing research focusing on cancer, the foundations also offer support to families whose children are afflicted by the disease. Support groups are offered both in hospitals and online and are funded by the different foundations.[32] The foundations for pediatric cancers organize in-person and online support groups and direct families toward books that aid in the coping process. The foundations for pediatric cancer all fall under the 501(c)3 designation which means that they are non-profit organizations that are tax-exempt.[33] The "International Childhood Cancer Day" occurs annually on February 15.[5][34]
 This article incorporates public domain material from websites or documents of the Centers for Disease Control and Prevention.



Autopsy - Wikipedia
An autopsy (post-mortem examination, obduction, necropsy, or autopsia cadaverum) is a surgical procedure that consists of a thorough examination of a corpse by dissection to determine the cause and manner of death or to evaluate any disease or injury that may be present for research or educational purposes. (The term "necropsy" is generally reserved for non-human animals; see below). Autopsies  usually performed by a specialized medical doctor called a pathologist.  In most cases, a medical examiner or coroner can determine cause of death and only a small portion of deaths require an autopsy.  
Autopsies are performed for either legal or medical purposes. For example, a forensic autopsy is carried out when the cause of death may be a criminal matter, while a clinical or academic autopsy is performed to find the medical cause of death and is used in cases of unknown or uncertain death, or for research purposes. Autopsies can be further classified into cases where external examination suffices, and those where the body is dissected and internal examination is conducted. Permission from next of kin may be required for internal autopsy in some cases. Once an internal autopsy is complete the body is reconstituted by sewing it back together.
The term "autopsy" derives from the Ancient Greek αὐτοψία autopsia, "to see for oneself", derived from αὐτός (autos, "oneself") and ὄψις (opsis, "sight, view").[1] The word “autopsy” has been used since around the 17th century, it refers to the examination of inside the dead human body to discover diseases and cause of death.[2]
The principal aims of an autopsy is to determine the cause of death, the state of health of the person before he or she died, and whether any medical diagnosis and treatment before death was appropriate.
In most Western countries the number of autopsies performed in hospitals has been decreasing every year since 1955. Critics, including pathologist and former JAMA editor George D. Lundberg, have charged that the reduction in autopsies is negatively affecting the care delivered in hospitals, because when mistakes result in death, they are often not investigated and lessons therefore remain unlearned.
When a person has given permission in advance of their death, autopsies may also be carried out for the purposes of teaching or medical research.
An autopsy is frequently performed in cases of sudden death, where a doctor is not able to write a death certificate, or when death is believed to result from an unnatural cause. These examinations are performed under a legal authority (Medical Examiner or Coroner or Procurator Fiscal) and do not require the consent of relatives of the deceased. The most extreme example is the examination of murder victims, especially when medical examiners are looking for signs of death or the murder method, such as bullet wounds and exit points, signs of strangulation, or traces of poison.
Some religions including Judaism and Islam usually discourage the performing of autopsies on their adherents.[3] Organizations such as ZAKA in Israel and Misaskim in the United States generally guide families how to ensure that an unnecessary autopsy is not made. Also in the Filipino culture autopsies are taboo. 
Autopsies are used in clinical medicine to identify medical error, or a previously unnoticed condition that may endanger the living, such as infectious diseases or exposure to hazardous materials[4].
A study that focused on myocardial infarction (heart attack) as a cause of death found significant errors of omission and commission,[5] i.e. a sizable number cases ascribed to myocardial infarctions (MIs) were not MIs and a significant number of non-MIs were actually MIs.
A systematic review of studies of the autopsy calculated that in about 25% of autopsies a major diagnostic error will be revealed.[6] However, this rate has decreased over time and the study projects that in a contemporary US institution, 8.4% to 24.4% of autopsies will detect major diagnostic errors.
A large meta-analysis suggested that approximately one-third of death certificates are incorrect and that half of the autopsies performed produced findings that were not suspected before the person died.[7]  Also, it is thought that over one fifth of unexpected findings can only be diagnosed histologically, i.e., by biopsy or autopsy, and that approximately one quarter of unexpected findings, or 5% of all findings, are major and can similarly only be diagnosed from tissue.
One study found that (out of 694 diagnoses) "Autopsies revealed 171 missed diagnoses, including 21 cancers, 12 strokes, 11  myocardial infarctions, 10 pulmonary emboli, and 9 endocarditis, among others".[8]
Focusing on intubated patients, one study found "abdominal pathologic conditions — abscesses, bowel perforations, or  infarction — were as frequent as pulmonary emboli as a cause of class I errors.  While patients with abdominal pathologic conditions generally complained of  abdominal pain, results of examination of the abdomen were considered unremarkable in most patients, and the symptom was not pursued".[9]
There are four main types of autopsies:[10]
A forensic autopsy is used to determine the cause and manner of death. Forensic science involves the application of the sciences to answer questions of interest to the legal system.
Medical examiners attempt to determine the time of death, the exact cause of death, and what, if anything, preceded the death, such as a struggle.  A forensic autopsy may include obtaining biological specimens from the deceased for toxicological testing, including stomach contents.  Toxicology tests may reveal the presence of one or more chemical "poisons" (all chemicals, in sufficient quantities, can be classified as a poison) and their quantity. Because post-mortem deterioration of the body, together with the gravitational pooling of bodily fluids, will necessarily alter the bodily environment, toxicology tests may overestimate, rather than underestimate, the quantity of the suspected chemical.[12]
Following an in-depth examination of all the evidence, a medical examiner or coroner will assign one of the manners of death provided for in the fact-finder's jurisdiction and will detail the evidence on the mechanism of the death.
In most[which?] United States jurisdictions[citation needed], each death is categorized as taking place in one of five "manners of death":
Most states require the state medical examiner to complete an autopsy report, and many mandate that the autopsy be videotaped.[citation needed]
Inquests may declare a death is caused by, among other verdicts:
Some jurisdictions[which?] place deaths in absentia, such as deaths at sea and missing persons declared dead in a court of law, in the "Undetermined" category on the grounds that due to the fact-finder's lack of ability to examine the body, the examiner has no personal knowledge of the manner of (assumed) death; others[which?] classify such deaths in an additional category "Other," reserving "Undetermined" for deaths in which the fact-finder has access to the body, but the information provided by the body and examination of it is insufficient to provide sufficient grounds for a determination.
Clinical autopsies serve two major purposes. They are performed to gain more insight into pathological processes and determine what factors contributed to a patient's death. Autopsies are also performed to ensure the standard of care at hospitals. Autopsies can yield insight into how patient deaths can be prevented in the future.
Within the United Kingdom, clinical autopsies can be carried out only with the consent of the family of the deceased person, as opposed to a medico-legal autopsy instructed by a Coroner (England & Wales) or Procurator Fiscal (Scotland) to which the family cannot object.[citation needed]
Over time, autopsies have not only been able to determine the cause of death, but also lead to discoveries of various diseases such as fetal alcohol syndrome, Legionnaire's disease, and even viral hepatitis.[13]
In 2004 in England and Wales, there were 514,000 deaths of which 225,500 were referred to the coroner. Of those, 115,800 (22.5% of all deaths) resulted in post-mortem examinations and there were 28,300 inquests, 570 with a jury.[14]
The rate of consented (hospital) autopsy in the UK and worldwide has declined rapidly over the past 50 years. In the UK in 2013 only 0.7% of inpatient adult deaths were followed by consented autopsy.[15]
In the United States, autopsy rates fell from 17% in 1980[16] to 14% in 1985[16] and 11.5% in 1989,[17] although the figures vary notably from county to county.[18]
The body is received at a medical examiner's office, municipal mortuary, or hospital in a body bag or evidence sheet. A new body bag is used for each body to ensure that only evidence from that body is contained within the bag. Evidence sheets are an alternative way to transport the body. An evidence sheet is a sterile sheet that covers the body when it is moved. If it is believed there may be any significant evidence on the hands, for example, gunshot residue or skin under the fingernails, a separate paper sack is put around each hand and taped shut around the wrist.
There are two parts to the physical examination of the body: the external and internal examination. Toxicology, biochemical tests or genetic testing/molecular autopsy often supplement these and frequently assist the pathologist in assigning the cause or causes of death.
At many institutions the person responsible for handling, cleaning, and moving the body is called a diener, the German word for servant. In the UK this role is performed by an Anatomical Pathology Technician (APT), who will also assist the pathologist in eviscerating the body and reconstruction after the autopsy. After the body is received, it is first photographed. The examiner then notes the kind of clothes and their position on the body before they are removed. Next, any evidence such as residue, flakes of paint or other material is collected from the external surfaces of the body. Ultraviolet light may also be used to search body surfaces for any evidence not easily visible to the naked eye. Samples of hair, nails and the like are taken, and the body may also be radiographically imaged. Once the external evidence is collected, the body is removed from the bag, undressed, and any wounds present are examined. The body is then cleaned, weighed, and measured in preparation for the internal examination.
A general description of the body as regards ethnic group, sex, age, hair colour and length, eye colour and other distinguishing features (birthmarks, old scar tissue, moles, tattoos, etc.) is then made. A voice recorder or a standard examination form is normally used to record this information.
In some countries[citation needed], e.g., Scotland, France, Germany, and Canada, an autopsy may comprise an external examination only. This concept is sometimes termed a "view and grant". The principle behind this is that the medical records, history of the deceased and circumstances of death have all indicated as to the cause and manner of death without the need for an internal examination.[19]
If not already in place, a plastic or rubber brick called a "head block" is placed under the shoulders of the deceased, hyperflexing the neck making the spine arch backward while stretching and pushing the chest upward to make it easier to incise. This gives the APT, or pathologist, maximum exposure to the trunk. After this is done, the internal examination begins. The internal examination consists of inspecting the internal organs of the body by dissection for evidence of trauma or other indications of the cause of death. For the internal examination there are a number of different approaches available:
There is no need for any incision to be made, which will be visible after completion of the examination when the deceased is dressed in a shroud.
In all of the above cases the incision then extends all the way down to the pubic bone (making a deviation to either side of the navel) and avoiding, where possible; transsecting any scars which may be present.
Bleeding from the cuts is minimal, or non-existent, because the pull of gravity is producing the only blood pressure at this point, related directly to the complete lack of cardiac functionality. However, in certain cases there is anecdotal evidence that bleeding can be quite profuse, especially in cases of drowning.
At this point, shears are used to open the chest cavity.  It is also possible to utilise a simple scalpel blade. The prosector uses the tool to cut through the ribs on the costal cartilage, to allow the sternum to be removed; this is done so that the heart and lungs can be seen in situ and that the heart, in particular the pericardial sac is not damaged or disturbed from opening. A PM 40 knife is used to remove the sternum from the soft tissue which attaches it to the mediastinum. Now the lungs and the heart are exposed. The sternum is set aside and will be eventually replaced at the end of the autopsy.
At this stage the organs are exposed. Usually, the organs are removed in a systematic fashion.  Making a decision as to what order the organs are to be removed will depend highly on the case in question. Organs can be removed in several ways: The first is the en masse technique of Letulle whereby all the organs are removed as one large mass. The second is the en bloc method of Ghon. The most popular in the UK is a modified version of this method, which is divided into four groups of organs. Although these are the two predominant evisceration techniques, in the UK variations on these are widespread.
One method is described here: The pericardial sac is opened to view the heart.  Blood for chemical analysis may be removed from the inferior vena cava or the pulmonary veins.  Before removing the heart, the pulmonary artery is opened in order to search for a blood clot.  The heart can then be removed by cutting the inferior vena cava, the pulmonary veins, the aorta and pulmonary artery, and the superior vena cava.  This method leaves the aortic arch intact, which will make things easier for the embalmer.  The left lung is then easily accessible and can be removed by cutting the bronchus, artery, and vein at the hilum.  The right lung can then be similarly removed.  The abdominal organs can be removed one by one after first examining their relationships and vessels.
Most pathologists, however, prefer the organs to be removed all in one "block".  Using dissection of the fascia, blunt dissection; using the fingers or hands and traction; the organs are dissected out in one piece for further inspection and sampling. During autopsies of infants, this method is used almost all of the time. The various organs are examined, weighed and tissue samples in the form of slices are taken. Even major blood vessels are cut open and inspected at this stage. Next the stomach and intestinal contents are examined and weighed. This could be useful to find the cause and time of death, due to the natural passage of food through the bowel during digestion. The more area empty, the longer the deceased had gone without a meal before death.
The body block that was used earlier to elevate the chest cavity is now used to elevate the head. To examine the brain, an incision is made from behind one ear, over the crown of the head, to a point behind the other ear. When the autopsy is completed, the incision can be neatly sewn up and is not noticed when the head is resting on a pillow in an open casket funeral. The scalp is pulled away from the skull in two flaps with the front flap going over the face and the rear flap over the back of the neck. The skull is then cut with a circular (or semicircular) bladed reciprocating saw to create a "cap" that can be pulled off, exposing the brain. The brain is then observed in situ. Then the brain's connection to the cranial nerves and spinal cord are severed, and the brain is lifted out of the skull for further examination. If the brain needs to be preserved before being inspected, it is contained in a large container of formalin (15 percent solution of formaldehyde gas in buffered water) for at least two, but preferably four weeks. This not only preserves the brain, but also makes it firmer, allowing easier handling without corrupting the tissue.
An important component of the autopsy is the reconstitution of the body such that it can be viewed, if desired, by relatives of the deceased following the procedure. After the examination, the body has an open and empty thoracic cavity with chest flaps open on both sides, the top of the skull is missing, and the skull flaps are pulled over the face and neck. It is unusual to examine the face, arms, hands or legs internally.
In the UK, following the Human Tissue Act 2004 all organs and tissue must be returned to the body unless permission is given by the family to retain any tissue for further investigation. Normally the internal body cavity is lined with cotton, wool, or a similar material, and the organs are then placed into a plastic bag to prevent leakage and are returned to the body cavity. The chest flaps are then closed and sewn back together and the skull cap is sewed back in place. Then the body may be wrapped in a shroud, and it is common for relatives to not be able to tell the procedure has been done when the body is viewed in a funeral parlor after embalming.
Around 3000 BC ancient Egyptians were one of the first civilizations to practice the removal and examination of the internal organs of humans in the religious practice of mummification.[1][20]
Autopsies that opened the body to determine the cause of death were attested at least in the early third millennium BC, although they were opposed in many ancient societies where it was believed that the outward disfigurement of dead persons prevented them from entering the afterlife[21] (as with the Egyptians, who removed the organs through tiny slits in the body).[1] Notable Greek autopsists were Galen (AD 129- c. 200/ 216),[22] Erasistratus and
Herophilus of Chalcedon, who lived in 3rd century BC Alexandria, but in general, autopsies were rare in ancient Greece.[21] In 44 BC, Julius Caesar was the subject of an official autopsy after his murder by rival senators, the physician's report noting that the second stab wound Caesar received was the fatal one.[21] Julius Caesar had been stabbed a total of 23 times.[23] By around 150 BC, ancient Roman legal practice had established clear parameters for autopsies.[1]
The dissection of human remains for medical or scientific reasons continued to be practiced irregularly after the Romans, for instance by the Arab physicians Avenzoar and Ibn al-Nafis. In Europe they were done with enough regularity to become skilled, as early as 1200, and successful efforts to preserve the body, by filling the veins with wax and metals.[22] Until the 20th century,[22] it was thought that the modern autopsy process derived from the anatomists of the Renaissance. Giovanni Battista Morgagni (1682–1771), celebrated as the father of anatomical pathology,[24] wrote the first exhaustive work on pathology, De Sedibus et Causis Morborum per Anatomen Indagatis (The Seats and Causes of Diseases Investigated by Anatomy, 1769).[1]
In the mid-1800s, Carl von Rokitansky and colleagues at the Second Vienna Medical School began to undertake dissections as a means to improve diagnostic medicine.[23]
In 1543 Andreas Vesalius conducted a public dissection of the body of a former criminal. He asserted and articulated the bones, this became the worlds oldest surviving anatomical preparation. It is still displayed at the Anatomical museum at the University of Basel.[25]
The 19th-century medical researcher Rudolf Virchow, in response to a lack of standardization of autopsy procedures, established and published specific autopsy protocols (one such protocol still bears his name). He also developed the concept of pathological processes.
Post-mortem examination, or necropsy, is far more common in veterinary medicine than in human medicine. For many species that exhibit few external symptoms (sheep), or that are not suited to detailed clinical examination (poultry, cage birds, zoo animals), it is a common method used by veterinary physicians to come to a diagnosis. A necropsy is mostly used like an autopsy to determine cause of death. The entire body is examined at the gross visual level, and samples are collected for additional analyses.[26]



Fear - Wikipedia
Fear is a feeling induced by perceived danger or threat that occurs in certain types of organisms, which causes a change in metabolic and organ functions and ultimately a change in behavior, such as fleeing, hiding, or freezing from perceived traumatic events. Fear in human beings may occur in response to a specific stimulus occurring in the present, or in anticipation or expectation of a future threat perceived as a risk to body or life. The fear response arises from the perception of danger leading to confrontation with or escape from/avoiding the threat (also known as the fight-or-flight response), which in extreme cases of fear (horror and terror) can be a freeze response or paralysis. 
In humans and animals, fear is modulated by the process of cognition and learning. Thus fear is judged as rational or appropriate and irrational or inappropriate. An irrational fear is called a phobia.
Psychologists such as John B. Watson, Robert Plutchik, and Paul Ekman have suggested that there is only a small set of basic or innate emotions and that fear is one of them. This hypothesized set includes such emotions as acute stress reaction, anger, angst, anxiety, fright, horror, joy, panic, and sadness. Fear is closely related to, but should be distinguished from, the emotion anxiety, which occurs as the result of threats that are perceived to be uncontrollable or unavoidable.[1] The fear response serves survival by generating appropriate behavioral responses, so it has been preserved throughout evolution.[2] Sociological and organizational research also suggests that individuals’ fears are not solely dependent on their nature but are also shaped by their social relations and culture, which guide their understanding of when and how much fear to feel.[3]
Many physiological changes in the body are associated with fear, summarized as the fight-or-flight response. An inborn response for coping with danger, it works by accelerating the breathing rate (hyperventilation), heart rate, vasoconstriction of the peripheral blood vessels leading to blushing and sanskadania of the central vessels (pooling),  increasing muscle tension including the muscles attached to each hair follicle to contract and causing "goose bumps", or more clinically, piloerection (making a cold person warmer or a frightened animal look more impressive), sweating, increased blood glucose (hyperglycemia), increased serum calcium, increase in white blood cells called neutrophilic leukocytes, alertness leading to sleep disturbance and "butterflies in the stomach" (dyspepsia). This primitive mechanism may help an organism survive by either running away or fighting the danger.[4] With the series of physiological changes, the consciousness realizes an emotion of fear.
People develop specific fears as a result of learning. This has been studied in psychology as fear conditioning, beginning with John B. Watson's Little Albert experiment in 1920, which was inspired after observing a child with an irrational fear of dogs. In this study, an 11-month-old boy was conditioned to fear a white rat in the laboratory. The fear became generalized to include other white, furry objects, such as a rabbit, dog, and even a ball of cotton.
Fear can be learned by experiencing or watching a frightening traumatic accident. For example, if a child falls into a well and struggles to get out, he or she may develop a fear of wells, heights (acrophobia), enclosed spaces (claustrophobia), or water (aquaphobia). There are studies looking at areas of the brain that are affected in relation to fear. When looking at these areas (such as the amygdala), it was proposed that a person learns to fear regardless of whether they themselves have experienced trauma, or if they have observed the fear in others. In a study completed by Andreas Olsson, Katherine I. Nearing and Elizabeth A. Phelps, the amygdala were affected both when subjects observed someone else being submitted to an aversive event, knowing that the same treatment awaited themselves, and when subjects were subsequently placed in a fear-provoking situation.[5] This suggests that fear can develop in both conditions, not just simply from personal history.
Fear is affected by cultural and historical context. For example, in the early 20th century, many Americans feared polio, a disease that can lead to paralysis.[6] There are consistent cross-cultural differences in how people respond to fear.[citation needed] Display rules affect how likely people are to show the facial expression of fear and other emotions.
Although many fears are learned, the capacity to fear is part of human nature. Many studies[citation needed] have found that certain fears (e.g. animals, heights) are much more common than others (e.g. flowers, clouds). These fears are also easier to induce in the laboratory. This phenomenon is known as preparedness. Because early humans that were quick to fear dangerous situations were more likely to survive and reproduce, preparedness is theorized to be a genetic effect that is the result of natural selection.[7]
From an evolutionary psychology perspective, different fears may be different adaptations that have been useful in our evolutionary past. They may have developed during different time periods. Some fears, such as fear of heights, may be common to all mammals and developed during the mesozoic period. Other fears, such as fear of snakes, may be common to all simians and developed during the cenozoic time period. Still others, such as fear of mice and insects, may be unique to humans and developed during the paleolithic and neolithic time periods (when mice and insects become important carriers of infectious diseases and harmful for crops and stored foods).[8]
Fear is high only if the observed risk and seriousness both are high, and it is low if risk or seriousness is low.[9]
In a 2005 Gallup Poll (U.S.), a national sample of adolescents between the ages of 13 and 17 were asked what they feared the most. The question was open-ended and participants were able to say whatever they wanted. The top ten fears were, in order: terrorist attacks, spiders, death, failure, war, criminal or gang violence, being alone, the future, and nuclear war.[10]
In an estimate of what people fear the most, book author Bill Tancer analyzed the most frequent online queries that involved the phrase, "fear of..." following the assumption that people tend to seek information on the issues that concern them the most. His top ten list of fears published 2008 consisted of flying, heights, clowns, intimacy, death, rejection, people, snakes, failure, and driving.[11]
According to surveys, some of the most common fears are of demons and ghosts, the existence of evil powers, cockroaches, spiders, snakes, heights,Trypophobia, water, enclosed spaces, tunnels, bridges, needles, social rejection, failure, examinations, and public speaking.[12][13][14]
Death anxiety is multidimensional; it covers "fears related to one's own death, the death of others, fear of the unknown after death, fear of obliteration, and fear of the dying process, which includes fear of a slow death and a painful death".[15]
The Yale philosopher Shelly Kagan examined fear of death in a 2007 Yale open course[16] by examining the following questions: Is fear of death a reasonable appropriate response? What conditions are required and what are appropriate conditions for feeling fear of death? What is meant by fear, and how much fear is appropriate? According to Kagan for fear in general to make sense, three conditions should be met:
The amount of fear should be appropriate to the size of "the bad". If the three conditions are not met, fear is an inappropriate emotion. He argues, that death does not meet the first two criteria, even if death is a "deprivation of good things" and even if one believes in a painful afterlife. Because death is certain, it also does not meet the third criterion, but he grants that the unpredictability of when one dies may be cause to a sense of fear.[16]
In a 2003 study of 167 women and 121 men, aged 65–87, low self-efficacy predicted fear of the unknown after death and fear of dying for women and men better than demographics, social support, and physical health. Fear of death was measured by a "Multidimensional Fear of Death Scale" which included the 8 subscales Fear of Dying, Fear of the Dead, Fear of Being Destroyed, Fear for Significant Others, Fear of the Unknown, Fear of Conscious Death, Fear for the Body After Death, and Fear of Premature Death. In hierarchical multiple regression analysis the most potent predictors of death fears were low "spiritual health efficacy", defined as beliefs relating to one's perceived ability to generate spiritually based faith and inner strength, and low "instrumental efficacy", defined as beliefs relating to one's perceived ability to manage activities of daily living.[15]
Psychologists have tested the hypotheses that fear of death motivates religious commitment, and that assurances about an afterlife alleviate the fear; however, empirical research on this topic has been equivocal.[citation needed] Religiosity can be related to fear of death when the afterlife is portrayed as time of punishment. "Intrinsic religiosity", as opposed to mere "formal religious involvement", has been found to be negatively correlated with death anxiety.[15] In a 1976 study of people of various Christian denominations, those who were most firm in their faith, who attended religious services weekly, were the least afraid of dying. The survey found a negative correlation between fear of death and "religious concern".[17][better source needed]
In a 2006 study of white, Christian men and women the hypothesis was tested that traditional, church-centered religiousness and de-institutionalized spiritual seeking are ways of approaching fear of death in old age. Both religiousness and spirituality were related to positive psychosocial functioning, but only church-centered religiousness protected subjects against the fear of death.[18][better source needed]
Fear of the unknown or irrational fear is caused by negative thinking (worry) which arises from anxiety accompanied with a subjective sense of apprehension or dread. Irrational fear shares a common neural pathway with other fears, a pathway that engages the nervous system to mobilize bodily resources in the face of danger or threat. Many people are scared of the "unknown". The irrational fear can branch out to many areas such as the hereafter, the next ten years or even tomorrow. Chronic irrational fear has deleterious effects since the elicitor stimulus is commonly absent or perceived from delusions. In these cases specialists use False Evidence Appearing Real as a definition, alternatively therapists use it as acronym for Feeling frightened, Expecting bad things to happen, Actions and attitudes that can help, and Rewards and results in Rx programs like Coping Cat. Such fear can create comorbidity with the anxiety disorder umbrella.[19] Being scared may cause people to experience anticipatory fear of what may lie ahead rather than planning and evaluating for the same. For example, "continuation of scholarly education" is perceived by many educators as a risk that may cause them fear and stress,[20] and they would rather teach things they've been taught than go and do research. That can lead to habits such as laziness and procrastination.[citation needed] The ambiguity of situations that tend to be uncertain and unpredictable can cause anxiety in addition to other psychological and physical problems in some populations; especially those who engage it constantly, for example, in war-ridden places or in places of conflict, terrorism, abuse, etc. Poor parenting that instills fear can also debilitate a child's psyche development or personality. For example, parents tell their children not to talk to strangers in order to protect them. In school they would be motivated to not show fear in talking with strangers, but to be assertive and also aware of the risks and the environment in which it takes place. Ambiguous and mixed messages like this can affect their self-esteem and self-confidence. Researchers say talking to strangers isn't something to be thwarted but allowed in a parent's presence if required.[21] Developing  a sense of equanimity to handle various situations is often advocated as an antidote to irrational fear and as an essential skill by a number of ancient philosophies.

Often laboratory studies with rats are conducted to examine the acquisition and extinction of conditioned fear responses.[22] In 2004, researchers conditioned rats (Rattus norvegicus) to fear a certain stimulus, through electric shock.[23] The researchers were able to then cause an extinction of this conditioned fear, to a point that no medications or drugs were able to further aid in the extinction process. However the rats did show signs of avoidance learning, not fear, but simply avoiding the area that brought pain to the test rats. The avoidance learning of rats is seen as a conditioned response, and therefore the behavior can be unconditioned, as supported by the earlier research.
Species-specific defense reactions (SSDRs) or avoidance learning in nature is the specific tendency to avoid certain threats or stimuli, it is how animals survive in the wild. Humans and animals both share these species-specific defense reactions, such as the flight-or-fight, which also include pseudo-aggression, fake or intimidating aggression and freeze response to threats, which is controlled by the sympathetic nervous system. These SSDRs are learned very quickly through social interactions between others of the same species, other species, and interaction with the environment.[24] These acquired sets of reactions or responses are not easily forgotten. The animal that survives is the animal that already knows what to fear and how to avoid this threat. An example in humans is the reaction to the sight of a snake, many jump backwards before cognitively realizing what they are jumping away from, and in some cases it is a stick rather than a snake.
As with many functions of the brain, there are various regions of the brain involved in deciphering fear in humans and other nonhuman species.[25]  The amygdala communicates both directions between the prefrontal cortex, hypothalamus, the sensory cortex, the hippocampus, thalamus, septum, and the  brainstem. The amygdala plays an important role in SSDR, such as the ventral amygdalofugal, which is essential for associative learning, and SSDRs are learned through interaction with the environment and others of the same species. An emotional response is created only after the signals have been relayed between the different regions of the brain, and activating the sympathetic nervous systems; which controls the flight, fight, freeze, fright, and faint response.[26][27] Often a damaged amygdala can cause impairment in the recognition of fear (like the human case of patient S.M.).[28] This impairment can cause different species to lack the sensation of fear, and often can become overly confident, confronting larger peers, or walking up to predatory creatures.
Robert C. Bolles (1970), a researcher at University of Washington, wanted to understand species-specific defense reactions and avoidance learning among animals, but found that the theories of avoidance learning and the tools that were used to measure this tendency were out of touch with the natural world.[29] He theorized the species-specific defense reaction (SSDR).[30] There are three forms of SSDRs: flight, fight (pseudo-aggression), or freeze. Even domesticated animals have SSDRs, and in those moments it is seen that animals revert to atavistic standards and become "wild" again. Dr. Bolles states that responses are often dependent on the reinforcement of a safety signal, and not the aversive conditioned stimuli. This safety signal can be a source of feedback or even stimulus change. Intrinsic feedback or information coming from within, muscle twitches, increased heart rate, are seen to be more important in SSDRs than extrinsic feedback, stimuli that comes from the external environment. Dr. Bolles found that most creatures have some intrinsic set of fears, to help assure survival of the species. Rats will run away from any shocking event, and pigeons will flap their wings harder when threatened. The wing flapping in pigeons and the scattered running of rats are considered species-specific defense reactions or behaviors. Bolles believed that SSDRs are conditioned through Pavlovian conditioning, and not operant conditioning; SSDRs arise from the association between the environmental stimuli and adverse events.[31] Michael S. Fanselow conducted an experiment, to test some specific defense reactions, he observed that rats in two different shock situations responded differently, based on instinct or defensive topography, rather than contextual information.[32]
Species-specific defense responses are created out of fear, and are essential for survival.[33] Rats that lack the gene stathmin show no avoidance learning, or a lack of fear, and will often walk directly up to cats and be eaten.[34] Animals use these SSDRs to continue living, to help increase their chance of fitness, by surviving long enough to procreate. Humans and animals alike have created fear to know what should be avoided, and this fear can be learned through association with others in the community, or learned through personal experience with a creature, species, or situations that should be avoided. SSDRs are an evolutionary adaptation that has been seen in many species throughout the world including rats, chimpanzees, prairie dogs, and even humans, an adaptation created to help individual creatures survive in a hostile world.
Fear learning changes across the lifetime due to natural developmental changes in the brain.[35][36] This includes changes in the prefrontal cortex and the amygdala.[37]
The brain structures that are the center of most neurobiological events associated with fear are the two amygdalae, located behind the pituitary gland. Each amygdala is part of a circuitry of fear learning.[2] They are essential for proper adaptation to stress and specific modulation of emotional learning memory. In the presence of a threatening stimulus, the amygdalae generate the secretion of hormones that influence fear and aggression.[38] Once a response to the stimulus in the form of fear or aggression commences, the amygdalae may elicit the release of hormones into the body to put the person into a state of alertness, in which they are ready to move, run, fight, etc. This defensive response is generally referred to in physiology as the fight-or-flight response regulated by the hypothalamus, part of the limbic system.[39] Once the person is in safe mode, meaning that there are no longer any potential threats surrounding them, the amygdalae will send this information to the medial prefrontal cortex (mPFC) where it is stored for similar future situations, which is known as memory consolidation.[40]
Some of the hormones involved during the state of fight-or-flight include epinephrine, which regulates heart rate and metabolism as well as dilating blood vessels and air passages, norepinephrine increasing heart rate, blood flow to skeletal muscles and the release of glucose from energy stores,[41] and cortisol which increases blood sugar, increases circulating neutrophilic leukocytes, calcium amongst other things.[42]
After a situation which incites fear occurs, the amygdalae and hippocampus record the event through synaptic plasticity.[43]  The stimulation to the hippocampus will cause the individual to remember many details surrounding the situation.[44] Plasticity and memory formation in the amygdala are generated by activation of the neurons in the region. Experimental data supports the notion that synaptic plasticity of the neurons leading to the lateral amygdalae occurs with fear conditioning.[45] In some cases, this forms permanent fear responses such as posttraumatic stress disorder (PTSD) or a phobia.[46] MRI and fMRI scans have shown that the amygdalae in individuals diagnosed with such disorders including bipolar or panic disorder are larger and wired for a higher level of fear.[47]
Pathogens can suppress amygdala activity. Rats infected with the toxoplasmosis parasite become less fearful of cats, sometimes even seeking out their urine-marked areas. This behavior often leads to them being eaten by cats. The parasite then reproduces within the body of the cat. There is evidence that the parasite concentrates itself in the amygdala of infected rats.[48] In a separate experiment, rats with lesions in the amygdala did not express fear or anxiety towards unwanted stimuli. These rats pulled on levers supplying food that sometimes sent out electrical shocks. While they learned to avoid pressing on them, they did not distance themselves from these shock-inducing levers.[49]
Several brain structures other than the amygdalae have also been observed to be activated when individuals are presented with fearful vs. neutral faces, namely the occipitocerebellar regions including the fusiform gyrus and the inferior parietal / superior temporal gyri.[50] Fearful eyes, brows and mouth seem to separately reproduce these brain responses.[50] Scientists from Zurich studies show that the hormone oxytocin related to stress and sex reduces activity in your brain fear center.[51]
In threatening situations insects, aquatic organisms, birds, reptiles, and mammals emit odorant substances, initially called alarm substances, which are chemical signals now called alarm pheromones ("Schreckstoff" in German). This is to defend themselves and at the same time to inform members of the same species of danger and leads to observable behavior change like freezing, defensive behavior, or dispersion depending on circumstances and species. For example, stressed rats release odorant cues that cause other rats to move away from the source of the signal.
After the discovery of pheromones in 1959, alarm pheromones were first described in 1968 in ants[52] and earthworms,[53] and four years later also found in mammals, both mice and rats.[54] Over the next two decades identification and characterization of these pheromones proceeded in all manner of insects and sea animals, including fish, but it was not until 1990 that more insight into mammalian alarm pheromones was gleaned.
Earlier, in 1985, a link between odors released by stressed rats and pain perception was discovered: unstressed rats exposed to these odors developed opioid-mediated analgesia.[55] In 1997, researchers found that bees became less responsive to pain after they had been stimulated with isoamyl acetate, a chemical smelling of banana, and a component of bee alarm pheromone.[56] The experiment also showed that the bees' fear-induced pain tolerance was mediated by an endorphine.
By using the forced swimming test in rats as a model of fear-induction, the first mammalian "alarm substance" was found.[57] In 1991, this "alarm substance" was shown to fulfill criteria for pheromones: well-defined behavioral effect, species specificity, minimal influence of experience and control for nonspecific arousal. Rat activity testing with the alarm pheromone, and their preference/avoidance for odors from cylinders containing the pheromone, showed that the pheromone had very low volatility.[58]
In 1993 a connection between alarm chemosignals in mice and their immune response was found.[59] Pheromone production in mice was found to be associated with or mediated by the pituitary gland in 1994.[60]
In 2004, it was demonstrated that rats' alarm pheromones had different effects on the "recipient" rat (the rat perceiving the pheromone) depending which body region they were released from: Pheromone production from the face modified behavior in the recipient rat, e.g. caused sniffing or movement, whereas pheromone secreted from the rat's anal area induced autonomic nervous system stress responses, like an increase in core body temperature.[61] Further experiments showed that when a rat perceived alarm pheromones, it increased its defensive and risk assessment behavior,[62] and its acoustic startle reflex was enhanced.
It was not until 2011 that a link between severe pain, neuroinflammation and alarm pheromones release in rats was found: real time RT-PCR analysis of rat brain tissues indicated that shocking the footpad of a rat increased its production of proinflammatory cytokines in deep brain structures, namely of IL-1β, heteronuclear Corticotropin-releasing hormone and c-fos mRNA expressions in both the paraventricular nucleus and the bed nucleus of the stria terminalis, and it increased stress hormone levels in plasma (corticosterone).[63]
The neurocircuit for how rats perceive alarm pheromones was shown to be related to the hypothalamus, brainstem, and amygdalae, all of which are evolutionary ancient structures deep inside or in the case of the brainstem underneath the brain away from the cortex, and involved in the fight-or-flight response, as is the case in humans.[64]
Alarm pheromone-induced anxiety in rats has been used to evaluate the degree to which anxiolytics can alleviate anxiety in humans. For this the change in the acoustic startle reflex of rats with alarm pheromone-induced anxiety (i.e. reduction of defensiveness) has been measured. Pretreatment of rats with one of five anxiolytics used in clinical medicine was able to reduce their anxiety: namely midazolam, phenelzine (a nonselective monoamine oxidase (MAO) inhibitor), propranolol, a nonselective beta blocker, clonidine, an alpha 2 adrenergic agonist or CP-154,526, a corticotropin-releasing hormone antagonist.[65]
Faulty development of odor discrimination impairs the perception of pheromones and pheromone-related behavior, like aggressive behavior and mating in male rats: The enzyme Mitogen-activated protein kinase 7 (MAPK7) has been implicated in regulating the development of the olfactory bulb and odor discrimination and it is highly expressed in developing rat brains, but absent in most regions of adult rat brains. Conditional deletion of the MAPK7gene in mouse neural stem cells impairs several pheromone-mediated behaviors, including aggression and mating in male mice. These behavior impairments were not caused by a reduction in the level of testosterone, by physical immobility, by heightened fear or anxiety or by depression. Using mouse urine as a natural pheromone-containing solution, it has been shown that the impairment was associated with defective detection of related pheromones, and with changes in their inborn preference for pheromones related to sexual and reproductive activities.[66]
Lastly, alleviation of an acute fear response because a friendly peer (or in biological language: an affiliative conspecific) tends and befriends is called "social buffering". The term is in analogy to the 1985 "buffering" hypothesis in psychology, where social support has been proven to mitigate the negative health effects of alarm pheromone mediated distress.[67] The role of a "social pheromone" is suggested by the recent discovery that olfactory signals are responsible in mediating the "social buffering" in male rats.[68] "Social buffering" was also observed to mitigate the conditioned fear responses of honeybees.  A bee colony exposed to an environment of high threat of predation did not show increased aggression and aggressive-like gene expression patterns in individual bees, but decreased aggression. That the bees did not simply habituate to threats is suggested by the fact that the disturbed colonies also decreased their foraging.[69]
Biologists have proposed in 2012 that fear pheromones evolved as molecules of "keystone significance", a term coined in analogy to keystone species. Pheromones may determine species compositions and affect rates of energy and material exchange in an ecological community. Thus pheromones generate structure in a food web and play critical roles in maintaining natural systems.[70]
Evidence of chemosensory alarm signals in humans has emerged slowly: Although alarm pheromones have not been physically isolated and their chemical structures have not been identified in humans so far, there is evidence for their presence. Androstadienone, for example, a steroidal, endogenous odorant, is a pheromone candidate found in human sweat, axillary hair and plasma. The closely related compound androstenone is involved in communicating dominance, aggression or competition; sex hormone influences on androstenone perception in humans showed a high testosterone level related to heightened androstenone sensitivity in men, a high testosterone level related to unhappiness in response to androstenone in men, and a high estradiol level related to disliking of androstenone in women.[71]
A German study from 2006 showed when anxiety-induced versus exercise-induced human sweat from a dozen people was pooled and offered to seven study participants, of five able to olfactorily distinguish exercise-induced sweat from room air, three could also distinguish exercise-induced sweat from anxiety induced sweat. The acoustic startle reflex response to a sound when sensing anxiety sweat was larger than when sensing exercise-induced sweat, as measured by electromyograph analysis of the orbital muscle, which is responsible for the eyeblink component. This showed for the first time that fear chemosignals can modulate the startle reflex in humans without emotional mediation; fear chemosignals primed the recipient's "defensive behavior" prior to the subjects' conscious attention on the acoustic startle reflex level.[72]
In analogy to the social buffering of rats and honeybees in response to chemosignals, induction of empathy by "smelling anxiety" of another person has been found in humans.[73]
A study from 2013 provided brain imaging evidence that human responses to fear chemosignals may be gender-specific. Researchers collected alarm-induced sweat and exercise-induced sweat from donors extracted it, pooled it and presented it to 16 unrelated people undergoing functional brain MRI. While stress-induced sweat from males produced a comparably strong emotional response in both females and males, stress-induced sweat from females produced a markedly stronger arousal in women than in men. Statistical tests pinpointed this gender-specificity to the right amygdala and strongest in the superficial nuclei. Since no significant differences were found in the olfactory bulb, the response to female fear-induced signals is likely based on processing the meaning, i.e. on the emotional level, rather than the strength of chemosensory cues from each gender, i.e. the perceptual level.[74]
An approach-avoidance task was set up where volunteers seeing either an angry or a happy cartoon face on a computer screen pushed away or pulled toward them a joystick as fast as possible. Volunteers smelling anandrostadienone, masked with clove oil scent responded faster, especially to angry faces, than those smelling clove oil only, which was interpreted as anandrostadienone-related activation of the fear system.[75] A potential mechanism of action is, that androstadienone alters the "emotional face processing". Androstadienone is known to influence activity of the fusiform gyrus which is relevant for face recognition.
A drug treatment for fear conditioning and phobias via the amygdalae is the use of glucocorticoids.[76] In one study, glucocorticoid receptors in the central nuclei of the amygdalae were disrupted in order to better understand the mechanisms of fear and fear conditioning. The glucocorticoid receptors were inhibited using lentiviral vectors containing Cre-recombinase injected into mice. Results showed that disruption of the glucocorticoid receptors prevented conditioned fear behavior. The mice were subjected to auditory cues which caused them to freeze normally. However, a reduction of freezing was observed in the mice that had inhibited glucocorticoid receptors.[77]
Cognitive behavioral therapy has been successful in helping people overcome fear. Because fear is more complex than just forgetting or deleting memories, an active and successful approach involves people repeatedly confronting their fears. By confronting their fears in a safe manner a person can suppress the fear-triggering memory or stimulus.[78] Known as "exposure therapy", this practice has helped up to 90% of people with specific phobias to significantly decrease their fear over time.[40][78]
The fear of the end of life and its existence is in other words the fear of death. The fear of death ritualized the lives of our ancestors. These rituals were designed to reduce that fear; they helped collect the cultural ideas that we now have in the present.[citation needed] These rituals also helped preserve the cultural ideas. The results and methods of human existence had been changing at the same time that social formation was changing. One can say[by whom?] that the formation of communities happened because people lived in fear. The result of this fear forced people to unite to fight dangers together rather than fight alone.[citation needed]
Religions are filled with different fears that humans have had throughout many centuries. The fears aren't just metaphysical (including the problems of life and death) but are also moral. Death is seen as a boundary to another world. That world would always be different depending on how each individual lived their lives. The origins of this intangible fear are not found in the present world. In a sense we can assume that fear was a big influence on things such as morality. This assumption, however, flies in the face of concepts such as moral absolutism and moral universalism – which would hold that our morals are rooted in either the divine or natural laws of the universe, and would not be generated by any human feeling, thought or emotion.[citation needed]
Fear may be politically and culturally manipulated to persuade citizenry of ideas which would otherwise be widely rejected or dissuade citizenry from ideas which would otherwise be widely supported. In contexts of disasters, nation-states manage the fear not only to provide their citizens with an explanation about the event or blaming some minorities, but also to adjust their previous beliefs.
Fear is found and reflected in mythology and folklore as well as in works of fiction such as novels and films.
Works of dystopian and (post)apocalyptic fiction convey the fears and anxieties of societies.[79][80]
The fear of the world's end is about as old as civilization itself.[81] In a 1967 study Frank Kermode suggests that the failure of religious prophecies led to a shift in how society apprehends this ancient mode.[82] Scientific and critical thought supplanting religious and mythical thought as well as a public emancipation may be the cause of eschatology becoming replaced by more realistic scenarios. Such might constructively provoke discussion and steps to be taken to prevent depicted catastrophes.
The Story of the Youth Who Went Forth to Learn What Fear Was is a German fairy tale dealing with the topic of not knowing fear.
Many stories also include characters who fear the antagonist of the plot. One important characteristic of historical and mythical heroes across cultures is to be fearless in the face of big and often lethal enemies.[citation needed]
In the world of athletics fear is often used as a means of motivation to not fail.[83] This situation involves using fear in a way that increases the chances of a positive outcome. In this case the fear that is being created is initially a cognitive state to the receiver.[84] This initial state is what generates the first response of the athlete, this response generates a possibility of fight or flight reaction by the athlete (receiver), which in turn will increase or decrease the possibility of success or failure in the certain situation for the athlete.[85] The amount of time that the athlete has to determine this decision is small but it is still enough time for the receiver to make a determination through cognition.[86] Even though the decision is made quickly, the decision is determined through past events that have been experienced by the athlete.[87] The results of these past events will determine how the athlete will make his cognitive decision in the spilt second that he or she has.[83]
Fear of failure as described above has been studied frequently in the field of sport psychology. Many scholars have tried to determine how often fear of failure is triggered within athletes. As well as what personalities of athletes most often choose to use this type of motivation. Studies have also been conducted to determine the success rate of this method of motivation.
Murray's Exploration in Personal (1938) was one of the first studies that actually identified fear of failure as an actual motive to avoid failure or to achieve success. His studies suggested that inavoidance, the need to avoid failure, was found in many college-aged men during the time of his research in 1938.[88]  This was a monumental finding in the field of psychology because it allowed other researchers to better clarify how fear of failure can actually be a determinant of creating achievement goals as well as how it could be used in the actual act of achievement.[89]
In the context of sport, a model was created by R.S. Lazarus in 1991 that uses the cognitive-motivational-relational theory of emotion.[90]
It holds that Fear of Failure results when beliefs or cognitive schemas about aversive consequences of failing are activated by situations in which failure is possible. These belief systems predispose the individual to make appraisals of threat and experience the state anxiety that is associated with Fear of Failure in evaluative situations.[89][84]
Another study was done in 2001 by Conroy, Poczwardowski, and Henschen that created five aversive consequences of failing that have been repeated over time. The five categories include (a) experiencing shame and embarrassment, (b) devaluing one's self-estimate, (c) having an uncertain future, (d) important others losing interest, (e) upsetting important others.[83] These five categories can help one infer the possibility of an individual to associate failure with one of these threat categories, which will lead them to experiencing fear of failure.
In summary, the two studies that were done above created a more precise definition of fear of failure, which is "a dispositional tendency to experience apprehension and anxiety in evaluative situations because individuals have learned that failure is associated with aversive consequences".[89]
People who have damage to their amygdalae, such as from Urbach–Wiethe disease, are unable to experience fear.  This is not debilitating; however, a lack of fear can allow someone to get into a dangerous situation they otherwise would have avoided. Fear is an important aspect of a human being's development.[91]



Patients' rights - Wikipedia
A patient's bill of rights is a list of guarantees for those receiving medical care. It may take the form of a law or a non-binding declaration. Typically a patient's bill of rights guarantees patients information, fair treatment, and autonomy over medical decisions, among other rights.
In the UK, the Patient's Charter was introduced and revised in the 1990s. It was replaced by the NHS Constitution for England.
In the United States there have been a number of attempts to enshrine a patient's bill of rights in law, including a bill rejected by Congress in 2001.
The United States Congress considered a bill designed to safeguard patients' rights in 2001. The "Bipartisan Patient Protection Act" (S.1052), sponsored by Senators Edward Kennedy and John McCain,  contained new rules for what health maintenance organizations had to cover and granted new rights for patients to sue in state or federal courts, if they are denied needed care.[1]
The House of Representatives and Senate passed differing versions of the proposed law. Although both bills would have provided patients key rights, such as prompt access to emergency care and medical specialists, only the Senate-passed measure would provide patients with adequate means to enforce their rights.  The Senate's proposal would have conferred a broad array of rights on patients. It would have ensured that patients with health care plans had the right to:
The bill was passed by the US Senate by a vote of 59–36 in 2001,[2] it was then amended by the House of Representatives and returned to the Senate. However, it ultimately failed.
Wendell Potter, former senior executive[3] at Cigna turned whistleblower, has written that the insurance industry has worked to kill "any reform that might interfere with insurers' ability to increase profits" by engaging in extensive and well-funded, anti-reform campaigns. The industry, however, "goes to great lengths to keep its involvement in these campaigns hidden from public view," including the use of "front groups." Indeed, in a successful 1998 effort to kill the Patient Bill of Rights then,
"the insurers formed a front group
called the Health Benefits Coalition to kill efforts to pass a
Patients Bill of Rights. While it was billed as a broad-based business
coalition that was led by the National Federation of Independent Business and included the U.S. Chamber of Commerce, the Health
Benefits Coalition in reality got the lion’s share of its funding and
guidance from the big insurance companies and their trade
associations. Like most front groups, the Health Benefits Coalition was set up and run out of one of Washington’s biggest P.R. firms. The P.R. firm

provided all the staff work for the Coalition. The tactics worked. Industry allies in Congress made sure the Patients’ Bill of Rights would not become law."[4]Some have cited differences between positive rights and personal freedoms. Asserting that medical care "must be rendered under conditions that are acceptable to both patient and physician," the Association of American Physicians and Surgeons adopted a list of patient freedoms in 1990, which was modified and adopted as a 'patients' bill of rights' in 1995:
"All patients should be guaranteed the following freedoms:



War on Cancer - Wikipedia
The War on Cancer refers to the effort to find a cure for cancer by increased research to improve the understanding of cancer biology and the development of more effective cancer treatments, such as targeted drug therapies. The aim of such efforts is to eradicate cancer as a major cause of death. The signing of the National Cancer Act of 1971 by United States president Richard Nixon is generally viewed as the beginning of this effort, though it was not described as a "war" in the legislation itself.[1]
Despite significant progress in the treatment of certain forms of cancer (such as childhood leukemia[2]), cancer in general remains a major cause of death 40+ years after this war on cancer began,[3] leading to a perceived lack of progress[4][5][6] and to new legislation aimed at augmenting the original National Cancer Act of 1971.[7]
New research directions, in part based on the results of the Human Genome Project, hold promise for a better understanding of the genetic factors underlying cancer, and the development of new diagnostics, therapies, preventive measures, and early detection ability. However, targeting cancer proteins can be difficult, as a protein can be undruggable.
The war on cancer began with the National Cancer Act of 1971, a United States federal law.[9]
The act was intended "to amend the Public Health Service Act so as to strengthen the National Cancer Institute in order to more effectively carry out the national effort against cancer".[1]
It was signed into law by President Nixon on December 23, 1971.[10]
Health activist and philanthropist Mary Lasker was instrumental in persuading the United States Congress to pass the National Cancer Act.[11] She and her husband Albert Lasker were strong supporters of medical research. They established the Lasker Foundation which awarded people for their research. In the year of 1943, Mary Lasker began changing the American Cancer Society to get more funding for research.  Five years later she contributed to getting federal funding for the National Cancer Institute and the National Heart Institute. In 1946 the funding was around $2.8 million and had grown to over $1.4 billion by 1972.  In addition to all of these accomplishments, Mary became the president of the Lasker Foundation due to the death of her husband in 1952. Lasker's devotion to medical research and experience in the field eventually contributed to the passing of the National Cancer Act.[12]
The improved funding for cancer research has been quite beneficial over the last 40 years.  In 1971, the number of survivors in the U.S. was 3 million and as of 2007 has increased to more than 12 million.[13]
In 2003, Andrew von Eschenbach, the director of the National Cancer Institute (who served as FDA Commissioner from 2006-2009 and is now a Director at biotechnology company BioTime) issued a challenge "to eliminate the suffering and death from cancer, and to do so by 2015".[14][15]
This was supported by the American Association for Cancer Research in 2005[16]
though some scientists felt this goal was impossible to reach and undermined von Eschenbach's credibility.[17]
John E. Niederhuber, who succeeded Andrew von Eschenbach as NCI director, noted that cancer is a global health crisis, with 12.9 million new cases diagnosed in 2009 worldwide and that by 2030, this number could rise to 27 million including 17 million deaths "unless we take more pressing action."[18]
Harold Varmus, former director of the NIH and current director of the NCI,[19][20]
held a town hall meeting in 2010[21]
in which he outlined his priorities for improving the cancer research program, including the following: 
Recent years have seen an increased perception of a lack of progress in the war on cancer, and renewed motivation to confront the disease.[5][22]
On July 15, 2008, the United States Senate Committee on Health, Education, Labor, and Pensions convened a panel discussion titled, Cancer: Challenges and Opportunities in the 21st Century.[23]
It included interviews with noted cancer survivors such as Arlen Specter, Elizabeth Edwards and Lance Armstrong, who came out of retirement in 2008, returning to competitive cycling "to raise awareness of the global cancer burden."[24]
The Livestrong Foundation created the Livestrong Global Cancer Campaign to address the burden of cancer worldwide and encourage nations to make commitments to battle the disease and provide better access to care.[25]
In April 2009, the foundation announced that the Hashemite Kingdom of Jordan pledged $300 million to fund three important cancer control initiatives – building a cutting-edge cancer treatment and research facility, developing a national cancer control plan and creating an Office of Advocacy and Survivorship.[26]
The Livestrong Foundation encourages similar commitments from other nations to combat the disease.
Livestrong Day is an annual event established by the LAF to serve as "a global day of action to raise awareness about the fight against cancer." Individuals from around the world are encouraged to host cancer-oriented events in their local communities and then register their events with the Livestrong website.[27]
The US Senate on 26 March 2009 issued a new bill (S. 717), the 21st Century Cancer Access to Life-Saving Early detection, Research and Treatment (ALERT) Act[28]
intended to "overhaul the 1971 National Cancer Act."[7]
The bill aims to improve patient access to prevention and early detection by:
During their 2008 U.S. presidential campaign then Senators Barack Obama and Joe Biden published a plan to combat cancer that entailed doubling "federal funding for cancer research within 5 years, focusing on NIH and NCI" as well as working "with Congress to increase funding for the 
Food and Drug Administration."[29][30]
Their plan would provide additional funding for:
President Obama's 2009 economic stimulus package includes $10 billion for the NIH, which funds much of the cancer research in the U.S., and he has pledged to increase federal funding for cancer research by a third for the next two years as part of a drive to find "a cure for cancer in our time."[31][32]
In a message published in the July 2009 issue of Harper's Bazaar, President Obama described his mother's battle with ovarian cancer and, noting the additional funding his administration has slated for cancer research, stated: "Now is the time to commit ourselves to waging a war against cancer as aggressive as the war cancer wages against us."[33]
On 30 September 2009, Obama announced that $1 billion of a $5 billion medical research spending plan would be earmarked for research into the genetic causes of cancer and targeted cancer treatments.[34]
Cancer-related federal spending of money from the 2009 Recovery Act can be tracked online.[35]
The International Union Against Cancer (UICC) has organized a World Cancer campaign in 2009 with the theme, "I love my healthy active childhood," to promote healthy habits in children and thereby reduce their lifestyle-based cancer risk as adults.[36]
The World Health Organization is also promoting this campaign[37]
and joins with the UICC in annually promoting World Cancer Day on 4 February.[38]
Though there has been significant progress in the understanding of cancer biology, risk factors, treatments, and prognosis of some types of cancer (such as childhood leukemia[2]) since the inception of the National Cancer Act of 1971, progress in reducing the overall cancer mortality rate has been disappointing.[5][32] Many types of cancer remain largely incurable (such as pancreatic cancer[39])
and the overall death rate from cancer has not decreased appreciably since the 1970s.[40]
The death rate for cancer in the U.S., adjusted for population size and age, dropped only 5 percent from 1950 to 2005.[3]
Cancer was expected to surpass cardiovascular disease as the leading cause of death in the world by 2010, according to the World Health Organization's World Cancer Report 2008.[41][42] As of 2012, WHO reported 8.2 million annual deaths from cancer[43]
Heart disease (including both Ischaemic and hypertensive) accounted for 8.5 million annual deaths.  Stroke accounted for 6.7 million annual deaths.
[44]
There is evidence for progress in reducing cancer mortality.[45] Age-specific analysis of cancer mortality rates has had progress in reducing cancer mortality in the United States since 1955. An August 2009 study found that age-specific cancer mortality rates have been steadily declining since the early 1950s for individuals born since 1925, with the youngest age groups experiencing the steepest decline in mortality rate at 25.9 percent per decade, and the oldest age groups experiencing a 6.8 percent per decade decline.[46]
Dr. Eric Kort, the lead author of this study, claims that public reports often focus on cancer incidence rates and underappreciate the progress that has been achieved in reduced cancer mortality rates.[47]
The effectiveness and expansion of available therapies has seen significant improvements since the 1970s. For example, lumpectomy replaced more invasive mastectomy surgery for the treatment of breast cancer.[48]
Treatment of childhood leukemia[2] and chronic myeloid leukemia (CML) have undergone major advances since the war on cancer began. The drug Gleevec now cures most CML patients, compared to previous therapy with interferon, which extended life for approximately 1 year in only 20-30 percent of patients.[49]
Dr. Steven Rosenberg, chief of surgery at the NCI has said that as of the year 2000, 50% of all diagnosed cases of cancer are curable
through a combination of surgery, radiation, and chemotherapy.[48][50] Cancer surveillance experts have reported a 15.8 percent decrease in the age-standardized death rate from all cancers combined between 1991 and 2006 along with an approximately  1 percent annual decrease in the rate of new diagnoses between 1999 and 2006.[45] A large portion of this decreased mortality for men was attributable to smoking cessation efforts in the United States.
A 2010 report from the American Cancer Society found that death rates for all cancers combined decreased 1.3% per year from 2001 to 2006 in males and 0.5% per year from 1998 to 2006 in females, largely due to decreases in the 3 major cancer sites in men (lung, prostate, and colorectum) and 2 major cancer sites in women (breast and colorectum). Cancer death rates between 1990 and 2006 for all races combined decreased by 21.0% among men and by 12.3% among women. This reduction in the overall cancer death rates translates to the avoidance of approximately 767,000 deaths from cancer over the 16-year period. Despite these reductions, the report noted, cancer still accounts for more deaths than heart disease in persons younger than 85 years.[51][52]
An improvement in the number of cancer survivors living in the U.S. was indicated in a 2011 report by the CDC and the NCI, which noted that the number of cancer survivors in 2007 (11.7 million) increased by 19% from 2001 (9.8 million survivors). The number of cancer survivors in 1971 was 3 million. Breast, prostate, and colorectal cancers were the most common types of cancer among survivors, accounting for 51% of diagnoses. As of January 1, 2007, an estimated 64.8% of cancer survivors had lived ≥5 years after their diagnosis of cancer, and 59.5% of survivors were aged ≥65 years.[53][54] A continued decline in cancer rates in the U.S. among both women and men, across most major racial groups, and in the most common cancer sites (lung, colon and rectum), was indicated in a 2013 report by the National Cancer Institute. However, the same report indicated an increase from 2000 to 2009 in cancers of the liver, pancreas and uterus.[55]
A multitude of factors have been cited as impeding progress in finding a cure for cancer[5][6] and key areas have been identified and suggested as important to accelerate progress in cancer research.[56]
Since there are many different forms of cancer with distinct causes, each form requires different treatment approaches. However, this research could still lead to therapies and cures for many forms of cancer. Some of the factors that have posed challenges for the development of preventive measures and anti-cancer drugs and therapies include the following:
“The public is so jaded by cancer research media attention at the moment... And let’s face it, rather embarrassingly, most claimed ‘breakthroughs’ are not proving to significantly advance cancer therapies... It is a real conundrum for researchers today, because ‘early publicity’ is needed for funding, capital raising and professional kudos, but not too helpful for the public who then think that an immediate cure might be just around the corner.” Professor Brendon Coventry, 9 July 2013[70]
The rise of a new class of molecular technologies developed during the Human Genome Project opens up new ways to study cancer and holds the promise for the discovery of new aspects of cancer biology that could eventually lead to novel, more effective diagnostics and therapies for cancer patients.[71]
[72]
[73]
These new technologies are capable of screening many biomolecules and genetic variations such as SNPs[74]
and copy number variations in a single experiment and are employed within functional genomics and personalized medicine studies.
Speaking on the occasion of the announcement of $1 billion in new funding for genome-based cancer research, Dr. Francis Collins, director of the NIH claimed, "We are about to see a quantum leap in our understanding of cancer."[34] Harold Varmus, after his appointment to be the director of the NCI, said we are in a "golden era for cancer research," poised to profit from advances in our understanding of the cancer genome.[20]
High-throughput DNA sequencing has been used to study the whole genome sequence of two different cancer tissues: a small-cell lung cancer metastasis and a malignant melanoma cell line.[75]
The sequence information provides a comprehensive catalog of approximately 90% of the somatic mutations in the cancerous tissue, providing a more detailed molecular and genetic understanding of cancer biology than was previously possible, and offering hope for the development of new therapeutic strategies gleaned from these insights.[76][77]
The Cancer Genome Atlas (TCGA), a collaborative effort between the National Cancer Institute and the National Human Genome Research Institute, is an example of a basic research project that is employing some of these new molecular approaches.[78]
One TCGA publication notes the following:
Here we report the interim integrative analysis of DNA copy number, gene expression and DNA methylation aberrations in 206 glioblastomas...Together, these findings establish the feasibility and power of TCGA, demonstrating that it can rapidly expand knowledge of the molecular basis of cancer.[79]
In a cancer research funding announcement made by president Obama in September 2009, TCGA project is slated to receive $175 million in funding to collect comprehensive gene sequence data on 20,000 tissue samples from people with more than 20 different types of cancer, in order to help researchers understand the genetic changes underlying cancer. New, targeted therapeutic approaches are expected to arise from the insights resulting from such studies.[34]
The Cancer Genome Project at the Wellcome Trust Sanger Institute aims to identify sequence variants/mutations critical in the development of human cancers. The Cancer Genome Project combines knowledge of the human genome sequence with high throughput mutation detection techniques.[80]
Advances in information technology supporting cancer research, such as the NCI's caBIG project, promise to improve data sharing among cancer researchers and accelerate "the discovery of new approaches for the detection, diagnosis, treatment, and prevention of cancer, ultimately improving patient outcomes."[81]
Researchers are considering ways to improve the efficiency, cost-effectiveness, and overall success rate of cancer clinical trials.[82]
Increased participation in rigorously designed clinical trials would increase the pace of research.  Currently, about 3% of people with cancer participate in clinical trials; more than half of them are patients for whom no other options are left, patients who are participating in "exploratory" trials designed to burnish the researchers' résumés or promote a drug rather than to produce meaningful information, or in trials that will not enroll enough patients to produce a statistically significant result.[citation needed]
A major challenge in cancer treatment is to find better ways to specifically target tumors with drugs and chemotherapeutic agents in order to provide a more effective, localized dose and to minimize exposure of healthy tissue in other parts of the body to the potentially adverse effects of the treatments. The accessibility of different tissues and organs to anti-tumor drugs contributes to this challenge. For example, the blood–brain barrier blocks many drugs that may otherwise be effective against brain tumors. In November 2009, a new, experimental therapeutic approach for treating glioblastoma was published in which the anti-tumor drug Avastin was delivered to turmor site within the brain through the use of microcatheters, along with mannitol to temporarily open the blood–brain barrier permitting delivery of the chemotherapy into the brain.[83][84]
An important aspect to the war on cancer is improving public access to educational and supportive resources, to provide individuals with the latest information about cancer prevention and treatment, as well as access to support communities. Resources have been created by governmental and other organizations to provide support for cancer patients, their families and caregivers, to help them share information and find advice to guide decision making.[85][86][87][88][89][90][91]



Alternative cancer treatments - Wikipedia

Alternative cancer treatments are alternative or complementary treatments for cancer that have not been approved by the government agencies responsible for the regulation of therapeutic goods.  They include diet and exercise, chemicals, herbs, devices, and manual procedures. The treatments are not supported by evidence, either because no proper testing has been conducted, or because testing did not demonstrate statistically significant efficacy.  Concerns have been raised about the safety of some of them. Some treatments that have been proposed in the past have been found in clinical trials to be useless or unsafe.  Some of these obsolete or disproven treatments continue to be promoted, sold, and used. Promoting or marketing such treatments is illegal in most of the developed world including the United States and European Union.
A distinction is typically made between complementary treatments which do not disrupt conventional medical treatment, and alternative treatments which may replace conventional treatment. 
Alternative cancer treatments are typically contrasted with experimental cancer treatments – which are treatments for which experimental testing is underway – and with complementary treatments, which are non-invasive practices used alongside other treatment. All approved chemotherapeutic cancer treatments were considered experimental cancer treatments before their safety and efficacy testing was completed.
Since the 1940s, medical science has developed chemotherapy, radiation therapy, adjuvant therapy and the newer targeted therapies, as well as refined surgical techniques for removing cancer.  Before the development of these modern, evidence-based treatments, 90% of cancer patients died within five years.[2]  With modern mainstream treatments, only 34% of cancer patients die within five years.[3]  However, while mainstream forms of cancer treatment generally prolong life or permanently cure cancer, most treatments also have side effects ranging from unpleasant to fatal, such as pain, blood clots, fatigue, and infection.[4] These side effects and the lack of a guarantee that treatment will be successful create appeal for alternative treatments for cancer, which purport to cause fewer side effects or to increase survival rates despite evidence to suggest a 2–5 fold increase in death with alternative medicines.[5]
Alternative cancer treatments have typically not undergone properly conducted, well-designed clinical trials, or the results have not been published due to publication bias (a refusal to publish results of a treatment outside that journal's focus area, guidelines or approach).  Among those that have been published, the methodology is often poor.  A 2006 systematic review of 214 articles covering 198 clinical trials of alternative cancer treatments concluded that almost none conducted dose-ranging studies, which are necessary to ensure that the patients are being given a useful amount of the treatment.[6]  These kinds of treatments appear and vanish frequently, and have throughout history.[7]
Complementary and alternative cancer treatments are often grouped together, in part because of the adoption of the phrase "complementary and alternative medicine" by the United States Congress.[8] However, according to Barrie R. Cassileth, in cancer treatment the distinction between complementary and alternative therapies is "crucial".[7]
Complementary treatments are used in conjunction with proven mainstream treatments. They tend to be pleasant for the patient, not involve substances with any pharmacological effects, inexpensive, and intended to treat side effects rather than to kill cancer cells.[9] Medical massage and self-hypnosis to treat pain are examples of complementary treatments.
About half the practitioners who dispense complementary treatments are physicians, although they tend to be generalists rather than oncologists. As many as 60% of American physicians have referred their patients to a complementary practitioner for some purpose.[7]
Alternative treatments, by contrast, are used in place of mainstream treatments. The most popular alternative cancer therapies include restrictive diets, mind-body interventions, bioelectromagnetics, nutritional supplements, and herbs.[7] The popularity and prevalence of different treatments varies widely by region.[10]  While conventional physicians should always be kept aware of any complementary treatments used by a patient, many physicians in the United Kingdom are at least tolerant of their use, and some might recommend them.[11]
Survey data about how many cancer patients use alternative or complementary therapies vary from nation to nation as well as from region to region. A 2000 study published by the European Journal of Cancer evaluated a sample of 1023 women from a British cancer registry suffering from breast cancer and found that 22.4% had consulted with a practitioner of complementary therapies in the previous twelve months. The study concluded that the patients had spent many thousands of pounds on such measures and that use "of practitioners of complementary therapies following diagnosis is a significant and possibly growing phenomenon".[12]
In Australia, one study reported that 46% of children suffering from cancer have been treated with at least one non-traditional therapy. Further 40% of those of any age receiving palliative care had tried at least one such therapy. Some of the most popular alternative cancer treatments were found to be dietary therapies, antioxidants, high dose vitamins, and herbal therapies.[13]
Use of unconventional cancer treatments in the United States has been influenced by the U.S. federal government's National Center for Complementary and Alternative Medicine (NCCAM), initially known as the Office of Alternative Medicine (OAM), which was established in 1992 as a National Institutes of Health (NIH) adjunct by the U.S. Congress. Over thirty American medical schools have offered general courses in alternative medicine, including the Georgetown, Columbia, and Harvard university systems, among others.[7]
People who choose alternative treatments tend to believe that evidence-based medicine is extremely invasive or ineffective, while still believing that their own health could be improved.[14]  They are loyal to their alternative healthcare providers and believe that "treatment should concentrate on the whole person".[14]
Cancer patients who choose alternative treatments instead of conventional treatments believe themselves less likely to die than patients who choose only conventional treatments.[15]  They feel a greater sense of control over their destinies, and report less anxiety and depression.[15]  They are more likely to engage in benefit finding, which is the psychological process of adapting to a traumatic situation and deciding that the trauma was valuable, usually because of perceived personal and spiritual growth during the crisis.[16]
However, patients who use alternative treatments have a poorer survival time, even after controlling for type and stage of disease.[17] In 2017, researchers at Yale School of Medicine published a paper which suggested that people who choose alternative medicine over conventional cancer treatments were more than twice as likely to die within five years of diagnosis. And specifically, in those with breast cancer, people choosing alternative medicine were 5.68 times more likely to die within five years.[18]
The reason that patients using alternative treatments die sooner may be because patients who accurately perceive that they are likely to survive do not attempt unproven remedies, and patients who accurately perceive that they are unlikely to survive are attracted to unproven remedies.[17]  Among patients who believe their condition to be untreatable by evidence-based medicine, "desperation drives them into the hands of anyone with a promise and a smile."[19] Con artists have long exploited patients' perceived lack of options to extract payments for ineffectual and even harmful treatments.[19]
In a survey of American cancer patients, Baby Boomers were more likely to support complementary and alternative treatments than people from an older generation.[20]  White, female, college-educated patients who had been diagnosed more than a year ago were more likely than others to report a favorable impression of at least some complementary and alternative benefits.[20]
Many therapies have been (and continue to be) promoted to treat or prevent cancer in humans but lack good scientific and medical evidence of effectiveness.  In many cases, there is good scientific evidence that the alleged treatments do not work. Unlike accepted cancer treatments, unproven and disproven treatments are generally ignored or avoided by the medical community, and are often pseudoscientific.[21]
Despite this, many of these therapies have continued to be promoted as effective, particularly by promoters of alternative medicine. Scientists consider this practice quackery,[22][23] and some of those engaged in it have been investigated and prosecuted by public health regulators such as the US Federal Trade Commission,[24] the Mexican Secretariat of Health[25] and the Canadian Competition Bureau.[26] In the United Kingdom, the Cancer Act makes the unauthorized promotion of cancer treatments a criminal offense.[27][28]
Most studies of complementary and alternative medicine in the treatment of cancer pain are of low quality in terms of scientific evidence. Studies of massage therapy have produced mixed results, but overall show some temporary benefit for reducing pain, anxiety, and depression and a very low risk of harm, unless the patient is at risk for bleeding disorders.[34][35]  There is weak evidence for a modest benefit from hypnosis, supportive psychotherapy and cognitive therapy.  Results about Reiki and touch therapy were inconclusive.  The most studied such treatment, acupuncture, has demonstrated no benefit as an adjunct analgesic in cancer pain. The evidence for music therapy is equivocal, and some herbal interventions such as PC-SPES, mistletoe, and saw palmetto are known to be toxic to some cancer patients. The most promising evidence, though still weak, is for mind–body interventions such as biofeedback and relaxation techniques.[36]
As stated in the scientific literature, the measures listed below are defined as 'complementary' because they are applied in conjunction with mainstream anti-cancer measures such as chemotherapy, in contrast to the ineffective therapies viewed as 'alternative' since they are offered as substitutes for mainstream measures.[7]
Some alternative cancer treatments are based on unproven or disproven theories of how cancer begins or is sustained in the body.  Some common concepts are:
Government agencies around the world routinely investigate purported alternative cancer treatments in an effort to protect their citizens from fraud and abuse.
In 2008, the United States Federal Trade Commission acted against companies that made unsupported claims that their products, some of which included highly toxic chemicals, could cure cancer.[46] Targets included Omega Supply, Native Essence Herb Company,  Daniel Chapter One, Gemtronics, Inc., Herbs for Cancer, Nu-Gen Nutrition, Inc., Westberry Enterprises, Inc., Jim Clark's All Natural Cancer Therapy, Bioque Technologies, Inc., Cleansing Time Pro, and Premium-essiac-tea-4less.



Just-world hypothesis - Wikipedia
The just-world hypothesis or just-world fallacy is the cognitive bias (or assumption) that a person's actions are inherently inclined to bring morally fair and fitting consequences to that person, to the end of all noble actions being eventually rewarded and all evil actions eventually punished. In other words, the just-world hypothesis is the tendency to attribute consequences to—or expect consequences as the result of—a universal force that restores moral balance. This belief generally implies the existence of cosmic justice, destiny, divine providence, desert, stability, or order, and has high potential to result in fallacy, especially when used to rationalize people's misfortune on the grounds that they "deserve" it.
The hypothesis popularly appears in the English language in various figures of speech that imply guaranteed negative reprisal, such as: "you got what was coming to you", "what goes around comes around", "chickens come home to roost", everything happens for a reason, and "you reap what you sow". This hypothesis has been widely studied by social psychologists since Melvin J. Lerner conducted seminal work on the belief in a just world in the early 1960s.[1] Research has continued since then, examining the predictive capacity of the hypothesis in various situations and across cultures, and clarifying and expanding the theoretical understandings of just-world beliefs.[2]
Many philosophers and social theorists have observed and considered the phenomenon of belief in a just world, going back to at least as early as the Pyrrhonist philosopher Sextus Empiricus writing around 180 CE who argued against this belief.[3] Lerner's work made the just-world hypothesis a focus of research in the field of social psychology.
Lerner was prompted to study justice beliefs and the just-world hypothesis in the context of social psychological inquiry into negative social and societal interactions.[4] Lerner saw his work as extending Stanley Milgram's work on obedience. He sought to answer the questions of how regimes that cause cruelty and suffering maintain popular support, and how people come to accept social norms and laws that produce misery and suffering.[5]
Lerner's inquiry was influenced by repeatedly witnessing the tendency of observers to blame victims for their suffering. During his clinical training as a psychologist, he observed treatment of mentally ill persons by the health care practitioners with whom he worked. Although he knew them to be kindhearted, educated people, they often blamed patients for the patients' own suffering.[6] Lerner also describes his surprise at hearing his students derogate (disparage, belittle) the poor, seemingly oblivious to the structural forces that contribute to poverty.[4] In a study on rewards, he observed that when one of two men was chosen at random to receive a reward for a task, that caused him to be more favorably evaluated by observers, even when the observers had been informed that the recipient of the reward was chosen at random.[7][8] Existing social psychological theories, including cognitive dissonance, could not fully explain these phenomena.[8] The desire to understand the processes that caused these phenomena led Lerner to conduct his first experiments on what is now called the just-world hypothesis.
In 1966, Lerner and his colleagues began a series of experiments that used shock paradigms to investigate observer responses to victimization. In the first of these experiments conducted at the University of Kansas, 72 female subjects were made to watch a confederate receiving electrical shocks under a variety of conditions. Initially, subjects were upset by observing the apparent suffering. But as the suffering continued and observers remained unable to intervene, the observers began to derogate the victim. Derogation was greater when the observed suffering was greater. But when subjects were told the victim would receive compensation for her suffering, subjects did not derogate the victim.[5] Lerner and colleagues replicated these findings in subsequent studies, as did other researchers.[7]
To explain these studies' findings, Lerner theorized that there was a prevalent belief in a just world. A just world is one in which actions and conditions have predictable, appropriate consequences. These actions and conditions are typically individuals' behaviors or attributes. The specific conditions that correspond to certain consequences are socially determined by a society's norms and ideologies. Lerner presents the belief in a just world as functional: it maintains the idea that one can influence the world in a predictable way. Belief in a just world functions as a sort of "contract" with the world regarding the consequences of behavior. This allows people to plan for the future and engage in effective, goal-driven behavior. Lerner summarized his findings and his theoretical work in his 1980 monograph The Belief in a Just World: A Fundamental Delusion.[6]
Lerner hypothesized that the belief in a just world is crucially important for people to maintain for their own well-being. But people are confronted daily with evidence that the world is not just: people suffer without apparent cause. Lerner explained that people use strategies to eliminate threats to their belief in a just world. These strategies can be rational or irrational. Rational strategies include accepting the reality of injustice, trying to prevent injustice or provide restitution, and accepting one's own limitations. Non-rational strategies include denial, withdrawal, and reinterpretation of the event.[citation needed]
There are a few modes of reinterpretation that could make an event fit the belief in a just world. One can reinterpret the outcome, the cause, and/or the character of the victim.  In the case of observing the injustice of the suffering of innocent people, one major way to rearrange the cognition of an event is to interpret the victim of suffering as deserving.[1] Specifically, observers can blame victims for their suffering on the basis of their behaviors and/or their characteristics.[7] Much psychological research on the belief in a just world has focused on these negative social phenomena of victim blaming and victim derogation in different contexts.[2]
An additional effect of this thinking is that individuals experience less personal vulnerability because they do not believe they have done anything to deserve or cause negative outcomes.[2] This is related to the self-serving bias observed by social psychologists.[9]
Many researchers have interpreted just-world beliefs as an example of causal attribution. In victim blaming, the causes of victimization are attributed to an individual rather than to a situation. Thus, the consequences of belief in a just world may be related to or explained in terms of particular patterns of causal attribution.[10]
Others have suggested alternative explanations for the derogation of victims. One suggestion is that derogation effects are based on accurate judgments of a victim's character. In particular, in relation to Lerner's first studies, some have hypothesized that it would be logical for observers to derogate an individual who would allow himself to be shocked without reason.[11] A subsequent study by Lerner challenged this alternative hypothesis by showing that individuals are only derogated when they actually suffer; individuals who agreed to undergo suffering but did not were viewed positively.[12]
Another alternative explanation offered for the derogation of victims early in the development of the just-world hypothesis was that observers derogate victims to reduce their own feelings of guilt. Observers may feel responsible, or guilty, for a victim's suffering if they themselves are involved in the situation or experiment. In order to reduce the guilt, they may devalue the victim.[13][14][15] Lerner and colleagues claim that there has not been adequate evidence to support this interpretation. They conducted one study that found derogation of victims occurred even by observers who were not implicated in the process of the experiment and thus had no reason to feel guilty.[7]
Alternatively, victim derogation and other strategies may only be ways to alleviate discomfort after viewing suffering. This would mean that the primary motivation is not to restore a belief in a just world, but to reduce discomfort caused by empathizing.  Studies have shown that victim derogation does not suppress subsequent helping activity and that empathizing with the victim plays a large role when assigning blame. According to Ervin Staub,[16] devaluing the victim should lead to lesser compensation if restoring belief in a just world was the primary motive; instead, there is virtually no difference in compensation amounts whether the compensation precedes or follows devaluation. Psychopathy has been linked to the lack of just-world maintaining strategies, possibly due to dampened emotional reactions and lack of empathy.[17]
After Lerner's first studies, other researchers replicated these findings in other settings in which individuals are victimized. This work, which began in the 1970s and continues today, has investigated how observers react to victims of random calamities like traffic accidents, as well as rape and domestic violence, illnesses, and poverty.[1] Generally, researchers have found that observers of the suffering of innocent victims tend to both derogate and blame victims for their suffering. Observers thus maintain their belief in a just world by changing their cognitions about the victims' character.[18]
In the early 1970s, social psychologists Zick Rubin and Letitia Anne Peplau developed a measure of belief in a just world.[19] This measure and its revised form published in 1975 allowed for the study of individual differences in just-world beliefs.[20] Much of the subsequent research on the just-world hypothesis used these measurement scales.
Researchers have looked at how observers react to victims of rape and other violence. In a formative experiment on rape and belief in a just world by Linda Carli and colleagues, researchers gave two groups of subjects a narrative about interactions between a man and a woman. The description of the interaction was the same until the end; one group received a narrative that had a neutral ending and the other group received a narrative that ended with the man raping the woman. Subjects judged the rape ending as inevitable and blamed the woman in the narrative for the rape on the basis of her behavior, but not her characteristics.[21] These findings have been replicated repeatedly, including using a rape ending and a 'happy ending' (a marriage proposal).[2][22]
Other researchers have found a similar phenomenon for judgments of battered partners. One study found that observers' labels of blame of female victims of relationship violence increase with the intimacy of the relationship. Observers blamed the perpetrator only in the most significant case of violence, in which a male struck an acquaintance.[23]
Researchers have employed the just-world hypothesis to understand bullying. Given other research on beliefs in a just world, it would be expected that observers would derogate and blame bullying victims, but the opposite has been found: individuals high in just-world belief have stronger anti-bullying attitudes.[24] Other researchers have found that strong belief in a just world is associated with lower levels of bullying behavior.[25] This finding is in keeping with Lerner's understanding of belief in a just world as functioning as a "contract" that governs behavior.[6] There is additional evidence that belief in a just world is protective of the well-being of children and adolescents in the school environment,[26] as has been shown for the general population.
Other researchers have found that observers judge sick people as responsible for their illnesses. One experiment showed that persons suffering from a variety of illnesses were derogated on a measure of attractiveness more than healthy individuals were. In comparison to healthy people, victim derogation was found for persons presenting with indigestion, pneumonia, and stomach cancer. Moreover, derogation was found to be higher for those suffering from severer illnesses, except for those presenting with cancer.[27] Stronger belief in a just world has also been found to correlate with greater derogation of AIDS victims.[28]
More recently, researchers have explored how people react to poverty through the lens of the just-world hypothesis. Strong belief in a just world is associated with blaming the poor, with weak belief in a just world associated with identifying external causes of poverty including world economic systems, war, and exploitation.[29][30]
Some research on belief in a just world has examined how people react when they themselves are victimized. An early paper by Dr. Ronnie Janoff-Bulman found that rape victims often blame their own behavior, but not their own characteristics, for their victimization.[31] It was hypothesized that this may be because blaming one's own behavior makes an event more controllable.
These studies on victims of violence, illness, and poverty and others like them have provided consistent support for the link between observers' just-world beliefs and their tendency to blame victims for their suffering.[1] As a result, the existence of the just-world hypothesis as a psychological phenomenon has become widely accepted.
Subsequent work on measuring belief in a just world has focused on identifying multiple dimensions of the belief. This work has resulted in the development of new measures of just-world belief and additional research.[2] Hypothesized dimensions of just-world beliefs include belief in an unjust world,[32] beliefs in immanent justice and ultimate justice,[33] hope for justice, and belief in one's ability to reduce injustice.[34]  Other work has focused on looking at the different domains in which the belief may function; individuals may have different just-world beliefs for the personal domain, the sociopolitical domain, the social domain, etc.[28] An especially fruitful distinction is between the belief in a just world for the self (personal) and the belief in a just world for others (general). These distinct beliefs are differentially associated with positive mental health.[35]
Researchers have used measures of belief in a just world to look at correlates of high and low levels of belief in a just world.
Limited studies have examined ideological correlates of the belief in a just world. These studies have found sociopolitical correlates of just-world beliefs, including right-wing authoritarianism and the protestant work ethic.[36][37] Studies have also found belief in a just world to be correlated with aspects of religiousness.[38][39]
Studies of demographic differences, including gender and racial differences, have not shown systematic differences, but do suggest racial differences, with blacks and African Americans having the lowest levels of belief in a just world.[40][41]
The development of measures of just-world beliefs has also allowed researchers to assess cross-cultural differences in just-world beliefs. Much research conducted shows that beliefs in a just world are evident cross-culturally. One study tested beliefs in a just world of students in 12 countries. This study found that in countries where the majority of inhabitants are powerless, belief in a just world tends to be weaker than in other countries.[42] This supports the theory of the just-world hypothesis because the powerless have had more personal and societal experiences that provided evidence that the world is not just and predictable.[43][clarification needed]
Belief in unjust world has been linked to increased self-handicapping, criminality, defensive coping, anger and perceived future risk. It may also serve as ego-protective belief for certain individuals by justifying maladaptive behavior.[2][44][45]
Although much of the initial work on belief in a just world focused on its negative social effects, other research suggests that belief in a just world is good, and even necessary, for mental health.[46] Belief in a just world is associated with greater life satisfaction and well-being and less depressive affect.[35][47] Researchers are actively exploring the reasons why the belief in a just world might have this relationship to mental health; it has been suggested that such beliefs could be a personal resource or coping strategy that buffers stress associated with daily life and with traumatic events.[48] This hypothesis suggests that belief in a just world can be understood as a positive illusion.[49]
Some studies also show that beliefs in a just world are correlated with internal locus of control.[20] Strong belief in a just world is associated with greater acceptance of and less dissatisfaction with negative events in one's life.[48] This may be one way in which belief in a just world affects mental health. Others have suggested that this relationship holds only for beliefs in a just world for oneself. Beliefs in a just world for others are related instead to the negative social phenomena of victim blaming and victim derogation observed in other studies.[50]
More than 40 years after Lerner's seminal work on belief in a just world, researchers continue to study the phenomenon. Work continues primarily in the United States, Europe, Australia, and Asia.[8] Researchers in Germany have contributed disproportionately to recent research.[4] Their work resulted in a volume edited by Lerner and German researcher Leo Montada titled Responses to Victimizations and Belief in a Just World.[51]



Equal Employment Opportunity Commission - Wikipedia
The U.S. Equal Employment Opportunity Commission (EEOC) is a federal agency that administers and enforces civil rights laws against workplace discrimination. The EEOC investigates discrimination complaints based on an individual's race, children, national origin, religion, sex, age, disability, sexual orientation, gender identity, genetic information, and retaliation for reporting, participating in, and/or opposing a discriminatory practice.[3]
On March 6, 1961, President John F. Kennedy signed Executive Order 10925, which required government contractors to "take affirmative action to ensure that applicants are employed and that employees are treated during employment without regard to their race, creed, color, or national origin."[4] It established the President's Committee on Equal Employment Opportunity of which then Vice President Lyndon Johnson was appointed to head. This was the forerunner of the EEOC.
The EEOC was established on July 2, 1965; its mandate is specified under Title VII of the Civil Rights Act of 1964, the Age Discrimination in Employment Act of 1967 (ADEA),[5] the Rehabilitation Act of 1973, the Americans with Disabilities Act (ADA) of 1990, and the ADA Amendments Act of 2008. The EEOC's first complainants were female flight attendants.[6] However, the EEOC at first ignored sex discrimination complaints, and the prohibition against sex discrimination in employment went unenforced for the next few years.[7] One EEOC director called the prohibition "a fluke... conceived out of wedlock."[7]
All Commission seats and the post of general counsel to the commission are filled by the US President, subject to confirmation by the Senate.[8] Stuart J. Ishimaru, a Commissioner who was confirmed in 2003 and 2006,[9] served as Acting Chair of the Commission from January 20, 2009 until December 22, 2010, when the Senate confirmed Jacqueline Berrien to be the chairwoman. She had been nominated as chairwoman by President Barack Obama in July 2009.[10] In September 2009, Obama chose Chai Feldblum to fill another vacant seat.[11]
On March 27, 2010, President Obama made recess appointments of three Commission posts: Berrien, Feldblum, and Victoria Lipnic. With the appointments, the Commission had its full five Commissioners: Ishimaru, Berrien, Feldblum, Lipnic, and Constance Barker, who was confirmed by the Senate in 2008 to be a Commissioner. President Obama also made a recess appointment of P. David Lopez to be the EEOC's General Counsel.[12]
On December 22, 2010, the Senate gave full confirmation to Berrien, Feldblum, Lipnic, and Lopez. In 2014, President Obama renominated Lopez and he was reconfirmed by the Senate the same year.[13]
In 2011, the Commission included "sex-stereotyping" of lesbian, gay, and bisexual individuals, as a form of sex discrimination illegal under Title VII of the Civil Rights Act of 1964.[14][15] In 2012, the Commission expanded protection provided by Title VII to transgender status and gender identity.[14][16]
After the departure of Ishimaru, the commission returned to its full five commissioners on April 25, 2013, with the Senate confirmation of Jenny Yang.
In 2015, it concluded that for Title VII, sex discrimination includes discrimation based on sexual orientation.[17][18]
However, the rulings, while persuasive, are not binding on courts and would need to be addressed by the Supreme Court for a final decision. The Commission also mediates and settles thousands of discrimination complaints each year prior to their investigation.  The EEOC is also empowered to file civil discrimination suits against employers on behalf of alleged victims and to adjudicate claims of discrimination brought against federal agencies.[19][20]
In 1975, when the backlog reached more than 100,000 charges to be investigated, President Gerald Ford's full requested budget of $62 million was approved. A "Backlog Unit" was created in Philadelphia in 1978 to resolve the thousands of federal equal employment complaints inherited from the Civil Service Commission. In 1980, Eleanor Holmes Norton began re-characterizing the backlog cases as "workload" in her reports to Congress, thus fulfilling her promise to eliminate the backlog.[21]
In June 2006, civil rights and labor union advocates publicly complained that the effectiveness of the EEOC was being undermined by budget and staff cuts and the outsourcing of complaint screening to a private contractor whose workers were poorly trained. In 2006, a partial budget freeze prevented the agency from filling vacant jobs, and its staff had shrunk by nearly 20 percent from 2001. A Bush administration official stated that the cuts had been made because it was necessary to direct more money to defense and homeland security.[22] By 2008, the EEOC had lost 25 percent of its staff over the previous eight years, including investigators and lawyers who handle the cases. The number of complaints to investigate grew to 95,400 in fiscal 2008, up 26 percent from 2006.[23]
Although full-time staffing of the EEOC was cut between 2002 and 2006, Congress increased the commission's budget during that period, as it has almost every year since 1980. The budget was $303 million in fiscal year 2001[2] to $327 million in fiscal year 2006.[23]
The outsourcing to Pearson Government Solutions in Kansas cost the agency $4.9 million and was called a "huge waste of money" by the president of the EEOC employees' union in 2006.[22]
The EEOC uses monetary fines as the primary form of deterrence and, as the fines have not adjusted for inflation, the backlog of EEOC cases illustrates a decline in its effectiveness.
The EEOC requires employers to report various information about their employees, in particular their racial/ethnic categories, to prevent discrimination based on race/ethnicity. The definitions used in the report have been different at different times.
In 1997, the Office of Management and Budget gave a Federal Register Notice, the "Revisions to the Standards for the Classification of Federal Data on Race and Ethnicity," which defined new racial and ethnic definitions.[24] As of September 30, 2007, the EEO's EEO-1 report must use the new racial and ethnic definitions in establishing grounds for racial or ethnic discrimination.[25] If an employee identifies their ethnicity as "Hispanic or Latino" as well as a race, the race is not reported in EEO-1, but it is kept as part of the employment record.
A person's skin color or physical appearance can also be grounds for a case of racial discrimination.[26][27] Discrimination based on national origin can be grounds for a case on discrimination as well.[28]
EEOC applies an investigative compliance policy when respondents are uncooperative in providing information during an investigation of a charge. If a respondent fails to turn over requested information, field offices are to subpoena the information, file a direct suit on the merits of a charge, or use the legal principle of adverse inference, which assumes the withheld information is against the respondent.[29]
In 2008, disability-based charges handled by the EEOC rose to a record 19,543, up 10.2 percent from the prior year and the highest level since 1995.[30]
That may again be showing that because the EEOC has not adjusted many of their initial 1991 fines for inflation, the backlog of EEOC cases illustrates erosion of deterrence.
In September 2012, Home Depot agreed to pay $100,000 and furnish other relief to settle a disability discrimination lawsuit filed by the EEOC for the alleged failure to provide reasonable accommodation for a cashier with cancer at its Towson, Maryland, store and for later purportedly firing her because of her condition.[31]
The U.S. Equal Employment Opportunity Commission (EEOC) announced that it received 99,412 private sector workplace discrimination charges during fiscal year 2012, down slightly from the previous year. The year-end data also show that retaliation (37,836), race (33,512), and sex discrimination (30,356), which includes allegations of sexual harassment and pregnancy were the most frequently filed charges.[32]
Additionally, the EEOC achieved a second consecutive year of a significant reduction in the charge inventory, something not seen since fiscal year 2002.  Due to a concerted effort, the EEOC reduced the pending inventory of private sector charges by 10 percent from fiscal year 2011, bringing the inventory level to 70,312.  This inventory reduction is the second consecutive decrease of almost ten percent in charge inventory.  Also this fiscal year, the agency obtained the largest amount of monetary recovery from private sector and state and local government employers through its administrative process — $365.4 million.
In fiscal year 2012, the EEOC filed 122 lawsuits, including 86 individual suits, 26 multiple-victim suits, with fewer than 20 victims, and 10 systemic suits. The EEOC's legal staff resolved 254 lawsuits for a total monetary recovery of $44.2 million.
EEOC also continued its emphasis on eliminating alleged systemic patterns of discrimination in the workplace. In fiscal year 2012, EEOC completed 240 systemic investigations which in part resulted in 46 settlements or conciliation agreements. These settlements, achieved without litigation, secured 36.2 million dollars for the victims of unlawful discrimination. In addition, the agency filed 12 systemic lawsuits in fiscal year 2012.
Overall, the agency secured both monetary and non-monetary benefits for more than 23,446 people through administrative enforcement activities – mediation, settlements, conciliations, and withdrawals with benefits. The number of charges resolved through successful conciliation, the last step in the EEOC administrative process prior to litigation, increased by 18 percent over 2011.
On May 1, 2013, a Davenport, Iowa jury awarded the U.S. Equal Employment Opportunity Commissission damages totaling $240 million — the largest verdict in the federal agency's hstory — for disability discrimination and severe abuse.[33]
The jury agreed with the EEOC that Hill County Farms, doing business as Henry's Turkey Service subjected a group of 32 men with intellectual disabilities to severe abuse and discrimination for a period between 2007 and 2009, after 20 years of similar mistreatment.[33] This victory received international attention and was profiled in the New York Times.[34]
On June 1, 2015, the U.S. Supreme Court held in an 8-1 decision written by Justice Antonin Scalia that an employer may not refuse to hire an applicant if the employer was motivated by avoiding the need to accommodate a religious practice. Such behavior violates the prohibition on religious discrimination contained in Title VII of the Civil Rights Act of 1964.[35]
EEOC General Counsel David Lopez hailed the decision. "At its root, this case is about defending the quintessentially American principles of religious freedom and tolerance," Lopez said. "This decision is a victory for our increasingly diverse society and we applaud Samantha Elauf's courage and tenacity in pursuing this matter.”[35]
Some employment-law professionals criticized the agency after it issued advice that requiring a high school diploma from job applicants could violate the Americans with Disabilities Act. The advice letter stated that the longtime lowest common denominator of employee screening must be "job-related for the position in question and consistent with business necessity." A Ballard Spahr lawyer suggested, "There will be less incentive for the general public to obtain a high school diploma if many employers eliminate that requirement for job applicants in their workplace."[36]
The EEOC has been criticized for alleged heavy-handed tactics in their 1980 lawsuit against retailer Sears, Roebuck & Co. Based on a statistical analysis of personnel and promotions, EEOC argued that Sears both was systematically excluding women from high-earning positions in commission sales and was paying female management lower wages than male management. Sears, represented by lawyer Charles Morgan, Jr., counter-argued that the company had encouraged female applicants for sales and management, but women preferred lower-paying positions with more stable daytime working hours, as compared to commission sales, which demanded evening and weekend shifts and featured drastically-varying paychecks, depending on the numbers of sales in a given pay period. In 1986, the court ruled in favor of Sears on all counts and noted that the EEOC had neither produced a single witness who alleged discrimination nor identified any Sears policy that discriminated against women.[37][38]
In a 2011 ruling against the EEOC, Judge Loretta A. Preska declared that It relied too heavily on anecdotal claims rather than on hard data, in a lawsuit against Bloomberg, L.P. that alleged discrimination against pregnant employees. In a ruling described in the New York Times[39] as "strongly worded," Preska wrote, "the law does not mandate 'work-life balance' and added that while Bloomberg had expected high levels of dedication from employees, the company did not treat women who took pregnancy leave differently from those who took leave for other reasons.



Experimental cancer treatment - Wikipedia

Experimental cancer treatments are non-medical therapies intended to treat cancer by improving on, supplementing or replacing conventional methods (surgery, chemotherapy, radiation, and immunotherapy). Experimental cancer treatments cannot make medical claims. The term experimental cancer treatment could thus be substituted for "non FDA approved cancer treatment."
The entries listed below vary between theoretical therapies to unproven controversial therapies. Many of these treatments are alleged to help against only specific forms of cancer. It is not a list of treatments widely available at hospitals.
The twin goals of research are to determine whether the treatment actually works (called efficacy) and whether it is sufficiently safe. Regulatory processes attempt to balance the potential benefits with the potential harms, so that people given the treatment are more likely to benefit from it than to be harmed by it.
Medical research for cancer begins much like research for any disease. In organized studies of new treatments for cancer, the pre-clinical development of drugs, devices, and techniques begins in laboratories, either with isolated cells or in small animals, most commonly rats or mice. In other cases, the proposed treatment for cancer is already in use for some other medical condition, in which case more is known about its safety and potential efficacy.
Clinical trials are the study of treatments in humans. The first-in-human tests of a potential treatment are called Phase I studies. Early clinical trials typically enroll a very small number of patients, and the purpose is to identify major safety issues and the maximum tolerated dose, which is the highest dose that does not produce serious or fatal adverse effects. The dose given in these trials may be far too small to produce any useful effect. In most research, these early trials may involve healthy people, but cancer studies normally enroll only people with relatively severe forms of the disease in this stage of testing. On average, 95% of the participants in these early trials receive no benefit, but all are exposed to the risk of adverse effects.[1]  Most participants show signs of optimism bias (the irrational belief that they will beat the odds).
Later studies, called Phase II and Phase III studies, enroll more people, and the goal is to determine whether the treatment actually works. Phase III studies are frequently randomized controlled trials, with the experimental treatment being compared to the current best available treatment rather than to a placebo. In some cases, the Phase III trial provides the best available treatment to all participants, in addition to some of the patients receiving the experimental treatment.
Chemotherapeutic drugs have a hard time penetrating tumors to kill them at their core because these cells may lack a good blood supply. Researchers have been using anaerobic bacteria, such as Clostridium novyi, to consume the interior of oxygen-poor tumours. These should then die when they come in contact with the tumor's oxygenated sides, meaning they would be harmless to the rest of the body. A major problem has been that bacteria do not consume all parts of the malignant tissue. However, combining the therapy with chemotheraputic treatments can help to solve this problem.
Another strategy is to use anaerobic bacteria that have been transformed with an enzyme that can convert a non-toxic prodrug into a toxic drug. With the proliferation of the bacteria in the necrotic and hypoxic areas of the tumor, the enzyme is expressed solely in the tumor. Thus, a systemically applied prodrug is metabolised to the toxic drug only in the tumor. This has been demonstrated to be effective with the nonpathogenic anaerobe Clostridium sporogenes.[2]
HAMLET (human alpha-lactalbumin made lethal to tumor cells) is a molecular complex derived from human breast milk that kills tumor cells by a process resembling programmed cell death (apoptosis). It has been tested in humans with skin papillomas and bladder cancer.[3]
Dichloroacetate (DCA) has been found to shrink tumors in vivo in rats, and has a plausible scientific mechanism: DCA appears to reactivate suppressed mitochondria in some types of oxygen-starved tumor cells, and thus promotes apoptosis.[4]  Because it was tested for other conditions, DCA is known to be relatively safe, available, and inexpensive, and it can be taken by mouth as a pill, which is convenient. Five patients with brain cancer have been treated with DCA in a clinical trial, and the authors say that the lives of four were 'probably' extended.[5][6]  However, without a large controlled trial it is impossible to say whether the drug is truly effective against cancer.[7][8]
Quercetin is a principal flavonoid compound and an excellent free-radical-scavenging antioxidant that promotes apoptosis. In vitro it shows some antitumor activity in oral cancer and leukemia.[9][10][11] Cultured skin and prostate cancer cells showed significant mortality (compared to nonmalignant cells) when treated with a combination of quercetin and ultrasound.[12]  Note that ultrasound also promotes topical absorption by up to 1,000 times, making the use of topical quercetin and ultrasound wands an interesting proposition.[13]
High dietary intake of fruits and vegetables is associated with reduction in cancer, and some scientists, such as Gian Luigi Russo at the Institute of Food Sciences in Italy, suspect quercetin may be partly responsible.[14][15]  Research shows that quercetin influences cellular mechanisms in vitro and in animal studies.[16] According to the American Cancer society, "there is no reliable clinical evidence that quercetin can prevent or treat cancer in humans".[17]
Insulin potentiation therapy is practice of injecting insulin, usually alongside conventional cancer drugs, in the belief that this improves the overall effect of the treatment. Quackwatch state: "Insulin Potentiation Therapy (IPT) is one of several unproven, dangerous treatments that is promoted by a small group of practitioners without trustworthy evidence that it works."[18]
Several drug therapies are being developed based on p53, the tumour suppressor gene that protects the cell in response to damage and stress. It is analogous to deciding what to do with a damaged car: p53 brings everything to a halt, and then decides whether to fix the cell or, if the cell is beyond repair, to destroy the cell. This protective function of p53 is disabled in most cancer cells, allowing them to multiply without check. Restoration of p53 activity in tumours (where possible) has been shown to inhibit tumour growth and can even shrink the tumour.[19][20][21]
As p53 protein levels are usually kept low, one could block its degradation and allow large amounts of p53 to accumulate, thus stimulating p53 activity and its antitumour effects. Drugs that utilize this mechanism include nutlin and MI-219, which are both in phase I clinical trials.[22] There are also other drugs that are still in the preclinical stage of testing, such as RITA[23] and MITA.[24]
BI811283 is a small molecule inhibitor of the aurora B kinase protein being developed by Boehringer Ingelheim for use as an anti-cancer agent. BI 811283 is currently in the early stages of clinical development and is undergoing first-in-human trials in patients with solid tumors and Acute Myeloid Leukaemia.[25]
Introduction of tumor suppressor genes into rapidly dividing cells has been thought to slow down or arrest tumor growth. Adenoviruses are a commonly utilized vector for this purpose. Much research has focused on the use of adenoviruses that cannot reproduce, or reproduce only to a limited extent, within the patient to ensure safety via the avoidance of cytolytic destruction of noncancerous cells infected with the vector. However, new studies focus on adenoviruses that can be permitted to reproduce, and destroy cancerous cells in the process, since the adenoviruses' ability to infect normal cells is substantially impaired, potentially resulting in a far more effective treatment.[26][27]
Another use of gene therapy is the introduction of enzymes into these cells that make them susceptible to particular chemotherapy agents; studies with introducing thymidine kinase in gliomas, making them susceptible to aciclovir, are in their experimental stage.
Epigenetics is the study of heritable changes in gene activity that are not caused by changes in the DNA sequence, often a result of environmental or dietary damage to the histone receptors within the cell. Current research has shown that epigenetic pharmaceuticals could be a putative replacement or adjuvant therapy for currently accepted treatment methods such as radiation and chemotherapy, or could enhance the effects of these current treatments.[28] It has been shown that the epigenetic control of the proto-onco regions and the tumor suppressor sequences by conformational changes in histones directly affects the formation and progression of cancer.[29] Epigenetics also has the factor of reversibility, a characteristic that other cancer treatments do not offer.[30]
Some investigators, like Randy Jirtle, PhD, of Duke University Medical Center, think epigenetics may ultimately turn out to have a greater role in disease than genetics.[31]
Because most malignant cells rely on the activity of the protein telomerase for their immortality, it has been proposed that a drug that inactivates telomerase might be effective against a broad spectrum of malignancies. At the same time, most healthy tissues in the body express little if any telomerase, and would function normally in its absence. Currently, inositol hexaphosphate, which is available over-the-counter, is undergoing testing in cancer research due to its telomerase-inhibiting abilities.[32]
A number of research groups have experimented with the use of telomerase inhibitors in animal models, and as of 2005 and 2006 phase I and II human clinical trials are underway. Geron Corporation is currently conducting two clinical trials involving telomerase inhibitors. One uses a vaccine (GRNVAC1) and the other uses a lipidated oligonucleotide (GRN163L).
Photodynamic therapy (PDT) is generally a non-invasive treatment using a combination of light and a photosensitive drug, such as 5-ALA, Foscan, Metvix, Tookad, WST09, WST11, Photofrin, or Visudyne. The drug is triggered by light of a specific wavelength.
Localized and whole-body application of heat has been proposed as a technique for the treatment of malignant tumours. Intense heating will cause denaturation and coagulation of cellular proteins, rapidly killing cells within a tumour.
More prolonged moderate heating to temperatures just a few degrees above normal (39.5 °C) can cause more subtle changes. A mild heat treatment combined with other stresses can cause cell death by apoptosis. There are many biochemical consequences to the heat shock response within the cell, including slowed cell division and increased sensitivity to ionizing radiation therapy. The purpose of overheating the tumor cells is to create a lack of oxygen so that the heated cells become overacidified, which leads to a lack of nutrients in the tumor. This in turn disrupts the metabolism of the cells so that cell death (apoptosis) can set in. In certain cases chemotherapy or radiation that has previously not had any effect can be made effective. Hyperthermia alters the cell walls by means of so-called heat shock proteins. The cancer cells then react very much more effectively to the cytostatics and radiation. If hyperthermia is used conscientiously it has no serious side effects.[33]
There are many techniques by which heat may be delivered. Some of the most common involve the use of focused ultrasound (FUS or HIFU), microwave heating, induction heating, magnetic hyperthermia, and direct application of heat through the use of heated saline pumped through catheters. Experiments with carbon nanotubes that selectively bind to cancer cells have been performed. Lasers are then used that pass harmlessly through the body, but heat the nanotubes, causing the death of the cancer cells. Similar results have also been achieved with other types of nanoparticles, including gold-coated nanoshells and nanorods that exhibit certain degrees of 'tunability' of the absorption properties of the nanoparticles to the wavelength of light for irradiation. The success of this approach to cancer treatment rests on the existence of an 'optical window' in which biological tissue (i.e., healthy cells) are completely transparent at the wavelength of the laser light, while nanoparticles are highly absorbing at the same wavelength. Such a 'window' exists in the so-called near-infrared region of the electromagnetic spectrum. In this way, the laser light can pass through the system without harming healthy tissue, and only diseased cells, where the nanoparticles reside, get hot and are killed.
Magnetic hyperthermia makes use of magnetic nanoparticles, which can be injected into tumours and then generate heat when subjected to an alternating magnetic field.[34]
One of the challenges in thermal therapy is delivering the appropriate amount of heat to the correct part of the patient's body. A great deal of current research focuses on precisely positioning heat delivery devices (catheters, microwave, and ultrasound applicators, etc.) using ultrasound or magnetic resonance imaging, as well as of developing new types of nanoparticles that make them particularly efficient absorbers while offering little or no concerns about toxicity to the circulation system. Clinicians also hope to use advanced imaging techniques to monitor heat treatments in real time—heat-induced changes in tissue are sometimes perceptible using these imaging instruments. In magnetic hyperthermia or magnetic fluid hyperthermia method, it will be easier to control temperature distribution by controlling the velocity of ferrofluid injection and size of magnetic nanoparticles.[35][36][37]
This preclinical treatment involves using radio waves to heat up tiny metals that are implanted in cancerous tissue. Gold nanoparticles or carbon nanotubes are the most likely candidate. Promising preclinical trials have been conducted,[38][39] although clinical trials may not be held for another few years.[40]
Another method that is entirely non-invasive referred to as Tumor Treating Fields has already reached clinical trial stage in many countries. The concept applies an electric field through a tumour region using electrodes external to the body. Successful trials have shown the process effectiveness to be greater than chemotherapy and there are no side-effects and only negligible time spent away from normal daily activities.[41][42] This treatment is still in very early development stages for many types of cancer.
High-intensity focused ultrasound (HIFU) is still in investigatory phases in many places around the world.[43] In China it has CFDA approval and over 180 treatment centres have been established in China, Hong Kong, and Korea. HIFU has been successfully used to treat cancer to destroy tumours of the bone, brain, breast, liver, pancreas, rectum, kidney, testes, and prostate. Several thousand patients have been treated with various types of tumours. HIFU has CE approval for palliative care for bone metastasis. Experimentally, palliative care has been provided for cases of advanced pancreatic cancer. High-energy therapeutic ultrasound could increase higher-density anti-cancer drug load and nanomedicines to target tumor sites by 20x fold higher than traditional target cancer therapy.[44]
Cold atmospheric plasma or CAP for short is an emerging modality for the trreatment of solid tumors[45]  Recently, cold atmospheric plasma (CAP) indicated promising anti-neoplastic effects on several tumors, e.g. melanoma, glioma, and pancreatic cancer cells [5, 6, 7], and therefore could be an efficient method for anti-cancer treatment in clinical urology in the future.[46] One example of an experimental technology utilizing Cold Atmospheric plasma is Theraphi
Tumor Treating Fields is a novel FDA-approved cancer treatment therapy that uses alternating electric field to disturb the rapid cell division exhibited by cancer cells.[47]
Complementary and alternative medicine (CAM) treatments are the diverse group of medical and healthcare systems, practices, and products that are not part of conventional medicine and have not been proven to be effective.[48] Complementary medicine usually refers to methods and substances used along with conventional medicine, while alternative medicine refers to compounds used instead of conventional medicine.[49] CAM use is common among people with cancer.[50]
Most complementary and alternative medicines for cancer have not been rigorously studied or tested. Some alternative treatments that have been proven ineffective continue to be marketed and promoted.[51]



Molecular biology - Wikipedia
Molecular biology /məˈlɛkjʊlər/ is a branch of biology that concerns the molecular basis of biological activity between biomolecules in the various systems of a cell, including the interactions between DNA, RNA, proteins and their biosynthesis, as well as the regulation of these interactions.[1] Writing in Nature in 1961, William Astbury described molecular biology as:
...not so much a technique as an approach, an approach from the viewpoint of the so-called basic sciences with the leading idea of searching below the large-scale manifestations of classical biology for the corresponding molecular plan. It is concerned particularly with the forms of biological molecules and [...] is predominantly three-dimensional and structural—which does not mean, however, that it is merely a refinement of morphology. It must at the same time inquire into genesis and function.[2]
Researchers in molecular biology use specific techniques native to molecular biology but increasingly combine these with techniques and ideas from genetics and biochemistry.  There is not a defined line between these disciplines. The figure to the right is a schematic that depicts one possible view of the relationships between the fields:[3]
Much of molecular biology is quantitative, and recently much work has been done at its interface with computer science in bioinformatics and computational biology. In the early 2000s, the study of gene structure and function, molecular genetics, has been among the most prominent sub-fields of molecular biology. Increasingly many other areas of biology focus on molecules, either directly studying interactions in their own right such as in cell biology and developmental biology, or indirectly, where molecular techniques are used to infer historical attributes of populations or species, as in fields in evolutionary biology such as population genetics and phylogenetics. There is also a long tradition of studying biomolecules "from the ground up" in biophysics.[citation needed]
One of the most basic techniques of molecular biology to study protein function is molecular cloning. In this technique, DNA coding for a protein of interest is cloned using polymerase chain reaction (PCR), and/or restriction enzymes into a plasmid (expression vector). A vector has 3 distinctive features: an origin of replication, a multiple cloning site (MCS), and a selective marker usually antibiotic resistance. Located upstream of the multiple cloning site are the promoter regions and the transcription start site which regulate the expression of cloned gene.
This plasmid can be inserted into either bacterial or animal cells. Introducing DNA into bacterial cells can be done by transformation via uptake of naked DNA, conjugation via cell-cell contact or by transduction via viral vector. Introducing DNA into eukaryotic cells, such as animal cells, by physical or chemical means is called transfection. Several different transfection techniques are available, such as calcium phosphate transfection, electroporation, microinjection and liposome transfection. The plasmid may be integrated into the genome, resulting in a stable transfection, or may remain independent of the genome, called transient transfection.[6][7]
DNA coding for a protein of interest is now inside a cell, and the protein can now be expressed. A variety of systems, such as inducible promoters and specific cell-signaling factors, are available to help express the protein of interest at high levels. Large quantities of a protein can then be extracted from the bacterial or eukaryotic cell. The protein can be tested for enzymatic activity under a variety of situations, the protein may be crystallized so its tertiary structure can be studied, or, in the pharmaceutical industry, the activity of new drugs against the protein can be studied.[citation needed]
Polymerase chain reaction (PCR) is an extremely versatile technique for copying DNA. In brief, PCR allows a specific DNA sequence to be copied or modified in predetermined ways. The reaction is extremely powerful and under perfect conditions could amplify one DNA molecule to become 1.07 billion molecules in less than two hours. The PCR technique can be used to introduce restriction enzyme sites to ends of DNA molecules, or to mutate particular bases of DNA, the latter is a method referred to as site-directed mutagenesis. PCR can also be used to determine whether a particular DNA fragment is found in a cDNA library. PCR has many variations, like reverse transcription PCR (RT-PCR) for amplification of RNA, and, more recently, quantitative PCR which allow for quantitative measurement of DNA or RNA molecules.[8][9]
Gel electrophoresis is one of the principal tools of molecular biology. The basic principle is that DNA, RNA, and proteins can all be separated by means of an electric field and size. In agarose gel electrophoresis, DNA and RNA can be separated on the basis of size by running the DNA through an electrically charged agarose gel. Proteins can be separated on the basis of size by using an SDS-PAGE gel, or on the basis of size and their electric charge by using what is known as a 2D gel electrophoresis.[10]
The terms northern, western and eastern blotting are derived from what initially was a molecular biology joke that played on the term Southern blotting, after the technique described by Edwin Southern for the hybridisation of blotted DNA. Patricia Thomas, developer of the RNA blot which then became known as the northern blot, actually didn't use the term.[11]
Named after its inventor, biologist Edwin Southern, the Southern blot is a method for probing for the presence of a specific DNA sequence within a DNA sample. DNA samples before or after restriction enzyme (restriction endonuclease) digestion are separated by gel electrophoresis and then transferred to a membrane by blotting via capillary action. The membrane is then exposed to a labeled DNA probe that has a complement base sequence to the sequence on the DNA of interest.[12] Southern blotting is less commonly used in laboratory science due to the capacity of other techniques, such as PCR, to detect specific DNA sequences from DNA samples. These blots are still used for some applications, however, such as measuring transgene copy number in transgenic mice or in the engineering of gene knockout embryonic stem cell lines.[citation needed]
The northern blot is used to study the expression patterns of a specific type of RNA molecule as relative comparison among a set of different samples of RNA.  It is essentially a combination of denaturing RNA gel electrophoresis, and a blot.  In this process RNA is separated based on size and is then transferred to a membrane that is then probed with a labeled complement of a sequence of interest. The results may be visualized through a variety of ways depending on the label used; however, most result in the revelation of bands representing the sizes of the RNA detected in sample.  The intensity of these bands is related to the amount of the target RNA in the samples analyzed.  The procedure is commonly used to study when and how much gene expression is occurring by measuring how much of that RNA is present in different samples.  It is one of the most basic tools for determining at what time, and under what conditions, certain genes are expressed in living tissues.[13][14]
In western blotting, proteins are first separated by size, in a thin gel sandwiched between two glass plates in a technique known as SDS-PAGE. The proteins in the gel are then transferred to a polyvinylidene fluoride (PVDF), nitrocellulose, nylon, or other support membrane. This membrane can then be probed with solutions of antibodies. Antibodies that specifically bind to the protein of interest can then be visualized by a variety of techniques, including colored products, chemiluminescence, or autoradiography. Often, the antibodies are labeled with enzymes. When a chemiluminescent substrate is exposed to the enzyme it allows detection. Using western blotting techniques allows not only detection but also quantitative analysis. Analogous methods to western blotting can be used to directly stain specific proteins in live cells or tissue sections.[15][16]
The eastern blotting technique is used to detect post-translational modification of proteins. Proteins blotted on to the PVDF or nitrocellulose membrane are probed for modifications using specific substrates.[17]
DNA microarray is a collection of spots attached to a solid support such as a microscope slide where each spot contains one or more single-stranded DNA oligonucleotide fragment. Arrays make it possible to put down large quantities of very small (100 micrometre diameter) spots on a single slide. Each spot has a DNA fragment molecule that is complementary to a single DNA sequence. A variation of this technique allows the gene expression of an organism at a particular stage in development to be qualified (expression profiling). In this technique the RNA in a tissue is isolated and converted to labeled cDNA. This cDNA is then hybridized to the fragments on the array and visualization of the hybridization can be done. Since multiple arrays can be made with exactly the same position of fragments they are particularly useful for comparing the gene expression of two different tissues, such as a healthy and cancerous tissue. Also, one can measure what genes are expressed and how that expression changes with time or with other factors.  
There are many different ways to fabricate microarrays; the most common are silicon chips, microscope slides with spots of ~100 micrometre diameter, custom arrays, and arrays with larger spots on porous membranes (macroarrays). There can be anywhere from 100 spots to more than 10,000 on a given array. Arrays can also be made with molecules other than DNA.[18][19][20][21]
Allele-specific oligonucleotide (ASO) is a technique that allows detection of single base mutations without the need for PCR or gel electrophoresis. Short (20-25 nucleotides in length), labeled probes are exposed to the non-fragmented target DNA, hybridization occurs with high specificity due to the short length of the probes and even a single base change will hinder hybridization. The target DNA is then washed and the labeled probes that didn't hybridize are removed. The target DNA is then analyzed for the presence of the probe via radioactivity or fluorescence. In this experiment, as in most molecular biology techniques, a control must be used to ensure successful experimentation.[22][23]
In molecular biology, procedures and technologies are continually being developed and older technologies abandoned. For example, before the advent of DNA gel electrophoresis (agarose or polyacrylamide), the size of DNA molecules was typically determined by rate sedimentation in sucrose gradients, a slow and labor-intensive technique requiring expensive instrumentation; prior to sucrose gradients, viscometry was used. Aside from their historical interest, it is often worth knowing about older technology, as it is occasionally useful to solve another new problem for which the newer technique is inappropriate.[citation needed]
While molecular biology was established in the 1930s, the term was coined by Warren Weaver in 1938. Weaver was the director of Natural Sciences for the Rockefeller Foundation at the time and believed that biology was about to undergo a period of significant change given recent advances in fields such as X-ray crystallography.[24][25]
Clinical research and medical therapies arising from molecular biology are partly covered under gene therapy. The use of molecular biology or molecular cell biology approaches in medicine is now called molecular medicine. Molecular biology also plays important role in understanding formations, actions, and regulations of various parts of cells which can be used to efficiently target new drugs, diagnosis disease, and understand the physiology of the cell.[26]



Virotherapy - Wikipedia
Virotherapy is a treatment using biotechnology to convert viruses into therapeutic agents by reprogramming viruses to treat diseases. There are three main branches of virotherapy: anti-cancer oncolytic viruses, viral vectors for gene therapy and viral immunotherapy.  In a slightly different context, virotherapy can also refer more broadly to the use of viruses to treat certain medical conditions by killing pathogens.
Oncolytic virotherapy is not a new idea – as early as the mid 1950s doctors were noticing that cancer patients who suffered a non-related viral infection, or who had been vaccinated recently, showed signs of improvement;[1] this has been largely attributed to the production of interferon and tumour necrosis factors in response to viral infection, but oncolytic viruses are being designed that selectively target and lyse only cancerous cells.
In the 1940s and 1950s, studies were conducted in animal models to evaluate the use of viruses in the treatment of tumours.[2]  In the 1940s–1950s some of the earliest human clinical trials with oncolytic viruses were started.[3][4]
In 2015 the FDA approved the marketing of talimogene laherparepvec, a genetically engineered herpes virus, to treat melanoma lesions that cannot be operated on; it is injected directly into the lesion.[5]  As of 2016 there was no evidence that it extends the life of people with melanoma, or that it prevents metastasis.[6]  Two genes were removed from the virus – one that shuts down an individual cell's defenses, and another that helps the virus evade the immune system – and a gene for human GM-CSF was added.  The drug works by replicating in cancer cells, causing them to burst; it was also designed to stimulate an immune response but as of 2016, there was no evidence of this.[7][5]  The drug was created and initially developed by BioVex, Inc. and was continued by Amgen, which acquired BioVex in 2011.[8] It was the first oncolytic virus approved in the West.[7]
Viral gene therapy most frequently uses non-replicating viruses to deliver therapeutic genes to cells with genetic malfunctions. Early efforts while technically successful, faced considerable delays due to safety issues as the uncontrolled delivery of a gene into a host genome has the potential to disrupt tumour suppressing genes and induce cancer, and did so in two cases. Immune responses to viral therapies also pose a barrier to successful treatment, for this reason eye therapy for genetic blindness is attractive as the eye is an immune privileged site, preventing an immune response.
An alternative form of viral gene therapy is to deliver a gene which may be helpful in preventing disease that would not normally be expressed in the natural disease condition. For example, the growth of new blood vessels in cancer, known as angiogenesis, enables tumours to grow larger. However, a virus introducing anti-angiogenic factors to the tumour may be able to slow or halt growth.
Viral immunotherapy uses viruses to introduce specific antigens to the patient's immune system. Unlike traditional vaccines, in which attenuated or killed virus/bacteria is used to generate an immune response, viral immunotherapy uses genetically engineered viruses to present a specific antigen to the immune system. That antigen could be from any species of virus/bactera or even human disease antigens, for example cancer antigens.
RIGVIR is a virotherapy drug that was approved by the State Agency of Medicines of the Republic of Latvia in 2004.[9]  It is wild type ECHO-7, a member of echovirus family.[10]  The potential use of echovirus as an oncolytic virus to treat cancer was discovered by Latvian scientist Aina Muceniece in the 1960s and 1970s.[10]  The data used to register the drug in Latvia is not sufficient to obtain approval to use it in the US, Europe, or Japan.[10][11]  As of 2017 there was no good evidence that RIGVIR is an effective cancer treatment.[12][13]
In 2004, researchers from University of Texas genetically programmed a type of common cold virus Adenovirus Delta-24-RGD to attack glioblastoma multiforme. Later other researchers[14] have tried tests on mice where 9 out of 10 mice have shown degeneration of tumours and prolonged survival. A drug grade virus was approved for clinical trials on humans in 2009.[15]
In 2006 researchers from the Hebrew University succeeded in isolating a variant of the Newcastle disease virus (NDV-HUJ), which usually affects birds, in order to specifically target cancer cells.[16] The researchers tested the new virotherapy on patients with glioblastoma multiforme and achieved promising results for the first time.
Vaccinia virus, a virus credited for the eradication of smallpox, is being developed as an oncolytic virus, e.g. GL-ONC1 and JX-594.[17] Promising research results[18][19] warrant its clinical development in human patients.[20]
ProSavin is one of a number of therapies in the Lentivector platform under development by Oxford BioMedica. It delivers to the brain the genes for three enzymes important in the production of dopamine, a deficiency of which causes Parkinson's disease.
TNFerade (a non replicating TNF gene therapy virus) failed a phase III trial for pancreatic cancer.[21]
Recent papers have proposed the use of viruses to treat infections caused by protozoa.[22][23]
Chester M. Southam, a researcher at Memorial Sloan Kettering Cancer Center, pioneered the study of viruses as potential agents to treat cancer.[24]



Magnetic resonance imaging - Wikipedia
Magnetic resonance imaging (MRI) is a medical imaging technique used in radiology to form pictures of the anatomy and the physiological processes of the body in both health and disease. MRI scanners use strong magnetic fields, magnetic field gradients, and radio waves to generate images of the organs in the body. MRI does not involve X-rays or the use of ionizing radiation, which distinguishes it from CT or CAT scans and PET scans. Magnetic resonance imaging is a medical application of nuclear magnetic resonance (NMR). NMR can also be used for imaging in other NMR applications such as NMR spectroscopy.
While the hazards of X-rays are now well-controlled in most medical contexts, MRI may still be seen as a better choice than a CT scan. MRI is widely used in hospitals and clinics for medical diagnosis, staging of disease and follow-up without exposing the body to radiation. However, MRI may often yield different diagnostic information compared with CT. There may be risks and discomfort associated with MRI scans. Compared with CT scans, MRI scans typically take longer and are louder, and they usually need the subject to enter a narrow, confining tube. In addition, people with some medical implants or other non-removable metal inside the body may be unable to undergo an MRI examination safely.
MRI was originally called NMRI (nuclear magnetic resonance imaging) and is a form of NMR, though the use of 'nuclear' in the acronym was dropped to avoid negative associations with the word.[1] Certain atomic nuclei are able to absorb and emit radio frequency energy when placed in an external magnetic field. In clinical and research MRI, hydrogen atoms are most often used to generate a detectable radio-frequency signal that is received by antennas in close proximity to the anatomy being examined. Hydrogen atoms are naturally abundant in people and other biological organisms, particularly in water and fat. For this reason, most MRI scans essentially map the location of water and fat in the body. Pulses of radio waves excite the nuclear spin energy transition, and magnetic field gradients localize the signal in space. By varying the parameters of the pulse sequence, different contrasts may be generated between tissues based on the relaxation properties of the hydrogen atoms therein.
Since its development in the 1970s and 1980s, MRI has proven to be a highly versatile imaging technique. While MRI is most prominently used in diagnostic medicine and biomedical research, it also may be used to form images of non-living objects. MRI scans are capable of producing a variety of chemical and physical data, in addition to detailed spatial images. The sustained increase in demand for MRI within health systems has led to concerns about cost effectiveness and overdiagnosis.[2][3]
To perform a study, the person is positioned within an MRI scanner that forms a strong magnetic field around the area to be imaged. In most medical applications, protons (hydrogen atoms) in tissues containing water molecules create a signal that is processed to form an image of the body. First, energy from an oscillating magnetic field temporarily is applied to the patient at the appropriate resonance frequency. The excited hydrogen atoms emit a radio frequency signal, which is measured by a receiving coil. The radio signal may be made to encode position information by varying the main magnetic field using gradient coils. As these coils are rapidly switched on and off they create the characteristic repetitive noise of an MRI scan. The contrast between different tissues is determined by the rate at which excited atoms return to the equilibrium state. Exogenous contrast agents may be given to the person to make the image clearer.[4]
The major components of an MRI scanner are the main magnet, which polarizes the sample, the shim coils for correcting shifts in the homogeneity of the main magnetic field, the gradient system which is used to localize the MR signal and the RF system, which excites the sample and detects the resulting NMR signal. The whole system is controlled by one or more computers.
MRI requires a magnetic field that is both strong and uniform. The field strength of the magnet is measured in teslas – and while the majority of systems operate at 1.5 T, commercial systems are available between 0.2 and 7 T. Most clinical magnets are superconducting magnets, which require liquid helium. Lower field strengths can be achieved with permanent magnets, which are often used in "open" MRI scanners for claustrophobic patients.[5] Recently, MRI has been demonstrated also at ultra-low fields, i.e., in the microtesla-to-millitesla range, where sufficient signal quality is made possible by prepolarization (on the order of 10-100 mT) and by measuring the Larmor precession fields at about 100 microtesla with highly sensitive superconducting quantum interference devices (SQUIDs).[6][7][8]
Each tissue returns to its equilibrium state after excitation by the independent relaxation processes of T1 (spin-lattice; that is, magnetization in the same direction as the static magnetic field) and T2 (spin-spin; transverse to the static magnetic field).

To create a T1-weighted image, magnetization is allowed to recover before measuring the MR signal by changing the repetition time (TR). This image weighting is useful for assessing the cerebral cortex, identifying fatty tissue, characterizing focal liver lesions and in general for obtaining morphological information, as well as for post-contrast imaging.

To create a T2-weighted image, magnetization is allowed to decay before measuring the MR signal by changing the echo time (TE). This image weighting is useful for detecting edema and inflammation, revealing white matter lesions and assessing zonal anatomy in the prostate and uterus.
The standard display of MRI images is to represent fluid characteristics in black and white images, where different tissues turn out as follows:
MRI has a wide range of applications in medical diagnosis and more than 25,000 scanners are estimated to be in use worldwide.[12] MRI affects diagnosis and treatment in many specialties although the effect on improved health outcomes is uncertain.[13]
MRI is the investigation of choice in the preoperative staging of rectal and prostate cancer and, has a role in the diagnosis, staging, and follow-up of other tumors.[14]
MRI is the investigative tool of choice for neurological cancers, as it has better resolution than CT and offers better visualization of the posterior cranial fossa, containing the brainstem and the cerebellum. The contrast provided between grey and white matter makes MRI the best choice for many conditions of the central nervous system, including demyelinating diseases, dementia, cerebrovascular disease, infectious diseases, Alzheimer's disease and epilepsy.[15][16][17] Since many images are taken milliseconds apart, it shows how the brain responds to different stimuli, enabling researchers to study both the functional and structural brain abnormalities in psychological disorders.[18] MRI also is used in guided stereotactic surgery and radiosurgery for treatment of intracranial tumors, arteriovenous malformations, and other surgically treatable conditions using a device known as the N-localizer.[19][20][21]
Cardiac MRI is complementary to other imaging techniques, such as echocardiography, cardiac CT, and nuclear medicine. Its applications include assessment of myocardial ischemia and viability, cardiomyopathies, myocarditis, iron overload, vascular diseases, and congenital heart disease.[22]
Applications in the musculoskeletal system include spinal imaging, assessment of joint disease, and soft tissue tumors.[23]
Hepatobiliary MR is used to detect and characterize lesions of the liver, pancreas, and bile ducts. Focal or diffuse disorders of the liver may be evaluated using diffusion-weighted, opposed-phase imaging, and dynamic contrast enhancement sequences. Extracellular contrast agents are used widely in liver MRI and newer hepatobiliary contrast agents also provide the opportunity to perform functional biliary imaging. Anatomical imaging of the bile ducts is achieved by using a heavily T2-weighted sequence in magnetic resonance cholangiopancreatography (MRCP). Functional imaging of the pancreas is performed following administration of secretin. MR enterography provides non-invasive assessment of inflammatory bowel disease and small bowel tumors. MR-colonography may play a role in the detection of large polyps in patients at increased risk of colorectal cancer.[24][25][26][27]
Magnetic resonance angiography (MRA) generates pictures of the arteries to evaluate them for stenosis (abnormal narrowing) or aneurysms (vessel wall dilatations, at risk of rupture). MRA is often used to evaluate the arteries of the neck and brain, the thoracic and abdominal aorta, the renal arteries, and the legs (called a "run-off"). A variety of techniques can be used to generate the pictures, such as administration of a paramagnetic contrast agent (gadolinium) or using a technique known as "flow-related enhancement" (e.g., 2D and 3D time-of-flight sequences), where most of the signal on an image is due to blood that recently moved into that plane (see also FLASH MRI).
Techniques involving phase accumulation (known as phase contrast angiography) can also be used to generate flow velocity maps easily and accurately. Magnetic resonance venography (MRV) is a similar procedure that is used to image veins. In this method, the tissue is now excited inferiorly, while the signal is gathered in the plane immediately superior to the excitation plane—thus imaging the venous blood that recently moved from the excited plane.[28]
MRI for imaging anatomical structures or blood flow do not require contrast agents as the varying properties of the tissues or blood provide natural contrasts. However, for more specific types of imaging, exogenous contrast agents may be given intravenously, orally, or intra-articularly.[4] The most commonly used intravenous contrast agents are based on chelates of gadolinium.[29] In general, these agents have proved safer than the iodinated contrast agents used in X-ray radiography or CT. Anaphylactoid reactions are rare, occurring in approx. 0.03–0.1%.[30] Of particular interest is the lower incidence of nephrotoxicity, compared with iodinated agents, when given at usual doses—this has made contrast-enhanced MRI scanning an option for patients with renal impairment, who would otherwise not be able to undergo contrast-enhanced CT.[31]
Although gadolinium agents have proved useful for patients with renal impairment, in patients with severe renal failure requiring dialysis there is a risk of a rare but serious illness, nephrogenic systemic fibrosis, which may be linked to the use of certain gadolinium-containing agents. The most frequently linked is gadodiamide, but other agents have been linked too.[32] Although a causal link has not been definitively established, current guidelines in the United States are that dialysis patients should only receive gadolinium agents where essential, and that dialysis should be performed as soon as possible after the scan to remove the agent from the body promptly.[33][34]
In Europe, where more gadolinium-containing agents are available, a classification of agents according to potential risks has been released.[35][36] Recently, a new contrast agent named gadoxetate, brand name Eovist (US) or Primovist (EU), was approved for diagnostic use: this has the theoretical benefit of a dual excretion path.[37]
An MRI sequence is a particular setting of radiofrequency pulses and gradients, resulting in a particular image appearance.[38] The T1 and T2 weighting can also be described as MRI sequences.
editThis table does not include uncommon and experimental sequences.
Standard foundation and comparison for other sequences
Standard foundation and comparison for other sequences
Magnetic resonance spectroscopy (MRS) is used to measure the levels of different metabolites in body tissues. The MR signal produces a spectrum of resonances that corresponds to different molecular arrangements of the isotope being "excited". This signature is used to diagnose certain metabolic disorders, especially those affecting the brain,[66] and to provide information on tumor metabolism.[67]
Magnetic resonance spectroscopic imaging (MRSI) combines both spectroscopic and imaging methods to produce spatially localized spectra from within the sample or patient. The spatial resolution is much lower (limited by the available SNR), but the spectra in each voxel contains information about many metabolites. Because the available signal is used to encode spatial and spectral information, MRSI requires high SNR achievable only at higher field strengths (3 T and above).[68] The high procurement and maintenance costs of MRI with extremely high field strengths[69] inhibit their popularity. However, recent compressed sensing-based software algorithms (e.g., SAMV[70]) have been proposed to achieve super-resolution without requiring such high field strengths.
Real-time MRI refers to the continuous imaging of moving objects (such as the heart) in real time. One of the many different strategies developed since the early 2000s is based on radial FLASH MRI, and iterative reconstruction. This gives a temporal resolution of 20–30 ms for images with an in-plane resolution of 1.5–2.0 mm.[71] This method is likely to add important information on diseases of the heart and the joints, and in many cases may make MRI  examinations easier and more comfortable for patients.[72]
The lack of harmful effects on the patient and the operator make MRI well-suited for interventional radiology, where the images produced by an MRI scanner guide minimally invasive procedures. Such procedures use no ferromagnetic instruments.[citation needed]
A specialized growing subset of interventional MRI is intraoperative MRI, in which an MRI is used in surgery. Some specialized MRI systems allow imaging concurrent with the surgical procedure. More typically, the surgical procedure is temporarily interrupted so that MRI can assess the success of the procedure or guide subsequent surgical work.[citation needed]
In guided therapy, high-intensity focused ultrasound (HIFU) beams are focused on a tissue, that are controlled using MR thermal imaging. Due to the high energy at the focus, the temperature rises to above 65 °C (150 °F) which completely destroys the tissue. This technology can achieve precise ablation of diseased tissue. MR imaging provides a three-dimensional view of the target tissue, allowing for the precise focusing of ultrasound energy. The MR imaging provides quantitative, real-time, thermal images of the treated area. This allows the physician to ensure that the temperature generated during each cycle of ultrasound energy is sufficient to cause thermal ablation within the desired tissue and if not, to adapt the parameters to ensure effective treatment.[73]
Hydrogen has the most frequently imaged nucleus in MRI because it is present in biological tissues in great abundance, and because its high gyromagnetic ratio gives a strong signal. However, any nucleus with a net nuclear spin could potentially be imaged with MRI. Such nuclei include helium-3, lithium-7, carbon-13, fluorine-19, oxygen-17, sodium-23, phosphorus-31 and xenon-129. 23Na and 31P are naturally abundant in the body, so can be imaged directly. Gaseous isotopes such as 3He or 129Xe must be hyperpolarized and then inhaled as their nuclear density is too low to yield a useful signal under normal conditions. 17O and 19F can be administered in sufficient quantities in liquid form (e.g. 17O-water) that hyperpolarization is not a necessity.[citation needed] Using helium or xenon has the advantage of reduced background noise, and therefore increased contrast for the image itself, because these elements are not normally present in biological tissues.[74]
Moreover, the nucleus of any atom that has a net nuclear spin and that is bonded to a hydrogen atom could potentially be imaged via heteronuclear magnetization transfer MRI that would image the high-gyromagnetic-ratio hydrogen nucleus instead of the low-gyromagnetic-ratio nucleus that is bonded to the hydrogen atom.[75] In principle, hetereonuclear magnetization transfer MRI could be used to detect the presence or absence of specific chemical bonds.[76][77]
Multinuclear imaging is primarily a research technique at present. However, potential applications include functional imaging and imaging of organs poorly seen on 1H MRI (e.g., lungs and bones) or as alternative contrast agents. Inhaled hyperpolarized 3He can be used to image the distribution of air spaces within the lungs. Injectable solutions containing 13C or stabilized bubbles of hyperpolarized 129Xe have been studied as contrast agents for angiography and perfusion imaging. 31P can potentially provide information on bone density and structure, as well as functional imaging of the brain. Multinuclear imaging holds the potential to chart the distribution of lithium in the human brain, this element finding use as an important drug for those with conditions such as bipolar disorder.[citation needed]
MRI has the advantages of having very high spatial resolution and is very adept at morphological imaging and functional imaging. MRI does have several disadvantages though. First, MRI has a sensitivity of around 10−3 mol/L  to 10−5 mol/L, which, compared to other types of imaging, can be very limiting. This problem stems from the fact that the population difference between the nuclear spin states is very small at room temperature. For example, at 1.5 teslas, a typical field strength for clinical MRI, the difference between high and low energy states is approximately 9 molecules per 2 million. Improvements to increase MR sensitivity include increasing magnetic field strength, and hyperpolarization via optical pumping or dynamic nuclear polarization. There are also a variety of signal amplification schemes based on chemical exchange that increase sensitivity.[citation needed]
To achieve molecular imaging of disease biomarkers using MRI, targeted MRI contrast agents with high specificity and high relaxivity (sensitivity) are required. To date, many studies have been devoted to developing targeted-MRI contrast agents to achieve molecular imaging by MRI. Commonly, peptides, antibodies, or small ligands, and small protein domains, such as HER-2 affibodies, have been applied to achieve targeting. To enhance the sensitivity of the contrast agents, these targeting moieties are usually linked to high payload MRI contrast agents or MRI contrast agents with high relaxivities.[78] A new class of gene targeting MR contrast agents (CA) has been introduced to show gene action of unique mRNA and gene transcription factor proteins.[79][80] This new CA can trace cells with unique mRNA, microRNA and virus; tissue response to inflammation in living brains.[81] The MR reports change in gene expression with positive correlation to TaqMan analysis, optical and electron microscopy.[82]
In the UK, the price of a clinical 1.5-tesla MRI scanner is around £920,000/US$1.4 million, with the lifetime maintenance cost broadly similar to the purchase cost.[83] In the Netherlands, the average MRI scanner costs around €1 million,[84] with a 7-T MRI having been taken in use by the UMC Utrecht in December 2007, costing €7 million.[85] Construction of MRI suites could cost up to US$500,000/€370.000 or more, depending on project scope. Pre-polarizing MRI (PMRI) systems using resistive electromagnets have shown promise as a low-cost alternative and have specific advantages for joint imaging near metal implants, however they are likely unsuitable for routine whole-body or neuroimaging applications.[86][87]
MRI scanners have become significant sources of revenue for healthcare providers in the US. This is because of favorable reimbursement rates from insurers and federal government programs. Insurance reimbursement is provided in two components, an equipment charge for the actual performance and operation of the MRI scan and a professional charge for the radiologist's review of the images and/or data. In the US Northeast, an equipment charge might be $3,500/€2,600 and a professional charge might be $350/€260,[88] although the actual fees received by the equipment owner and interpreting physician are often significantly less and depend on the rates negotiated with insurance companies or determined by the Medicare fee schedule. For example, an orthopedic surgery group in Illinois billed a charge of $1,116/€825 for a knee MRI in 2007, but the Medicare reimbursement in 2007 was only $470.91/€350.[89] Many insurance companies require advance approval of an MRI procedure as a condition for coverage.
In the US, the Deficit Reduction Act of 2005 significantly reduced reimbursement rates paid by federal insurance programs for the equipment component of many scans, shifting the economic landscape. Many private insurers have followed suit.[citation needed]
In the United States, an MRI of the brain with and without contrast billed to Medicare Part B entails, on average, a technical payment of US$403/€300 and a separate payment to the radiologist of US$93/€70.[90] In France, the cost of an MRI exam is approximately €150/US$205. This covers three basic scans including one with an intravenous contrast agent as well as a consultation with the technician and a written report to the patient's physician.[91] In Japan, the cost of an MRI examination (excluding the cost of contrast material and films) ranges from US$155/€115 to US$180/€133, with an additional radiologist professional fee of US$17/€12.50.[92] In India, the cost of an MRI examination including the fee for the radiologist's opinion comes to around Rs 3000–4000 (€37–49/US$50–60), excluding the cost of contrast material. In the UK the retail price for an MRI scan privately ranges between £350 and £700 (€405–810).[93]
Control console
Bore camera
Operator performing a scan
Technical area
MRI is in general a safe technique, although injuries may occur as a result of failed safety procedures or human error.[94] Contraindications to MRI include most cochlear implants and cardiac pacemakers, shrapnel, and metallic foreign bodies in the eyes. The safety of MRI during the first trimester of pregnancy is uncertain, but it may be preferable to other options.[95] Since MRI does not use any ionizing radiation, its use is generally favored in preference to CT when either modality could yield the same information.[96] In certain cases, MRI is not preferred as it may be more expensive, time-consuming, and claustrophobia-exacerbating.
MRI uses powerful magnets and can therefore cause magnetic materials to move at great speeds posing risk. Deaths have occurred.[97] However, as millions of MRIs are performed globally each year.,[98] fatalities are extremely rare.
Medical societies issue guidelines for when physicians should use MRI on patients and recommend against overuse. MRI can detect health problems or confirm a diagnosis, but medical societies often recommend that MRI not be the first procedure for creating a plan to diagnose or manage a patient's complaint. A common case is to use MRI to seek a cause of low back pain; the American College of Physicians, for example, recommends against this procedure as unlikely to result in a positive outcome for the patient.[99][100]
An MRI artifact is a visual artifact, that is, an anomaly during visual representation. Many different artifacts can occur
during magnetic resonance imaging (MRI), some affecting the diagnostic quality, while others may be confused with pathology. Artifacts can be classified as patient-related, signal processing-dependent and hardware (machine)-related.[101]
MRI is used industrially mainly for routine analysis of chemicals. The nuclear magnetic resonance technique is also used, for example, to measure the ratio between water and fat in foods, monitoring of flow of corrosive fluids in pipes, or to study molecular structures such as catalysts.[102]
In 1971, Paul Lauterbur applied magnetic field gradients in all three dimensions and a back-projection technique to create NMR images. He published the first images of two tubes of water in 1973 in the journal Nature, followed by the picture of a living animal, a clam, and in 1974 by the image of the thoracic cavity of a mouse. Lauterbur called his imaging method zeugmatography, a term which was later replaced by (N)MR imaging.[103] In the late 1970s, physicists Peter Mansfield and Paul Lauterbur, developed MRI-related techniques, like the echo-planar imaging (EPI) technique.[104] Mansfield and Lauterbur were awarded the 2003 Nobel Prize in Physiology or Medicine for their "discoveries concerning magnetic resonance imaging".



Pregnancy - Wikipedia

Pregnancy, also known as gestation, is the time during which one or more offspring develops inside a woman.[4] A multiple pregnancy involves more than one offspring, such as with twins.[13] Pregnancy can occur by sexual intercourse or assisted reproductive technology.[6] Childbirth typically occurs around 40 weeks from the last menstrual period (LMP).[4][5] This is just over nine months, where each month averages 31 days.[4][5] When measured from fertilization it is about 38 weeks.[5] An embryo is the developing offspring during the first eight weeks following fertilization, after which, the term fetus is used until birth.[5] Symptoms of early pregnancy may include  missed periods, tender breasts, nausea and vomiting, hunger, and frequent urination.[1] Pregnancy may be confirmed with a pregnancy test.[7]
Pregnancy is typically divided into three trimesters.[4] The first trimester is from week one through 12 and includes conception, which is when the sperm fertilizes the egg.[4] The fertilized egg then travels down the fallopian tube and attaches to the inside of the uterus, where it begins to form the embryo and placenta.[4] During the first trimester, the possibility of miscarriage (natural death of embryo or fetus) is at its highest.[2] The second trimester is from week 13 through 28.[4] Around the middle of the second trimester, movement of the fetus may be felt.[4] At 28 weeks, more than 90% of babies can survive outside of the uterus if provided with high-quality medical care.[4] The third trimester is from 29 weeks through 40 weeks.[4]
Prenatal care improves pregnancy outcomes.[9] Prenatal care may include taking extra folic acid, avoiding drugs and alcohol, regular exercise, blood tests, and regular physical examinations.[9] Complications of pregnancy may include disorders of high blood pressure, gestational diabetes, iron-deficiency anemia, and severe nausea and vomiting among others.[3] In the ideal childbirth labor begins on its own when a woman is "at term".[14] Pregnancy is considered at full term when gestation has lasted 39 to 41 weeks.[4] After 41 weeks, it is known as late term and after 42 weeks post term.[4] Babies born before 39 weeks are considered early term while those before 37 weeks are preterm.[4] Preterm babies are at higher risk of health problems such as cerebral palsy.[4] Delivery before 39 weeks by labor induction or caesarean section is not recommended unless required for other medical reasons.[15]
About 213 million pregnancies occurred in 2012, of which, 190 million were in the developing world and 23 million were in the developed world.[11] The number of pregnancies in women ages 15 to 44 is 133 per 1,000 women.[11] About 10% to 15% of recognized pregnancies end in miscarriage.[2] In 2016, complications of pregnancy resulted in 230,600 deaths, down from 377,000 deaths in 1990.[12] Common causes include bleeding, infections, hypertensive diseases of pregnancy, obstructed labor, and complications associated with miscarriage, ectopic pregnancy, or elective abortion.[16] Globally, 44% of pregnancies are unplanned.[17] Over half (56%) of unplanned pregnancies are aborted.[17] Among unintended pregnancies in the United States, 60% of the women used birth control to some extent during the month pregnancy occurred.[18]
Associated terms for pregnancy are gravid and parous. Gravidus and gravid come from the Latin for "heavy" and a pregnant female is sometimes referred to as a gravida.[19] Gravidity is a term used to describe the number of times that a female has been pregnant.  Similarly, the term parity is used for the number of times that a female carries a pregnancy to a viable stage.[20] Twins and other multiple births are counted as one pregnancy and birth.  A woman who has never been pregnant is referred to as a nulligravida.  A woman who is (or has been only) pregnant for the first time is referred to as a primigravida,[21] and a woman in subsequent pregnancies as a multigravida or as multiparous.[19][22] Therefore, during a second pregnancy a woman would be described as gravida 2, para 1 and upon live delivery as gravida 2, para 2. In-progress pregnancies, abortions, miscarriages and/or stillbirths account for parity values being less than the gravida number. In the case of a multiple birth the gravida number and parity value are increased by one only. Women who have never carried a pregnancy achieving more than 20 weeks of gestation age are referred to as nulliparous.[23]
The terms preterm and postterm  have largely replaced earlier terms of premature and postmature. Preterm and postterm are defined above, whereas premature and postmature have historical meaning and relate more to the infant's size and state of development rather than to the stage of pregnancy.[24][25]
The usual symptoms and discomforts of pregnancy do not significantly interfere with activities of daily living or pose a health-threat to the mother or baby. However, pregnancy complications can cause other more severe symptoms, such as those associated with anemia.
Common symptoms and discomforts of pregnancy include:
The chronology of pregnancy is, unless otherwise specified, generally given as gestational age, where the starting point is the woman's last menstrual period (LMP), or the corresponding age of the gestation as estimated by a more accurate method if available. Sometimes, timing may also use the fertilization age which is the age of the embryo.
According to American Congress of Obstetricians and Gynecologists, the main methods to calculate gestational age are:[29]
Due date estimation basically follows two steps:
Naegele's rule is a standard way of calculating the due date for a pregnancy when assuming a gestational age of 280 days at childbirth. The rule estimates the expected date of delivery (EDD) by adding a year, subtracting three months, and adding seven days to the origin of gestational age. Alternatively there are mobile apps, which essentially always give consistent estimations compared to each other and correct for leap year, while pregnancy wheels made of paper can differ from each other by 7 days and generally do not correct for leap year.[33]
Furthermore, actual childbirth has only a certain probability of occurring within the limits of the estimated due date. A study of singleton live births came to the result that childbirth has a standard deviation of 14 days when gestational age is estimated by first trimester ultrasound, and 16 days when estimated directly by last menstrual period.[31]
Through an interplay of hormones that includes follicle stimulating hormone that stimulates folliculogenesis and oogenesis creates a mature egg cell, the female gamete. Fertilization is the event where the egg cell fuses with the male gamete, spermatozoon.  After the point of fertilization, the fused product of the female and male gamete is referred to as a zygote or fertilized egg. The fusion of male and female gametes usually occurs following the act of sexual intercourse. Pregnancy rates for sexual intercourse are highest during the menstrual cycle time from some 5 days before until 1 to 2 days after ovulation.[34] Fertilization can also occur by assisted reproductive technology such as artificial insemination and in vitro fertilisation.
Fertilization (conception) is sometimes used as the initiation of pregnancy, with the derived age being termed fertilization age. Fertilization usually occurs about two weeks before the next expected menstrual period.
A third point in time is also considered by some people to be the true beginning of a pregnancy:  This is time of implantation, when the future fetus attaches to the lining of the uterus.  This is about a week to ten days after fertilization.[35]  In this model, during the time between conception and implantation, the future fetus exists, but the woman is not considered pregnant.
The sperm and the egg cell, which has been released from one of the female's two ovaries, unite in one of the two fallopian tubes. The fertilized egg, known as a zygote, then moves toward the uterus, a journey that can take up to a week to complete. Cell division begins approximately 24 to 36 hours after the male and female cells unite. Cell division continues at a rapid rate and the cells then develop into what is known as a blastocyst. The blastocyst arrives at the uterus and attaches to the uterine wall, a process known as implantation.
The development of the mass of cells that will become the infant is called embryogenesis during the first approximately ten weeks of gestation. During this time, cells begin to differentiate into the various body systems. The basic outlines of the organ, body, and nervous systems are established. By the end of the embryonic stage, the beginnings of features such as fingers, eyes, mouth, and ears become visible. Also during this time, there is development of structures important to the support of the embryo, including the placenta and umbilical cord. The placenta connects the developing embryo to the uterine wall to allow nutrient uptake, waste elimination, and gas exchange via the mother's blood supply. The umbilical cord is the connecting cord from the embryo or fetus to the placenta.
After about ten weeks of gestational age, the embryo becomes known as a fetus. At the beginning of the fetal stage, the risk of miscarriage decreases sharply.[36] At this stage, a fetus is about 30 mm (1.2 inches) in length, the heartbeat is seen via ultrasound, and the fetus makes involuntary motions.[37] During continued fetal development, the early body systems, and structures that were established in the embryonic stage continue to develop. Sex organs begin to appear during the third month of gestation. The fetus continues to grow in both weight and length, although the majority of the physical growth occurs in the last weeks of pregnancy.
Electrical brain activity is first detected between the fifth and sixth week of gestation.  It is considered primitive neural activity rather than the beginning of conscious thought. Synapses begin forming at 17 weeks, and begin to multiply quickly at week 28 until 3 to 4 months after birth.[38]
Embryo at 4 weeks after fertilization. (Gestational age of 6 weeks.)
Fetus at 8 weeks after fertilization. (Gestational age of 10 weeks.)
Fetus at 18 weeks after fertilization. (Gestational age of 20 weeks.)
Fetus at 38 weeks after fertilization. (Gestational age of 40 weeks.)
Relative size in 1st month (simplified illustration)
Relative size in 3rd month (simplified illustration)
Relative size in 5th month (simplified illustration)
Relative size in 9th month (simplified illustration)
During pregnancy, the woman undergoes many physiological changes, which are entirely normal, including behavioral, cardiovascular, hematologic, metabolic, renal, and respiratory changes. Increases in blood sugar, breathing, and cardiac output are all required. Levels of progesterone and oestrogens rise continually throughout pregnancy, suppressing the hypothalamic axis and therefore also the menstrual cycle.
The fetus is genetically different from the woman and can be viewed as an unusually successful allograft.[39] The main reason for this success is increased immune tolerance during pregnancy.[40] Immune tolerance is the concept that the body is able to not mount an immune system response against certain triggers.[39]
Pregnancy is typically broken into three periods, or trimesters, each of about three months.[41][42] Each trimester is defined as 14 weeks, for a total duration of 42 weeks, although the average duration of pregnancy is 40 weeks.[43] While there are no hard and fast rules, these distinctions are useful in describing the changes that take place over time.
Minute ventilation increases by 40% in the first trimester.[44] The womb will grow to the size of a lemon by eight weeks. Many symptoms and discomforts of pregnancy like nausea and tender breasts appear in the first trimester.[45]
Weeks 13 to 28 of the pregnancy are called the second trimester. Most women feel more energized in this period, and begin to put on weight as the symptoms of morning sickness subside and eventually fade away. The uterus, the muscular organ that holds the developing fetus, can expand up to 20 times its normal size during pregnancy.
Although the fetus begins to move during the first trimester, it is not until the second trimester that movement, known as quickening, can be felt. This typically happens in the fourth month, more specifically in the 20th to 21st week, or by the 19th week if the woman has been pregnant before. It is common for some women not to feel the fetus move until much later. During the second trimester, most women begin to wear maternity clothes.
Final weight gain takes place, which is the most weight gain throughout the pregnancy. The woman's abdomen will transform in shape as it drops due to the fetus turning in a downward position ready for birth. During the second trimester, the woman's abdomen would have been upright, whereas in the third trimester it will drop down low. The fetus moves regularly, and is felt by the woman. Fetal movement can become strong and be disruptive to the woman. The woman's navel will sometimes become convex, "popping" out, due to the expanding abdomen.
Head engagement, where the fetal head descends into cephalic presentation, relieves pressure on the upper abdomen with renewed ease in breathing. It also severely reduces bladder capacity, and increases pressure on the pelvic floor and the rectum.
It is also during the third trimester that maternal activity and sleep positions may affect fetal development due to restricted blood flow. For instance, the enlarged uterus may impede blood flow by compressing the vena cava when lying flat, which is relieved by lying on the left side.[46]
Childbirth, referred to as labor and delivery in the medical field, is the process whereby an infant is born.[47]
A woman is considered to be in labour when she begins experiencing regular uterine contractions, accompanied by changes of her cervix – primarily effacement and dilation. While childbirth is widely experienced as painful, some women do report painless labours, while others find that concentrating on the birth helps to quicken labour and lessen the sensations. Most births are successful vaginal births, but sometimes complications arise and a woman may undergo a cesarean section.
During the time immediately after birth, both the mother and the baby are hormonally cued to bond, the mother through the release of oxytocin, a hormone also released during breastfeeding.  Studies show that skin-to-skin contact between a mother and her newborn immediately after birth is beneficial for both the mother and baby.  A review done by the World Health Organization found that skin-to-skin contact between mothers and babies after birth reduces crying, improves mother–infant interaction, and helps mothers to breastfeed successfully.  They recommend that neonates be allowed to bond with the mother during their first two hours after birth, the period that they tend to be more alert than in the following hours of early life.[48]

In the ideal childbirth labor begins on its own when a woman is "at term".[14]
Events before completion of 37 weeks are considered preterm.[49] Preterm birth is associated with a range of complications and should be avoided if possible.[51]
Sometimes if a woman's water breaks or she has contractions before 39 weeks, birth is unavoidable.[50] However, spontaneous birth after 37 weeks is considered term and is not associated with the same risks of a pre-term birth.[47] Planned birth before 39 weeks by Caesarean section or labor induction, although "at term", results in an increased risk of complications.[52] This is from factors including underdeveloped lungs of newborns, infection due to underdeveloped immune system, feeding problems due to underdeveloped brain, and jaundice from underdeveloped liver.[53]
Babies born between 39 and 41 weeks gestation have better outcomes than babies born either before or after this range.[50] This special time period is called "full term".[50] Whenever possible, waiting for labor to begin on its own in this time period is best for the health of the mother and baby.[14] The decision to perform an induction must be made after weighing the risks and benefits, but is safer after 39 weeks.[14]
Events after 42 weeks are considered postterm.[50] When a pregnancy exceeds 42 weeks, the risk of complications for both the woman and the fetus increases significantly.[54][55] Therefore, in an otherwise uncomplicated pregnancy, obstetricians usually prefer to induce labour at some stage between 41 and 42 weeks.[56]
The postnatal period, also referred to as the puerperium, begins immediately after delivery and extends for about six weeks.[47] During this period, the mother's body begins the return to pre-pregnancy conditions that includes changes in hormone levels and uterus size.[47]
The beginning of pregnancy may be detected either based on symptoms by the woman herself, or by using pregnancy tests. However, an important condition with serious health implications that is quite common is the denial of pregnancy by the pregnant woman. About one in 475 denials will last until around the 20th week of pregnancy. The proportion of cases of denial, persisting until delivery is about 1 in 2500.[57] Conversely, some non-pregnant women have a very strong belief that they are pregnant along with some of the physical changes. This condition is known as a false pregnancy.[58]
Most pregnant women experience a number of symptoms,[59] which can signify pregnancy. A number of early medical signs are associated with pregnancy.[60][61] These signs include: 
Pregnancy detection can be accomplished using one or more various pregnancy tests,[63] which detect hormones generated by the newly formed placenta, serving as biomarkers of pregnancy.[64] Blood and urine tests can detect pregnancy 12 days after implantation.[65] Blood pregnancy tests are more sensitive than urine tests (giving fewer false negatives).[66] Home pregnancy tests are urine tests, and normally detect a pregnancy 12 to 15 days after fertilization.[67] A quantitative blood test can determine approximately the date the embryo was conceived because HCG doubles every 36 to 48 hours.[47] A single test of progesterone levels can also help determine how likely a fetus will survive in those with a threatened miscarriage (bleeding in early pregnancy).[68]
Obstetric ultrasonography can detect fetal abnormalities, detect  multiple pregnancies, and improve gestational dating at 24 weeks.[69] The resultant estimated gestational age and due date of the fetus are slightly more accurate than methods based on last menstrual period.[70] Ultrasound is used to measure the nuchal fold in order to screen for Downs syndrome.[71]
Pre-conception counseling is care that is provided to a woman and/ or couple to discuss conception, pregnancy, current health issues and recommendations for the period before pregnancy.[73]
Prenatal medical care is the medical and nursing care recommended for women during pregnancy, time intervals and exact goals of each visit differ by country.[74] Women who are high risk have better outcomes if they are seen regularly and frequently by a medical professional than women who are low risk.[75] A woman can be labeled as high risk for different reasons including previous complications in pregnancy, complications in the current pregnancy, current medical diseases, or social issues.[76][77]
The aim of good prenatal care is prevention, early identification, and treatment of any medical complications.[78] A basic prenatal visit consists of measurement of blood pressure, fundal height, weight and fetal heart rate, checking for symptoms of labor, and guidance for what to expect next.[73]
Nutrition during pregnancy is important to ensure healthy growth of the fetus.[79]  Nutrition during pregnancy is different from the non-pregnant state.[79]  There are increased energy requirements and specific micronutrient requirements.[79] Women benefit from education to encourage a balanced energy and protein intake during pregnancy.[80] Some women may need professional medical advice if their diet is affected by medical conditions, food allergies, or specific religious/ ethical beliefs.[81]
Adequate periconceptional (time before and right after conception) folic acid (also called folate or Vitamin B9) intake has been shown to decrease the risk of fetal neural tube defects, such as spina bifida.[82]  The neural tube develops during the first 28 days of pregnancy, a urine pregnancy test is not usually positive until 14 days post-conception, explaining the necessity to guarantee adequate folate intake before conception.[67][83] Folate is abundant in green leafy vegetables, legumes, and citrus.[84] In the United States and Canada, most wheat products (flour, noodles) are fortified with folic acid.[85]
DHA omega-3 is a major structural fatty acid in the brain and retina, and is naturally found in breast milk.[86] It is important for the woman to consume adequate amounts of DHA during pregnancy and while nursing to support her well-being and the health of her infant.[86]  Developing infants cannot produce DHA efficiently, and must receive this vital nutrient from the woman through the placenta during pregnancy and in breast milk after birth.[87]
Several micronutrients are important for the health of the developing fetus, especially in areas of the world where insufficient nutrition is common.[10] Women living in low and middle income countries are suggested to take multiple micronutrient supplements containing iron and folic acid.[10] These supplements have been shown to improve birth outcomes in developing countries, but do not have an effect on perinatal mortality.[10][88] Adequate intake of folic acid, and iron is often recommended.[89][90] In developed areas, such as Western Europe and the United States, certain nutrients such as Vitamin D and calcium, required for bone development, may also require supplementation.[91][92][93] Vitamin E supplementation has not been shown to improve birth outcomes.[94] Zinc supplementation has been associated with a decrease in preterm birth, but it is unclear whether it is causative.[95] Daily iron supplementation reduces the risk of maternal anemia.[96] Studies of routine daily iron supplementation for pregnant women found improvement in blood iron levels, without a clear clinical benefit.[97] The nutritional needs for women carrying twins or triplets are higher than those of women carrying one baby.[98]
Women are counseled to avoid certain foods, because of the possibility of contamination with bacteria or parasites that can cause illness.[99] Careful washing of fruits and raw vegetables may remove these pathogens, as may thoroughly cooking leftovers, meat, or processed meat.[100]  Unpasteurized dairy and deli meats may contain Listeria, which can cause neonatal meningitis, stillbirth and miscarriage.[101] Pregnant women are also more prone to Salmonella infections, can be in eggs and poultry, which should be thoroughly cooked.[102] Cat feces and undercooked meats may contain the parasite Toxoplasma gondii and can cause toxoplasmosis.[100] Practicing good hygiene in the kitchen can reduce these risks.[103]
Women are also counseled to eat seafood in moderation and to eliminate seafood known to be high in mercury because of the risk of birth defects.[102] Pregnant women are counseled to consume caffeine in moderation, because large amounts of caffeine are associated with miscarriage.[47]  However, the relationship between caffeine, birthweight, and preterm birth is unclear.[104]
The amount of healthy weight gain during a pregnancy varies.[105] Weight gain is related to the weight of the baby, the placenta, extra circulatory fluid, larger tissues, and fat and protein stores.[79] Most needed weight gain occurs later in pregnancy.[106]
The Institute of Medicine recommends an overall pregnancy weight gain for those of normal weight (body mass index of 18.5–24.9), of 11.3–15.9 kg (25–35 pounds) having a singleton pregnancy.[107] Women who are underweight (BMI of less than 18.5), should gain between 12.7–18 kg (28–40 lbs), while those who are overweight (BMI of 25–29.9) are advised to gain between 6.8–11.3 kg (15–25 lbs) and those who are obese (BMI>30) should gain between 5–9 kg (11–20 lbs).[108] These values reference the expectations for a term pregnancy.
During pregnancy, insufficient or excessive weight gain can compromise the health of the mother and fetus.[106] The most effective intervention for weight gain in underweight women is not clear.[106] Being or becoming overweight in pregnancy increases the risk of complications for mother and fetus, including cesarean section, gestational hypertension, pre-eclampsia, macrosomia and shoulder dystocia.[105] Excessive weight gain can make losing weight after the pregnancy difficult.[105][109]
Around 50% of women of childbearing age in developed countries like the United Kingdom are overweight or obese before pregnancy.[109] Diet modification is the most effective way to reduce weight gain and associated risks in pregnancy.[109] A diet that has foods with a low glycemic index may help prevent the onset of gestational diabetes.[110][needs update]
Drugs used during pregnancy can have temporary or permanent effects on the fetus.[111] Anything (including drugs) that can cause permanent deformities in the fetus are labeled as teratogens.[112]  In the U.S., drugs were classified into categories A, B, C, D and X based on the Food and Drug Administration (FDA) rating system to provide therapeutic guidance based on potential benefits and fetal risks.[113] Drugs, including some multivitamins, that have demonstrated no fetal risks after controlled studies in humans are classified as Category A.[111] On the other hand, drugs like thalidomide with proven fetal risks that outweigh all benefits are classified as Category X.[111]
The use of recreational drugs in pregnancy can cause various pregnancy complications.[47]
Intrauterine exposure to environmental toxins in pregnancy has the potential to cause adverse effects on the development of the embryo/fetus and to cause pregnancy complications.[47] Air pollution has been associated with low birth weight infants.[120] Conditions of particular severity in pregnancy include mercury poisoning and lead poisoning.[47] To minimize exposure to environmental toxins, the American College of Nurse-Midwives recommends: checking whether the home has lead paint, washing all fresh fruits and vegetables thoroughly and buying organic produce, and avoiding cleaning products labeled "toxic" or any product with a warning on the label.[121]
Pregnant women can also be exposed to toxins in the workplace, including airborne particles. The effects of wearing N95 filtering facepiece respirators are similar for pregnant women as for non-pregnant women, and wearing a respirator for one hour does not affect the fetal heart rate.[122]
Most women can continue to engage in sexual activity throughout pregnancy.[123] Most research suggests that during pregnancy both sexual desire and frequency of sexual relations decrease.[124][125] In context of this overall decrease in desire, some studies indicate a second-trimester increase, preceding a decrease during the third trimester.[126][127]
Sex during pregnancy is a low-risk behavior except when the healthcare provider advises that sexual intercourse be avoided for particular medical reasons.[123] For a healthy pregnant woman, there is no safe or right way to have sex during pregnancy.[123]  Pregnancy alters the vaginal flora with a reduction in microscopic species/genus diversity.[128]
Regular aerobic exercise during pregnancy appears to improve (or maintain) physical fitness.[129] Physical exercise during pregnancy does appear to decrease the need for C-section.[130] Bed rest, outside of research studies, is not recommended as there is no evidence of benefit and potential harm.[131]
The Clinical Practice Obstetrics Committee of Canada recommends that "All women without contraindications should be encouraged to participate in aerobic and strength-conditioning exercises as part of a healthy lifestyle during their pregnancy".[132] Although an upper level of safe exercise intensity has not been established, women who were regular exercisers before pregnancy and who have uncomplicated pregnancies should be able to engage in high intensity exercise programs.[132] In general, participation in a wide range of recreational activities appears to be safe, with the avoidance of those with a high risk of falling such as horseback riding or skiing or those that carry a risk of abdominal trauma, such as soccer or hockey.[133]
The American College of Obstetricians and Gynecologists reports that in the past, the main concerns of exercise in pregnancy were focused on the fetus and any potential maternal benefit was thought to be offset by potential risks to the fetus. However, they write that more recent information suggests that in the uncomplicated pregnancy, fetal injuries are highly unlikely.[133]  They do, however, list several circumstances when a woman should contact her health care provider before continuing with an exercise program: vaginal bleeding, dyspnea before exertion, dizziness, headache, chest pain, muscle weakness, preterm labor, decreased fetal movement, amniotic fluid leakage, and calf pain or swelling (to rule out thrombophlebitis).[133]
It has been suggested that shift work and exposure to bright light at night should be avoided at least during the last trimester of pregnancy to decrease the risk of psychological and behavioral problems in the newborn.[134]
The increased levels of progesterone and estrogen during in pregnancy can develop gingivitis; the gums become edematous, red in colour, and tend to bleed.[135] Also a pyogenic granuloma or “pregnancy tumor,” is commonly seen on the labial surface of the papilla. Lesions can be treated by local debridement or deep incision depending on their size, and by following adequate oral hygiene measures.[136] There have been suggestions that severe periodontitis may increase the risk of having preterm birth and low birth weight, however, a Cochrane review found insufficient evidence to determine if periodontitis can develop adverse birth outcomes.[137]
Each year, ill health as a result of pregnancy is experienced (sometimes permanently) by more than 20 million women around the world.[138] In 2016, complications of pregnancy resulted in 230,600 deaths down from 377,000 deaths in 1990.[12] Common causes include bleeding (72,000), infections (20,000), hypertensive diseases of pregnancy (32,000), obstructed labor (10,000), and pregnancy with abortive outcome (20,000), which includes miscarriage, ectopic pregnancy, and elective abortion.[12]
The following are some examples of pregnancy complications:
There is also an increased susceptibility and severity of certain infections in pregnancy.
A pregnant woman may have intercurrent diseases, defined as disease not directly caused by the pregnancy, but that may become worse or be a potential risk to the pregnancy.
Medical imaging may be indicated in pregnancy because of pregnancy complications, intercurrent diseases or routine prenatal care. Magnetic resonance imaging (MRI) without MRI contrast agents as well as obstetric ultrasonography are not associated with any risk for the mother or the fetus, and are the imaging techniques of choice for pregnant women.[146] Projectional radiography, X-ray computed tomography and nuclear medicine imaging result in some degree of ionizing radiation exposure, but in most cases the absorbed doses are not associated with harm to the baby.[146] At higher dosages, effects can include miscarriage, birth defects and intellectual disability.[146]
About 213 million pregnancies occurred in 2012 of which 190 million were in the developing world and 23 million were in the developed world.[11] This is about 133 pregnancies per 1,000 women between the ages of 15 and 44.[11] About 10% to 15% of recognized pregnancies end in miscarriage.[2] Globally, 44% of pregnancies are unplanned. Over half (56%) of unplanned pregnancies are aborted. In countries where abortion is prohibited, or only carried out in circumstances where the mother's life is at risk, 48% of unplanned pregnancies are aborted illegally. Compared to the rate in countries where abortion is legal, at 69%.[17]
Of pregnancies in 2012, 120 million occurred in Asia, 54 million in Africa, 19 million in Europe, 18 million in Latin America and the Caribbean, 7 million in North America, and 1 million in Oceania.[11] Pregnancy rates are 140 per 1000 women of childbearing age in the developing world and 94 per 1000 in the developed world.[11]
The rate of pregnancy, as well as the ages at which it occurs, differ by country and region. It is influenced by a number of factors, such as cultural, social and religious norms; access to contraception; and rates of education. The total fertility rate (TFR) in 2013 was estimated to be highest in Niger (7.03 children/woman) and lowest in Singapore (0.79 children/woman).[147]
In Europe, the average childbearing age has been rising continuously for some time. In Western, Northern, and Southern Europe, first-time mothers are on average 26 to 29 years old, up from 23 to 25 years at the start of the 1970s. In a number of European countries (Spain), the mean age of women at first childbirth has crossed the 30-year threshold.
This process is not restricted to Europe. Asia, Japan and the United States are all seeing average age at first birth on the rise, and increasingly the process is spreading to countries in the developing world like China, Turkey and Iran. In the US, the average age of first childbirth was 25.4 in 2010.[148]
In the United States and United Kingdom, 40% of pregnancies are unplanned, and between a quarter and half of those unplanned pregnancies were unwanted pregnancies.[149][150]
In most cultures, pregnant women have a special status in society and receive particularly gentle care.[151] At the same time, they are subject to expectations that may exert great psychological pressure, such as having to produce a son and heir. In many traditional societies, pregnancy must be preceded by marriage, on pain of ostracism of mother and (illegitimate) child.
Overall, pregnancy is accompanied by numerous customs that are often subject to ethnological research, often rooted in traditional medicine or religion. The baby shower is an example of a modern custom.
Pregnancy is an important topic in sociology of the family. The prospective child may preliminarily be placed into numerous social roles. The parents' relationship and the relation between parents and their surroundings are also affected.
A belly cast may be made during pregnancy as a keepsake.
Images of pregnant women, especially small figurines, were made in traditional cultures in many places and periods, though it is rarely one of the most common types  of image.  These include ceramic figures from some Pre-Columbian cultures, and a few figures from most of the ancient Mediterranean cultures. Many of these seem to be connected with fertility.  Identifying whether such figures are actually meant to show pregnancy is often a problem, as well as understanding their role in the culture concerned.
Among the oldest surviving examples of the depiction of pregnancy are prehistoric figurines found across much of Eurasia and collectively known as Venus figurines.  Some of these appear to be pregnant.
Due to the important role of the Mother of God in Christianity, the Western visual arts have a long tradition of depictions of pregnancy, especially in the biblical scene of the Visitation, and devotional images called a Madonna del Parto.[152]
The unhappy scene usually called Diana and Callisto, showing the moment of discovery of Callisto's forbidden pregnancy, is sometimes painted from the Renaissance onwards.  Gradually, portraits of pregnant women began to appear, with a particular fashion for "pregnancy portraits" in elite portraiture of the years around 1600.
Pregnancy, and especially pregnancy of unmarried women, is also an important motif in literature. Notable examples include Hardy's Tess of the d'Urbervilles and Goethe's Faust.
Anatomical model of a pregnant woman; Stephan Zick (1639-1715); 1700; Germanisches Nationalmuseum
Statue of a pregnant woman, Macedonia
Bronze figure of a pregnant naked woman by Danny Osborne, Merrion Square
Marcus Gheeraerts the Younger Portrait of Susanna Temple, second wife of Sir Martin Lister, 1620
Octave Tassaert, The Waif aka L'abandonnée 1852, Musée Fabre, Montpellier
Modern reproductive medicine offers many forms of assisted reproductive technology for couples who stay childless against their will, such as fertility medication, artificial insemination, in vitro fertilization and surrogacy.
An abortion is the termination of an embryo or fetus, either naturally or via medical methods.[153] When done electively, it is more often done within the first trimester than the second, and rarely in the third.[36]  Not using contraception, contraceptive failure, poor family planning or rape can lead to undesired pregnancies. Legality of socially indicated abortions varies widely both internationally and through time. In most countries of Western Europe, abortions during the first trimester were a criminal offense a few decades ago[when?] but have since been legalized, sometimes subject to mandatory consultations. In Germany, for example, as of 2009 less than 3% of abortions had a medical indication.
Many countries have various legal regulations in place to protect pregnant women and their children. Maternity Protection Convention ensures that pregnant women are exempt from activities such as night shifts or carrying heavy stocks. Maternity leave typically provides paid leave from work during roughly the last trimester of pregnancy and for some time after birth. Notable extreme cases include Norway (8 months with full pay) and the United States (no paid leave at all except in some states). Moreover, many countries have laws against pregnancy discrimination.
In 2014, the American state of Tennessee passed a law which allows prosecutors to charge a woman with criminal assault if she uses illegal drugs during her pregnancy and her fetus or newborn is considered harmed as a result.[154]
In the United States, laws make some actions that result in miscarriage or stillbirth crimes.  One such law is the federal Unborn Victims of Violence Act.



Veterinary oncology - Wikipedia
Veterinary oncology is a subspecialty of veterinary medicine that deals with cancer diagnosis and treatment in animals. Cancer is a major cause of death in pet animals. In one study, 45% of the dogs that reached 10 years of age or older died of cancer.[1]
Skin tumors are the most frequently diagnosed type of tumor in domestic animals for two reasons: 1. constant exposure of animal skin to the sun and external environment, 2. skin tumors are easy to see because they are on the outside of the animal.[2]
[3][4][5]
[3][4][5]
These statistics, being from the 1960s, may not be an accurate representation of cancer in dogs currently.
Companion animals such as dogs and cats suffer from many of the same types of cancer as humans. Cancer research with dogs has helped in the design of clinical trials for cancer therapy for humans. In the spirit of the One Health movement (global collaborative research on human and animal health) such human-animal connections in cancer research could benefit both humans and animals with cancer in the future.[1][6]
Animals with cancer also sometimes function as animal sentinels that provide an early warning of carcinogens and an environmental health hazard to humans.[7][8]
Veterinarians use the HHHHHMM Scale to discuss animal quality of life with pet owners before a euthanasia decision is made for a pet with an incurable disease such as cancer (the letters stand for Hurt Hunger Hydration Hygiene Happiness Mobility and "More good days than bad days").[9][10]



Clonally transmissible cancer - Wikipedia
A transmissible cancer is a cancer cell or cluster of cancer cells that can be transferred between individuals without the involvement of an infectious agent, such as an oncovirus.[1][2] Transmission of cancer between humans is rare.[2]
Contagious cancers are known to occur in dogs, Tasmanian devils, Syrian hamsters, and some marine bivalves including soft-shell clams. These cancers have a relatively stable genome as they are transmitted.[3]
In humans, a significant fraction of Kaposi's sarcoma occurring after transplantation may be due to tumorous outgrowth of donor cells.[4] Although Kaposi's sarcoma is caused by a virus (Kaposi's sarcoma-associated herpesvirus), in these cases, it appears likely that transmission of virus-infected tumor cells—rather than the free virus—caused tumors in the transplant recipients.[2]
Clonally transmissible cancer, caused by a clone of malignant cells rather than a virus,[8] is an extremely rare disease modality,[9] with few transmissible cancers being known[1] Animals that have undergone population bottlenecks may be at greater risks of contracting transmissible cancers.[10] Because of their transmission, it was initially thought that these diseases were caused by the transfer of oncoviruses, in the manner of cervical cancer caused by HPV.[2] However, canine transmissible venereal tumor mutes the expression of the immune response, whereas the Syrian hamster disease spreads due to lack of genetic diversity.[11]
