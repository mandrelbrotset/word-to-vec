Programming language - Wikipedia

A programming language is a formal language, which comprises a set of instructions used to produce various kinds of output. Programming languages are used to create programs that implement specific algorithms.
Most programming languages consist of instructions for computers, although there are programmable machines that use a limited set of specific instructions, rather than the general programming languages of modern computers. Early ones preceded the invention of the digital computer, the first probably being the automatic flute player described in the 9th century by the brothers Musa in Baghdad, during the Islamic Golden Age.[1] From the early 1800s, programs were used to direct the behavior of machines such as Jacquard looms, music boxes and player pianos.[2] However, their programs (such as a player piano's scrolls) could not produce different behavior in response to some input or condition.
Thousands of different programming languages have been created, mainly in the computer field, and many more still are being created every year. Many programming languages require computation to be specified in an imperative form (i.e., as a sequence of operations to perform) while other languages use other forms of program specification such as the declarative form (i.e. the desired result is specified, not how to achieve it).
The description of a programming language is usually split into the two components of syntax (form) and semantics (meaning). Some languages are defined by a specification document (for example, the C programming language is specified by an ISO Standard) while other languages (such as Perl) have a dominant implementation that is treated as a reference. Some languages have both, with the basic language defined by a standard and extensions taken from the dominant implementation being common.
A programming language is a notation for writing programs, which are specifications of a computation or algorithm.[3] Some, but not all, authors restrict the term "programming language" to those languages that can express all possible algorithms.[3][4] Traits often considered important for what constitutes a programming language include:
Markup languages like XML, HTML, or troff, which define structured data, are not usually considered programming languages.[13][14][15] Programming languages may, however, share the syntax with markup languages if a computational semantics is defined. XSLT, for example, is a Turing complete language entirely using XML syntax.[16][17][18] Moreover, LaTeX, which is mostly used for structuring documents, also contains a Turing complete subset.[19][20]
The term computer language is sometimes used interchangeably with programming language.[21] However, the usage of both terms varies among authors, including the exact scope of each. One usage describes programming languages as a subset of computer languages.[22] In this vein, languages used in computing that have a different goal than expressing computer programs are generically designated computer languages. For instance, markup languages are sometimes referred to as computer languages to emphasize that they are not meant to be used for programming.[23]
Another usage regards programming languages as theoretical constructs for programming abstract machines, and computer languages as the subset thereof that runs on physical computers, which have finite hardware resources.[24] John C. Reynolds emphasizes that formal specification languages are just as much programming languages as are the languages intended for execution. He also argues that textual and even graphical input formats that affect the behavior of a computer are programming languages, despite the fact they are commonly not Turing-complete, and remarks that ignorance of programming language concepts is the reason for many flaws in input formats.[25]
Very early computers, such as Colossus, were programmed without the help of a stored program, by modifying their circuitry or setting banks of physical controls.
Slightly later, programs could be written in machine language, where the programmer writes each instruction in a numeric form the hardware can execute directly. For example, the instruction to add the value in two memory location might consist of 3 numbers: a "opcode" that selects the "add" operation, and two memory locations. The programs, in decimal or binary form, were read in from punched cards or paper tape or magnetic tape or toggled in on switches on the front panel of the computer.  Machine languages were later termed first-generation programming languages (1GL).
The next step was development of so-called second-generation programming languages (2GL) or assembly languages, which were still closely tied to the instruction set architecture of the specific computer. These served to make the program much more human-readable and relieved the programmer of tedious and error-prone address calculations.
The first high-level programming languages, or third-generation programming languages (3GL), were written in the 1950s. An early high-level programming language to be designed for a computer was Plankalkül, developed for the German Z3 by Konrad Zuse between 1943 and 1945. However, it was not implemented until 1998 and 2000.[26]
John Mauchly's Short Code, proposed in 1949, was one of the first high-level languages ever developed for an electronic computer.[27] Unlike machine code, Short Code statements represented mathematical expressions in understandable form. However, the program had to be translated into machine code every time it ran, making the process much slower than running the equivalent machine code.
At the University of Manchester, Alick Glennie developed Autocode in the early 1950s. A programming language, it used a compiler to automatically convert the language into machine code. The first code and compiler was developed in 1952 for the Mark 1 computer at the University of Manchester and is considered to be the first compiled high-level programming language.[28][29]
The second autocode was developed for the Mark 1 by R. A. Brooker in 1954 and was called the "Mark 1 Autocode". Brooker also developed an autocode for the Ferranti Mercury in the 1950s in conjunction with the University of Manchester. The version for the EDSAC 2 was devised by D. F. Hartley of  University of Cambridge Mathematical Laboratory in 1961. Known as EDSAC 2 Autocode, it was a straight development from Mercury Autocode adapted for local circumstances and was noted for its object code optimisation and source-language diagnostics which were advanced for the time. A contemporary but separate thread of development, Atlas Autocode was developed for the University of Manchester Atlas 1 machine.
In 1954, FORTRAN was invented at IBM by John Backus. It was the first widely used high-level general purpose programming language to have a functional implementation, as opposed to just a design on paper.[30][31] It is still a popular language for high-performance computing[32] and is used for programs that benchmark and rank the world's fastest supercomputers.[33]
Another early programming language was devised by Grace Hopper in the US, called FLOW-MATIC. It was developed for the UNIVAC I at Remington Rand during the period from 1955 until 1959. Hopper found that business data processing customers were uncomfortable with mathematical notation, and in early 1955, she and her team wrote a specification for an English programming language and implemented a prototype.[34] The FLOW-MATIC compiler became publicly available in early 1958 and was substantially complete in 1959.[35] Flow-Matic was a major influence in the design of COBOL, since only it and its direct descendant AIMACO were in actual use at the time.[36]
The increased use of high-level languages introduced a requirement for low-level programming languages or system programming languages. These languages, to varying degrees, provide facilities between assembly languages and high-level languages and can be used to perform tasks which require direct access to hardware facilities but still provide higher-level control structures and error-checking.
The period from the 1960s to the late 1970s brought the development of the major language paradigms now in use:
Each of these languages spawned descendants, and most modern programming languages count at least one of them in their ancestry.
The 1960s and 1970s also saw considerable debate over the merits of structured programming, and whether programming languages should be designed to support it.[39] Edsger Dijkstra, in a famous 1968 letter published in the Communications of the ACM, argued that GOTO statements should be eliminated from all "higher level" programming languages.[40]
The 1980s were years of relative consolidation. C++ combined object-oriented and systems programming. The United States government standardized Ada, a systems programming language derived from Pascal and intended for use by defense contractors. In Japan and elsewhere, vast sums were spent investigating so-called "fifth generation" languages that incorporated logic programming constructs.[41] The functional languages community moved to standardize ML and Lisp. Rather than inventing new paradigms, all of these movements elaborated upon the ideas invented in the previous decades.
One important trend in language design for programming large-scale systems during the 1980s was an increased focus on the use of modules or large-scale organizational units of code. Modula-2, Ada, and ML all developed notable module systems in the 1980s, which were often wedded to generic programming constructs.[42]
The rapid growth of the Internet in the mid-1990s created opportunities for new languages. Perl, originally a Unix scripting tool first released in 1987, became common in dynamic websites. Java came to be used for server-side programming, and bytecode virtual machines became popular again in commercial settings with their promise of "Write once, run anywhere" (UCSD Pascal had been popular for a time in the early 1980s). These developments were not fundamentally novel, rather they were refinements of many existing languages and paradigms (although their syntax was often based on the C family of programming languages).
Programming language evolution continues, in both industry and research. Current directions include security and reliability verification, new kinds of modularity (mixins, delegates, aspects), and database integration such as Microsoft's LINQ.
Fourth-generation programming languages (4GL) are computer programming languages which aim to provide a higher level of abstraction of the internal computer hardware details than 3GLs. Fifth generation programming languages (5GL) are programming languages based on solving problems using constraints given to the program, rather than using an algorithm written by a programmer.
All programming languages have some primitive building blocks for the description of data and the processes or transformations applied to them (like the addition of two numbers or the selection of an item from a collection). These primitives are defined by syntactic and semantic rules which describe their structure and meaning respectively.
A programming language's surface form is known as its syntax. Most programming languages are purely textual; they use sequences of text including words, numbers, and punctuation, much like written natural languages. On the other hand, there are some programming languages which are more graphical in nature, using visual relationships between symbols to specify a program.
The syntax of a language describes the possible combinations of symbols that form a syntactically correct program. The meaning given to a combination of symbols is handled by semantics (either formal or hard-coded in a reference implementation). Since most languages are textual, this article discusses textual syntax.
Programming language syntax is usually defined using a combination of regular expressions (for lexical structure) and Backus–Naur form (for grammatical structure). Below is a simple grammar, based on Lisp:
This grammar specifies the following:
The following are examples of well-formed token sequences in this grammar: 12345, () and (a b c232 (1)).
Not all syntactically correct programs are semantically correct. Many syntactically correct programs are nonetheless ill-formed, per the language's rules; and may (depending on the language specification and the soundness of the implementation) result in an error on translation or execution. In some cases, such programs may exhibit undefined behavior. Even when a program is well-defined within a language, it may still have a meaning that is not intended by the person who wrote it.
Using natural language as an example, it may not be possible to assign a meaning to a grammatically correct sentence or the sentence may be false:
The following C language fragment is syntactically correct, but performs operations that are not semantically defined (the operation *p >> 4 has no meaning for a value having a complex type and p->im is not defined because the value of p is the null pointer):
If the type declaration on the first line were omitted, the program would trigger an error on compilation, as the variable "p" would not be defined. But the program would still be syntactically correct since type declarations provide only semantic information.
The grammar needed to specify a programming language can be classified by its position in the Chomsky hierarchy. The syntax of most programming languages can be specified using a Type-2 grammar, i.e., they are context-free grammars.[43] Some languages, including Perl and Lisp, contain constructs that allow execution during the parsing phase. Languages that have constructs that allow the programmer to alter the behavior of the parser make syntax analysis an undecidable problem, and generally blur the distinction between parsing and execution.[44] In contrast to Lisp's macro system and Perl's BEGIN blocks, which may contain general computations, C macros are merely string replacements and do not require code execution.[45]
The term semantics refers to the meaning of languages, as opposed to their form (syntax).
The static semantics defines restrictions on the structure of valid texts that are hard or impossible to express in standard syntactic formalisms.[3] For compiled languages, static semantics essentially include those semantic rules that can be checked at compile time. Examples include checking that every identifier is declared before it is used (in languages that require such declarations) or that the labels on the arms of a case statement are distinct.[46] Many important restrictions of this type, like checking that identifiers are used in the appropriate context (e.g. not adding an integer to a function name), or that subroutine calls have the appropriate number and type of arguments, can be enforced by defining them as rules in a logic called a type system. Other forms of static analyses like data flow analysis may also be part of static semantics. Newer programming languages like Java and C# have definite assignment analysis, a form of data flow analysis, as part of their static semantics.
Once data has been specified, the machine must be instructed to perform operations on the data. For example, the semantics may define the strategy by which expressions are evaluated to values, or the manner in which control structures conditionally execute statements. The dynamic semantics (also known as execution semantics) of a language defines how and when the various constructs of a language should produce a program behavior. There are many ways of defining execution semantics. Natural language is often used to specify the execution semantics of languages commonly used in practice. A significant amount of academic research went into formal semantics of programming languages, which allow execution semantics to be specified in a formal manner. Results from this field of research have seen limited application to programming language design and implementation outside academia.
A type system defines how a programming language classifies values and expressions into types, how it can manipulate those types and how they interact. The goal of a type system is to verify and usually enforce a certain level of correctness in programs written in that language by detecting certain incorrect operations. Any decidable type system involves a trade-off: while it rejects many incorrect programs, it can also prohibit some correct, albeit unusual programs. In order to bypass this downside, a number of languages have type loopholes, usually unchecked casts that may be used by the programmer to explicitly allow a normally disallowed operation between different types. In most typed languages, the type system is used only to type check programs, but a number of languages, usually functional ones, infer types, relieving the programmer from the need to write type annotations. The formal design and study of type systems is known as type theory.
A language is typed if the specification of every operation defines types of data to which the operation is applicable, with the implication that it is not applicable to other types.[47] For example, the data represented by "this text between the quotes" is a string, and in many programming languages dividing a number by a string has no meaning and will be rejected by the compilers. The invalid operation may be detected when the program is compiled ("static" type checking) and will be rejected by the compiler with a compilation error message, or it may be detected when the program is run ("dynamic" type checking), resulting in a run-time exception. Many languages allow a function called an exception handler to be written to handle this exception and, for example, always return "-1" as the result.
A special case of typed languages are the single-type languages. These are often scripting or markup languages, such as REXX or SGML, and have only one data type[dubious  – discuss]-—most commonly character strings which are used for both symbolic and numeric data.
In contrast, an untyped language, such as most assembly languages, allows any operation to be performed on any data, which are generally considered to be sequences of bits of various lengths.[47] High-level languages which are untyped include BCPL, Tcl, and some varieties of Forth.
In practice, while few languages are considered typed from the point of view of type theory (verifying or rejecting all operations), most modern languages offer a degree of typing.[47] Many production languages provide means to bypass or subvert the type system, trading type-safety for finer control over the program's execution (see casting).
In static typing, all expressions have their types determined prior to when the program is executed, typically at compile-time. For example, 1 and (2+2) are integer expressions; they cannot be passed to a function that expects a string, or stored in a variable that is defined to hold dates.[47]
Statically typed languages can be either manifestly typed or type-inferred. In the first case, the programmer must explicitly write types at certain textual positions (for example, at variable declarations). In the second case, the compiler infers the types of expressions and declarations based on context. Most mainstream statically typed languages, such as C++, C# and Java, are manifestly typed. Complete type inference has traditionally been associated with less mainstream languages, such as Haskell and ML. However, many manifestly typed languages support partial type inference; for example, C++, Java and C# all infer types in certain limited cases.[48] Additionally, some programming languages allow for some types to be automatically converted to other types; for example, an int can be used where the program expects a float.
Dynamic typing, also called latent typing, determines the type-safety of operations at run time; in other words, types are associated with run-time values rather than textual expressions.[47] As with type-inferred languages, dynamically typed languages do not require the programmer to write explicit type annotations on expressions. Among other things, this may permit a single variable to refer to values of different types at different points in the program execution. However, type errors cannot be automatically detected until a piece of code is actually executed, potentially making debugging more difficult. Lisp, Smalltalk, Perl, Python, JavaScript, and Ruby are all examples of dynamically typed languages.
Weak typing allows a value of one type to be treated as another, for example treating a string as a number.[47] This can occasionally be useful, but it can also allow some kinds of program faults to go undetected at compile time and even at run time.
Strong typing prevents the above. An attempt to perform an operation on the wrong type of value raises an error.[47] Strongly typed languages are often termed type-safe or safe.
An alternative definition for "weakly typed" refers to languages, such as Perl and JavaScript, which permit a large number of implicit type conversions. In JavaScript, for example, the expression 2 * x implicitly converts x to a number, and this conversion succeeds even if x is null, undefined, an Array, or a string of letters. Such implicit conversions are often useful, but they can mask programming errors.
Strong and static are now generally considered orthogonal concepts, but usage in the literature differs. Some use the term strongly typed to mean strongly, statically typed, or, even more confusingly, to mean simply statically typed. Thus C has been called both strongly typed and weakly, statically typed.[49][50]
It may seem odd to some professional programmers that C could be "weakly, statically typed". However, notice that the use of the generic pointer, the void* pointer, does allow for casting of pointers to other pointers without needing to do an explicit cast. This is extremely similar to somehow casting an array of bytes to any kind of datatype in C without using an explicit cast, such as (int) or (char).
Most programming languages have an associated core library (sometimes known as the 'standard library', especially if it is included as part of the published language standard), which is conventionally made available by all implementations of the language. Core libraries typically include definitions for commonly used algorithms, data structures, and mechanisms for input and output.
The line between a language and its core library differs from language to language. In some cases, the language designers may treat the library as a separate entity from the language. However, a language's core library is often treated as part of the language by its users, and some language specifications even require that this library be made available in all implementations. Indeed, some languages are designed so that the meanings of certain syntactic constructs cannot even be described without referring to the core library. For example, in Java, a string literal is defined as an instance of the java.lang.String class; similarly, in Smalltalk, an anonymous function expression (a "block") constructs an instance of the library's BlockContext class. Conversely, Scheme contains multiple coherent subsets that suffice to construct the rest of the language as library macros, and so the language designers do not even bother to say which portions of the language must be implemented as language constructs, and which must be implemented as parts of a library.
Programming languages share properties with natural languages related to their purpose as vehicles for communication, having a syntactic form separate from its semantics, and showing language families of related languages branching one from another.[51][52] But as artificial constructs, they also differ in fundamental ways from languages that have evolved through usage. A significant difference is that a programming language can be fully described and studied in its entirety, since it has a precise and finite definition.[53] By contrast, natural languages have changing meanings given by their users in different communities. While constructed languages are also artificial languages designed from the ground up with a specific purpose, they lack the precise and complete semantic definition that a programming language has.
Many programming languages have been designed from scratch, altered to meet new needs, and combined with other languages. Many have eventually fallen into disuse. Although there have been attempts to design one "universal" programming language that serves all purposes, all of them have failed to be generally accepted as filling this role.[54] The need for diverse programming languages arises from the diversity of contexts in which languages are used:
One common trend in the development of programming languages has been to add more ability to solve problems using a higher level of abstraction. The earliest programming languages were tied very closely to the underlying hardware of the computer. As new programming languages have developed, features have been added that let programmers express ideas that are more remote from simple translation into underlying hardware instructions. Because programmers are less tied to the complexity of the computer, their programs can do more computing with less effort from the programmer. This lets them write more functionality per time unit.[55]

Natural language programming has been proposed as a way to eliminate the need for a specialized language for programming. However, this goal remains distant and its benefits are open to debate. Edsger W. Dijkstra took the position that the use of a formal language is essential to prevent the introduction of meaningless constructs, and dismissed natural language programming as "foolish".[56] Alan Perlis was similarly dismissive of the idea.[57] Hybrid approaches have been taken in Structured English and SQL.
A language's designers and users must construct a number of artifacts that govern and enable the practice of programming. The most important of these artifacts are the language specification and implementation.
The specification of a programming language is an artifact that the language users and the implementors can use to agree upon whether a piece of source code is a valid program in that language, and if so what its behavior shall be.
A programming language specification can take several forms, including the following:
An implementation of a programming language provides a way to write programs in that language and execute them on one or more configurations of hardware and software. There are, broadly, two approaches to programming language implementation: compilation and interpretation. It is generally possible to implement a language using either technique.
The output of a compiler may be executed by hardware or a program called an interpreter. In some implementations that make use of the interpreter approach there is no distinct boundary between compiling and interpreting. For instance, some implementations of BASIC compile and then execute the source a line at a time.
Programs that are executed directly on the hardware usually run several orders of magnitude faster than those that are interpreted in software.[citation needed]
One technique for improving the performance of interpreted programs is just-in-time compilation. Here the virtual machine, just before execution, translates the blocks of bytecode which are going to be used to machine code, for direct execution on the hardware.
Although most of the most commonly used programming languages have fully open specifications and implementations, many programming languages exist only as proprietary programming languages with the implementation available only from a single vendor, which may claim that such a proprietary language is their intellectual property. Proprietary programming languages are commonly domain specific languages or internal scripting languages for a single product; some proprietary languages are used only internally within a vendor, while others are available to external users.
Some programming languages exist on the border between proprietary and open; for example, Oracle Corporation asserts proprietary rights to some aspects of the Java programming language,[61] and Microsoft's C# programming language, which has open implementations of most parts of the system, also has Common Language Runtime (CLR) as a closed environment.[62]
Many proprietary languages are widely used, in spite of their proprietary nature; examples include MATLAB, VBScript, and Wolfram Language.  Some languages may make the transition from closed to open; for example, Erlang was originally an Ericsson's internal programming language.[63]
Thousands of different programming languages have been created, mainly in the computing field.[64]
Software is commonly built with 5 programming languages or more.[65]
Programming languages differ from most other forms of human expression in that they require a greater degree of precision and completeness. When using a natural language to communicate with other people, human authors and speakers can be ambiguous and make small errors, and still expect their intent to be understood. However, figuratively speaking, computers "do exactly what they are told to do", and cannot "understand" what code the programmer intended to write. The combination of the language definition, a program, and the program's inputs must fully specify the external behavior that occurs when the program is executed, within the domain of control of that program. On the other hand, ideas about an algorithm can be communicated to humans without the precision required for execution by using pseudocode, which interleaves natural language with code written in a programming language.
A programming language provides a structured mechanism for defining pieces of data, and the operations or transformations that may be carried out automatically on that data. A programmer uses the abstractions present in the language to represent the concepts involved in a computation. These concepts are represented as a collection of the simplest elements available (called primitives).[66] Programming is the process by which programmers combine these primitives to compose new programs, or adapt existing ones to new uses or a changing environment.
Programs for a computer might be executed in a batch process without human interaction, or a user might type commands in an interactive session of an interpreter. In this case the "commands" are simply programs, whose execution is chained together. When a language can run its commands through an interpreter (such as a Unix shell or other command-line interface), without compiling, it is called a scripting language.[67]
It is difficult to determine which programming languages are most widely used, and what usage means varies by context. One language may occupy the greater number of programmer hours, a different one have more lines of code, and a third may consume the most CPU time. Some languages are very popular for particular kinds of applications. For example, COBOL is still strong in the corporate data center, often on large mainframes;[68][69] Fortran in scientific and engineering applications; Ada in aerospace, transportation, military, real-time and embedded applications; and C in embedded applications and operating systems. Other languages are regularly used to write many different kinds of applications.
Various methods of measuring language popularity, each subject to a different bias over what is measured, have been proposed:
Combining and averaging information from various internet sites, stackify.com reported the ten most popular programming languages as (in descending order by overall popularity): Java, C, C++, Python, C#, JavaScript, VB .NET, R, PHP, and MATLAB.[73]
A dialect of a programming language or a data exchange language is a (relatively small) variation or extension of the language that does not change its intrinsic nature. With languages such as Scheme and Forth, standards may be considered insufficient, inadequate or illegitimate by implementors, so often they will deviate from the standard, making a new dialect. In other cases, a dialect is created for use in a domain-specific language, often a subset. In the Lisp world, most languages that use basic S-expression syntax and Lisp-like semantics are considered Lisp dialects, although they vary wildly, as do, say, Racket and Clojure. As it is common for one language to have several dialects, it can become quite difficult for an inexperienced programmer to find the right documentation. The BASIC programming language has many dialects.
The explosion of Forth dialects led to the saying "If you've seen one Forth... you've seen one Forth."
There is no overarching classification scheme for programming languages. A given programming language does not usually have a single ancestor language. Languages commonly arise by combining the elements of several predecessor languages with new ideas in circulation at the time. Ideas that originate in one language will diffuse throughout a family of related languages, and then leap suddenly across familial gaps to appear in an entirely different family.
The task is further complicated by the fact that languages can be classified along multiple axes. For example, Java is both an object-oriented language (because it encourages object-oriented organization) and a concurrent language (because it contains built-in constructs for running multiple threads in parallel). Python is an object-oriented scripting language.
In broad strokes, programming languages divide into programming paradigms and a classification by intended domain of use, with general-purpose programming languages distinguished from domain-specific programming languages. Traditionally, programming languages have been regarded as describing computation in terms of imperative sentences, i.e. issuing commands. These are generally called imperative programming languages. A great deal of research in programming languages has been aimed at blurring the distinction between a program as a set of instructions and a program as an assertion about the desired answer, which is the main feature of declarative programming.[74] More refined paradigms include procedural programming, object-oriented programming, functional programming, and logic programming; some languages are hybrids of paradigms or multi-paradigmatic. An assembly language is not so much a paradigm as a direct model of an underlying machine architecture. By purpose, programming languages might be considered general purpose, system programming languages, scripting languages, domain-specific languages, or concurrent/distributed languages (or a combination of these).[75] Some general purpose languages were designed largely with educational goals.[76]
A programming language may also be classified by factors unrelated to programming paradigm. For instance, most programming languages use English language keywords, while a minority do not. Other languages may be classified as being deliberately esoteric or not.



Formal language - Wikipedia

In mathematics, computer science, and linguistics, a formal language is a set of strings of symbols together with a set of rules that are specific to it.
The alphabet of a formal language is the set of symbols, letters, or tokens from which the strings of the language may be formed.[1] The strings formed from this alphabet are called words, and the words that belong to a particular formal language are sometimes called well-formed words or well-formed formulas. A formal language is often defined by means of a formal grammar such as a regular grammar or context-free grammar, also called its formation rule.
The field of formal language theory studies primarily the purely syntactical aspects of such languages—that is, their internal structural patterns. Formal language theory sprang out of linguistics, as a way of understanding the syntactic regularities of natural languages.
In computer science, formal languages are used among others as the basis for defining the grammar of programming languages and formalized versions of subsets of natural languages in which the words of the language represent concepts that are associated with particular meanings or semantics. In computational complexity theory, decision problems are typically defined as formal languages, and complexity classes are defined as the sets of the formal languages that can be parsed by machines with limited computational power. In logic and the foundations of mathematics, formal languages are used to represent the syntax of axiomatic systems, and mathematical formalism is the philosophy that all of mathematics can be reduced to the syntactic manipulation of formal languages in this way.
The first formal language is thought to be the one used by Gottlob Frege in his Begriffsschrift (1879), literally meaning "concept writing", and which Frege described as a "formal language of pure thought."[2]
Axel Thue's early semi-Thue system, which can be used for rewriting strings, was influential on formal grammars.
An alphabet, in the context of formal languages, can be any set, although it often makes sense to use an alphabet in the usual sense of the word, or more generally a character set such as ASCII or Unicode.  The elements of an alphabet are called its letters.  An alphabet may contain an infinite number of elements;[3] however, most definitions in formal language theory specify alphabets with a finite number of elements, and most results apply only to them.
A word over an alphabet can be any finite sequence (i.e., string) of letters. The set of all words over an alphabet Σ is usually denoted by Σ* (using the Kleene star). The length of a word is the number of letters it is composed of. For any alphabet, there is only one word of length 0, the empty word, which is often denoted by e, ε, λ or even Λ. By concatenation one can combine two words to form a new word, whose length is the sum of the lengths of the original words. The result of concatenating a word with the empty word is the original word.
In some applications, especially in logic, the alphabet is also known as the vocabulary and words are known as formulas or sentences; this breaks the letter/word metaphor and replaces it by a word/sentence metaphor.
A formal language L over an alphabet Σ is a subset of Σ*, that is, a set of words over that alphabet. Sometimes the sets of words are grouped into expressions, whereas rules and constraints may be formulated for the creation of 'well-formed expressions'.
In computer science and mathematics, which do not usually deal with natural languages, the adjective "formal" is often omitted as redundant.
While formal language theory usually concerns itself with formal languages that are described by some syntactical rules, the actual definition of the concept "formal language" is only as above: a (possibly infinite) set of finite-length strings composed from a given alphabet, no more no less.  In practice, there are many languages that can be described by rules, such as regular languages or context-free languages.  The notion of a formal grammar may be closer to the intuitive concept of a "language," one described by syntactic rules. By an abuse of the definition, a particular formal language is often thought of as being equipped with a formal grammar that describes it.
The following rules describe a formal language L over the alphabet Σ = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, +, = }:
Under these rules, the string "23+4=555" is in L, but the string "=234=+" is not. This formal language expresses natural numbers, well-formed additions, and well-formed addition equalities, but it expresses only what they look like (their syntax), not what they mean (semantics). For instance, nowhere in these rules is there any indication that "0" means the number zero, or that "+" means addition.
For finite languages, one can explicitly enumerate all well-formed words. For example, we can describe a language L as just L = {"a", "b", "ab", "cba"}. The degenerate case of this construction is the empty language, which contains no words at all (L = ∅).
However, even over a finite (non-empty) alphabet such as Σ = {a, b} there are an infinite number of finite-length words that can potentially be expressed: "a", "abb", "ababba", "aaababbbbaab", …. Therefore, formal languages are typically infinite, and describing an infinite formal language is not as simple as writing L = {"a", "b", "ab", "cba"}. Here are some examples of formal languages:
Formal languages are used as tools in multiple disciplines. However, formal language theory rarely concerns itself with particular languages (except as examples), but is mainly concerned with the study of various types of formalisms to describe languages. For instance, a language can be given as
Typical questions asked about such formalisms include:
Surprisingly often, the answer to these decision problems is "it cannot be done at all", or "it is extremely expensive" (with a characterization of how expensive). Therefore, formal language theory is a major application area of computability theory and complexity theory. Formal languages may be classified in the Chomsky hierarchy based on the expressive power of their generative grammar as well as the complexity of their recognizing automaton. Context-free grammars and regular grammars provide a good compromise between expressivity and ease of parsing, and are widely used in practical applications.
Certain operations on languages are common. This includes the standard set operations, such as union, intersection, and complement. Another class of operation is the element-wise application of string operations.
Examples: suppose L1 and L2 are languages over some common alphabet.
Such string operations are used to investigate closure properties of classes of languages. A class of languages is closed under a particular operation when the operation, applied to languages in the class, always produces a language in the same class again. For instance, the context-free languages are known to be closed under union, concatenation, and intersection with regular languages, but not closed under intersection or complement. The theory of trios and abstract families of languages studies the most common closure properties of language families in their own right.[4]
A compiler usually has two distinct components. A lexical analyzer, generated by a tool like lex, identifies the tokens of the programming language grammar, e.g. identifiers or keywords, which are themselves expressed in a simpler formal language, usually by means of regular expressions. At the most basic conceptual level, a parser, usually generated by a parser generator like yacc, attempts to decide if the source program is valid, that is if it belongs to the programming language for which the compiler was built.
Of course, compilers do more than just parse the source code – they usually translate it into some executable format. Because of this, a parser usually outputs more than a yes/no answer, typically an abstract syntax tree. This is used by subsequent stages of the compiler to eventually generate an executable containing machine code that runs directly on the hardware, or some intermediate code that requires a virtual machine to execute.
In mathematical logic, a formal theory is a set of sentences expressed in a formal language.
A formal system (also called a logical calculus, or a logical system) consists of a formal language together with a deductive apparatus (also called a deductive system). The deductive apparatus may consist of a set of transformation rules, which may be interpreted as valid rules of inference, or a set of axioms, or have both. A formal system is used to derive one expression from one or more other expressions. Although a formal language can be identified with its formulas, a formal system cannot be likewise identified by its theorems. Two formal systems 





F
S




{\displaystyle {\mathcal {FS}}}

 and 





F

S
′





{\displaystyle {\mathcal {FS'}}}

 may have all the same theorems and yet differ in some significant proof-theoretic way (a formula A may be a syntactic consequence of a formula B in one but not another for instance).
A formal proof or derivation is a finite sequence of well-formed formulas (which may be interpreted as sentences, or propositions) each of which is an axiom or follows from the preceding formulas in the sequence by a rule of inference. The last sentence in the sequence is a theorem of a formal system. Formal proofs are useful because their theorems can be interpreted as true propositions.
Formal languages are entirely syntactic in nature but may be given semantics that give meaning to the elements of the language. For instance, in mathematical logic, the set of possible formulas of a particular logic is a formal language, and an interpretation assigns a meaning to each of the formulas—usually, a truth value.
The study of interpretations of formal languages is called formal semantics.  In mathematical logic, this is often done in terms of model theory.  In model theory, the terms that occur in a formula are interpreted as objects within mathematical structures, and fixed compositional interpretation rules determine how the truth value of the formula can be derived from the interpretation of its terms; a model for a formula is an interpretation of terms such that the formula becomes true.



Machine code - Wikipedia
Machine code is a computer program written in machine language instructions that can be executed directly by a computer's central processing unit (CPU). Each instruction causes the CPU to perform a very specific task, such as a load, a jump, or an ALU operation on a unit of data in a CPU register or memory.
Machine code is a strictly numerical language which is intended to run as fast as possible, and may be regarded as the lowest-level representation of a compiled or assembled computer program or as a primitive and hardware-dependent programming language. While it is possible to write programs directly in machine code, it is tedious and error prone to manage individual bits and calculate numerical addresses and constants manually. For this reason, programs are very rarely written directly in machine code in modern contexts, but may be done for low level debugging, program patching, and assembly language disassembly.
The overwhelming majority of practical programs today are written in higher-level languages or assembly language. The source code is then translated to executable machine code by utilities such as compilers, assemblers, and linkers, with the important exception of interpreted programs,[1] which are not translated into machine code. However, the interpreter itself, which may be seen as an executor or processor, performing the instructions of the source code, typically consists of directly executable machine code (generated from assembly or high-level language source code).
Machine code is by definition the lowest level of programming detail visible to the programmer, but internally many processors use microcode or optimise and transform machine code instructions into sequences of micro-ops, this is not generally considered to be a machine code per se.
Every processor or processor family has its own instruction set. Instructions are patterns of bits that by physical design correspond to different commands to the machine. Thus, the instruction set is specific to a class of processors using (mostly) the same architecture. Successor or derivative processor designs often include all the instructions of a predecessor and may add additional instructions. Occasionally, a successor design will discontinue or alter the meaning of some instruction code (typically because it is needed for new purposes), affecting code compatibility to some extent; even nearly completely compatible processors may show slightly different behavior for some instructions, but this is rarely a problem. Systems may also differ in other details, such as memory arrangement, operating systems, or peripheral devices. Because a program normally relies on such factors, different systems will typically not run the same machine code, even when the same type of processor is used.
A processor's instruction set may have all instructions of the same length, or it may have variable-length instructions. How the patterns are organized varies strongly with the particular architecture and often also with the type of instruction. Most instructions have one or more opcode fields which specifies the basic instruction type (such as arithmetic, logical, jump, etc.) and the actual operation (such as add or compare) and other fields that may give the type of the operand(s), the addressing mode(s), the addressing offset(s) or index, or the actual value itself (such constant operands contained in an instruction are called immediates).[2]
Not all machines or individual instructions have explicit operands. An accumulator machine has a combined left operand and result in an implicit accumulator for most arithmetic instructions. Other architectures (such as 8086 and the x86-family) have accumulator versions of common instructions, with the accumulator regarded as one of the general registers by longer instructions. A stack machine has most or all of its operands on an implicit stack. Special purpose instructions also often lack explicit operands (CPUID in the x86 architecture writes values into four implicit destination registers, for instance). This distinction between explicit and implicit operands is important in  code generators, especially in the register allocation and live range tracking parts. A good code optimizer can track implicit as well as explicit operands which may allow more frequent constant propagation, constant folding of registers (a register assigned the result of a constant expression freed up by replacing it by that constant) and other code enhancements.
A computer program is a list of instructions that can be executed by a central processing unit. A program's execution is done in order for the CPU that is executing it to solve a specific problem and thus accomplish a specific result. While simple processors are able to execute instructions one after another, superscalar processors are capable of executing a variety of different instructions at once.
Program flow may be influenced by special 'jump' instructions that transfer execution to an instruction other than the numerically following one. Conditional jumps are taken (execution continues at another address) or not (execution continues at the next instruction) depending on some condition.
A much more readable rendition of machine language, called assembly language, uses mnemonic codes to refer to machine code instructions, rather than using the instructions' numeric values directly. For example, on the Zilog Z80 processor, the machine code 00000101, which causes the CPU to decrement the B processor register, would be represented in assembly language as DEC B.
The MIPS architecture provides a specific example for a machine code whose instructions are always 32 bits long. The general type of instruction is given by the op (operation) field, the highest 6 bits. J-type (jump) and I-type (immediate) instructions are fully specified by op. R-type (register) instructions include an additional field funct to determine the exact operation. The fields used in these types are:
rs, rt, and rd indicate register operands; shamt gives a shift amount; and the address or immediate fields contain an operand directly.
For example, adding the registers 1 and 2 and placing the result in register 6 is encoded:
Load a value into register 8, taken from the memory cell 68 cells after the location listed in register 3:
Jumping to the address 1024:
In some computer architectures, the machine code is implemented by an even more fundamental underlying layer called microcode, providing a common machine language interface across a line or family of different models of computer with widely different underlying dataflows. This is done to facilitate porting of machine language programs between different models. An example of this use is the IBM System/360 family of computers and their successors. With dataflow path widths of 8 bits to 64 bits and beyond, they nevertheless present a common architecture at the machine language level across the entire line.
Using microcode to implement an emulator enables the computer to present the architecture of an entirely different computer. The System/360 line used this to allow porting programs from earlier IBM machines to the new family of computers, e.g. an IBM 1401/1440/1460 emulator on the IBM S/360 model 40.
Machine code is generally different from bytecode (also known as p-code), which is either executed by an interpreter or itself compiled into machine code for faster (direct) execution. An exception is when a processor is designed to use a particular bytecode directly as its machine code, such as is the case with Java processors.
Machine code and assembly code are sometimes called native code when referring to platform-dependent parts of language features or libraries.[3]
The Harvard architecture is a computer architecture with physically separate storage and signal pathways for the code (instructions) and data. Today, most processors implement such separate signal pathways for performance reasons but actually implement a Modified Harvard architecture,[citation needed] so they can support tasks like loading an executable program from disk storage as data and then executing it. Harvard architecture is contrasted to the Von Neumann architecture, where data and code are stored in the same memory which is read by the processor allowing the computer to execute commands.
From the point of view of a process, the code space is the part of its address space where the code in execution is stored. In multitasking systems this comprises the program's code segment and usually shared libraries. In multi-threading environment, different threads of one process share code space along with data space, which reduces the overhead of context switching considerably as compared to process switching.
Pamela Samuelson wrote that machine code is so unreadable that the United States Copyright Office cannot identify whether a particular encoded program is an original work of authorship;[4] however, the US Copyright Office does allow for copyright registration of computer programs[5] and a program's machine code can sometimes be decompiled in order to make its functioning more easily understandable to humans.[6]
Cognitive science professor Douglas Hofstadter has compared machine code to genetic code, saying that "Looking at a program written in machine language is vaguely comparable to looking at a DNA molecule atom by atom."[7]



Imperative programming - Wikipedia
In computer science, imperative programming is a  programming paradigm that uses statements that change a program's state. In much the same way that the imperative mood in natural languages expresses commands, an imperative program consists of commands for the computer to perform. Imperative programming focuses on describing how a program operates.
The term is often used in contrast to declarative programming, which focuses on what the program should accomplish without specifying how the program should achieve the result.
Procedural programming is a type of imperative programming in which the program is built from one or more procedures (also termed subroutines or functions). The terms are often used as synonyms, but the use of procedures has a dramatic effect on how imperative programs appear and how they are constructed. Heavily-procedural programming, in which state changes are localized to procedures or restricted to explicit arguments and returns from procedures, is a form of structured programming. From the 1960s onwards, structured programming and modular programming in general have been promoted as techniques to improve the maintainability and overall quality of imperative programs. The concepts behind object-oriented programming attempt to extend this approach.
Procedural programming could be considered a step towards declarative programming. A programmer can often tell, simply by looking at the names, arguments, and return types of procedures (and related comments), what a particular procedure is supposed to do, without necessarily looking at the details of how it achieves its result. At the same time, a complete program is still imperative since it fixes the statements to be executed and their order of execution to a large extent.
The hardware implementation of almost all computers is imperative.[note 1] Nearly all computer hardware is designed to execute machine code, which is native to the computer and is written in the imperative style. From this low-level perspective, the program state is defined by the contents of memory, and the statements are instructions in the native machine language of the computer. Higher-level imperative languages use variables and more complex statements, but still follow the same paradigm. Recipes and process checklists, while not computer programs, are also familiar concepts that are similar in style to imperative programming; each step is an instruction, and the physical world holds the state. Since the basic ideas of imperative programming are both conceptually familiar and directly embodied in the hardware, most computer languages are in the imperative style.
Assignment statements, in imperative paradigm, perform an operation on information located in memory and store the results in memory for later use. High-level imperative languages, in addition, permit the evaluation of complex expressions, which may consist of a combination of arithmetic operations and function evaluations, and the assignment of the resulting value to memory. Looping statements (as in while loops, do while loops, and for loops) allow a sequence of statements to be executed multiple times. Loops can either execute the statements they contain a predefined number of times, or they can execute them repeatedly until some condition changes. Conditional branching statements allow a sequence of statements to be executed only if some condition is met. Otherwise, the statements are skipped and the execution sequence continues from the statement following them. Unconditional branching statements allow an execution sequence to be transferred to another part of a program. These include the jump (called goto in many languages), switch, and the subprogram, subroutine, or procedure call (which usually returns to the next statement after the call).
Early in the development of high-level programming languages, the introduction of the block enabled the construction of programs in which a group of statements and declarations could be treated as if they were one statement. This, alongside the introduction of subroutines, enabled complex structures to be expressed by hierarchical decomposition into simpler procedural structures.
Many imperative programming languages (such as Fortran, BASIC, and C) are abstractions of assembly language.[1]
The earliest imperative languages were the machine languages of the original computers. In these languages, instructions were very simple, which made hardware implementation easier, but hindered the creation of complex programs. FORTRAN, developed by John Backus at International Business Machines (IBM) starting in 1954, was the first major programming language to remove the obstacles presented by machine code in the creation of complex programs. FORTRAN was a compiled language that allowed named variables, complex expressions, subprograms, and many other features now common in imperative languages. The next two decades saw the development of many other major high-level imperative programming languages. In the late 1950s and 1960s, ALGOL was developed in order to allow mathematical algorithms to be more easily expressed, and even served as the operating system's target language for some computers. MUMPS (1966) carried the imperative paradigm to a logical extreme, by not having any statements at all, relying purely on commands, even to the extent of making the IF and ELSE commands independent of each other, connected only by an intrinsic variable named $TEST. COBOL (1960) and BASIC (1964) were both attempts to make programming syntax look more like English. In the 1970s, Pascal was developed by Niklaus Wirth, and C was created by Dennis Ritchie while he was working at Bell Laboratories. Wirth went on to design Modula-2 and Oberon. For the needs of the United States Department of Defense, Jean Ichbiah and a team at Honeywell began designing Ada in 1978, after a 4-year project to define the requirements for the language. The specification was first published in 1983, with revisions in 1995, 2005 and 2012.
The 1980s saw a rapid growth in interest in object-oriented programming. These languages were imperative in style, but added features to support objects. The last two decades of the 20th century saw the development of many such languages. Smalltalk-80, originally conceived by Alan Kay in 1969, was released in 1980, by the Xerox Palo Alto Research Center (PARC). Drawing from concepts in another object-oriented language—Simula (which is considered the world's first object-oriented programming language, developed in the 1960s)—Bjarne Stroustrup designed C++, an object-oriented language based on C. Design of C++ began in 1979 and the first implementation was completed in 1983. In the late 1980s and 1990s, the notable imperative languages drawing on object-oriented concepts were Perl, released by Larry Wall in 1987; Python, released by Guido van Rossum in 1990; Visual Basic and Visual C++ (which included Microsoft Foundation Class Library (MFC) 2.0), released by Microsoft in 1991 and 1993 respectively; PHP, released by Rasmus Lerdorf in 1994; Java, released by Sun Microsystems in 1995, JavaScript, by Brendan Eich (Netscape), and Ruby, by Yukihiro "Matz" Matsumoto, both released in 1995. Microsoft's .NET Framework (2002) is imperative at its core, as are its main target languages, VB.NET and C# that run on it; however Microsoft's F#, a functional language, also runs on it.
Originally based on the article 'Imperative programming' by Stan Seibert, from Nupedia, licensed under the GNU Free Documentation License.



Syntax (programming languages) - Wikipedia
In computer science, the syntax of a computer language is the set of rules that defines the combinations of symbols that are considered to be a correctly structured document or fragment in that language. This applies both to programming languages, where the document represents source code, and markup languages, where the document represents data. The syntax of a language defines its surface form.[1] Text-based computer languages are based on sequences of characters, while visual programming languages are based on the spatial layout and connections between symbols (which may be textual or graphical). Documents that are syntactically invalid are said to have a syntax error.
Syntax – the form – is contrasted with semantics – the meaning. In processing computer languages, semantic processing generally comes after syntactic processing, but in some cases semantic processing is necessary for complete syntactic analysis, and these are done together or concurrently. In a compiler, the syntactic analysis comprises the frontend, while semantic analysis comprises the backend (and middle end, if this phase is distinguished).
Computer language syntax is generally distinguished into three levels:
Distinguishing in this way yields modularity, allowing each level to be described and processed separately, and often independently. First a lexer turns the linear sequence of characters into a linear sequence of tokens; this is known as "lexical analysis" or "lexing". Second the parser turns the linear sequence of tokens into a hierarchical syntax tree; this is known as "parsing" narrowly speaking. Thirdly the contextual analysis resolves names and checks types. This modularity is sometimes possible, but in many real-world languages an earlier step depends on a later step – for example, the lexer hack in C is because tokenization depends on context. Even in these cases, syntactical analysis is often seen as approximating this ideal model.
The parsing stage itself can be divided into two parts: the parse tree or "concrete syntax tree" which is determined by the grammar, but is generally far too detailed for practical use, and the abstract syntax tree (AST), which simplifies this into a usable form. The AST and contextual analysis steps can be considered a form of semantic analysis, as they are adding meaning and interpretation to the syntax, or alternatively as informal, manual implementations of syntactical rules that would be difficult or awkward to describe or implement formally.
The levels generally correspond to levels in the Chomsky hierarchy. Words are in a regular language, specified in the lexical grammar, which is a Type-3 grammar, generally given as regular expressions. Phrases are in a context-free language (CFL), generally a deterministic context-free language (DCFL), specified in a phrase structure grammar, which is a Type-2 grammar, generally given as production rules in Backus–Naur form (BNF). Phrase grammars are often specified in much more constrained grammars than full context-free grammars, in order to make them easier to parse; while the LR parser can parse any DCFL in linear time, the simple LALR parser and even simpler LL parser are more efficient, but can only parse grammars whose production rules are constrained. In principle, contextual structure can be described by a context-sensitive grammar, and automatically analyzed by means such as attribute grammars, though in general this step is done manually, via name resolution rules and type checking, and implemented via a symbol table which stores names and types for each scope.
Tools have been written that automatically generate a lexer from a lexical specification written in regular expressions and a parser from the phrase grammar written in BNF: this allows one to use declarative programming, rather than need to have procedural or functional programming. A notable example is the lex-yacc pair. These automatically produce a concrete syntax tree; the parser writer must then manually write code describing how this is converted to an abstract syntax tree. Contextual analysis is also generally implemented manually. Despite the existence of these automatic tools, parsing is often implemented manually, for various reasons – perhaps the phrase structure is not context-free, or an alternative implementation improves performance or error-reporting, or allows the grammar to be changed more easily. Parsers are often written in functional languages, such as Haskell, or in scripting languages, such as Python or Perl, or in C or C++.
As an example, (add 1 1) is a syntactically valid Lisp program (assuming the 'add' function exists, else name resolution fails), adding 1 and 1. However, the following are invalid:
Note that the lexer is unable to identify the first error – all it knows is that, after producing the token LEFT_PAREN, '(' the remainder of the program is invalid, since no word rule begins with '_'. The second error is detected at the parsing stage: The parser has identified the "list" production rule due to the '(' token (as the only match), and thus can give an error message; in general it may be ambiguous. 
Type errors and undeclared variable errors are sometimes considered to be syntax errors when they are detected at compile-time (which is usually the case when compiling strongly-typed languages), though it is common to classify these kinds of error as semantic errors instead.[2][3][4]
As an example, the Python code 
contains a type error because it adds a string literal to an integer literal. Type errors of this kind can be detected at compile-time: They can be detected during parsing (phrase analysis) if the compiler uses separate rules that allow "integerLiteral + integerLiteral" but not "stringLiteral + integerLiteral", though it is more likely that the compiler will use a parsing rule that allows all expressions of the form "LiteralOrIdentifier + LiteralOrIdentifier" and then the error will be detected during contextual analysis (when type checking occurs). In some cases this validation is not done by the compiler, and these errors are only detected at runtime.
In a dynamically typed language, where type can only be determined at runtime, many type errors can only be detected at runtime. For example, the Python code
is syntactically valid at the phrase level, but the correctness of the types of a and b can only be determined at runtime, as variables do not have types in Python, only values do. Whereas there is disagreement about whether a type error detected by the compiler should be called a syntax error (rather than a static semantic error), type errors which can only be detected at program execution time are always regarded as semantic rather than syntax errors.
The syntax of textual programming languages is usually defined using a combination of regular expressions (for lexical structure) and Backus–Naur form (for grammatical structure) to inductively specify syntactic categories (nonterminals) and terminal symbols. Syntactic categories are defined by rules called productions, which specify the values that belong to a particular syntactic category.[1] Terminal symbols are the concrete characters or strings of characters (for example keywords such as define, if, let, or void) from which syntactically valid programs are constructed. 
A language can have different equivalent grammars, such as equivalent regular expressions (at the lexical levels), or different phrase rules which generate the same language. Using a broader category of grammars, such as LR grammars, can allow shorter or simpler grammars compared with more restricted categories, such as LL grammar, which may require longer grammars with more rules. Different but equivalent phrase grammars yield different parse trees, though the underlying language (set of valid documents) is the same.
Below is a simple grammar, defined using the notation of regular expressions and Extended Backus–Naur form. It describes the syntax of S-expressions, a data syntax of the programming language Lisp, which defines productions for the syntactic categories expression, atom, number, symbol, and list:
This grammar specifies the following:
Here the decimal digits, upper- and lower-case characters, and parentheses are terminal symbols.
The following are examples of well-formed token sequences in this grammar: '12345', '()', '(a b c232 (1))'
The grammar needed to specify a programming language can be classified by its position in the Chomsky hierarchy. The phrase grammar of most programming languages can be specified using a Type-2 grammar, i.e., they are context-free grammars,[5] though the overall syntax is context-sensitive (due to variable declarations and nested scopes), hence Type-1. However, there are exceptions, and for some languages the phrase grammar is Type-0 (Turing-complete).
In some languages like Perl and Lisp the specification (or implementation) of the language allows constructs that execute during the parsing phase. Furthermore, these languages have constructs that allow the programmer to alter the behavior of the parser. This combination effectively blurs the distinction between parsing and execution, and makes syntax analysis an undecidable problem in these languages, meaning that the parsing phase may not finish. For example, in Perl it is possible to execute code during parsing using a BEGIN statement, and Perl function prototypes may alter the syntactic interpretation, and possibly even the syntactic validity of the remaining code.[6] Colloquially this is referred to as "only Perl can parse Perl" (because code must be executed during parsing, and can modify the grammar), or more strongly "even Perl cannot parse Perl" (because it is undecidable). Similarly, Lisp macros introduced by the defmacro syntax also execute during parsing, meaning that a Lisp compiler must have an entire Lisp run-time system present. In contrast, C macros are merely string replacements, and do not require code execution.[7][8]
The syntax of a language describes the form of a valid program, but does not provide any information about the meaning of the program or the results of executing that program. The meaning given to a combination of symbols is handled by semantics (either formal or hard-coded in a reference implementation). Not all syntactically correct programs are semantically correct. Many syntactically correct programs are nonetheless ill-formed, per the language's rules; and may (depending on the language specification and the soundness of the implementation) result in an error on translation or execution. In some cases, such programs may exhibit undefined behavior. Even when a program is well-defined within a language, it may still have a meaning that is not intended by the person who wrote it.
Using natural language as an example, it may not be possible to assign a meaning to a grammatically correct sentence or the sentence may be false:
The following C language fragment is syntactically correct, but performs an operation that is not semantically defined (because p is a null pointer, the operations p->real and p->im have no meaning):
As a simpler example,
is syntactically valid, but not semantically defined, as it uses an uninitialized variable. Even though compilers for some programming languages (e.g., Java and C#) would detect uninitialized variable errors of this kind, they should be regarded as semantic errors rather than syntax errors.[4][9]
To quickly compare syntax of various programming languages, take a look at the list of "Hello, World!" program examples:



Computer program - Wikipedia
A computer program is a collection of instructions[1] that performs a specific task when executed by a computer. A computer requires programs to function.
A computer program is usually written by a computer programmer in a programming language. From the program in its human-readable form of source code, a compiler can derive machine code—a form consisting of instructions that the computer can directly execute. Alternatively, a computer program may be executed with the aid of an interpreter.
A collection of computer programs, libraries, and related data are referred to as software. Computer programs may be categorized along functional lines, such as application software and system software.  The underlying method used for some calculation or manipulation is known as an algorithm.
The earliest programmable machines preceded the invention of the digital computer. In 1801, Joseph-Marie Jacquard devised a loom that would weave a pattern by following a series of perforated cards. Patterns could be woven and repeated by arranging the cards.[2]
In 1837, Charles Babbage was inspired by Jacquard's loom to attempt to build the Analytical Engine.[2]
The names of the components of the calculating device were borrowed from the textile industry. In the textile industry, yarn was brought from the store to be milled. The device would have had a "store"—memory to hold 1,000 numbers of 40 decimal digits each.  Numbers from the "store" would then have then been transferred to the "mill" (analogous to the CPU of a modern machine), for processing. It was programmed using two sets of perforated cards—one to direct the operation and the other for the input variables.[2]
[3] However, after more than 17,000 pounds of the British government's money, the thousands of cogged wheels and gears never fully worked together.[4]
During a nine-month period in 1842–43, Ada Lovelace translated the memoir of Italian mathematician Luigi Menabrea. The memoir covered the Analytical Engine. The translation contained Note G which completely detailed a method for calculating Bernoulli numbers using the Analytical Engine. This note is recognized by some historians as the world's first written computer program.[5]
In 1936, Alan Turing introduced the Universal Turing machine—a theoretical device that can model every computation that can be performed on a Turing complete computing machine.[6]
It is a finite-state machine that has an infinitely long read/write tape. The machine can move the tape back and forth, changing its contents as it performs an algorithm. The machine starts in the initial state, goes through a sequence of steps, and halts when it encounters the halt state.[7]
This machine is considered by some to be the origin of the stored-program computer—used by John von Neumann (1946) for the "Electronic Computing Instrument" that now bears the von Neumann architecture name.[8]
The Z3 computer, invented by Konrad Zuse (1941) in Germany, was a digital and programmable computer.[9] A digital computer uses electricity as the calculating component. The Z3 contained 2,400 relays to create the circuits. The circuits provided a binary, floating-point, nine-instruction computer. Programming the Z3 was through a specially designed keyboard and punched tape.
The Electronic Numerical Integrator And Computer (Fall 1945) was a Turing complete, general-purpose computer that used 17,468 vacuum tubes to create the circuits. At its core, it was a series of Pascalines wired together.[10] Its 40 units weighed 30 tons, occupied 1,800 square feet (167 m2), and consumed $650 per hour (in 1940s currency) in electricity when idle.[10] It had 20 base-10 accumulators. Programming the ENIAC took up to two months.[10] Three function tables were on wheels and needed to be rolled to fixed function panels. Function tables were connected to function panels using heavy black cables. Each function table had 728 rotating knobs. Programming the ENIAC also involved setting some of the 3,000 switches. Debugging a program took a week.[10] The ENIAC featured parallel operations. Different sets of accumulators could simultaneously work on different algorithms. It used punched card machines for input and output, and it was controlled with a clock signal. It ran for eight years, calculating hydrogen bomb parameters, predicting weather patterns, and producing firing tables to aim artillery guns.
The Manchester Small-Scale Experimental Machine (June 1948) was a stored-program computer.[11] Programming transitioned away from moving cables and setting dials; instead, a computer program was stored in memory as numbers. Only three bits of memory were available to store each instruction, so it was limited to eight instructions. 32 switches were available for programming.
Computers manufactured until the 1970s had front-panel switches for programming. The computer program was written on paper for reference. An instruction was represented by a configuration of on/off settings. After setting the configuration, an execute button was pressed. This process was then repeated. Computer programs also were manually input via paper tape or punched cards. After the medium was loaded, the starting address was set via switches and the execute button pressed.[12]
In 1961, the Burroughs B5000 was built specifically to be programmed in the ALGOL 60 language. The hardware featured circuits to ease the compile phase.[13]
In 1964, the IBM System/360 was a line of six computers each having the same instruction set architecture. The Model 30 was the smallest and least expensive. Customers could upgrade and retain the same application software.[14] Each System/360 model featured multiprogramming. With operating system support, multiple programs could be in memory at once. When one was waiting for input/output, another could compute. Each model also could emulate other computers. Customers could upgrade to the System/360 and retain their IBM 7094 or IBM 1401 application software.[14]
Computer programming is the process of writing or editing source code. Editing source code involves testing, analyzing, refining, and sometimes coordinating with other programmers on a jointly developed program. A person who practices this skill is referred to as a computer programmer, software developer, and sometimes coder.
The sometimes lengthy process of computer programming is usually referred to as software development. The term software engineering is becoming popular as the process is seen as an engineering discipline.
Computer programs can be categorized by the programming language paradigm used to produce them. Two of the main paradigms are imperative and declarative.
Imperative programming languages specify a sequential algorithm using declarations, expressions, and statements:[15]
One criticism of imperative languages is the side effect of an assignment statement on a class of variables called non-local variables.[16]
Declarative programming languages describe what computation should be performed and not how to compute it. Declarative programs omit the control flow and are considered sets of instructions. Two broad categories of declarative languages are functional languages and logical languages. The principle behind functional languages (like Haskell) is to not allow side effects, which makes it easier to reason about programs like mathematical functions.[16] The principle behind logical languages (like Prolog) is to define the problem to be solved – the goal – and leave the detailed solution to the Prolog system itself.[17] The goal is defined by providing a list of subgoals. Then each subgoal is defined by further providing a list of its subgoals, etc. If a path of subgoals fails to find a solution, then that subgoal is backtracked and another path is systematically attempted.
A computer program in the form of a human-readable, computer programming language is called source code. Source code may be converted into an executable image by a compiler or executed immediately with the aid of an interpreter.
Compilers are used to translate source code from a programming language into either object code or machine code.[18]  Object code needs further processing to become machine code, and machine code consists of the central processing unit's native instructions, ready for execution. Compiled computer programs are commonly referred to as executables, binary images, or simply as binaries – a reference to the binary file format used to store the executable code.
Interpreters are used to execute source code from a programming language line-by-line. The interpreter decodes each statement and performs its behavior. One advantage of interpreters is that they can easily be extended to an interactive session. The programmer is presented with a prompt, and individual lines of code are typed in and performed immediately.
The main disadvantage of interpreters is computer programs run slower than when compiled. Interpreting code is slower because the interpreter must decode each statement and then perform it. However, software development may be faster using an interpreter because testing is immediate when the compiling step is omitted. Another disadvantage of interpreters is an interpreter must be present on the executing computer. By contrast, compiled computer programs need no compiler present during execution.
Just in time compilers pre-compile computer programs just before execution. For example, the Java virtual machine Hotspot contains a Just In Time Compiler which selectively compiles Java bytecode into machine code - but only code which Hotspot predicts is likely to be used many times.
Either compiled or interpreted programs might be executed in a batch process without human interaction.
Scripting languages are often used to create batch processes. One common scripting language is Unix shell, and its executing environment is called the command-line interface.
No properties of a programming language require it to be exclusively compiled or exclusively interpreted. The categorization usually reflects the most popular method of language execution. For example, Java is thought of as an interpreted language and C a compiled language, despite the existence of Java compilers and C interpreters.
Typically, computer programs are stored in non-volatile memory until requested either directly or indirectly to be executed by the computer user. Upon such a request, the program is loaded into random-access memory, by a computer program called an operating system, where it can be accessed directly by the central processor. The central processor then executes ("runs") the program, instruction by instruction, until termination. A program in execution is called a process.[19] Termination is either by normal self-termination or by error – software or hardware error.
Many operating systems support multitasking which enables many computer programs to appear to run simultaneously on one computer.  Operating systems may run multiple programs through process scheduling – a software mechanism to switch the CPU among processes often so users can interact with each program while it runs.[20] Within hardware, modern day multiprocessor computers or computers with multicore processors may run multiple programs.[21]
Multiple lines of the same computer program may be simultaneously executed using threads. Multithreading processors are optimized to execute multiple threads efficiently.
A computer program in execution is normally treated as being different from the data the program operates on. However, in some cases, this distinction is blurred when a computer program modifies itself. The modified computer program is subsequently executed as part of the same program. Self-modifying code is possible for programs written in machine code, assembly language, Lisp, C, COBOL, PL/1, and Prolog.
Computer programs may be categorized along functional lines. The main functional categories are application software and system software. System software includes the operating system which couples computer hardware with application software.[22] The purpose of the operating system is to provide an environment in which application software executes in a convenient and efficient manner.[22] In addition to the operating system, system software includes embedded programs, boot programs, and micro programs. Application software designed for end users have a user interface. Application software not designed for the end user includes middleware, which couples one application with another. Application software also includes utility programs. The distinction between system software and application software is under debate.
There are many types of application software:
Utility programs are application programs designed to aid system administrators and computer programmers.
An operating system is a computer program that acts as an intermediary between a user of a computer and the computer hardware.
[22]
In the 1950s, the programmer, who was also the operator, would write a program and run it.[12]
After the program finished executing, the output may have been printed, or it may have been punched onto paper tape or cards for later processing.[12]
More often than not the program did not work.
[23]
The programmer then looked at the console lights and fiddled with the console switches. If less fortunate, a memory printout was made for further study.[23]
In the 1960s, programmers reduced the amount of wasted time by automating the operator's job.[23] A program called an operating system was kept in the computer at all times.[23]
Originally, operating systems were programmed in assembly; however, modern operating systems are typically written in C.
A stored-program computer requires an initial computer program stored in its read-only memory to boot. The boot process is to identify and initialize all aspects of the system, from processor registers to device controllers to memory contents.[24] Following the initialization process, this initial computer program loads the operating system and sets the program counter to begin normal operations.
Independent of the host computer, a hardware device might have embedded firmware to control its operation. Firmware is used when the computer program is rarely or never expected to change, or when the program must not be lost when the power is off.[23]
Microcode programs control some central processing units and some other hardware. This code moves data between the registers, buses, arithmetic logic units, and other functional units in the CPU.  Unlike conventional programs, microcode is not usually written by, or even visible to, the end users of systems, and is usually provided by the manufacturer, and is considered internal to the device.



Markup language - Wikipedia
In computer text processing, a markup language is a system for annotating a document in a way that is syntactically distinguishable from the text.[1] The idea and terminology evolved from the "marking up" of paper manuscripts, i.e., the revision instructions by editors, traditionally written with a blue pencil on authors' manuscripts.[citation needed] In digital media,  this "blue pencil instruction text" was replaced by tags, that is, instructions are expressed directly by tags or "instruction text encapsulated by tags." However the whole idea of a mark up language is to avoid the formatting work for the text, as the tags in the mark up language serve the purpose to format the appropriate text (like a header or beginning of a next para...etc.). Every tag used in a Markup language has a property to format the text we write.
Examples include typesetting instructions such as those found in troff, TeX and LaTeX, or structural markers such as XML tags. Markup instructs the software that displays the text to carry out appropriate actions, but is omitted from the version of the text that users see.
Some markup languages, such as the widely used HTML, have pre-defined presentation semantics—meaning that their specification prescribes how to present the structured data. Others, such as XML, do not have them and are general purpose.
HyperText Markup Language (HTML), one of the document formats of the World Wide Web, is an instance of Standard Generalized Markup Language or SGML, and follows many of the markup conventions used in the publishing industry in the communication of printed work between authors, editors, and printers.[citation needed]
The term markup is derived from the traditional publishing practice of "marking up" a manuscript, which involves adding handwritten annotations in the form of conventional symbolic printer's instructions in the margins and text of a paper manuscript or printed. It is computer jargon used in coding proof. For centuries, this task was done primarily by skilled typographers known as "markup men"[2] or "copy markers"[3] who marked up text to indicate what typeface, style, and size should be applied to each part, and then passed the manuscript to others for typesetting by hand. Markup was also commonly applied by editors, proofreaders, publishers, and graphic designers, and indeed by document authors.
There are three main general categories of electronic markup:[4][5]
There is considerable blurring of the lines between the types of markup. In modern word-processing systems, presentational markup is often saved in descriptive-markup-oriented systems such as XML, and then processed procedurally by implementations. The programming constructs in procedural-markup systems such as TeX may be used to create higher-level markup systems that are more descriptive, such as LaTeX.
In recent years, a number of small and largely unstandardized markup languages have been developed to allow authors to create formatted text via web browsers, for use in wikis and web forums. These are sometimes called lightweight markup languages. Markdown or the markup language used by Wikipedia are examples of such wiki markup.
The first well-known public presentation of markup languages in computer text processing was made by William W. Tunnicliffe at a conference in 1967, although he preferred to call it generic coding. It can be seen as a response to the emergence of programs such as RUNOFF that each used their own control notations, often specific to the target typesetting device. In the 1970s, Tunnicliffe led the development of a standard called GenCode for the publishing industry and later was the first chair of the International Organization for Standardization committee that created SGML, the first standard descriptive markup language. Book designer Stanley Rice published speculation along similar lines in 1970.[7] Brian Reid, in his 1980 dissertation at Carnegie Mellon University, developed the theory and a working implementation of descriptive markup in actual use.
However, IBM researcher Charles Goldfarb is more commonly seen today as the "father" of markup languages. Goldfarb hit upon the basic idea while working on a primitive document management system intended for law firms in 1969, and helped invent IBM GML later that same year. GML was first publicly disclosed in 1973.
In 1975, Goldfarb moved from Cambridge, Massachusetts to Silicon Valley and became a product planner at the IBM Almaden Research Center. There, he convinced IBM's executives to deploy GML commercially in 1978 as part of IBM's Document Composition Facility product, and it was widely used in business within a few years.
SGML, which was based on both GML and GenCode, was developed by Goldfarb in 1974.[8] Goldfarb eventually became chair of the SGML committee. SGML was first released by ISO as the ISO 8879 standard in October 1986.
Some early examples of computer markup languages available outside the publishing industry can be found in typesetting tools on Unix systems such as troff and nroff. In these systems, formatting commands were inserted into the document text so that typesetting software could format the text according to the editor's specifications. It was a trial and error iterative process to get a document printed correctly.[9] Availability of WYSIWYG ("what you see is what you get") publishing software supplanted much use of these languages among casual users, though serious publishing work still uses markup to specify the non-visual structure of texts, and WYSIWYG editors now usually save documents in a markup-language-based format.
Another major publishing standard is TeX, created and refined by Donald Knuth in the 1970s and '80s. TeX concentrated on detailed layout of text and font descriptions to typeset mathematical books. This required Knuth to spend considerable time investigating the art of typesetting. TeX is mainly used in academia, where it is a de facto standard in many scientific disciplines. A TeX macro package known as LaTeX provides a descriptive markup system on top of TeX, and is widely used.
The first language to make a clean distinction between structure and presentation was Scribe, developed by Brian Reid and described in his doctoral thesis in 1980.[10] Scribe was revolutionary in a number of ways, not least that it introduced the idea of styles separated from the marked up document, and of a grammar controlling the usage of descriptive elements. Scribe influenced the development of Generalized Markup Language (later SGML) and is a direct ancestor to HTML and LaTeX[citation needed].
In the early 1980s, the idea that markup should be focused on the structural aspects of a document and leave the visual presentation of that structure to the interpreter led to the creation of SGML. The language was developed by a committee chaired by Goldfarb. It incorporated ideas from many different sources, including Tunnicliffe's project, GenCode. Sharon Adler, Anders Berglund, and James A. Marke were also key members of the SGML committee.
SGML specified a syntax for including the markup in documents, as well as one for separately describing what tags were allowed, and where (the Document Type Definition (DTD) or schema). This allowed authors to create and use any markup they wished, selecting tags that made the most sense to them and were named in their own natural languages. Thus, SGML is properly a meta-language, and many particular markup languages are derived from it. From the late '80s on, most substantial new markup languages have been based on SGML system, including for example TEI and DocBook. SGML was promulgated as an International Standard by International Organization for Standardization, ISO 8879, in 1986.
SGML found wide acceptance and use in fields with very large-scale documentation requirements. However, many found it cumbersome and difficult to learn—a side effect of its design attempting to do too much and be too flexible. For example, SGML made end tags (or start-tags, or even both) optional in certain contexts, because its developers thought markup would be done manually by overworked support staff who would appreciate saving keystrokes[citation needed].
In 1989, computer scientist Sir Tim Berners-Lee wrote a memo proposing an Internet-based hypertext system,[11] then specified HTML and wrote the browser and server software in the last part of 1990. The first publicly available description of HTML was a document called "HTML Tags", first mentioned on the Internet by Berners-Lee in late 1991.[12][13] It describes 18 elements comprising the initial, relatively simple design of HTML. Except for the hyperlink tag, these were strongly influenced by SGMLguid, an in-house SGML-based documentation format at CERN. Eleven of these elements still exist in HTML 4.[14]
Berners-Lee considered HTML an SGML application. The Internet Engineering Task Force (IETF) formally defined it as such with the mid-1993 publication of the first proposal for an HTML specification: "Hypertext Markup Language (HTML)" Internet-Draft by Berners-Lee and Dan Connolly, which included an SGML Document Type Definition to define the grammar.[15] Many of the HTML text elements are found in the 1988 ISO technical report TR 9537 Techniques for using SGML, which in turn covers the features of early text formatting languages such as that used by the RUNOFF command developed in the early 1960s for the CTSS (Compatible Time-Sharing System) operating system. These formatting commands were derived from those used by typesetters to manually format documents. Steven DeRose[16] argues that HTML's use of descriptive markup (and influence of SGML in particular) was a major factor in the success of the Web, because of the flexibility and extensibility that it enabled. HTML became the main markup language for creating web pages and other information that can be displayed in a web browser, and is quite likely the most used markup language in the world today.
XML (Extensible Markup Language) is a meta markup language that is now widely used. XML was developed by the World Wide Web Consortium, in a committee created and chaired by Jon Bosak. The main purpose of XML was to simplify SGML by focusing on a particular problem—documents on the Internet.[17] XML remains a meta-language like SGML, allowing users to create any tags needed (hence "extensible") and then describing those tags and their permitted uses.
XML adoption was helped because every XML document can be written in such a way that it is also an SGML document, and existing SGML users and software could switch to XML fairly easily. However, XML eliminated many of the more complex and human-oriented features of SGML to simplify implementation environments such as documents and publications. However, it appeared to strike a happy medium between simplicity and flexibility, and was rapidly adopted for many other uses. XML is now widely used for communicating data between applications.
Since January 2000, all W3C Recommendations for HTML have been based on XML rather than SGML, using the abbreviation XHTML (Extensible HyperText Markup Language). The language specification requires that XHTML Web documents must be well-formed XML documents. This allows for more rigorous and robust documents while using tags familiar from HTML.
One of the most noticeable differences between HTML and XHTML is the rule that all tags must be closed: empty HTML tags such as <br> must either be closed with a regular end-tag, or replaced by a special form: <br /> (the space before the '/' on the end tag is optional, but frequently used because it enables some pre-XML Web browsers, and SGML parsers, to accept the tag). Another is that all attribute values in tags must be quoted. Finally, all tag and attribute names within the XHTML namespace must be lowercase to be valid. HTML, on the other hand, was case-insensitive.
Many XML-based applications now exist, including the Resource Description Framework as RDF/XML, XForms, DocBook, SOAP, and the Web Ontology Language (OWL). For a partial list of these, see List of XML markup languages.
A common feature of many markup languages is that they intermix the text of a document with markup instructions in the same data stream or file. This is not necessary; it is possible to isolate markup from text content, using pointers, offsets, IDs, or other methods to co-ordinate the two. Such "standoff markup" is typical for the internal representations that programs use to work with marked-up documents. However, embedded or "inline" markup is much more common elsewhere. Here, for example, is a small section of text marked up in HTML:
The codes enclosed in angle-brackets <like this> are markup instructions (known as tags), while the text between these instructions is the actual text of the document. The codes h1, p, and em are examples of semantic markup, in that they describe the intended purpose or meaning of the text they include. Specifically, h1 means "this is a first-level heading", p means "this is a paragraph", and em means "this is an emphasized word or phrase". A program interpreting such structural markup may apply its own rules or styles for presenting the various pieces of text, using different typefaces, boldness, font size, indentation, colour, or other styles, as desired.
A tag such as "h1" (header level 1) might be presented in a large bold sans-serif typeface, for example, or in a monospaced (typewriter-style) document it might be underscored – or it might not change the presentation at all.
In contrast, the i tag in HTML is an example of presentational markup; it is generally used to specify a particular characteristic of the text (in this case, the use of an italic typeface) without specifying the reason for that appearance.
The Text Encoding Initiative (TEI) has published extensive guidelines[18] for how to encode texts of interest in the humanities and social sciences, developed through years of international cooperative work. These guidelines are used by projects encoding historical documents, the works of particular scholars, periods, or genres, and so on.
While the idea of markup language originated with text documents, there is increasing use of markup languages in the presentation of other types of information, including playlists, vector graphics, web services, content syndication, and user interfaces. Most of these are XML applications, because XML is a well-defined and extensible language.
The use of XML has also led to the possibility of combining multiple markup languages into a single profile, like XHTML+SMIL and XHTML+MathML+SVG.[19]
Because markup languages, and more generally data description languages (not necessarily textual markup), are not programming languages[20] (they are data without instructions), they are more easily manipulated than programming languages—for example, web pages are presented as HTML documents, not C code, and thus can be embedded within other web pages, displayed when only partially received, and so forth. This leads to the web design principle of the rule of least power, which advocates using the least (computationally) powerful language that satisfies a task to facilitate such manipulation and reuse.





Colossus computer - Wikipedia
Colossus was a set of computers developed by British codebreakers in the years 1943–1945 to help in the cryptanalysis of the Lorenz cipher. Colossus used thermionic valves (vacuum tubes) to perform Boolean and counting operations. Colossus is thus regarded[3] as the world's first programmable, electronic, digital computer, although it was programmed by switches and plugs and not by a stored program.[4]
Colossus was designed by research telephone engineer Tommy Flowers to solve a problem posed by mathematician Max Newman at the Government Code and Cypher School (GC&CS) at Bletchley Park. Alan Turing's use of probability in cryptanalysis (see Banburismus) contributed to its design. It has sometimes been erroneously stated that Turing designed Colossus to aid the cryptanalysis of the Enigma.[5] Turing's machine that helped decode Enigma was the electromechanical Bombe, not Colossus.[6]
The prototype, Colossus Mark 1, was shown to be working in December 1943 and was in use at Bletchley Park by early 1944. An improved Colossus Mark 2 that used shift registers to quintuple the processing speed, first worked on 1 June 1944, just in time for the Normandy landings on D-Day.[7] Ten Colossi were in use by the end of the war and an eleventh was being commissioned.[7] Bletchley Park's use of these machines allowed the Allies to obtain a vast amount of high-level military intelligence from intercepted radiotelegraphy messages between the German High Command (OKW) and their army commands throughout occupied Europe.
The existence of the Colossus machines was kept secret until the mid-1970s, although some of its personnel and secret information undoubtedly fueled further development in the U.S. in the late 1940s; the machines and the plans for building them had previously been destroyed in the 1960s as part of the effort to maintain the secrecy of the project.[8][9] This deprived most of those involved with Colossus of the credit for pioneering electronic digital computing during their lifetimes. A functioning rebuild of a Mark 2 Colossus was completed in 2008 by Tony Sale and some volunteers; it is on display at The National Museum of Computing at Bletchley Park.[10][11][12]
The Colossus computers were used to help decipher intercepted radio teleprinter messages that had been encrypted using an unknown device. Intelligence information revealed that the Germans called the wireless teleprinter transmission systems "Sägefisch" (sawfish). This led the British to call encrypted German teleprinter traffic "Fish",[14] and the unknown machine and its intercepted messages "Tunny" (tunafish).[15]
Before the Germans increased the security of their operating procedures, British cryptanalysts diagnosed how the unseen machine functioned and built an imitation of it called "British Tunny".
It was deduced that the machine had twelve wheels and used a Vernam ciphering technique on message characters in the standard 5-bit ITA2 telegraph code. It did this by combining the plaintext characters with a stream of key characters using the XOR Boolean function to produce the ciphertext.
In August 1941, a blunder by German operators led to the transmission of two versions of the same message with identical machine settings. These were intercepted and worked on at Bletchley Park. First, John Tiltman, a very talented GC&CS cryptanalyst, derived a key stream of almost 4000 characters.[16] Then Bill Tutte, a newly arrived member of the Research Section, used this key stream to work out the logical structure of the Lorenz machine. He deduced that the twelve wheels consisted of two groups of five, which he named the χ (chi) and ψ (psi) wheels, the remaining two he called μ (mu) or "motor" wheels. The chi wheels stepped regularly with each letter that was encrypted, while the psi wheels stepped irregularly, under the control of the motor wheels.[17]
With a sufficiently random key stream, a Vernam cipher removes the natural language property of a plaintext message of having an uneven frequency distribution of the different characters, to produce a uniform distribution in the ciphertext. The Tunny machine did this well. However, the cryptanalysts worked out that by examining the frequency distribution of the character-to-character changes in the ciphertext, instead of the plain characters, there was a departure from uniformity which provided a way into the system. This was achieved by "differencing" in which each bit or character was XOR-ed with its successor.[18] After Germany surrendered, allied forces captured a Tunny machine and discovered that it was the electromechanical Lorenz SZ (Schlüsselzusatzgerät, cipher attachment) in-line cipher machine.[14]
In order to decrypt the transmitted messages, two tasks had to be performed. The first was "wheel breaking", which was the discovery of the cam patterns for all the wheels. These patterns were set up on the Lorenz machine and then used for a fixed period of time for a succession of different messages. Each transmission, which often contained more than one message, was enciphered with a different start position of the wheels. Alan Turing invented a method of wheel-breaking that became known as Turingery.[19] Turing's technique was further developed into "Rectangling", for which Colossus could produce tables for manual analysis. Colossi 2, 4, 6, 7 and 9 had a "gadget" to aid this process.[20]
The second task was "wheel setting", which worked out the start positions of the wheels for a particular message, and could only be attempted once the cam patterns were known.[21] It was this task for which Colossus was initially designed. To discover the start position of the chi wheels for a message, Colossus compared two character streams, counting statistics from the evaluation of programmable Boolean functions. The two streams were the ciphertext, which was read at high speed from a paper tape, and the key stream, which was generated internally, in a  simulation of the unknown German machine. After a succession of different Colossus runs to discover the likely chi-wheel settings, they were checked by examining the frequency distribution of the characters in processed ciphertext.[22] Colossus produced these frequency counts.
By using differencing and knowing that the psi wheels did not advance with each character, Tutte worked out that trying just two differenced bits (impulses) of the chi-stream against the differenced ciphertext would produce a statistic that was non-random. This became known as Tutte's "1+2 break in".[27] It involved calculating the following Boolean function:
and counting the number of times it yielded "false" (zero). If this number exceeded a pre-defined threshold value known as the "set total", it was printed out. The cryptanalyst would examine the printout to determine which of the putative start positions was most likely to be the correct one for the chi-1 and chi-2 wheels.[28]
This technique would then be applied to other pairs of, or single, impulses to determine the likely start position of all five chi wheels. From this, the de-chi (D) of a ciphertext could be obtained, from which the psi component could be removed by manual methods.[29] If the frequency distribution of characters in the de-chi version of the ciphertext was within certain bounds, "wheel setting" of the chi wheels was considered to have been achieved,[22] and the message settings and de-chi were passed to the "Testery". This was the section at Bletchley Park led by Major Ralph Tester where the bulk of the decrypting work was done by manual and linguistic methods.[30]
Colossus could also derive the start position of the psi and motor wheels, but this was not much done until the last few months of the war, when there were plenty of Colossi available and the number of Tunny messages had declined.
Colossus was developed for the "Newmanry",[31] the section headed by the mathematician Max Newman that was responsible for machine methods against the twelve-rotor Lorenz SZ40/42 on-line teleprinter cipher machine (code named Tunny, for tunafish). The Colossus design arose out of a prior project that produced a counting machine dubbed "Heath Robinson". Although it proved the concept of machine analysis for this part of the process, it was initially unreliable. The electro-mechanical parts were relatively slow and it was difficult to synchronise two looped paper tapes, one containing the enciphered message, and the other representing part of the key stream of the Lorenz machine,[32] also the tapes tended to stretch when being read at up to 2000 characters per second.
Tommy Flowers MBE[34] was a senior electrical engineer and Head of the Switching Group at the Post Office Research Station at Dollis Hill. Prior to his work on Colossus, he had been involved with GC&CS at Bletchley Park from February 1941 in an attempt to improve the Bombes that were used in the cryptanalysis of the German Enigma cipher machine.[35] He was recommended to Max Newman by Alan Turing, who had been impressed by his work on the Bombes.[36] The main components of the Heath Robinson machine were as follows.
Flowers had been brought in to design the Heath Robinson's combining unit.[37] He was not impressed by the system of a key tape that had to be kept synchronised with the message tape and, on his own initiative, he designed an electronic machine which eliminated the need for the key tape by having an electronic analogue of the Lorenz (Tunny) machine.[38] He presented this design to Max Newman in February 1943, but the idea that the one to two thousand thermionic valves (vacuum tubes and thyratrons) proposed, could work together reliably, was greeted with great scepticism,[39] so more Robinsons were ordered from Dollis Hill. Flowers, however, knew from his pre-war work that most thermionic valve failures occurred as a result of the thermal stresses at power up, so not powering a machine down reduced failure rates to very low levels.[40] Additionally, the heaters were started at a low voltage then slowly brought up to full voltage to reduce the thermal stress. The valves themselves were soldered in to avoid problems with plug-in bases, which could be unreliable.[citation needed] Flowers persisted with the idea and obtained support from the Director of the Research Station, W Gordon Radley.[41] Flowers and his team of some fifty people in the switching group[42][43] spent eleven months from early February 1943 designing and building a machine that dispensed with the second tape of the Heath Robinson, by generating the wheel patterns electronically. Flowers used some of his own money for the project.[44][45]
This prototype, Mark 1 Colossus, performed satisfactorily at Dollis Hill on 8 December 1943[46] and was taken apart and shipped to Bletchley Park, where it was delivered on 18 January and re-assembled by Harry Fensom and Don Horwood.[47][48] It was operational in January, according to some sources,[49][50] and it successfully attacked its first message on 5 February 1944.[51] As it was a large structure it was quickly dubbed Colossus by the WRNS operators. This is incorrect as the machine was named Colossus before it arrived at Bletchley Park as established by a memo held in the National Archives written by Max Newman on the 18th January 1944 where he records that 'Colossus arrives today." [52] This machine contained 1600 thermionic valves (tubes).[42]
During the development of the prototype, an improved design had been developed – the Mark 2 Colossus. Four of these were ordered in March 1944 and by the end of April the number on order had been increased to twelve. Dollis Hill was put under pressure to have the first of these working by 1 June.[53] The first Mark 2 Colossus, containing 2400 valves, became operational at 08:00 on 1 June 1944 after which Allen Coombs took over leadership of Colossus production.[54] Subsequently, Colossi were delivered at the rate of about  one a month. By the time of V-E Day there were ten Colossi working at Bletchley Park and a start had been made on assembling an eleventh.[53]
The main units of the Mark 2 design were as follows.[38][55]
Most of the design of the electronics was the work of Tommy Flowers, assisted by William Chandler, Sidney Broadhurst and Allen Coombs; with Erie Speight and Arnold Lynch developing the photoelectric reading mechanism.[56] Coombs remembered Flowers, having produced a rough draft of his design, tearing it into pieces that he handed out to his colleagues for them to do the detailed design and get their team to manufacture it.[57] The Mark 2 Colossi were both 5 times faster and simpler to operate than the prototype.[58]
The design overcame the problem of synchronizing the electronics with the message tape by generating a clock signal by reading the sprocket holes of the message tape. The speed of operation was thus limited by the mechanics of reading the tape. The tape reader was tested up to 9700 characters per second (53 mph) before the tape disintegrated. So 5000 characters/second (40 ft/s (12.2 m/s; 27.3 mph)) was settled on as the speed for regular use. Flowers designed shift registers,[59] one being used for each of the five channels of the punched tape. The shift register stored successive bits from each of the tape channels and delivered five successive characters (either Z or ΔZ according to switch selection) to the processors. The five-way parallelism[60] enabled five simultaneous tests and counts to be performed giving an effective processing speed of 25,000 characters per second. [59] The computer used algorithms devised by W. T. Tutte and colleagues to decrypt a Tunny message.[61][62]
The Newmanry was staffed by cryptanalysts, operators from the Women's Royal Naval Service (WRNS) – known as “Wrens” – and engineers who were permanently on hand for maintenance and repair. By the end of the war the staffing was 272 Wrens and 27 men.[53]
The first job in operating Colossus for a new message, was to prepare the paper tape loop. This was performed by the Wrens who stuck the two ends together using Bostik glue, ensuring that there was a 150-character length of blank tape between the end and the start of the message.[63] Using a special hand punch they inserted a start hole between the third and fourth channels ​2 1⁄2 sprocket holes from the end of the blank section, and a stop hole between the fourth and fifth channels ​1 1⁄2 sprocket holes from the end of the characters of the message.[64][65] These were read by specially positioned photocells and indicated when the message was about to start and when it ended. The operator would then thread the paper tape through the gate and around the pulleys of the bedstead and adjust the tension. The two-tape bedstead design had been carried on from Heath Robinson so that one tape could be loaded whilst the previous one was being run. A switch on the Selection Panel specified the "near" or the "far" tape.[66]
After performing various resetting and zeroizing tasks, the Wren operators would, under instruction from the cryptanalyst, operate the "set total" decade switches and the K2 panel switches to set the desired algorithm. They would then start the bedstead tape motor and lamp and, when the tape was up to speed, operate the master start switch.[66]

Howard Campaigne, a mathematician and cryptanalyst from the US Navy's OP-20-G, wrote the following in a foreword to Flowers' 1983 paper "The Design of Colossus".My view of Colossus was that of cryptanalyst-programmer. I told the machine to make certain calculations and counts, and after studying the results, told it to do another job. It did not remember the previous result, nor could it have acted upon it if it did. Colossus and I alternated in an interaction that sometimes achieved an analysis of an unusual German cipher system, called "Geheimschreiber" by the Germans, and "Fish" by the cryptanalysts.[67]
Colossus was not a stored-program computer. The input data for the five parallel processors was read from the looped message paper tape and the electronic pattern generators for the chi, psi and motor wheels.[68] The programs for the processors were set and held on the switches and jack panel connections. Each processor could evaluate a Boolean function and count and display the number of times it yielded the specified value of "false" (0) or "true" (1) for each pass of the message tape.
Input to the processors came from two sources, the shift registers from tape reading and the thyratron rings that emulated the wheels of the Tunny machine.[69] The characters on the paper tape were called Z and the characters from the Tunny emulator were referred to by the Greek letters that Bill Tutte had given them when working out the logical structure of the machine. On the selection panel, switches specified either Z or ΔZ, either 



χ


{\displaystyle \chi }

 or Δ



χ


{\displaystyle \chi }

 and either 



ψ


{\displaystyle \psi }

 or Δ



ψ


{\displaystyle \psi }

 for the data to be passed to the jack field and 'K2 switch panel'. These signals from the wheel simulators could be specified as stepping on with each new pass of the message tape or not.
The K2 switch panel had a group of switches on the left hand side to specify the algorithm. The switches on the right hand side selected the counter to which the result was fed. The plugboard allowed less specialized conditions to be imposed. Overall the K2 switch panel switches and the plugboard allowed about five billion different combinations of the selected variables. [63]
As an example: a set of runs for a message tape might initially involve two chi wheels, as in Tutte's 1+2 algorithm. Such a two-wheel run was called a long run, taking on average eight minutes unless the parallelism was utilised to cut the time by a factor of five. The subsequent runs might only involve setting one chi wheel, giving a short run taking about two minutes. Initially, after the initial long run, the choice of next algorithm to be tried was specified by the cryptanalyst. Experience showed, however, that decision trees for this iterative process could be produced for use by the Wren operators in a proportion of cases.[70]
Although the Colossus was the first of the electronic digital machines with programmability, albeit limited by modern standards,[71] it was not a general-purpose machine, being designed for a range of cryptanalytic tasks, most involving counting the results of evaluating Boolean algorithms.
A Colossus computer was thus not a fully Turing complete machine. However, University of San Francisco professor Benjamin Wells has shown that if all ten Colossus machines made were rearranged in a specific cluster, then the entire set of computers could have simulated a universal Turing machine, and thus be Turing complete.[72] The notion of a computer as a general purpose machine — that is, as more than a calculator devoted to solving difficult but specific problems — did not become prominent until after World War II.[citation needed]
Colossus and the reasons for its construction were highly secret, and remained so for 30 years after the War. Consequently, it was not included in the history of computing hardware for many years, and Flowers and his associates were deprived of the recognition they were due. Colossi 1 to 10 were dismantled after the war and parts returned to the Post Office. Some parts, sanitised as to their original purpose, were taken to Max Newman's Royal Society Computing Machine Laboratory at Manchester University.[73] Tommy Flowers was ordered to destroy all documentation and burnt them in a furnace at Dollis Hill. He later said of that order:
That was a terrible mistake. I was instructed to destroy all the records, which I did. I took all the drawings and the plans and all the information about Colossus on paper and put it in the boiler fire. And saw it burn.[74]
 Colossi 11 and 12, along with two replica Tunny machines, were retained, being moved to GCHQ's new headquarters at Eastcote in April 1946, and again with GCHQ to Cheltenham between 1952 and 1954.[75] One of the Colossi, known as Colossus Blue, was dismantled in 1959; the other in 1960.[75] There had been attempts to adapt them to other purposes, with varying success; in their later years they had been used for training.[76] Jack Good related how he was the first to use Colossus after the war, persuading the US National Security Agency that it could be used to perform a function for which they were planning to build a special-purpose machine.[75] Colossus was also used to perform character counts on one-time pad tape to test for non-randomness.[75]
A small number of people who were associated with Colossus—and knew that large-scale, reliable, high-speed electronic digital computing devices were feasible—played significant roles in early computer work in the UK and probably in the US. However, being so secret, it had little direct influence on the development of later computers; it was EDVAC that was the seminal computer architecture of the time. In 1972 Herman Goldstine, who was unaware of Colossus and its legacy to the projects of people such as Alan Turing (ACE), Max Newman (Manchester computers) and Harry Huskey (Bendix G-15), wrote that,
Britain had such vitality that it could immediately after the war embark on so many well-conceived and well-executed projects in the computer field.[77]
Professor Brian Randell, who unearthed information about Colossus in the 1970s, commented on this, saying that:
It is my opinion that the COLOSSUS project was an important source of this vitality, one that has been largely unappreciated, as has the significance of its places in the chronology of the invention of the digital computer.[78]
Randell's efforts started to bear fruit in the mid-1970s, after the secrecy about Bletchley Park was broken when Group Captain Winterbotham published his 1974 book The Ultra Secret.[79] In October 2000, a 500-page technical report on the Tunny cipher and its cryptanalysis—entitled General Report on Tunny[80]—was released by GCHQ to the national Public Record Office, and it contains a fascinating paean to Colossus by the cryptographers who worked with it:
It is regretted that it is not possible to give an adequate idea of the fascination of a Colossus at work; its sheer bulk and apparent complexity; the fantastic speed of thin paper tape round the glittering pulleys; the childish pleasure of not-not, span, print main header and other gadgets; the wizardry of purely mechanical decoding letter by letter (one novice thought she was being hoaxed); the uncanny action of the typewriter in printing the correct scores without and beyond human aid; the stepping of the display; periods of eager expectation culminating in the sudden appearance of the longed-for score; and the strange rhythms characterizing every type of run: the stately break-in, the erratic short run, the regularity of wheel-breaking, the stolid rectangle interrupted by the wild leaps of the carriage-return, the frantic chatter of a motor run, even the ludicrous frenzy of hosts of bogus scores.[81]
Construction of a fully functional rebuild[82][83] of a Colossus Mark 2 was undertaken between 1993 and 2008 by a team led by Tony Sale.[12][11] In spite of the blueprints and hardware being destroyed, a surprising amount of material survived, mainly in engineers' notebooks, but a considerable amount of it in the U.S. The optical tape reader might have posed the biggest problem, but Dr. Arnold Lynch, its original designer, was able to redesign it to his own original specification. The reconstruction is on display, in the historically correct place for Colossus No. 9, at The National Museum of Computing, in H Block Bletchley Park in Milton Keynes, Buckinghamshire.
In November 2007, to celebrate the project completion and to mark the start of a fundraising initiative for The National Museum of Computing, a Cipher Challenge[84] pitted the rebuilt Colossus against radio amateurs worldwide in being first to receive and decode three messages enciphered using the Lorenz SZ42 and transmitted from radio station DL0HNF in the Heinz Nixdorf MuseumsForum computer museum. The challenge was easily won by radio amateur Joachim Schüth, who had carefully prepared[85] for the event and developed his own signal processing and code-breaking code using Ada.[86] The Colossus team were hampered by their wish to use World War II radio equipment,[87] delaying them by a day because of poor reception conditions. Nevertheless, the victor's 1.4 GHz laptop, running his own code, took less than a minute to find the settings for all 12 wheels. The German codebreaker said: "My laptop digested ciphertext at a speed of 1.2 million characters per second—240 times faster than Colossus. If you scale the CPU frequency by that factor, you get an equivalent clock of 5.8 MHz for Colossus. That is a remarkable speed for a computer built in 1944."[88]
The Cipher Challenge verified the successful completion of the rebuild project. "On the strength of today's performance Colossus is as good as it was six decades ago", commented Tony Sale. "We are delighted to have produced a fitting tribute to the people who worked at Bletchley Park and whose brainpower devised these fantastic machines which broke these ciphers and shortened the war by many months."[89]
There was a fictional computer named Colossus in the 1970 movie Colossus: The Forbin Project which was based on the 1966 novel Colossus by D. F. Jones. This was sheer coincidence as it pre-dates the public release of information about Colossus, or even its name.
Neal Stephenson's novel Cryptonomicon (1999) also contains a fictional treatment of the historical role played by Turing and Bletchley Park.




Machine code - Wikipedia
Machine code is a computer program written in machine language instructions that can be executed directly by a computer's central processing unit (CPU). Each instruction causes the CPU to perform a very specific task, such as a load, a jump, or an ALU operation on a unit of data in a CPU register or memory.
Machine code is a strictly numerical language which is intended to run as fast as possible, and may be regarded as the lowest-level representation of a compiled or assembled computer program or as a primitive and hardware-dependent programming language. While it is possible to write programs directly in machine code, it is tedious and error prone to manage individual bits and calculate numerical addresses and constants manually. For this reason, programs are very rarely written directly in machine code in modern contexts, but may be done for low level debugging, program patching, and assembly language disassembly.
The overwhelming majority of practical programs today are written in higher-level languages or assembly language. The source code is then translated to executable machine code by utilities such as compilers, assemblers, and linkers, with the important exception of interpreted programs,[1] which are not translated into machine code. However, the interpreter itself, which may be seen as an executor or processor, performing the instructions of the source code, typically consists of directly executable machine code (generated from assembly or high-level language source code).
Machine code is by definition the lowest level of programming detail visible to the programmer, but internally many processors use microcode or optimise and transform machine code instructions into sequences of micro-ops, this is not generally considered to be a machine code per se.
Every processor or processor family has its own instruction set. Instructions are patterns of bits that by physical design correspond to different commands to the machine. Thus, the instruction set is specific to a class of processors using (mostly) the same architecture. Successor or derivative processor designs often include all the instructions of a predecessor and may add additional instructions. Occasionally, a successor design will discontinue or alter the meaning of some instruction code (typically because it is needed for new purposes), affecting code compatibility to some extent; even nearly completely compatible processors may show slightly different behavior for some instructions, but this is rarely a problem. Systems may also differ in other details, such as memory arrangement, operating systems, or peripheral devices. Because a program normally relies on such factors, different systems will typically not run the same machine code, even when the same type of processor is used.
A processor's instruction set may have all instructions of the same length, or it may have variable-length instructions. How the patterns are organized varies strongly with the particular architecture and often also with the type of instruction. Most instructions have one or more opcode fields which specifies the basic instruction type (such as arithmetic, logical, jump, etc.) and the actual operation (such as add or compare) and other fields that may give the type of the operand(s), the addressing mode(s), the addressing offset(s) or index, or the actual value itself (such constant operands contained in an instruction are called immediates).[2]
Not all machines or individual instructions have explicit operands. An accumulator machine has a combined left operand and result in an implicit accumulator for most arithmetic instructions. Other architectures (such as 8086 and the x86-family) have accumulator versions of common instructions, with the accumulator regarded as one of the general registers by longer instructions. A stack machine has most or all of its operands on an implicit stack. Special purpose instructions also often lack explicit operands (CPUID in the x86 architecture writes values into four implicit destination registers, for instance). This distinction between explicit and implicit operands is important in  code generators, especially in the register allocation and live range tracking parts. A good code optimizer can track implicit as well as explicit operands which may allow more frequent constant propagation, constant folding of registers (a register assigned the result of a constant expression freed up by replacing it by that constant) and other code enhancements.
A computer program is a list of instructions that can be executed by a central processing unit. A program's execution is done in order for the CPU that is executing it to solve a specific problem and thus accomplish a specific result. While simple processors are able to execute instructions one after another, superscalar processors are capable of executing a variety of different instructions at once.
Program flow may be influenced by special 'jump' instructions that transfer execution to an instruction other than the numerically following one. Conditional jumps are taken (execution continues at another address) or not (execution continues at the next instruction) depending on some condition.
A much more readable rendition of machine language, called assembly language, uses mnemonic codes to refer to machine code instructions, rather than using the instructions' numeric values directly. For example, on the Zilog Z80 processor, the machine code 00000101, which causes the CPU to decrement the B processor register, would be represented in assembly language as DEC B.
The MIPS architecture provides a specific example for a machine code whose instructions are always 32 bits long. The general type of instruction is given by the op (operation) field, the highest 6 bits. J-type (jump) and I-type (immediate) instructions are fully specified by op. R-type (register) instructions include an additional field funct to determine the exact operation. The fields used in these types are:
rs, rt, and rd indicate register operands; shamt gives a shift amount; and the address or immediate fields contain an operand directly.
For example, adding the registers 1 and 2 and placing the result in register 6 is encoded:
Load a value into register 8, taken from the memory cell 68 cells after the location listed in register 3:
Jumping to the address 1024:
In some computer architectures, the machine code is implemented by an even more fundamental underlying layer called microcode, providing a common machine language interface across a line or family of different models of computer with widely different underlying dataflows. This is done to facilitate porting of machine language programs between different models. An example of this use is the IBM System/360 family of computers and their successors. With dataflow path widths of 8 bits to 64 bits and beyond, they nevertheless present a common architecture at the machine language level across the entire line.
Using microcode to implement an emulator enables the computer to present the architecture of an entirely different computer. The System/360 line used this to allow porting programs from earlier IBM machines to the new family of computers, e.g. an IBM 1401/1440/1460 emulator on the IBM S/360 model 40.
Machine code is generally different from bytecode (also known as p-code), which is either executed by an interpreter or itself compiled into machine code for faster (direct) execution. An exception is when a processor is designed to use a particular bytecode directly as its machine code, such as is the case with Java processors.
Machine code and assembly code are sometimes called native code when referring to platform-dependent parts of language features or libraries.[3]
The Harvard architecture is a computer architecture with physically separate storage and signal pathways for the code (instructions) and data. Today, most processors implement such separate signal pathways for performance reasons but actually implement a Modified Harvard architecture,[citation needed] so they can support tasks like loading an executable program from disk storage as data and then executing it. Harvard architecture is contrasted to the Von Neumann architecture, where data and code are stored in the same memory which is read by the processor allowing the computer to execute commands.
From the point of view of a process, the code space is the part of its address space where the code in execution is stored. In multitasking systems this comprises the program's code segment and usually shared libraries. In multi-threading environment, different threads of one process share code space along with data space, which reduces the overhead of context switching considerably as compared to process switching.
Pamela Samuelson wrote that machine code is so unreadable that the United States Copyright Office cannot identify whether a particular encoded program is an original work of authorship;[4] however, the US Copyright Office does allow for copyright registration of computer programs[5] and a program's machine code can sometimes be decompiled in order to make its functioning more easily understandable to humans.[6]
Cognitive science professor Douglas Hofstadter has compared machine code to genetic code, saying that "Looking at a program written in machine language is vaguely comparable to looking at a DNA molecule atom by atom."[7]



Second-generation programming language - Wikipedia
Second-generation programming language (2GL) is a generational way to categorize assembly languages.[1]
The term was coined to provide a distinction from higher level third-generation programming languages (3GL) such as COBOL and earlier first-generation programming language (machine code languages).
Second-generation programming languages have the following properties:
Second-generation languages are sometimes used in kernels and device drivers (though C is generally employed for this in modern kernels), but more often find use in extremely intensive processing such as games, video editing, graphic manipulation/rendering.
One method for creating such code is by allowing a compiler to generate a machine-optimized assembly language version of a particular function. This code is then hand-tuned, gaining both the brute-force insight of the machine optimizing algorithm and the intuitive abilities of the human optimizer.



High-level programming language - Wikipedia
In computer science, a high-level programming language is a programming language with strong abstraction from the details of the computer. In contrast to low-level programming languages, it may use natural language elements, be easier to use, or may automate (or even hide entirely) significant areas of computing systems (e.g. memory management), making the process of developing a program simpler and more understandable than when using a lower-level language. The amount of abstraction provided defines how "high-level" a programming language is.[1]
In the 1960s, low-level programming languages using a compiler were commonly called autocodes.[2]
Examples of autocodes are COBOL and Fortran.[3]
The first high-level programming language designed for computers was Plankalkül, created by Konrad Zuse.[4] However, it was not implemented in his time, and his original contributions were largely isolated from other developments due to World War II, aside from the language's influence on the "Superplan" language by Heinz Rutishauser and also to some degree Algol. The first significantly widespread high-level language was Fortran, a machine-independent development of IBM's earlier Autocode systems. Algol, defined in 1958 and 1960 by committees of European and American computer scientists, introduced recursion as well as nested functions under lexical scope. It was also the first language with a clear distinction between value and name-parameters and their corresponding semantics.[5] Algol also introduced several structured programming concepts, such as the while-do and if-then-else constructs and its syntax was the first to be described in formal notation – "Backus–Naur form" (BNF). During roughly the same period, Cobol introduced records (also called structs) and Lisp introduced a fully general lambda abstraction in a programming language for the first time.
"High-level language" refers to the higher level of abstraction from machine language. Rather than dealing with registers, memory addresses and call stacks, high-level languages deal with variables, arrays, objects, complex arithmetic or boolean expressions, subroutines and functions, loops, threads, locks, and other abstract computer science concepts, with a focus on usability over optimal program efficiency. Unlike low-level assembly languages, high-level languages have few, if any, language elements that translate directly into a machine's native opcodes. Other features, such as string handling routines, object-oriented language features, and file input/output, may also be present. One thing to note about high-level programming languages is that these languages allow the programmer to be detached and separated from the machine. That is, unlike low-level languages like assembly or machine language, high-level programming can amplify the programmer's instructions and trigger a lot of data movements in the background without their knowledge. The responsibility and power of executing instructions have been handed over to the machine from the programmer.
High-level languages intend to provide features which standardize common tasks, permit rich debugging, and maintain architectural agnosticism; while low-level languages often produce more efficient code through optimization for a specific system architecture. Abstraction penalty is the border that prevents high-level programming techniques from being applied in situations where computational limitations, standards conformance or physical constraints require access to low-level architectural resources (fi, response time(s), hardware integration). High-level programming exhibits features like more generic data structures/operations, run-time interpretation, and intermediate code files; which often result in execution of far more operations than necessary, higher memory consumption, and larger binary program size.[6][7][8] For this reason, code which needs to run particularly quickly and efficiently may require the use of a lower-level language, even if a higher-level language would make the coding easier. In many cases, critical portions of a program mostly in a high-level language can be hand-coded in assembly language, leading to a much faster, more efficient, or simply reliably functioning optimised program.
However, with the growing complexity of modern microprocessor architectures, well-designed compilers for high-level languages frequently produce code comparable in efficiency to what most low-level programmers can produce by hand, and the higher abstraction may allow for more powerful techniques providing better overall results than their low-level counterparts in particular settings.[9]
High-level languages are designed independent of a specific computing system architecture. This facilitates executing a program written in such a language on any computing system with compatible support for the Interpreted or JIT program. High-level languages can be improved as their designers develop improvements. In other cases, new high-level languages evolve from one or more others with the goal of aggregating the most popular constructs with new or improved features. An example of this is Scala which maintains backward compatibility with Java which means that programs and libraries written in Java will continue to be usable even if a programming shop switches to Scala; this makes the transition easier and the lifespan of such high-level coding indefinite. In contrast, low-level programs rarely survive beyond the system architecture which they were written for without major revision. This is the engineering 'trade-off' for the 'Abstraction Penalty'.
Examples of high-level programming languages in active use today include Python, Visual Basic, Delphi, Perl, PHP, ECMAScript, Ruby, C# and many others.
The terms high-level and low-level are inherently relative. Some decades ago, the C language, and similar languages, were most often considered "high-level", as it supported concepts such as expression evaluation, parameterised recursive functions, and data types and structures, while assembly language was considered "low-level".  Today, many programmers might refer to C as low-level, as it lacks a large runtime-system (no garbage collection, etc.), basically supports only scalar operations, and provides direct memory addressing. It, therefore, readily blends with assembly language and the machine level of CPUs and microcontrollers.
Assembly language may itself be regarded as a higher level (but often still one-to-one if used without macros) representation of machine code, as it supports concepts such as constants and (limited) expressions, sometimes even variables, procedures, and data structures. Machine code, in its turn, is inherently at a slightly higher level than the microcode or micro-operations used internally in many processors.
There are three general modes of execution for modern high-level languages:
Note that languages are not strictly "interpreted" languages or "compiled" languages. Rather, implementations of language behavior use interpretation or compilation. For example, Algol 60 and Fortran have both been interpreted (even though they were more typically compiled). Similarly, Java shows the difficulty of trying to apply these labels to languages, rather than to implementations; Java is compiled to bytecode and the bytecode is subsequently executed by either interpretation (in a JVM) or compilation (typically with a just-in-time compiler such as HotSpot, again in a JVM). Moreover, compilation, trans-compiling, and interpretation are not strictly limited to just a description of the compiler artifact (binary executable or IL assembly).
Alternatively, it is possible for a high-level language to be directly implemented by a computer – the computer directly executes the HLL code. This is known as a high-level language computer architecture – the computer architecture itself is designed to be targeted by a specific high-level language. The Burroughs large systems were target machines for Algol 60, for example.



John Mauchly - Wikipedia
John William Mauchly (August 30, 1907 – January 8, 1980) was an American physicist who, along with J. Presper Eckert, designed ENIAC, the first general purpose electronic digital computer, as well as EDVAC, BINAC and UNIVAC I, the first commercial computer made in the United States.
Together they started the first computer company, the Eckert-Mauchly Computer Corporation (EMCC), and pioneered fundamental computer concepts including the stored program, subroutines, and programming languages.  Their work, as exposed in the widely read First Draft of a Report on the EDVAC (1945) and as taught in the Moore School Lectures (1946), influenced an explosion of computer development in the late 1940s all over the world.
John W. Mauchly was born on August 30, 1907, to Sebastian and Rachel (Scheidermantel) Mauchly in Cincinnati, Ohio. He moved with his parents and sister, Helen Elizabeth (Betty), at an early age to Chevy Chase, Maryland, when Sebastian Mauchly obtained a position at the Carnegie Institution of Washington as head of its Section of Terrestrial Electricity. As a youth, Mauchly was interested in science, and in particular with electricity, and as a young teenager was known to fix neighbors' electric systems. Mauchly attended E.V. Brown Elementary School in Chevy Chase and McKinley Technical High School in Washington, DC. At McKinley, Mauchly was extremely active in the debate team, was a member of the national honor society, and became editor-in-chief of the school's newspaper, Tech Life. After graduating from high school in 1925, he earned a scholarship to study engineering at Johns Hopkins University. He subsequently transferred to the Physics Department, and without completing his undergraduate degree, instead earned a Ph.D. in physics in 1932.[1]
From 1932 to 1933, Mauchly served as a research assistant at Johns Hopkins University where he concentrated on calculating energy levels of the formaldehyde spectrum. Mauchly's teaching career truly began in 1933 at Ursinus College where he was appointed head of the physics department, where he was, in fact, the only staff member.[2]
In the summer of 1941, Mauchly took a Defense Training Course for Electronics at the University of Pennsylvania Moore School of Electrical Engineering. There he met the lab instructor, J. Presper Eckert (1919-1995), with whom he would form a long-standing working partnership. Following the course, Mauchly was hired as an instructor of electrical engineering and in 1943, he was promoted to assistant professor of electrical engineering. Following the outbreak of World War II, the United States Army Ordnance Department contracted the Moore School to build an electronic computer which, as proposed by Mauchly and Eckert, would accelerate the recomputation of artillery firing tables.[3]
In 1959, Mauchly left Sperry Rand and started Mauchly Associates, Inc. One of Mauchly Associates' notable achievements was the development of the Critical Path Method (CPM) which provided for automated construction scheduling. Mauchly also set up a consulting organization, Dynatrend, in 1967 and worked as a consultant to Sperry UNIVAC from 1973 until his death in 1980.[4]
John Mauchly died on January 8, 1980, in Abington, Pennsylvania, during heart surgery and following a long illness. His first wife, Mary Augusta Walzl, a mathematician, whom he married on December 30, 1930, drowned in 1946. John and Mary Mauchly had two children, James (Jimmy) and Sidney. In 1948, Mauchly married Kathleen Kay McNulty (1921-2006), one of the six original ENIAC programmers; they had five children Sara (Sallie), Kathleen (Kathy), John, Virginia (Gini), and Eva.[5]
In 1941 Dr. Mauchly took a course in wartime electronics at the Moore School of Electrical Engineering, part of the University of Pennsylvania. There he met J. Presper Eckert, a recent Moore School graduate.  Mauchly accepted a teaching position at the Moore School, which was a center for wartime computing. Eckert encouraged Mauchly to believe that vacuum tubes could be made reliable with proper engineering practices. The critical problem that was consuming the Moore School was ballistics: the calculation of firing tables for the large number of new guns that the U.S. Army was developing for the war effort.
In 1942 Mauchly wrote a memo proposing the building of a general-purpose electronic computer.[6]  The proposal, which circulated within the Moore School (but the significance of which was not immediately recognized), emphasized the enormous speed advantage that could be gained by using digital electronics with no moving parts.  Lieutenant Herman Goldstine, who was the liaison between the United States Army and Moore School, picked up on the idea and asked Mauchly to write a formal proposal.  In April 1943, the Army contracted with the Moore School to build the Electronic Numerical Integrator and Computer (ENIAC).  Mauchly led the conceptual design while Eckert led the hardware engineering on ENIAC.  A number of other talented engineers contributed to the confidential "Project PX".
Because of its high-speed calculations, ENIAC could solve problems that were previously unsolvable. It was roughly a thousand times faster than the existing technology. It could add 5,000 numbers or do 357 10-digit multiplications in one second.
ENIAC could be programmed to perform sequences and loops of addition, subtraction, multiplication, division, square-root, input/output functions, and conditional branches.  Programming was initially accomplished with patch cords and switches, and reprogramming took days.  It was redesigned in 1948 to allow the use of stored programs with some loss in speed.
In 2002, for his work on ENIAC he was inducted, posthumously, into the National Inventors Hall of Fame.[7]
The ENIAC design was frozen in 1944 to allow construction. Eckert and Mauchly were already aware of the limitations of the machine and began plans on a second computer, to be called EDVAC. By January 1945 they had procured a contract to build this stored-program computer. Eckert had proposed a mercury delay line memory to store both program and data. Later that year, mathematician John von Neumann learned of the project and joined in some of the engineering discussions. He produced what was understood to be an internal document describing the EDVAC.
The term von Neumann architecture arose from von Neumann's paper, First Draft of a Report on the EDVAC.[8]  Dated June 30, 1945, it was an early written account of a general purpose stored-program computing machine (the EDVAC).  Goldstine, in a move that was to become controversial, removed any reference to Eckert or Mauchly and distributed the document to a number of von Neumann's associates across the country. The ideas became widely known within the very small world of computer designers.
Besides the lack of credit, Eckert and Mauchly suffered additional setbacks due to Goldstine's actions.  The ENIAC patent U.S. Patent 3,120,606, issued in 1964[9]  was filed on June 26, 1947, and granted February 4, 1964, but the public disclosure of design details of EDVAC in the First Draft (which were also common to ENIAC) was later cited as one cause for the 1973 invalidation of the ENIAC patent.
In March 1946, just after the ENIAC was announced, the Moore School decided to change their patent policy, in order to gain commercial rights to any future and past computer development there.  Eckert and Mauchly decided this was unacceptable; they resigned. However they had already been contracted to do one more thing at the Moore School: to give a series of talks on computer design.
The course "The Theory and Techniques for Design of Digital Computers", ran from July 8 to August 31, 1946. Eckert gave 11 of the lectures; Mauchly and Goldstine each delivered 6. "The Moore School Lectures", as they came to be known, were attended by representatives from the army, the navy, MIT, the National Bureau of Standards, Cambridge University, Columbia, Harvard, the Institute for Advanced Study, IBM, Bell Labs, Eastman Kodak, General Electric, and National Cash Register. A number of the attendees were to later go on to develop computers, such as Maurice Wilkes, of Cambridge, who built EDSAC.
In 1947 Eckert and Mauchly formed the first computer company, the Eckert-Mauchly Computer Corporation (EMCC); Mauchly was president. They secured a contract with the National Bureau of Standards to build an "EDVAC II", later named UNIVAC. UNIVAC, the first computer designed for business applications, had many significant technical advantages such as magnetic tape for mass storage. As an interim product, the company created and delivered a smaller computer, BINAC, but were still in a shaky financial situation. They were purchased by Remington Rand and became the UNIVAC division.
Very early in the history of EMCC, John Mauchly assumed responsibility for programming, coding, and applications for the planned computer systems. His early interaction with representatives of the Census Bureau in 1944 and 1945, and discussion with people interested in statistics, weather prediction, and various business problems in 1945 and 1946 focused his attention on the need to provide new users with the software to accomplish their objectives. He knew it would be difficult to sell computers without application materials, and without training in how to use the systems. And so, EMCC began to assemble a staff of mathematicians interested in coding in early 1947. (from Norberg)
Mauchly’s interest lay in the application of computers, as well as to their architecture and organization.  His experience with programming the ENIAC and its successors led him to create Short Code (see "The UNIVAC SHORT CODE"), the first programming language actually used on a computer (predated by Zuse’s conceptual Plankalkul). It was a pseudocode interpreter for mathematical problems proposed in 1949 and ran on the UNIVAC I and II. Mauchly's belief in the importance of languages led him to hire Grace Murray Hopper to develop a compiler for the UNIVAC.
John Mauchly has also been credited for being the first one using the verb "to program" in his 1942 paper on electronic computing, although in the context of ENIAC, not in its current meaning.
Dr. Mauchly stayed involved in computers for the rest of his life.  He was a founding member and president of the Association for Computing Machinery (ACM) and also helped found the Society for Industrial and Applied Mathematics (SIAM), serving as its fourth president. The Eckert-Mauchly Corporation was bought by Remington Rand in 1950 and for ten years Dr. Mauchly remained as Director of Univac Applications Research.  Leaving in 1959 he formed Mauchly Associates, a consulting company that later introduced the critical path method (CPM) for construction scheduling by computer. In 1967 he founded Dynatrend, a computer consulting organization. In 1973 he became a consultant to Sperry Univac.
Mauchly received numerous award and honors. He was a life member of the Franklin Institute, the National Academy of Engineering and the Society for Advancement of Management. He was elected a Fellow of the IRE, a predecessor society of IEEE, in 1957, and was a Fellow of the American Statistical Association. He received an LLD (Hon) degree from the University of Pennsylvania and aDSc(Hon) degree from Ursinus College. He was a recipient of the Philadelphia Award, the Scott Medal, the Goode Medal of AFIPS (American Federation of Information Processing Societies), the Pennsylvania Award, the Emanual R. Piore Award, the Howard N. Potts Medal, and numerous other awards.
Mauchly and Eckert's patent on the ENIAC was invalidated by U.S. Federal Court decision in October, 1973 for several reasons. Some had to do with the time between publication (the First Draft) and the patent filing date (1947).  The federal judge who presided over the case ruled that "the subject matter was derived" from the earlier Atanasoff–Berry Computer (ABC). This statement has become the center of a controversy.
Critics note that while the court said that the ABC was the first electronic digital computer, it did not define the term computer. It had originally referred to a person who computes, but was adapted to apply to a machine.
Critics of the court decision also note that there is, at a component level, nothing in common between the two machines.  The ABC was binary; the ENIAC was decimal. The ABC used regenerative drum memory; The ENIAC used electronic decade counters. The ABC used its tubes to implement a binary serial adder while the ENIAC used tubes to implement a complete set of decimal operations. The ENIAC's general-purpose instruction set, together with the ability to automatically sequence through them, made it a general-purpose computer.
Proponents for the court decision emphasize that the testimony established that Mauchly definitely had complete access to Atanasoff's machine and the documents describing it. Letters he wrote to Atanasoff show that he was at one time at least considering building on Atanasoff's approach.
Mauchly consistently maintained that it was the use of high-speed electronic flip-flops in cosmic-ray counting devices at Swarthmore College that gave him the idea for computing at electronic speeds.



University of Manchester - Wikipedia

Blue, gold, purple
The University of Manchester is a public research university in Manchester, England, formed in 2004 by the merger of the University of Manchester Institute of Science and Technology and the Victoria University of Manchester.[6][7] The University of Manchester is a red brick university, a product of the civic university movement of the late 19th century.
The main campus is south of Manchester city centre on Oxford Road. In 2016/17, the university had 40,490 students and 10,400 staff, making it the second largest university in the UK (out of 167 including the Open University), and the largest single-site university. The university had a consolidated income of £1 billion in 2016–17, of which £262.1 million was from research grants and contracts (6th place nationally behind Oxford, UCL, Cambridge, Imperial and Edinburgh).[1] It has the third-largest endowment of any university in England, after the universities of Cambridge and Oxford. It is a member of the worldwide Universities Research Association, the Russell Group of British research universities and the N8 Group. For 2018–19, the University of Manchester was ranked 29th in the world and 6th in the UK by QS World University Rankings. In 2017 it was ranked 38th in the world and 6th in the UK by Academic Ranking of World Universities, 55th in the world and 8th in the UK by Times Higher Education World University Rankings and 59th in the world by U.S. News and World Report. Manchester was ranked 15th in the UK amongst multi-faculty institutions for the quality (GPA) of its research[8] and 5th for its Research Power in the 2014 Research Excellence Framework.[9]
The university owns and operates major cultural assets such as the Manchester Museum, Whitworth Art Gallery, John Rylands Library and Jodrell Bank Observatory and its Grade I listed Lovell Telescope.[10]
The University of Manchester has 25 Nobel laureates among its past and present students and staff, the fourth-highest number of any single university in the United Kingdom. Four Nobel laureates are currently among its staff – more than any other British university.[11]
The University of Manchester traces its roots to the formation of the Mechanics' Institute (later UMIST) in 1824, and its heritage is linked to Manchester's pride in being the world's first industrial city.[12] The English chemist John Dalton, together with Manchester businessmen and industrialists, established the Mechanics' Institute to ensure that workers could learn the basic principles of science.
John Owens, a textile merchant, left a bequest of £96,942 in 1846 (around £5.6 million in 2005 prices)[13] to found a college to educate men on non-sectarian lines. His trustees established Owens College in 1851 in a house on the corner of Quay Street and Byrom Street which had been the home of the philanthropist Richard Cobden, and subsequently housed Manchester County Court. The locomotive designer, Charles Beyer became a governor of the college and was the largest single donor to the college extension fund, which raised the money to move to a new site and construct the main building now known as the John Owens building. He also campaigned and helped fund the engineering chair, the first applied science department in the north of England. He left the college the equivalent of £10 million in his will in 1876, at a time when it was in great financial difficulty. Beyer funded the total cost of construction of the Beyer building to house the biology and geology departments. His will also funded Engineering chairs and the Beyer Professor of Applied mathematics.
The university has a rich German heritage. The Owens College Extension Movement based their plans after a tour of mainly German universities and polytechnics.[14][15] Manchester mill owner, Thomas Ashton, chairman of the extension movement had studied at Heidelberg University. Sir Henry Roscoe also studied at Heidelberg under Robert Bunsen and they collaborated for many years on research projects. Roscoe promoted the German style of research led teaching that became the role model for the redbrick universities. Charles Beyer studied at Dresden Academy Polytechnic. There were many Germans on the staff, including Carl Schorlemmer, Britain's first chair in organic chemistry, and Arthur Schuster, professor of Physics.[16] There was even a German chapel on the campus.
In 1873 the college moved to new premises on Oxford Road, Chorlton-on-Medlock and from 1880 it was a constituent college of the federal Victoria University. The university was established and granted a Royal Charter in 1880 becoming England's first civic university; it was renamed the Victoria University of Manchester in 1903 and absorbed Owens College the following year.[17] By 1905, the institutions were large and active forces. The Municipal College of Technology, forerunner of UMIST, was the Victoria University of Manchester's Faculty of Technology while continuing in parallel as a technical college offering advanced courses of study. Although UMIST achieved independent university status in 1955, the universities continued to work together.[18]  However, in the late-20th century, formal connections between the university and UMIST diminished and in 1994 most of the remaining institutional ties were severed as new legislation allowed UMIST to become an autonomous university with powers to award its own degrees. A decade later the development was reversed.[19] The Victoria University of Manchester and the University of Manchester Institute of Science and Technology agreed to merge into a single institution in March 2003.[20][21]
Before the merger, Victoria University of Manchester and UMIST counted 23 Nobel Prize winners amongst their former staff and students, with two further Nobel laureates being subsequently added. Manchester has traditionally been strong in the sciences; it is where the nuclear nature of the atom was discovered by Ernest Rutherford, and the world's first electronic stored-program computer was built at the university. Notable scientists associated with the university include physicists Ernest Rutherford, Osborne Reynolds, Niels Bohr, James Chadwick, Arthur Schuster, Hans Geiger, Ernest Marsden and Balfour Stewart. Contributions in other fields such as mathematics were made by Paul Erdős, Horace Lamb and Alan Turing and in philosophy by Samuel Alexander, Ludwig Wittgenstein and Alasdair MacIntyre. The author Anthony Burgess, Pritzker Prize and RIBA Stirling Prize-winning architect Norman Foster and composer Peter Maxwell Davies all attended, or worked at, Manchester.
The current University of Manchester was officially launched on 1 October 2004 when Queen Elizabeth bestowed its Royal Charter.[22] The university was named the Sunday Times University of the Year in 2006 after winning the inaugural Times Higher Education Supplement University of the Year prize in 2005.[23]
The founding president and vice-chancellor of the new university was Alan Gilbert, former Vice-Chancellor of the University of Melbourne, who retired at the end of the 2009–2010 academic year.[24] His successor was Dame Nancy Rothwell,[3] who had held a chair in physiology at the university since 1994. One of the university's aims stated in the Manchester 2015 Agenda is to be one of the top 25 universities in the world, following on from Alan Gilbert's aim to "establish it by 2015 among the 25 strongest research universities in the world on commonly accepted criteria of research excellence and performance".[25] In 2011, four Nobel laureates were on its staff: Andre Geim,[26] Konstantin Novoselov,[27] Sir John Sulston and Joseph E. Stiglitz.
The EPSRC announced in February 2012 the formation of the National Graphene Institute. The University of Manchester is the "single supplier invited to submit a proposal for funding the new £45m institute, £38m of which will be provided by the government" – (EPSRC & Technology Strategy Board).[28] In 2013, an additional £23 million of funding from European Regional Development Fund was awarded to the institute taking investment to £61 million.[29]
In August 2012, it was announced that the university's Faculty of Engineering and Physical Sciences had been chosen to be the "hub" location for a new BP International Centre for Advanced Materials, as part of a $100 million initiative to create industry-changing materials.[30][31] The centre will be aimed at advancing fundamental understanding and use of materials across a variety of oil and gas industrial applications and will be modelled on a hub and spoke structure, with the hub located at Manchester, and the spokes based at the University of Cambridge, Imperial College London, and the University of Illinois at Urbana–Champaign.[32]
The university's main site contains most of its facilities and is often referred to as campus, however Manchester is not a campus university as the concept is commonly understood. It is centrally located in the city and its buildings are integrated into the fabric of Manchester, with non-university buildings and major roads between.
The campus occupies an area shaped roughly like a boot: the foot of which is aligned roughly south-west to north-east and is joined to the broader southern part of the boot by an area of overlap between former UMIST and former VUM buildings;[33] it comprises two parts:
The names are not officially recognised by the university, but are commonly used, including in parts of its website and roughly correspond to the campuses of the old UMIST and Victoria University respectively.
Fallowfield Campus is the main residential campus in Fallowfield, approximately 2 miles (3.2 km) south of the main site.
There are other university buildings across the city and the wider region, such as Jodrell Bank Observatory in Cheshire and One Central Park in Moston, a collaboration between the university and other partners which offers office space for start-up firms and venues for conferences and workshops,[34]
Following the merger, the university embarked on a £600 million programme of capital investment, to deliver eight new buildings and 15 major refurbishment projects by 2010, partly financed by a sale of unused assets.[35] These include:
The buildings around the Old Quadrangle date from the time of Owens College, and were designed in a Gothic style by Alfred Waterhouse and his son Paul Waterhouse. The first to be built was the John Owens Building (1873), formerly the Main Building; the others were added over the next thirty years. Today, the museum continues to occupy part of one side, including the tower. The grand setting of the Whitworth Hall is used for the conferment of degrees, and part of the old Christie Library (1898) now houses Christie's Bistro. The remainder of the buildings house administrative departments. The less easily accessed Rear Quadrangle, dating mostly from 1873, is older in its completed form than the Old Quadrangle.

Contact stages modern live performance for all ages, and participatory workshops primarily for young people aged 13 to 30. The building on Devas Street was completed in 1999 incorporating parts of its 1960s predecessor.[36] It has a unique energy-efficient ventilation system, using its high towers to naturally ventilate the building without the use of air conditioning. The colourful and curvaceous interior houses three performance spaces, a lounge bar and Hot Air, a reactive public artwork in the foyer.The Chancellors Hotel & Conference Centre was built around The Firs, a house built in 1850 for Sir Joseph Whitworth by Edward Walters, who also designed Manchester's Free Trade Hall. Whitworth used the house as a social, political and business base, entertaining radicals such as John Bright, Richard Cobden, William Forster and T.H. Huxley at the time of the Reform Bill of 1867. Whitworth, credited with raising the art of machine-tool building to a previously unknown level, supported the Mechanics Institute  – the birthplace of UMIST – and was a founder the Manchester School of Design. Whilst living there, Whitworth used land at the rear (now the site of the University's botanical glasshouses) for testing his "Whitworth rifle". In 1882, The Firs was leased to C.P. Scott, editor of the Manchester Guardian and after Scott's death became the property of Owens College. It was the Vice-Chancellor's residence until 1991.
The house now forms the western wing of the Chancellors Hotel & Conference Centre. The eastern wing houses the circular Flowers Theatre, six conference rooms and most of the hotel's bedrooms.
Other notable buildings in the Oxford Road Campus include the Stephen Joseph Studio, a former German Protestant church and the Samuel Alexander Building, a grade II listed building[37] erected in 1919 and home of the School of Arts, Languages and Cultures.
The University of Manchester was divided into four faculties, but from 1 August 2016 it was restructured into three faculties, each sub-divided into schools.
On 25 June 2015 Manchester University announced the results of a review of the position of life sciences as a separate faculty. As a result of this review the Faculty of Life Sciences was to be dismantled, most of its personnel to be incorporated into a single medical/biological faculty, with a substantial minority being incorporated into a science and engineering faculty.
The faculty is divided into the School of Biological Sciences, the School of Medical Sciences and the School of Health Sciences.
Biological Sciences have been taught at Manchester as far back as the foundation of Owens College in 1851. At UMIST, biological teaching and research began in 1959, with the creation of a Biochemistry department.[38] The present school, though unitary for teaching, is divided into a number of sections for research purposes.
The medical college was established in 1874 and is one of the largest in the country,[39] with more than 400 medical students trained in each clinical year and more than 350 students in the pre-clinical/phase 1 years.
The university is a founding partner of the Manchester Academic Health Science Centre, established to focus high-end healthcare research in Greater Manchester.[40]
In 1883, a department of pharmacy was established at the university and, in 1904, Manchester became the first British university to offer an honours degree in the subject. The School of Pharmacy[41] benefits from links with Manchester Royal Infirmary and Wythenshawe and Hope hospitals providing its undergraduate students with hospital experience.[42] The Pharmacy School's Centre for Pharmacy Postgraduate Education (CPPE) is considered a centre of excellence.[43]
Manchester Dental School was rated the country's best dental school by Times Higher Education in 2010 and 2011[44] and it is one of the best funded because of its emphasis on research and enquiry-based learning approach. The university has obtained multimillion-pound backing to maintain its high standard of dental education.[45] The University Dental Hospital of Manchester is part of Central Manchester University Hospitals NHS Foundation Trust. It was established in 1884 in association with the School of Medicine at Owens College. In 1905 the university established a degree and a diploma in dental surgery (first awarded in 1909 and 1908 respectively).[46]
The Faculty of Science and Engineering comprises the schools of Chemical Engineering and Analytical Science;Chemistry; Computer Science; Earth and Environmental Science; Physics and Astronomy; Electrical and Electronic Engineering; Materials; Mathematics; and Mechanical, Aerospace and Civil Engineering.
The Jodrell Bank Centre for Astrophysics comprises the university's astronomical academic staff in Manchester and Jodrell Bank Observatory on rural land near Goostrey, about ten miles (16 km) west of Macclesfield away from the lights of Greater Manchester. The observatory's Lovell Telescope, named after Sir Bernard Lovell, a professor at the Victoria University of Manchester who first proposed the telescope. Constructed in the 1950s, it is the third largest fully movable radio telescope in the world. It has played an important role in the research of quasars, pulsars and gravitational lenses, and in confirming Einstein's theory of General Relativity.
The Faculty of Humanities includes the School of Arts, Languages and Cultures (incorporating Archaeology; Art History & Visual Studies; Classics and Ancient History; Drama; English and American Studies; History; Linguistics; Modern Languages; Museology; Music; Religions and Theology and the University Language Centre) and the Schools of Combined Studies; Education; Environment and Development; Architecture; Law; Social Sciences and the Manchester Business School. The Faculty of Humanities also jointly administers the Manchester School of Architecture (MSA) in conjunction with Manchester Metropolitan University and MSA students are classified as students of both universities.
Additionally, the faculty comprises a number of research institutes: the Centre for New Writing, the Institute for Social Change, the Brooks World Poverty Institute, Humanitarian and Conflict Response Institute, the Manchester Institute for Innovation Research, the Research Institute for Cosmopolitan Cultures, the Centre for Chinese Studies, the Institute for Development Policy and Management, the Centre for Equity in Education and the Sustainable Consumption Institute.
In the financial year ending 31 July 2011, the University of Manchester had a total income of £808.58 million (2009/10 – £787.9 million) and total expenditure of £754.51 million (2009/10 – £764.55 million).[1] Key sources of income included £247.28 million from tuition fees and education contracts (2009/10 – £227.75 million), £203.22 million from funding body grants (2009/10 – £209.02 million), £196.24 million from research grants and contracts (2009/10 – £194.6 million) and £14.84 million from endowment and investment income (2009/10 – £11.38 million).[1] During the 2010/11 financial year the University of Manchester had a capital expenditure of £57.42 million (2009/10 – £37.95 million).[1]
At year end the University of Manchester had endowments of £158.7 million (2009/10 – £144.37 million) and total net assets of £731.66 million (2009/10 – £677.12 million).[1]
The University of Manchester has the largest number of full-time students in the UK, unless the University of London's colleges are counted as a single university. It teaches more academic subjects than any other British university.
Well-known figures among the university's current academic staff include computer scientist Steve Furber, economist Richard Nelson,[47] novelist Jeanette Winterson[48]  (who succeeded Colm Tóibín in 2012)[49] and biochemist Sir John Sulston, Nobel laureate of 2002.
The University of Manchester is a major centre for research and a member of the Russell Group of leading British research universities.[50] In the 2014 Research Excellence Framework, the university was ranked fifth in the UK in terms of research power and fifteenth for grade point average quality of staff submitted among multi-faculty institutions (seventeenth when including specialist institutions)[51][52] Manchester has the sixth largest research income of any English university (after Oxford, UCL, Cambridge, Imperial and King's College London),[53] and has been informally referred to as part of a "golden diamond" of research-intensive UK institutions (adding Manchester to the Oxford–Cambridge–London "Golden Triangle").[54] Manchester has a strong record in terms of securing funding from the three main UK research councils, EPSRC, MRC and BBSRC, being ranked fifth,[55] seventh[56] and first[57] respectively. In addition, the university is one of the richest in the UK in terms of income and interest from endowments: an estimate in 2008 placed it third, surpassed only by Oxford and Cambridge.[58]
The University of Manchester has attracted the most research income from UK industry of any institution in the country. The figures, from the Higher Education Statistics Agency (HESA), show that Manchester attracted £24,831,000 of research income in 2016-2017 from UK industry, commerce and public corporations.[59]
Historically, Manchester has been linked with high scientific achievement: the university and its constituent former institutions combined had 25 Nobel laureates among their students and staff, the third largest number of any single university in the United Kingdom (after Oxford and Cambridge) and the ninth largest of any university in Europe. Furthermore, according to an academic poll two of the top ten discoveries by university academics and researchers were made at the university (namely the first working computer and the contraceptive pill).[60] The university currently employs four Nobel Prize winners amongst its staff, more than any other in the UK.[61] The Langworthy Professorship, an endowed chair at the University's School of Physics and Astronomy, has been historically given to a long line of academic luminaries, including Ernest Rutherford (1907–19), Lawrence Bragg (1919–37), Patrick Blackett (1937–53) and more recently Konstantin Novoselov, all of whom have won the Nobel Prize. In 2013 Manchester was given the Regius Professorship in Physics, the only one of its kind in the UK; the current holder is Andre Geim.
The University of Manchester Library is the largest non-legal deposit library in the UK and the third-largest academic library after those of Oxford and Cambridge.[62] It has the largest collection of electronic resources of any library in the UK.[62]
The John Rylands Library, founded in memory of John Rylands by his wife Enriqueta Augustina Rylands as an independent institution, is situated in a Victorian Gothic building on Deansgate, in the city centre. It houses an important collection of historic books and other printed materials, manuscripts, including archives and papyri. The papyri are in ancient languages and include the oldest extant New Testament document, Rylands Library Papyrus P52, commonly known as the St John Fragment. In April 2007 the Deansgate site reopened to readers and the public after major improvements and renovations, including the construction of the pitched roof originally intended and a new wing.
The Manchester Museum holds nearly 4.25 million[63] items sourced from many parts of the world. The collections include butterflies and carvings from India, birds and bark-cloth from the Pacific, live frogs and ancient pottery from America, fossils and native art from Australia, mammals and ancient Egyptian craftsmanship from Africa, plants, coins and minerals from Europe, art from past civilisations of the Mediterranean, and beetles, armour and archery from Asia. In November 2004, the museum acquired a cast of a fossilised Tyrannosaurus rex called "Stan".
The museum's first collections were assembled in 1821 by the Manchester Society of Natural History, and subsequently expanded by the addition of the collections of Manchester Geological Society. Due to the society's financial difficulties and on the advice of evolutionary biologist Thomas Huxley, Owens College accepted responsibility for the collections in 1867. The college commissioned Alfred Waterhouse, architect of London's Natural History Museum, to design a museum on a site in Oxford Road to house the collections for the benefit of students and the public. The Manchester Museum was opened to the public in 1888.[64]
The Whitworth Art Gallery houses collections of internationally famous British watercolours, textiles and wallpapers, modern and historic prints, drawings, paintings and sculpture. It contains 31,000 items in its collection. A programme of temporary exhibitions runs throughout the year and the Mezzanine Court displays sculpture.
The gallery was founded by Robert Darbishire with a donation from Sir Joseph Whitworth in 1889, as The Whitworth Institute and Park. In 1959 the gallery became part of the Victoria University of Manchester.[65] In October 1995 the Mezzanine Court in the centre of the building was opened. It was designed to display sculptures and won a RIBA regional award.[citation needed]
In an employability ranking published by Emerging in 2015, where CEOs and chairmen were asked to select the top universities they recruited from, Manchester was placed 24th in the world and 5th nationally.[74] In the 2014 Research Excellence Framework,[75] Manchester came fifth in terms of research power and seventeenth for grade point average quality when including specialist institutions.[76][77] According to the 2017 High Fliers Report, Manchester is the second most targeted university by the largest number of leading graduate employers in the UK.[78]
According to The Sunday Times in 2006, "Manchester has a formidable reputation spanning most disciplines, but most notably in the life sciences, engineering, humanities, economics, sociology and the social sciences".[79] As of 2016, Manchester is ranked as the 8th, 10th and 49th most reputable university in the UK, Europe and the world respectively.[80]  Manchester was also given a prestigious award for Excellence and Innovation in the Arts by the Times Higher Education Awards 2010.[81] In a recent ranking published by the New York Times, Manchester was placed as the 9th most innovative university in Europe and 3rd nationally behind Imperial and Cambridge.[82]
The QS World University Rankings 2018-19 placed Manchester 29th in the world.[83]
The Academic Ranking of World Universities 2016 ranked Manchester 5th in the UK and 35th in the world. Manchester is ranked 56th in the world (and 8th in the UK) in the 2016 Round University Ranking.[84]
In 2017, the Alliance Manchester Business School was ranked 3rd in UK, 10th in Europe and 30th in the world by the Financial Times in its global MBA ranking.[85]
However, while world rankings (such as QS, ARWU, THE) typically place the university within the top 10 in the UK, in national studies the university ranks less favourably. In The Sunday Times 10-year (1998–2007) average ranking of British universities based on consistent league table performance, Manchester was ranked 17th overall in the UK.[86] The Times/Sunday Times 'Good University Guide 2015' ranked Manchester 28th out of universities in the UK, 'The Complete University Guide 2016'  placed it at 28th, whilst 'The Guardian University Guide 2016' ranked Manchester at 29th in the UK. This apparent paradox is mainly a reflection of the different ranking methodologies employed by each listing: global rankings focus on research and international reputation, whereas national rankings are largely based on entry standards, graduate prospects and student satisfaction with teaching at the university.[87] In fact, a recent poll voted Manchester as the third "most underrated university in the UK" [88]
More students apply to Manchester than to any other university in the country, with more than 55,000 applications for undergraduate courses in 2014 resulting in 6.5 applicants for every available place.[79][92] Manchester had the 17th highest average entry qualification for undergraduates of any UK university in 2015, with new students averaging 431 UCAS points,[93] equivalent to just above A*AAb or ABBab in A-level grades. In 2015, the university gave offers of admission to 73.4% of its applicants, the 10th lowest amongst the Russell Group.[94]
17.2% of Manchester's undergraduates are privately educated, the 23rd highest proportion amongst mainstream British universities.[95] In the 2016-17 academic year, the university had a domicile breakdown of 67:6:27 of UK:EU:non-EU students respectively with a female to male ratio of 53:47.[96]
Manchester University Press is the university's academic publishing house. It publishes academic monographs, textbooks and journals, most of which are works from authors based elsewhere in the international academic community, and is the third-largest university press in England after Oxford University Press and Cambridge University Press.
The University of Manchester Students' Union is the representative body of students at the university and the UK's largest students' union. It was formed out of the merger between UMIST Students' Association and University of Manchester Union when the parent organisations UMIST and the Victoria University of Manchester merged on 1 October 2004.
Unlike many other students' unions in the UK, it does not have a president, but is run by an eight-member executive team who share joint responsibility.
The University of Manchester operates sports clubs via the Athletics Union while student societies are operated by the Students' Union.
The university has more than 80 health and fitness classes while over 3,000 students are members of the 44 various Athletic Union clubs. The sports societies vary widely in their level and scope. Many more popular sports operate several university teams and departmental teams which compete in leagues against other teams within the university. Teams include: lacrosse, korfball, dodgeball, hockey, rugby league, rugby union, football, basketball, netball and cricket. The Manchester Aquatics Centre, the swimming pool used for the Manchester Commonwealth Games is on the campus.
The university competes annually in 28 different sports against Leeds and Liverpool universities in the Christie Cup, which Manchester has won for seven consecutive years.[98] The university has achieved success in the BUCS (British University & College Sports) competitions, with its men's water polo 1st team winning the national championships (2009, 2010, 2011) under the tutelage of coach Andy Howard.[99] It was positioned in eighth place in the overall BUCS rankings for 2009/10[100] The Christie Cup is an inter-university competition between Liverpool, Leeds and Manchester in numerous sports since 1886. After the Oxford and Cambridge rivalry, the Christie's Championships is the oldest Inter–University competition on the sporting calendar: the cup was a benefaction of Richard Copley Christie.
Every year elite sportsmen and sportswomen are selected for membership of the XXI Club, a society formed in 1932 to promote sporting excellence at the university. Most members have gained a Full Maroon for representing the university and many have excelled at a British Universities or National level.
In the eight years up to 2013 Manchester has won the BBC2 quiz programme University Challenge four times, drawing equal with Magdalen College, Oxford, for the highest number of series wins.[101] Since merging as the University of Manchester, the university has consistently reached the latter stages of the competition, progressing to at least the semi-finals every year since 2005.[102]
In 2006, Manchester beat Trinity Hall, Cambridge, to record the university's first win in the competition. The next year, the university finished in second place after losing to the University of Warwick in the final. In 2009, the team battled hard in the final against Corpus Christi College, Oxford. At the gong, the score was 275 to 190 in favour of Corpus Christi College after a winning performance from Gail Trimble. However, the title was eventually given to the University of Manchester after it was discovered that Corpus Christi team member Sam Kay had graduated eight months before the final was broadcast, so the team was disqualified.
Manchester reached the semi-finals in the 2010 competition before being beaten by Emmanuel College, Cambridge. The university did not enter the 2011 series for an unknown reason. However, Manchester did enter a year later and won University Challenge 2012.[102] Manchester has since defended its title to win University Challenge 2013, beating University College London, 190 to 140.
The University of Manchester attracts thousands of international students coming from 154 countries around the world.[103]
Before they merged, the two former universities had for some time been sharing their residential facilities.
Whitworth Park Halls of Residence is owned by the University of Manchester and houses 1,085 students.[104][105]  It is notable for its triangular shaped accommodation blocks which gave rise to the nickname of "Toblerones", after the chocolate bar. Their designer took inspiration from a hill created from excavated soil which had been left in 1962 from an archaeological dig led by John Gater. A consequence of the triangular design was a reduced cost for the construction company. A deal struck between the university and Manchester City Council meant the council would pay for the roofs of all student residential buildings in the area, Allan Pluen's team is believed to have saved thousands on the final cost of the halls. They were built in the mid-1970s.
The site of the halls was previously occupied by many small streets whose names have been preserved in the names of the halls. Grove House is an older building that has been used by the university for many different purposes over the last sixty years. Its first occupants in 1951 were the Appointments Board and the Manchester University Press.[106] The shops in Thorncliffe Place were part of the same plan and include banks and a convenience store.
Notable people associated with the halls include Friedrich Engels, whose residence is commemorated by a blue plaque on Aberdeen House; the physicist Brian Cox; and Irene Khan, Secretary General of Amnesty International.[107]
The former UMIST Campus has four halls of residence near to Sackville Street building (Weston, Lambert, Fairfield, and Wright Robinson). Chandos Hall, a former residence, has been closed prior to demolition.
Moberly Tower has been demolished. Other residences include Vaughn House, once the home of the clergy serving the Church of the Holy Name, and George Kenyon Hall at University Place; Crawford House and Devonshire House adjacent to the Manchester Business School and Victoria Hall on Upper Brook Street.
Victoria Park Campus comprises several halls of residence. Among these are St. Anselm Hall with Canterbury Court and Pankhurst Court, Dalton-Ellis Hall, Hulme Hall (including Burkhardt House), St Gabriel's Hall and Opal Gardens Hall. St. Anselm Hall is the only all-male hall in the United Kingdom.
The Fallowfield Campus, 2 miles (3.2 km) south of the Oxford Road Campus is the largest of the university's residential campuses. The Owens Park group of halls with a landmark tower is at its centre, while Oak House is another hall of residence. Woolton Hall is next to Oak House. Allen Hall is a traditional hall near Ashburne Hall (Sheavyn House being annexed to Ashburne). Richmond Park is a recent addition to the campus.
Many notable people have worked or studied at one or both of the two former institutions that now form the University of Manchester, including 25 Nobel prize laureates. Some of the best-known are: John Dalton (founder of modern atomic theory), Ernest Rutherford who proved the nuclear nature of the atom whilst working at Manchester, Ludwig Wittgenstein (considered one of the most significant philosophers of the 20th century, who studied for a doctorate in engineering), George E. Davis (founder of the discipline of Chemical Engineering), Marie Stopes (pioneer of birth control and campaigner for women's rights), Bernard Lovell (a pioneer of radio astronomy), Alan Turing (one of the founders of computer science and artificial intelligence), Tom Kilburn and Frederic Calland Williams (who developed the Manchester Baby, the world's first stored-program computer at Victoria University of Manchester in 1948), Irene Khan (former Secretary General of Amnesty International), physicist and television presenter Brian Cox, the author Anthony Burgess and Robert Bolt (two times Academy Award winner and three times Golden Globe winner for writing the screenplay for Lawrence of Arabia and Doctor Zhivago).
A number of politicians are associated with the university, including the current presidents of the Republic of Ireland and the Somaliland region of Somalia and prime ministers of Palestine and Iraq, as well as several ministers in the United Kingdom, Malaysia, Canada and Singapore. The vice president of Tanzania (November 2015 – present), Samia Hassan Suluhu, also attended the University of Manchester. Chaim Weizmann, a senior lecturer at the university, was also the first President of Israel.
The university educated some of the leading figures of Alternative Comedy: Ben Elton, Ade Edmonson and Rik Mayall. Additionally, a number of well-known actors have studied at the university, including Benedict Cumberbatch, who most notably portrays Sherlock Holmes in the TV series Sherlock, as well as playing the role of Manchester's own Alan Turing in the 2014 Oscar-winning biopic The Imitation Game.
The University of Manchester, inclusive of its predecessor institutions, numbers 25 Nobel Prize recipients amongst its current and former staff and students, with some of the most important discoveries of the modern age having been made in Manchester. Manchester University has the fourth largest number of Nobel laureates in the UK, only Cambridge, Oxford and UCL having a greater number.
Chemistry
Physics
Physiology and Medicine
Economics
Coordinates: 53°27′56″N 2°14′01″W﻿ / ﻿53.46556°N 2.23361°W﻿ / 53.46556; -2.23361



Tony Brooker - Wikipedia

Ralph Anthony "Tony" Brooker (born 1925, England[1]) is a British academic who was a computer scientist known for developing the Mark 1 Autocode.[1]
He was educated at Emanuel School and graduated in Mathematics from Imperial College in 1945 and returned there in 1947 as Assistant Lecturer. His first computer project was the construction of a fast multiplier unit from electro-mechanical relays. This was taken over by Sid Michaelson [2] and  K. D. Tocher and incorporated into ICCE, the Imperial College Computing Engine based on the same technology.[3] By then (1949) Tony had moved to the University of Cambridge Computer Laboratory to work for Maurice Wilkes on software development for EDSAC.
In October 1951 Tony joined the Computing Machine Laboratory at Manchester University, where he took over from Alan Turing the task of writing programming manuals and running a user service on the Ferranti Mark 1 computer. It was his experience with the rather tedious Manchester machine-coding conventions that led him to devise what was probably the world's first publicly available High-Level Language. This was the Mark 1 Autocode available from March 1954 and therefore about two years ahead of the first Fortran compiler.
Throughout the 1950s Tony led a group at Manchester working on the theoretical underpinnings of compilers. This culminated in the compiler-compiler, a seminal idea first presented at a British Computer Society Conference in July 1960 by Brooker and Morris. This was subsequently implemented on the Ferranti ATLAS and used for high-level language development. The ATLAS was regarded as the world's most powerful computer when it was brought into service in December 1962.
In the mid-1960s Tony helped to inaugurate the UK's first Computer Science degree course at Manchester. He moved to Essex University in 1967 to take up the University's founding Chair of Computer Science. The first Essex Computer Science graduates obtained their degrees in the summer of 1970. He retired in 1988.[4]



Fortran - Wikipedia

Fortran (/ˈfɔːrtræn/; formerly FORTRAN, derived from Formula Translation[2]) is a general-purpose, compiled imperative programming language that is especially suited to numeric computation and scientific computing.
Originally developed by IBM[3] in the 1950s for scientific and engineering applications, FORTRAN came to dominate this area of programming early on and has been in continuous use for over half a century in computationally intensive areas such as numerical weather prediction, finite element analysis, computational fluid dynamics, computational physics, crystallography and computational chemistry. It is a popular language for high-performance computing[4] and is used for programs that benchmark and rank the world's fastest supercomputers.[5]
Fortran encompasses a lineage of versions, each of which evolved to add extensions to the language while usually retaining compatibility with prior versions.  Successive versions have added support for structured programming
and processing of character-based data (FORTRAN 77), array programming, modular programming and generic programming (Fortran 90), high performance Fortran (Fortran 95), object-oriented programming (Fortran 2003) and concurrent programming (Fortran 2008).
Fortran's design was the basis for many other programming languages. Among the better known is BASIC, which is a based on FORTRAN II with a number of syntax cleanups, notably better logical structures,[6] and other changes to more easily work in an interactive environment.[7]
The names of earlier versions of the language through FORTRAN 77 were conventionally spelled in all-capitals (FORTRAN 77 was the last version in which the use of lowercase letters in keywords was strictly non-standard).  The capitalization has been dropped in referring to newer versions beginning with Fortran 90. The official language standards now refer to the language as "Fortran" rather than all-caps "FORTRAN".
In late 1953, John W. Backus submitted a proposal to his superiors at IBM to develop a more practical alternative to assembly language for programming their IBM 704 mainframe computer. Backus' historic FORTRAN team consisted of programmers Richard Goldberg, Sheldon F. Best, Harlan Herrick, Peter Sheridan, Roy Nutt, Robert Nelson, Irving Ziller, Lois Haibt, and David Sayre.[8]  Its concepts included easier entry of equations into a computer, an idea developed by J. Halcombe Laning and demonstrated in the Laning and Zierler system of 1952.[9]
A draft specification for The IBM Mathematical Formula Translating System was completed by mid-1954.  The first manual for FORTRAN appeared in October 1956, with the first FORTRAN compiler delivered in April 1957.  This was the first optimizing compiler, because customers were reluctant to use a high-level programming language unless its compiler could generate code with performance comparable to that of hand-coded assembly language.[10]
While the community was skeptical that this new method could possibly outperform hand-coding, it reduced the number of programming statements necessary to operate a machine by a factor of 20, and quickly gained acceptance.  John Backus said during a 1979 interview with Think, the IBM employee magazine, "Much of my work has come from being lazy. I didn't like writing programs, and so, when I was working on the IBM 701, writing programs for computing missile trajectories, I started work on a programming system to make it easier to write programs."[11]
The language was widely adopted by scientists for writing numerically intensive programs, which encouraged compiler writers to produce compilers that could generate faster and more efficient code.  The inclusion of a complex number data type in the language made Fortran especially suited to technical applications such as electrical engineering.[citation needed]
By 1960, versions of FORTRAN were available for the IBM 709, 650, 1620, and 7090 computers.  Significantly, the increasing popularity of FORTRAN spurred competing computer manufacturers to provide FORTRAN compilers for their machines, so that by 1963 over 40 FORTRAN compilers existed.  For these reasons, FORTRAN is considered to be the first widely used  programming language supported across a variety of computer architectures.
The development of Fortran paralleled the early evolution of compiler technology, and many advances in the theory and design of compilers were specifically motivated by the need to generate efficient code for Fortran programs.
The initial release of FORTRAN for the IBM 704 contained 32 statements, including:
The arithmetic IF statement was reminiscent of (but not readily implementable by) a three-way comparison instruction (CAS –  Compare Accumulator with Storage) available on the 704. The statement provided the only way to compare numbers –  by testing their difference, with an attendant risk of overflow. This deficiency was later overcome by "logical" facilities introduced in FORTRAN IV.
The FREQUENCY statement was used originally (and optionally) to give branch probabilities for the three branch cases of the arithmetic IF statement. The first FORTRAN compiler used this weighting to perform at compile time a Monte Carlo simulation of the generated code, the results of which were used to optimize the placement of basic blocks in memory –  a very sophisticated optimization for its time. The Monte Carlo technique is documented in Backus et al.'s paper on this original implementation, The FORTRAN Automatic Coding System:
The fundamental unit of program is the basic block; a basic block is a stretch of program which has one entry point and one exit point. The purpose of section 4 is to prepare for section 5 a table of predecessors (PRED table) which enumerates the basic blocks and lists for every basic block each of the basic blocks which can be its immediate predecessor in flow, together with the absolute frequency of each such basic block link. This table is obtained by running the program once in Monte-Carlo fashion, in which the outcome of conditional transfers arising out of IF-type statements and computed GO TO's is determined by a random number generator suitably weighted according to whatever FREQUENCY statements have been provided.[12]
Many years later, the FREQUENCY statement had no effect on the code, and was treated as a comment statement, since the compilers no longer did this kind of compile-time simulation. A similar fate has befallen compiler hints in several other programming languages; for example C's register keyword.[citation needed]
The first FORTRAN compiler reported diagnostic information by halting the program when an error was found and outputting an error code on its console. That code could be looked up by the programmer in an error messages table in the operator's manual, providing them with a brief description of the problem.[13][14]
Before the development of disk files, text editors and terminals, programs were most often entered on a keypunch keyboard onto 80-column punched cards, one line to a card. The resulting deck of cards would be fed into a card reader to be compiled. Punched card codes included no lower-case letters or many special characters, and special versions of the IBM 026 keypunch were offered that would correctly print the re-purposed special characters used in FORTRAN.
Reflecting punched card input practice, Fortran programs were originally written in a fixed-column format, with the first 72 columns read into twelve 36-bit words.
A letter "C" in column 1 caused the entire card to be treated as a comment and ignored by the compiler. Otherwise, the columns of the card were divided into four fields:
Columns 73 to 80 could therefore be used for identification information, such as punching a sequence number or text, which could be used to re-order cards if a stack of cards was dropped; though in practice this was reserved for stable, production programs. An IBM 519 could be used to copy a program deck and add sequence numbers. Some early compilers, e.g., the IBM 650's, had additional restrictions due to limitations on their card readers.[16] Keypunches could be programmed to tab to column 7 and skip out after column 72. Later compilers relaxed most fixed-format restrictions, and the requirement was eliminated in the Fortran 90 standard.
Within the statement field, whitespace characters (blanks) were ignored outside a text literal. This allowed omitting spaces between tokens for brevity or including spaces within identifiers for clarity. For example, AVG OF X was a valid identifier, equivalent to AVGOFX, and 101010DO101I=1,101 was a valid statement, equivalent to 
10101 DO 101 I = 1, 101 because the zero in column 6 is treated as if it were a space (!), while 101010DO101I=1.101 was instead 10101 DO101I = 1.101, the assignment of 1.101 to a variable called DO101I. Note the slight visual difference between a comma and a period.
Hollerith strings, originally allowed only in FORMAT and DATA statements, were prefixed by a character count and the letter H (e.g., 26HTHIS IS ALPHANUMERIC DATA.), allowing blanks to be retained within the character string. Miscounts were a problem.
IBM's FORTRAN II appeared in 1958.  The main enhancement was to support procedural programming by allowing user-written subroutines and functions which returned values, with parameters passed by reference.  The COMMON statement provided a way for subroutines to access common (or global) variables. Six new statements were introduced:
Over the next few years, FORTRAN II would also add support for the DOUBLE PRECISION and COMPLEX data types.
Early FORTRAN compilers supported no recursion in subroutines. Early computer architectures supported no concept of a stack, and when they did directly support subroutine calls, the return location was often stored in one fixed location adjacent to the subroutine code (e.g. the IBM 1130) or a specific machine register (IBM 360 et seq), which only allows recursion if a stack is maintained by software and the return address is stored on the stack before the call is made and restored after the call returns. Although not specified in FORTRAN 77, many F77 compilers supported recursion as an option, and the Burroughs mainframes, designed with recursion built-in, did so by default. It became a standard in Fortran 90 via the new keyword RECURSIVE.[17]
This program, for Heron's formula, reads data on a tape reel containing three 5-digit integers A, B, and C as input. There are no "type" declarations available: variables whose name starts with I, J, K, L, M, or N are "fixed-point" (i.e. integers), otherwise floating-point. Since integers are to be processed in this example, the names of the variables start with the letter "I". The name of a variable must start with a letter and can continue with both letters and digits, up to a limit of six characters in FORTRAN II.  If A, B, and C cannot represent the sides of a triangle in plane geometry, then the program's execution will end with an error code of "STOP 1".  Otherwise, an output line will be printed showing the input values for A, B, and C, followed by the computed AREA of the triangle as a floating-point number occupying ten spaces along the line of output and showing 2 digits after the decimal point, the .2 in F10.2 of the FORMAT statement with label 601.
IBM also developed a FORTRAN III in 1958 that allowed for inline assembly code among other features; however, this version was never released as a product.  Like the 704 FORTRAN and FORTRAN II, FORTRAN III included machine-dependent features that made code written in it unportable from machine to machine.  Early versions of FORTRAN provided by other vendors suffered from the same disadvantage.
FORTRAN was provided for the IBM 1401 computer by an innovative 63-phase compiler that ran entirely in its core memory of only 8000 (six-bit) characters.  The compiler could be run from tape, or from a 2200-card deck; it used no further tape or disk storage.  It kept the program in memory and loaded overlays that gradually transformed it, in place, into executable form, as described by Haines.[18] 
This article was reprinted, edited, in both editions of Anatomy of a Compiler [19] and in the IBM manual "Fortran Specifications and Operating Procedures, IBM 1401".[20]  The executable form was not entirely machine language; rather, floating-point arithmetic, sub-scripting, input/output, and function references were interpreted, preceding UCSD Pascal P-code by two decades.
IBM later provided a FORTRAN IV compiler for the 1400 series of computers.[21]
Starting in 1961, as a result of customer demands, IBM began development of a FORTRAN IV that removed the machine-dependent features of FORTRAN II (such as READ INPUT TAPE), while adding new features such as a LOGICAL data type, logical Boolean expressions and the logical IF statement as an alternative to the arithmetic IF statement.  FORTRAN IV was eventually released in 1962, first for the IBM 7030 ("Stretch") computer, followed by versions for the IBM 7090, IBM 7094, and later for the IBM 1401 in 1966.
By 1965, FORTRAN IV was supposed to be compliant with the standard being developed by the American Standards Association X3.4.3 FORTRAN Working Group.[22]
At about this time FORTRAN IV had started to become an important educational tool and implementations such as the University of Waterloo's WATFOR and WATFIV were created to simplify the complex compile and link processes of earlier compilers.
Perhaps the most significant development in the early history of FORTRAN was the decision by the American Standards Association (now American National Standards Institute (ANSI)) to form a committee sponsored by BEMA, the Business Equipment Manufacturers Association, to develop an American Standard Fortran.  The resulting two standards, approved in March 1966, defined two languages, FORTRAN (based on FORTRAN IV, which had served as a de facto standard), and Basic FORTRAN (based on FORTRAN II, but stripped of its machine-dependent features).  The FORTRAN defined by the first standard, officially denoted X3.9-1966, became known as FORTRAN 66 (although many continued to term it FORTRAN IV, the language on which the standard was largely based).  FORTRAN 66 effectively became the first industry-standard version of FORTRAN. FORTRAN 66 included:
After the release of the FORTRAN 66 standard, compiler vendors introduced several extensions to Standard Fortran, prompting ANSI committee X3J3 in 1969 to begin work on revising the 1966 standard, under sponsorship of CBEMA, the Computer Business Equipment Manufacturers Association (formerly BEMA).  Final drafts of this revised standard circulated in 1977, leading to formal approval of the new FORTRAN standard in April 1978.  The new standard, called FORTRAN 77 and officially denoted X3.9-1978, added a number of significant features to address many of the shortcomings of FORTRAN 66:
In this revision of the standard, a number of features were removed or altered in a manner that might invalidate formerly standard-conforming programs.
(Removal was the only allowable alternative to X3J3 at that time, since the concept of "deprecation" was not yet available for ANSI standards.)
While most of the 24 items in the conflict list (see Appendix A2 of X3.9-1978) addressed loopholes or pathological cases permitted by the prior standard but rarely used, a small number of specific capabilities were deliberately removed, such as:
Control Data Corporation computers had another version of FORTRAN 77, called Minnesota FORTRAN (MNF), designed especially for student use, with variations in output constructs, special uses of COMMONs and DATA statements, optimizations code levels for compiling, and detailed error listings, extensive warning messages, and debugs.[23]
The development of a revised standard to succeed FORTRAN 77 would be repeatedly delayed as the standardization process struggled to keep up with rapid changes in computing and programming practice.  In the meantime, as the "Standard FORTRAN" for nearly fifteen years, FORTRAN 77 would become the historically most important dialect.
An important practical extension to FORTRAN 77 was the release of MIL-STD-1753 in 1978.[24] This specification, developed by the U.S. Department of Defense, standardized a number of features implemented by most FORTRAN 77 compilers but not included in the ANSI FORTRAN 77 standard. These features would eventually be incorporated into the Fortran 90 standard.
The IEEE 1003.9 POSIX Standard, released in 1991, provided a simple means for FORTRAN 77 programmers to issue POSIX system calls.[25] Over 100 calls were defined in the document –  allowing access to POSIX-compatible process control, signal handling, file system control, device control, procedure pointing, and stream I/O in a portable manner.
The much-delayed successor to FORTRAN 77, informally known as Fortran 90 (and prior to that, Fortran 8X), was finally released as ISO/IEC standard 1539:1991 in 1991 and an ANSI Standard in 1992.  In addition to changing the official spelling from FORTRAN to Fortran, this major revision added many new features to reflect the significant changes in programming practice that had evolved since the 1978 standard:
Unlike the prior revision, Fortran 90 removed no features. (Appendix B.1 says, "The list of deleted features in this standard is empty.") Any standard-conforming FORTRAN 77 program is also standard-conforming under Fortran 90, and either standard should be usable to define its behavior.
A small set of features were identified as "obsolescent"
and expected to be removed in a future standard.
from outside a block
Fortran 95, published officially as ISO/IEC 1539-1:1997, was a minor revision, mostly to resolve some outstanding issues from the Fortran 90 standard.  Nevertheless, Fortran 95 also added a number of extensions, notably from the High Performance Fortran specification:
A number of intrinsic functions were extended (for example a dim argument was added to the maxloc intrinsic).
Several features noted in Fortran 90 to be "obsolescent" were removed from Fortran 95:
An important supplement to Fortran 95 was the ISO technical report TR-15581: Enhanced Data Type Facilities, informally known as the Allocatable TR.  This specification defined enhanced use of ALLOCATABLE arrays, prior to the availability of fully Fortran 2003-compliant Fortran compilers.  Such uses include ALLOCATABLE arrays as derived type components, in procedure dummy argument lists, and as function return values.  (ALLOCATABLE arrays are preferable to POINTER-based arrays because ALLOCATABLE arrays are guaranteed by Fortran 95 to be deallocated automatically when they go out of scope, eliminating the possibility of memory leakage.  In addition, elements of allocatable arrays are contiguous, and aliasing is not an issue for optimization of array references, allowing compilers to generate faster code than in the case of pointers.[27])
Another important supplement to Fortran 95 was the ISO technical report TR-15580: Floating-point exception handling, informally known as the IEEE TR.  This specification defined support for IEEE floating-point arithmetic and floating point exception handling.
In addition to the mandatory "Base language"
(defined in ISO/IEC 1539-1 : 1997),
the Fortran 95 language also includes two optional modules:
which, together, compose the multi-part International Standard (ISO/IEC 1539).
According to the standards developers, "the optional parts describe self-contained features which have been requested by a substantial body of users and/or implementors, but which are not deemed to be of sufficient generality for them to be required in all standard-conforming Fortran compilers." Nevertheless, if a standard-conforming Fortran does provide such options, then they "must be provided in accordance with the description of those facilities in the appropriate Part of the Standard".
Fortran 2003, officially published as ISO/IEC 1539-1:2004, is a major revision introducing many new features.[28] A comprehensive summary of the new features of Fortran 2003 is available at the Fortran Working Group (ISO/IEC JTC1/SC22/WG5) official Web site.[29]
From that article, the major enhancements for this revision include:
An important supplement to Fortran 2003 was the ISO technical report TR-19767: Enhanced module facilities in Fortran.  This report provided sub-modules, which make Fortran modules more similar to Modula-2 modules.  They are similar to Ada private child sub-units.  This allows the specification and implementation of a module to be expressed in separate program units, which improves packaging of large libraries, allows preservation of trade secrets while publishing definitive interfaces, and prevents compilation cascades.
The most recent standard, ISO/IEC 1539-1:2010, informally known as Fortran 2008, was approved in September 2010.[30][31]   As with Fortran 95, this is a minor upgrade, incorporating clarifications and corrections to Fortran 2003, as well as introducing a select few new capabilities.  The new capabilities include:
The Final Draft international Standard (FDIS) is available as document N1830.[32]
An important supplement to Fortran 2008 is the ISO Technical Specification (TS) 29113 on Further Interoperability of Fortran with C,[33][34] which has been submitted to ISO in May 2012 for approval. The specification adds support for accessing the array descriptor from C and allows ignoring the type and rank of arguments.
The next revision of the language (Fortran 2018) was earlier referred to as Fortran 2015.[35] It is a significant revision and is planned for release in mid-2018.[36]
Fortran 2018 incorporates two previously published Technical Specifications:
Additional changes and new features include support for ISO/IEC/IEEE 60559:2011, hexadecimal input/output, IMPLICIT NONE enhancements and other changes[39][40][41][42]
Although a 1968 journal article by the authors of BASIC already described FORTRAN as "old-fashioned",[43] Fortran has now been in use for several decades and there is a vast body of Fortran software in daily use throughout the scientific and engineering communities.[44] Jay Pasachoff wrote in 1984 that "physics and astronomy students simply have to learn FORTRAN.  So much exists in FORTRAN that it seems unlikely that scientists will change to Pascal, Modula-2, or whatever."[45] In 1993, Cecil E. Leith called FORTRAN the "mother tongue of scientific computing", adding that its replacement by any other possible language "may remain a forlorn hope".[46]
It is the primary language for some of the most intensive super-computing tasks, such as in astronomy, climate modeling, computational chemistry, computational economics, computational fluid dynamics, computational physics, data analysis, hydrological modeling, numerical linear algebra and numerical libraries (LAPACK, IMSL and NAG), optimization, satellite simulation, structural engineering, and weather prediction.  Many of the floating-point benchmarks to gauge the performance of new computer processors, such as CFP2006, the floating-point component of the SPEC CPU2006 benchmarks, are written in Fortran.
Apart from this, more modern codes in computational science generally use large program libraries, such as METIS for graph partitioning, PETSc or Trilinos for linear algebra capabilities, DUNE or FEniCS for mesh and finite element support, and other generic libraries.  Since the late 1990s, almost all of the most widely used support libraries have been written in C and, more often, C++. On the other hand, high-level languages such as Matlab, Python, or R are becoming popular in particular areas of computational science. Consequently, a growing fraction of scientific programs are also written in these languages.  For this reason, facilities for inter-operation with C were added to Fortran 2003 and enhanced by ISO/IEC technical specification 29113, which will be incorporated into Fortran 2018. This shift in the popularity of programming languages is also evident in the selection of applications between the SPEC CPU 2000 and SPEC CPU 2006 floating point benchmarks.[citation needed]
Software for NASA probes Voyager 1 and Voyager 2 was originally written in FORTRAN 5, and later ported to FORTRAN 77.  As of  25 September 2013[update], some of the software is still written in Fortran and some has been ported to C.[47]
The precise characteristics and syntax of Fortran 95 are discussed in Fortran 95 language features.
Portability was a problem in the early days because there was no agreed upon standard –  not even IBM's reference manual –  and computer companies vied to differentiate their offerings from others by providing incompatible features.  Standards have improved portability.  The 1966 standard provided a reference syntax and semantics, but vendors continued to provide incompatible extensions.  Although careful programmers were coming to realize that use of incompatible extensions caused expensive portability problems, and were therefore using programs such as The PFORT Verifier, it was not until after the 1977 standard, when the National Bureau of Standards (now NIST) published FIPS PUB 69, that processors purchased by the U.S. Government were required to diagnose extensions of the standard.  Rather than offer two processors, essentially every compiler eventually had at least an option to diagnose extensions.
Incompatible extensions were not the only portability problem.  For numerical calculations, it is important to take account of the characteristics of the arithmetic.  This was addressed by Fox et al. in the context of the 1966 standard by the PORT library.  The ideas therein became widely used, and were eventually incorporated into the 1990 standard by way of intrinsic inquiry functions.  The widespread (now almost universal) adoption of the IEEE 754 standard for binary floating-point arithmetic has essentially removed this problem.
Access to the computing environment (e.g., the program's command line, environment variables, textual explanation of error conditions) remained a problem until it was addressed by the 2003 standard.
Large collections of library software that could be described as being loosely related to engineering and scientific calculations, such as graphics libraries, have been written in C, and therefore access to them presented a portability problem.  This has been addressed by incorporation of C interoperability into the 2003 standard.
It is now possible (and relatively easy) to write an entirely portable program in Fortran, even without recourse to a preprocessor.
Fortran 5 was marketed by Data General Corp in the late 1970s and early 1980s, for the Nova, Eclipse, and MV line of computers.  It had an optimizing compiler that was quite good for minicomputers of its time.  The language most closely resembles FORTRAN 66.  The name is a pun on the earlier FORTRAN IV.
FORTRAN V was distributed by Control Data Corporation in 1968 for the CDC 6600 series. The language was based upon FORTRAN IV.[48]
Univac also offered a compiler for the 1100 series known as FORTRAN V.  A spinoff of Univac Fortran V was Athena FORTRAN.
Fortran 6 or Visual Fortran 2001 was licensed to Compaq by Microsoft. They have licensed Compaq Visual Fortran and have provided the Visual Studio 5 environment interface for Compaq v6 up to v6.1.[49]
Vendors of high-performance scientific computers (e.g., Burroughs, Control Data Corporation (CDC), Cray, Honeywell, IBM, Texas Instruments, and UNIVAC) added extensions to Fortran to take advantage of special hardware features such as instruction cache, CPU pipelines, and vector arrays.  For example, one of IBM's FORTRAN compilers (H Extended IUP) had a level of optimization which reordered the machine code instructions to keep multiple internal arithmetic units busy simultaneously.  Another example is CFD, a special variant of FORTRAN designed specifically for the ILLIAC IV supercomputer, running at NASA's Ames Research Center.
IBM Research Labs also developed an extended FORTRAN-based language called VECTRAN for processing vectors and matrices.
Object-Oriented Fortran was an object-oriented extension of Fortran, in which data items can be grouped into objects, which can be instantiated and executed in parallel.  It was available for Sun, Iris, iPSC, and nCUBE, but is no longer supported.
Such machine-specific extensions have either disappeared over time or have had elements incorporated into the main standards. The major remaining extension is OpenMP, which is a cross-platform extension for shared memory programming.  One new extension, Coarray Fortran, is intended to support parallel programming.
FOR TRANSIT was the name of a reduced version of the IBM 704 FORTRAN language,
which was implemented for the IBM 650, using a translator program developed
at Carnegie in the late 1950s.[50]
The following comment appears in the IBM Reference Manual (FOR TRANSIT Automatic Coding System C28-4038, Copyright 1957, 1959 by IBM):
The FORTRAN system was designed for a more complex machine than the 650, and consequently some of the 32 statements found in the FORTRAN Programmer's Reference Manual are not acceptable to the FOR TRANSIT system.  In addition, certain restrictions to the FORTRAN language have been added.  However, none of these restrictions make a source program written for FOR TRANSIT incompatible with the FORTRAN system for the 704.The permissible statements were:
Up to ten subroutines could be used in one program.
FOR TRANSIT statements were limited to columns 7 through 56, only.
Punched cards were used for input and output on the IBM 650.  Three passes were required to translate source code to the "IT" language, then to compile the IT statements into SOAP assembly language, and finally to produce the object program, which could then be loaded into the machine to run the program (using punched cards for data input, and outputting results onto punched cards).
Two versions existed for the 650s with a 2000 word memory drum:  FOR TRANSIT I (S) and FOR TRANSIT II, the latter for machines equipped with indexing registers and automatic floating point decimal (bi-quinary) arithmetic.  Appendix A of the manual included wiring diagrams for the IBM 533 card reader/punch control panel.
Prior to FORTRAN 77, a number of preprocessors were commonly used to provide a friendlier language, with the advantage that the preprocessed code could be compiled on any machine with a standard FORTRAN compiler.  These preprocessors would typically support structured programming, variable names longer than six characters, additional data types, conditional compilation, and even macro capabilities.  Popular preprocessors included FLECS, iftran, MORTRAN, SFtran, S-Fortran, Ratfor, and Ratfiv.  Ratfor and Ratfiv, for example, implemented a C-like language, outputting preprocessed code in standard FORTRAN 66.  Despite advances in the Fortran language, preprocessors continue to be used for conditional compilation and macro substitution.
One of the earliest versions of FORTRAN, introduced in the '60s, was popularly used in colleges and universities.  Developed, supported, and distributed by the University of Waterloo, WATFOR was based largely on FORTRAN IV.  A student using WATFOR could submit their batch FORTRAN job and, if there were no syntax errors, the program would move straight to execution.  This simplification allowed students to concentrate on their program's syntax and semantics, or execution logic flow, rather than dealing with submission Job Control Language (JCL), the compile/link-edit/execution successive process(es), or other complexities of the mainframe/minicomputer environment.  A down side to this simplified environment was that WATFOR was not a good choice for programmers needing the expanded abilities of their host processor(s), e.g., WATFOR typically had very limited access to I/O devices. WATFOR was succeeded by WATFIV and its later versions.
  (line programming)
LRLTRAN was developed at the Lawrence Radiation Laboratory to provide support for vector arithmetic and dynamic storage, among other extensions to support systems programming.  The distribution included the LTSS operating system.
The Fortran-95 Standard includes an optional Part 3 which defines an optional conditional compilation capability.  This capability is often referred to as "CoCo".
Many Fortran compilers have integrated subsets of the C preprocessor into their systems.
SIMSCRIPT is an application specific Fortran preprocessor for modeling and simulating large discrete systems.
The F programming language was designed to be a clean subset of Fortran 95 that attempted to remove the redundant, unstructured, and deprecated features of Fortran, such as the EQUIVALENCE statement.  F retains the array features added in Fortran 90, and removes control statements that were made obsolete by structured programming constructs added to both FORTRAN 77 and Fortran 90.  F is described by its creators as "a compiled, structured, array programming language especially well suited to education and scientific computing".[51]
Lahey and Fujitsu teamed up to create Fortran for the Microsoft .NET Framework.[52] Silverfrost FTN95 is also capable of creating .NET code.[53]
The following program illustrates dynamic memory allocation and array-based operations, two features introduced with Fortran 90.  Particularly noteworthy is the absence of DO loops and IF/THEN statements in manipulating the array; mathematical operations are applied to the array as a whole.  Also apparent is the use of descriptive variable names and general code formatting that conform with contemporary programming style.  This example computes an average over data entered interactively.
During the same FORTRAN standards committee meeting at which the name "FORTRAN 77" was chosen, a satirical technical proposal was incorporated into the official distribution bearing the title "Letter O Considered Harmful".  This proposal purported to address the confusion that sometimes arises between the letter "O" and the numeral zero, by eliminating the letter from allowable variable names.  However, the method proposed was to eliminate the letter from the character set entirely (thereby retaining 48 as the number of lexical characters, which the colon had increased to 49).  This was considered beneficial in that it would promote structured programming, by making it impossible to use the notorious GO TO statement as before. (Troublesome FORMAT statements would also be eliminated.)  It was noted that this "might invalidate some existing programs" but that most of these "probably were non-conforming, anyway".[54][55]
When assumed-length arrays were being added, there was a dispute as to the appropriate character to separate upper and lower bounds. In a comment examining these arguments, Dr. Walt Brainerd penned an article entitled "Astronomy vs. Gastroenterology" because some proponents had suggested using the star or asterisk ("*"), while others favored the colon (":").[citation needed]
In FORTRAN 77, variable names beginning with the letters I–N had a default type of integer, while variables starting with any other letters defaulted to real, although programmers could override the defaults with an explicit declaration.[56] This led to the joke: "In Fortran, GOD is REAL (unless declared INTEGER)."



Grace Hopper - Wikipedia

Grace Brewster Murray Hopper (née Murray; December 9, 1906 – January 1, 1992) was an American computer scientist and United States Navy rear admiral.[1] One of the first programmers of the Harvard Mark I computer, she was a pioneer of computer programming who invented one of the first compiler related tools.  She popularized the idea of machine-independent programming languages, which led to the development of COBOL, an early high-level programming language still in use today.
Hopper attempted to enlist in the Navy during World War II but was rejected because she was 34 years old. She instead joined the Navy Reserves. Hopper began her computing career in 1944 when she worked on the Harvard Mark I team led by Howard H. Aiken. In 1949, she joined the Eckert–Mauchly Computer Corporation and was part of the team that developed the UNIVAC I computer.  At Eckert–Mauchly she began developing the compiler. She believed that a programming language based on English was possible.  Her compiler converted English terms into machine code understood by computers. By 1952, Hopper had finished her program linker (originally called a compiler), which was written for the A-0 System.[2][3][4][5]
In 1954, Eckert–Mauchly chose Hopper to lead their department for automatic programming, and she led the release of some of the first compiled languages like FLOW-MATIC. In 1959, she participated in the CODASYL consortium, which consulted Hopper to guide them in creating a machine-independent programming language. This led to the COBOL language, which was inspired by her idea of a language being based on English words. In 1966, she retired from the Naval Reserve, but in 1967, the Navy recalled her to active duty.  She retired from the Navy in 1986 and found work as a consultant for the Digital Equipment Corporation, sharing her computing experiences.
Owing to her accomplishments and her naval rank, she was sometimes referred to as "Amazing Grace".[6][7] The U.S. Navy Arleigh Burke-class guided-missile destroyer USS Hopper was named for her, as was the Cray XE6 "Hopper" supercomputer at NERSC.[8] During her lifetime, Hopper was awarded 40 honorary degrees from universities across the world. A college at Yale University was renamed in her honor.  In 1991, she received the National Medal of Technology. On November 22, 2016, she was posthumously awarded the Presidential Medal of Freedom by President Barack Obama.[9]
Hopper was born in New York City. She was the eldest of three children. Her parents, Walter Fletcher Murray and Mary Campbell Van Horne, were of Scottish and Dutch descent, and attended West End Collegiate Church.[10] Her great-grandfather, Alexander Wilson Russell, an admiral in the US Navy, fought in the Battle of Mobile Bay during the Civil War.[citation needed]
Grace was very curious as a child; this was a lifelong trait. At the age of seven, she decided to determine how an alarm clock worked and dismantled seven alarm clocks before her mother realized what she was doing (she was then limited to one clock).[11] For her preparatory school education, she attended the Hartridge School in Plainfield, New Jersey. Hopper was initially rejected for early admission to Vassar College at age 16 (her test scores in Latin were too low), but she was admitted the following year.  She graduated Phi Beta Kappa from Vassar in 1928 with a bachelor's degree in mathematics and physics and earned her master's degree at Yale University in 1930.
In 1934, she earned a Ph.D. in mathematics from Yale[12] under the direction of Øystein Ore.[13][14] Her dissertation, New Types of Irreducibility Criteria, was published that same year.[15] Hopper began teaching mathematics at Vassar in 1931, and was promoted to associate professor in 1941.[16]
She was married to New York University professor Vincent Foster Hopper (1906–76) from 1930 until their divorce in 1945.[13][17] She did not marry again, but chose to retain his surname.
Hopper had tried to enlist in the Navy early in World War II. She was rejected for multiple reasons. At age 34, she was too old to enlist, and her weight to height ratio was too low. She was also denied on the basis that her job as a mathematician and mathematics professor at Vassar College was valuable to the war effort.[18] During the war in 1943, Hopper obtained a leave of absence from Vassar and was sworn into the United States Navy Reserve; she was one of many women who volunteered to serve in the WAVES. She had to get an exemption to enlist; she was 15 pounds (6.8 kg) below the Navy minimum weight of 120 pounds (54 kg). She reported in December and trained at the Naval Reserve Midshipmen's School at Smith College in Northampton, Massachusetts. Hopper graduated first in her class in 1944, and was assigned to the Bureau of Ships Computation Project at Harvard University as a lieutenant, junior grade. She served on the Mark I computer programming staff headed by Howard H. Aiken. Hopper and Aiken co-authored three papers on the Mark I, also known as the Automatic Sequence Controlled Calculator. Hopper's request to transfer to the regular Navy at the end of the war was declined due to her advanced age of 38. She continued to serve in the Navy Reserve. Hopper remained at the Harvard Computation Lab until 1949, turning down a full professorship at Vassar in favor of working as a research fellow under a Navy contract at Harvard.[19]
In 1949, Hopper became an employee of the Eckert–Mauchly Computer Corporation as a senior mathematician and joined the team developing the UNIVAC I.[16]  Hopper also served as UNIVAC director of Automatic Programming Development for Remington Rand. The UNIVAC was the first known large-scale electronic computer to be on the market in 1950,  and was more competitive at processing information than the Mark I.[20]
When Hopper recommended the development of a new programming language that would use entirely English words, she "was told very quickly that [she] couldn't do this because computers didn't understand English." Her idea was not accepted for 3 years, and she published her first paper on the subject, compilers, in 1952. In the early 1950s, the company was taken over by the Remington Rand corporation, and it was while she was working for them that her original compiler work was done.  The program was known as the A compiler and its first version was A-0.[21]:11
In 1952 she had an operational link-loader, which at the time was referred to as a compiler. She later said that "Nobody believed that," and that she "had a running compiler and nobody would touch it. They told me computers could only do arithmetic."[22] She goes on to say that her compiler "translated mathematical notation into machine code. Manipulating symbols was fine for mathematicians but it was no good for data processors who were not symbol manipulators. Very few people are really symbol manipulators. If they are they become professional mathematicians, not data processors. It's much easier for most people to write an English statement than it is to use symbols. So I decided data processors ought to be able to write their programs in English, and the computers would translate them into machine code. That was the beginning of COBOL, a computer language for data processors. I could say "Subtract income tax from pay" instead of trying to write that in octal code or using all kinds of symbols. COBOL is the major language used today in data processing."[23]
In 1954 Hopper was named the company's first director of automatic programming, and her department released some of the first compiler-based programming languages, including MATH-MATIC and FLOW-MATIC.[16]
In the spring of 1959, computer experts from industry and government were brought together in a two-day conference known as the Conference on Data Systems Languages (CODASYL). Hopper served as a technical consultant to the committee, and many of her former employees served on the short-term committee that defined the new language COBOL (an acronym for COmmon Business-Oriented Language). The new language extended Hopper's FLOW-MATIC language with some ideas from the IBM equivalent, COMTRAN.  Hopper's belief that programs should be written in a language that was close to English (rather than in machine code or in languages close to machine code, such as assembly languages) was captured in the new business language, and COBOL went on to be the most ubiquitous business language to date.[24] Among the members of the committee that worked on COBOL was Mount Holyoke College alumnus Jean E. Sammet.[25]
From 1967 to 1977, Hopper served as the director of the Navy Programming Languages Group in the Navy's Office of Information Systems Planning and was promoted to the rank of captain in 1973.[19] She developed validation software for COBOL and its compiler as part of a COBOL standardization program for the entire Navy.[19]
In the 1970s, Hopper advocated for the Defense Department to replace large, centralized systems with networks of small, distributed computers. Any user on any computer node could access common databases located on the network.[21]:119 She developed the implementation of standards for testing computer systems and components, most significantly for early programming languages such as FORTRAN and COBOL. The Navy tests for conformance to these standards led to significant convergence among the programming language dialects of the major computer vendors. In the 1980s, these tests (and their official administration) were assumed by the National Bureau of Standards (NBS), known today as the National Institute of Standards and Technology (NIST).
In accordance with Navy attrition regulations, Hopper retired from the Naval Reserve with the rank of commander at age 60 at the end of 1966.[26]  She was recalled to active duty in August 1967 for a six-month period that turned into an indefinite assignment.  She again retired in 1971 but was again asked to return to active duty in 1972. She was promoted to captain in 1973 by Admiral Elmo R. Zumwalt, Jr.[27]
After Republican Representative Philip Crane saw her on a March 1983 segment of 60 Minutes, he championed H.J.Res. 341, a joint resolution originating in the House of Representatives, which led to her promotion to commodore by special Presidential appointment.[27][28][29][30] She remained on active duty for several years beyond mandatory retirement by special approval of Congress.[31]  Effective November 8, 1985, the rank of commodore was renamed rear admiral (lower half) and Hopper became one of the Navy's few female admirals.
Following a career that spanned more than 42 years, Admiral Hopper took mandatory retirement from the Navy on August 14, 1986.  At a celebration held in Boston on the USS Constitution to commemorate her retirement, Hopper was awarded the Defense Distinguished Service Medal, the highest non-combat decoration awarded by the Department of Defense.
At the time of her retirement, she was the oldest active-duty commissioned officer in the United States Navy (79 years, eight months and five days), and had her retirement ceremony aboard the oldest commissioned ship in the United States Navy (188 years, nine months and 23 days).[32]  (Admirals William D. Leahy, Chester W. Nimitz, Hyman G. Rickover and Charles Stewart were the only other officers in the Navy's history to serve on active duty at a higher age.  Leahy and Nimitz served on active duty for life due to their promotions to the rank of fleet admiral.)
Following her retirement from the Navy, she was hired as a senior consultant to Digital Equipment Corporation (DEC). Hopper was initially offered the job position by Rita Yavinsky, but she insisted on applying for the position at DEC, and going through the typical formal interview process. She also sent a letter to Yavinsky's boss explaining that she would be available on alternating Thursdays, receiving a high salary, and have access to an unlimited expense account if she were to be exhibited at their museum of computing as a pioneer. After the proposal from Hopper, she was hired as a full-time senior consultant. As part of her position, she would report to Yavinsky. In this position, Hopper represented the company at industry forums, serving on various industry committees, along with other obligations.[10] She retained that position until her death at age 85 in 1992.
Hopper was a goodwill ambassador in her primary activity in this capacity. She lectured widely about the early days of computing, her career, and on efforts that computer vendors could take to make life easier for their users. She visited most of Digital's engineering facilities, where she generally received a standing ovation at the conclusion of her remarks.
She often recounted that during her service she was frequently asked by admirals and generals why satellite communication would take so long.  So during many of her lectures, she illustrated a nanosecond using salvaged obsolete Bell System 25 pair telephone cable, she cut it to lengths of 11.8 inches (30 cm), the distance that light travels in one nanosecond, and handed out the individual wires to her listeners. Although no longer a serving officer, she always wore her Navy full dress uniform to these lectures (which is not allowed under U.S. Department of Defense regulation 32 CFR 53.2(a)(2)).
The most important thing I've accomplished, other than building the compiler, is training young people. They come to me, you know, and say, 'Do you think we can do this?' I say, "Try it." And I back 'em up. They need that. I keep track of them as they get older and I stir 'em up at intervals so they don't forget to take chances.[33]
On New Year's Day 1992, Hopper died in her sleep of natural causes at her home in Arlington, Virginia; she was 85 years of age. She was interred with full military honors in Arlington National Cemetery.[40]
Grace Hopper Celebration of Women in Computing is a convention for Women in the field of Computer Science and Technology. It is named after Hopper to honor her for her work and influence in the field of computing, and her push for more women to enter and stay in the tech field. It features a wide array of educational and professional development courses and workshops, including a lesson on compilers, which Hopper invented and pioneered, and a career fair, in order to help connect women in the computing field with potential employers.
Her legacy was an inspiring factor in the creation of the Grace Hopper Celebration of Women in Computing.[70] Held yearly, this conference is designed to bring the research and career interests of women in computing to the forefront.[71]



Low-level programming language - Wikipedia
A low-level programming language is a programming language that provides little or no abstraction from a computer's instruction set architecture—commands or functions in the language map closely to processor instructions. Generally this refers to either machine code or assembly language. The word "low" refers to the small or nonexistent amount of abstraction between the language and machine language; because of this, low-level languages are sometimes described as being "close to the hardware". Programs written in low-level languages tend to be relatively non-portable.
Low-level languages can convert to machine code without a compiler or interpreter— second-generation programming languages use a simpler processor called an assembler— and the resulting code runs directly on the processor. A program written in a low-level language can be made to run very quickly, with a small memory footprint. An equivalent program in a high-level language can be less efficient and use more memory. Low-level languages are simple, but considered difficult to use, due to numerous technical details that the programmer must remember. By comparison, a high-level programming language isolates execution semantics of a computer architecture from the specification of the program, which simplifies development. 
Low-level programming languages are sometimes divided into two categories: first generation and second generation.[citation needed]
Machine code is the only language a computer can process directly without a previous transformation. Currently, programmers almost never write programs directly in machine code, because it requires attention to numerous details that a high-level language handles automatically. Furthermore it requires memorizing or looking up numerical codes for every instruction, and is extremely difficult to modify.
True machine code is a stream of raw, usually binary, data.  A programmer coding in "machine code" normally codes instructions and data in a more readable form such as decimal, octal, or hexadecimal which is translated to internal format by a program called a loader or toggled into the computer's memory from a front panel.
Although few programs are written in machine language, programmers often become adept at reading it through working with core dumps or debugging from the front panel.    
Example: A function in hexadecimal representation of 32-bit x86 machine code to calculate the nth Fibonacci number:
Second-generation languages provide one abstraction level on top of the machine code. In the early days of coding on computers like the TX-0 and PDP-1, the first thing MIT hackers did was write assemblers.[1]
Assembly language has little semantics or formal specification, being only a mapping of human-readable symbols, including symbolic addresses, to opcodes, addresses, numeric constants, strings and so on. Typically, one machine instruction is represented as one line of assembly code.  Assemblers produce object files that can link with other object files or be loaded on their own.
Most assemblers provide macros to generate common sequences of instructions.
Example: The same Fibonacci number calculator as above, but in x86 assembly language using MASM syntax:
In this code example, hardware features of the x86 processor (its registers) are named and manipulated directly. The function loads its input from a precise location in the stack (8 bytes higher than the location stored in the ESP stack pointer) and performs its calculation by manipulating values in the EAX, EBX, ECX and EDX registers until it has finished and returns.  Note that in this assembly language, there is no concept of returning a value. The result having been stored in the EAX register, the RET command simply moves code processing to the code location stored on the stack (usually the instruction immediately after the one that called this function) and it is up to the author of the calling code to know that this function stores its result in EAX and to retrieve it from there.  x86 assembly language imposes no standard for returning values from a function (and so, in fact, has no concept of a function); it is up to the calling code to examine state after the procedure returns if it needs to extract a value.
Compare this with the same function in C:
This code is very similar in structure to the assembly language example but there are significant differences in terms of abstraction:
These abstractions make the C code compilable without modification on any architecture for which a C compiler has been written. The x86 assembly language code is specific to the x86 architecture.
In the late 1960s, high-level languages such as PL/S, BLISS, BCPL, extended ALGOL (for Burroughs large systems) and C included some degree of access to low-level programming functions. One method for this is Inline assembly, in which assembly code is embedded in a high-level language that supports this feature.  Some of these languages also allow architecture-dependent compiler optimization directives to adjust the way a compiler uses the target processor architecture.
For example low level language is known as c and 'O' level language.



Structured programming - Wikipedia
Structured programming is a programming paradigm aimed at improving the clarity, quality, and development time of a computer program by making extensive use of the structured control flow constructs of selection (if/then/else) and repetition (while and for), block structures, and subroutines.
It emerged in the late 1950s with the appearance of the ALGOL 58 and ALGOL 60 programming languages,[1] with the latter including support for block structures. Contributing factors to its popularity and widespread acceptance, at first in academia and later among practitioners, include the discovery of what is now known as the structured program theorem in 1966,[2] and the publication of the influential "Go To Statement Considered Harmful" open letter in 1968 by Dutch computer scientist Edsger W. Dijkstra, who coined the term "structured programming".[3]
Structured programming is most frequently used with deviations that allow for clearer programs in some particular cases, such as when exception handling has to be performed.
Following the structured program theorem, all programs are seen as composed of control structures:
Subroutines; callable units such as procedures, functions, methods, or subprograms are used to allow a sequence to be referred to by a single statement.
Blocks are used to enable groups of statements to be treated as if they were one statement. Block-structured languages have a syntax for enclosing structures in some formal way, such as an if-statement bracketed by if..fi as in ALGOL 68, or a code section bracketed by BEGIN..END, as in PL/I and Pascal, whitespace indentation as in Python - or the curly braces {...} of C and many later languages.
It is possible to do structured programming in any programming language, though it is preferable to use something like a procedural programming language. Some of the languages initially used for structured programming include: ALGOL, Pascal, PL/I and Ada, but most new procedural programming languages since that time have included features to encourage structured programming, and sometimes deliberately left out features – notably GOTO – in an effort to make unstructured programming more difficult.
Structured programming (sometimes known as modular programming) enforces a logical structure on the program being written to make it more efficient and easier to understand and modify.
The structured program theorem provides the theoretical basis of structured programming. It states that three ways of combining programs—sequencing, selection, and iteration—are sufficient to express any computable function. This observation did not originate with the structured programming movement; these structures are sufficient to describe the instruction cycle of a central processing unit, as well as the operation of a Turing machine. Therefore, a processor is always executing a "structured program" in this sense, even if the instructions it reads from memory are not part of a structured program. However, authors usually credit the result to a 1966 paper by Böhm and Jacopini, possibly because Dijkstra cited this paper himself.[4] The structured program theorem does not address how to write and analyze a usefully structured program. These issues were addressed during the late 1960s and early 1970s, with major contributions by Dijkstra, Robert W. Floyd, Tony Hoare, Ole-Johan Dahl, and David Gries.
P. J. Plauger, an early adopter of structured programming, described his reaction to the structured program theorem:
Donald Knuth accepted the principle that programs must be written with provability in mind, but he disagreed (and still disagrees[citation needed]) with abolishing the GOTO statement. In his 1974 paper, "Structured Programming with Goto Statements",[6] he gave examples where he believed that a direct jump leads to clearer and more efficient code without sacrificing provability. Knuth proposed a looser structural constraint: It should be possible to draw a program's flow chart with all forward branches on the left, all backward branches on the right, and no branches crossing each other. Many of those knowledgeable in compilers and graph theory have advocated allowing only reducible flow graphs[when defined as?].[who?]
Structured programming theorists gained a major ally in the 1970s after IBM researcher Harlan Mills applied his interpretation of structured programming theory to the development of an indexing system for The New York Times research file. The project was a great engineering success, and managers at other companies cited it in support of adopting structured programming, although Dijkstra criticized the ways that Mills's interpretation differed from the published work.[citation needed]
As late as 1987 it was still possible to raise the question of structured programming in a computer science journal. Frank Rubin did so in that year with an open letter titled ""GOTO considered harmful" considered harmful".[7] Numerous objections followed, including a response from Dijkstra that sharply criticized both Rubin and the concessions other writers made when responding to him.
By the end of the 20th century nearly all computer scientists were convinced that it is useful to learn and apply the concepts of structured programming. High-level programming languages that originally lacked programming structures, such as FORTRAN, COBOL, and BASIC, now have them.
While goto has now largely been replaced by the structured constructs of selection (if/then/else) and repetition (while and for), few languages are purely structured. The most common deviation, found in many languages, is the use of a return statement for early exit from a subroutine. This results in multiple exit points, instead of the single exit point required by structured programming. There are other constructions to handle cases that are awkward in purely structured programming.
The most common deviation from structured programming is early exit from a function or loop. At the level of functions, this is a return statement. At the level of loops, this is a break statement (terminate the loop) or continue statement (terminate the current iteration, proceed with next iteration). In structured programming, these can be replicated by adding additional branches or tests, but for returns from nested code this can add significant complexity. C is an early and prominent example of these constructs. Some newer languages also have "labeled breaks", which allow breaking out of more than just the innermost loop. Exceptions also allow early exit, but have further consequences, and thus are treated below.
Multiple exits can arise for a variety of reasons, most often either that the subroutine has no more work to do (if returning a value, it has completed the calculation), or has encountered "exceptional" circumstances that prevent it from continuing, hence needing exception handling.
The most common problem in early exit is that cleanup or final statements are not executed – for example, allocated memory is not deallocated, or open files are not closed, causing memory leaks or resource leaks. These must be done at each return site, which is brittle and can easily result in bugs. For instance, in later development, a return statement could be overlooked by a developer, and an action which should be performed at the end of a subroutine (e.g., a trace statement) might not be performed in all cases. Languages without a return statement, such as standard Pascal, do not have this problem.
Most modern languages provide language-level support to prevent such leaks;[8] see detailed discussion at resource management. Most commonly this is done via unwind protection, which ensures that certain code is guaranteed to be run when execution exits a block; this is a structured alternative to having a cleanup block and a goto. This is most often known as try...finally, and considered a part of exception handling. Various techniques exist to encapsulate resource management. An alternative approach, found primarily in C++, is Resource Acquisition Is Initialization, which uses normal stack unwinding (variable deallocation) at function exit to call destructors on local variables to deallocate resources.
Kent Beck, Martin Fowler and co-authors have argued in their refactoring books that nested conditionals may be harder to understand than a certain type of flatter structure using multiple exits predicated by guard clauses. Their 2009 book flatly states that "one exit point is really not a useful rule. Clarity is the key principle: If the method is clearer with one exit point, use one exit point; otherwise don’t". They offer a cookbook solution for transforming a function consisting only of nested conditionals into a sequence of guarded return (or throw) statements, followed by a single unguarded block, which is intended to contain the code for the common case, while the guarded statements are supposed to deal with the less common ones (or with errors).[9] Herb Sutter and Andrei Alexandrescu also argue in their 2004 C++ tips book that the single-exit point is an obsolete requirement.[10]
In his 2004 textbook, David Watt writes that "single-entry multi-exit control flows are often desirable". Using Tennent's framework notion of sequencer, Watt uniformly describes the control flow constructs found in contemporary programming languages and attempts to explain why certain types of sequencers are preferable to others in the context of multi-exit control flows. Watt writes that unrestricted gotos (jump sequencers) are bad because the destination of the jump is not self-explanatory to the reader of a program until the reader finds and examines the actual label or address that is the target of the jump. In contrast, Watt argues that the conceptual intent of a return sequencer is clear from its own context, without having to examine its destination. Watt writes that a class of sequencers known as escape sequencers, defined as a "sequencer that terminates execution of a textually enclosing command or procedure", encompasses both breaks from loops (including multi-level breaks) and return statements. Watt also notes that while jump sequencers (gotos) have been somewhat restricted in languages like C, where the target must be an inside the local block or an encompassing outer block, that restriction alone is not sufficient to make the intent of gotos in C self-describing and so they can still produce "spaghetti code". Watt also examines how exception sequencers differ from escape and jump sequencers; this is explained in the next section of this article.[11]
In contrast to the above, Bertrand Meyer wrote in his 2009 textbook that instructions like break and continue "are just the old goto in sheep's clothing" and strongly advised against their use.[12]
Based on the coding error from the Ariane 501 disaster, software developer Jim Bonang argues that any exceptions thrown from a function violate the single-exit paradigm, and proposes that all inter-procedural exceptions should be forbidden. In C++ syntax, this is done by declaring all function signatures as noexcept (since C++11) or throw().[13] Bonang proposes that all single-exit conforming C++ should be written along the lines of:
Peter Ritchie also notes that, in principle, even a single throw right before the return in a function constitutes a violation of the single-exit principle, but argues that Dijkstra's rules were written in a time before exception handling became a paradigm in programming languages, so he proposes to allow any number of throw points in addition to a single return point. He notes that solutions which wrap exceptions for the sake of creating a single-exit have higher nesting depth and thus are more difficult to comprehend, and even accuses those who propose to apply such solutions to programming languages which support exceptions of engaging in cargo cult thinking.[14]
David Watt also analyzes exception handling in the framework of sequencers (introduced in this article in the previous section on early exits.) Watt notes that an abnormal situation (generally exemplified with arithmetic 
overflows or input/output failures like file not found) is a kind of error that "is detected in some low-level program unit, but [for which] a handler is more naturally located in a high-level program unit". For example, a program might contain several calls to read files, but the action to perform when a file is not found depends on the meaning (purpose) of the file in question to the program and thus a handling routine for this abnormal situation cannot be located in low-level system code. Watts further notes that introducing status flags testing in the caller, as single-exit structured programming or even (multi-exit) return sequencers would entail, results in a situation where "the application code tends to get cluttered by tests of status flags" and that "the programmer might forgetfully or lazily omit to test a status flag. In fact, abnormal situations represented by status flags are by default ignored!" He notes that in contrast to status flags testing, exceptions have the opposite default behavior, causing the program to terminate unless the programmer explicitly deals with the exception in some way, possibly by adding code to willfully ignore it. Based on these arguments, Watt concludes that jump sequencers or escape sequencers (discussed in the previous section) aren't as suitable as a dedicated exception sequencer with the semantics discussed above.[15]
The textbook by Louden and Lambert emphasizes that exception handling differs from structured programming constructs like while loops because the transfer of control "is set up at a different point in the program than that where the actual transfer takes place. At the point where the transfer actually occurs, there may be no syntactic indication that control will in fact be transferred."[16] Computer science professor Arvind Kumar Bansal also notes that in languages which implement exception handling, even control structures like for, which have the single-exit property in absence of exceptions, no longer have it in presence of exceptions, because an exception can prematurely cause an early exit in any part of the control structure; for instance if init() throws an exception in for (init(); check(); increm()), then the usual exit point after check() is not reached.[17] Citing multiple prior studies by others (1999-2004) and their own results, Westley Weimer and George Necula wrote that a significant problem with exceptions is that they "create hidden control-flow paths that are difficult for programmers to reason about".[18]:8:27
The necessity to limit code to single-exit points appears in some contemporary programming environments focused on parallel computing, such as OpenMP. The various parallel constructs from OpenMP, like parallel do, do not allow early exits from inside to the outside of the parallel construct; this restriction includes all manner of exits, from break to C++ exceptions, but all of these are permitted inside the parallel construct if the jump target is also inside it.[19]
More rarely, subprograms allow multiple entry. This is most commonly only re-entry into a coroutine (or generator/semicoroutine), where a subprogram yields control (and possibly a value), but can then be resumed where it left off. There are a number of common uses of such programming, notably for streams (particularly input/output), state machines, and concurrency. From a code execution point of view, yielding from a coroutine is closer to structured programming than returning from a subroutine, as the subprogram has not actually terminated, and will continue when called again – it is not an early exit. However, coroutines mean that multiple subprograms have execution state – rather than a single call stack of subroutines – and thus introduce a different form of complexity.
It is very rare for subprograms to allow entry to an arbitrary position in the subprogram, as in this case the program state (such as variable values) is uninitialized or ambiguous, and this is very similar to a goto.
Some programs, particularly parsers and communications protocols, have a number of states that follow each other in a way that is not easily reduced to the basic structures, and some programmers implement the state-changes with a jump to the new state. This type of state-switching is often used in the Linux kernel.[citation needed]
However, it is possible to structure these systems by making each state-change a separate subprogram and using a variable to indicate the active state (see trampoline). Alternatively, these can be implemented via coroutines, which dispense with the trampoline.



C++ - Wikipedia

C++ (/ˌsiːˌplʌsˈplʌs/ "see plus plus") is a general-purpose programming language. It has imperative, object-oriented and generic programming features, while also providing facilities for low-level memory manipulation.
It was designed with a bias toward system programming and embedded, resource-constrained and large systems, with performance, efficiency and flexibility of use as its design highlights.[6] C++ has also been found useful in many other contexts, with key strengths being software infrastructure and resource-constrained applications,[6] including desktop applications, servers (e.g. e-commerce, Web search or SQL servers), and performance-critical applications (e.g. telephone switches or space probes).[7]  C++ is a compiled language, with implementations of it available on many platforms.  Many vendors provide C++ compilers, including the Free Software Foundation, Microsoft, Intel, and IBM.
C++ is standardized by the International Organization for Standardization (ISO), with the latest standard version ratified and published by ISO in December 2017 as ISO/IEC 14882:2017 (informally known as C++17).[8] The C++ programming language was initially standardized in 1998 as ISO/IEC 14882:1998, which was then amended by the C++03, C++11 and C++14 standards. The current C++17 standard supersedes these with new features and an enlarged standard library.  Before the initial standardization in 1998, C++ was developed by Bjarne Stroustrup at Bell Labs since 1979, as an extension of the C language as he wanted an efficient and flexible language similar to C, which also provided high-level features for program organization.[9] C++20 is the next planned standard thereafter.
Many other programming languages have been influenced by C++, including C#, D, Java, and newer versions of C.
In 1979, Bjarne Stroustrup, a Danish computer scientist, began work on "C with Classes", the predecessor to C++.[10] The motivation for creating a new language originated from Stroustrup's experience in programming for his Ph.D. thesis. Stroustrup found that Simula had features that were very helpful for large software development, but the language was too slow for practical use, while BCPL was fast but too low-level to be suitable for large software development. When Stroustrup started working in AT&T Bell Labs, he had the problem of analyzing the UNIX kernel with respect to distributed computing. Remembering his Ph.D. experience, Stroustrup set out to enhance the C language with Simula-like features.[11] C was chosen because it was general-purpose, fast, portable and widely used. As well as C and Simula's influences, other languages also influenced C++, including ALGOL 68, Ada, CLU and ML.
Initially, Stroustrup's "C with Classes" added features to the C compiler, Cpre, including classes, derived classes, strong typing, inlining and default arguments.[12]
In 1983, "C with Classes" was renamed to "C++" (++ being the increment operator in C), adding new features that included virtual functions, function name and operator overloading, references, constants, type-safe free-store memory allocation (new/delete), improved type checking, and BCPL style single-line comments with two forward slashes (//).  Furthermore, it included the development of a standalone compiler for C++, Cfront.
In 1985, the first edition of The C++ Programming Language was released, which became the definitive reference for the language, as there was not yet an official standard.[13] The first commercial implementation of C++ was released in October of the same year.[10]
In 1989, C++ 2.0 was released, followed by the updated second edition of The C++ Programming Language in 1991.[14] New features in 2.0 included multiple inheritance, abstract classes, static member functions, const member functions, and protected members. In 1990, The Annotated C++ Reference Manual was published. This work became the basis for the future standard. Later feature additions included templates, exceptions, namespaces, new casts, and a boolean type.
After the 2.0 update, C++ evolved relatively slowly until, in 2011, the C++11 standard was released, adding numerous new features, enlarging the standard library further, and providing more facilities to C++ programmers. After a minor C++14 update released in December 2014, various new additions were introduced in C++17, and further changes planned for 2020.[15]
As of 2017, C++ remains the third most popular programming language, behind Java and C.[16][17]
On January 3, 2018, Stroustrup was announced as the 2018 winner of the Charles Stark Draper Prize for Engineering, "for conceptualizing and developing the C++ programming language."[18]
According to Stroustrup: "the name signifies the evolutionary nature of the changes from C".[19] This name is credited to Rick Mascitti (mid-1983)[12] and was first used in December 1983. When Mascitti was questioned informally in 1992 about the naming, he indicated that it was given in a tongue-in-cheek spirit. The name comes from C's ++ operator (which increments the value of a variable) and a common naming convention of using "+" to indicate an enhanced computer program.
During C++'s development period, the language had been referred to as "new C" and "C with Classes"[12][20] before acquiring its final name.
Throughout C++'s life, its development and evolution has been guided by a set of principles:[11]
C++ is standardized by an ISO working group known as JTC1/SC22/WG21. So far, it has published five revisions of the C++ standard and is currently working on the next revision, C++20.
In 1998, the ISO working group standardized C++ for the first time as ISO/IEC 14882:1998, which is informally known as C++98. In 2003, it published a new version of the C++ standard called ISO/IEC 14882:2003, which fixed problems identified in C++98.
The next major revision of the standard was informally referred to as "C++0x", but it was not released until 2011.[25] C++11 (14882:2011) included many additions to both the core language and the standard library.[23]
In 2014, C++14 (also known as C++1y) was released as a small extension to C++11, featuring mainly bug fixes and small improvements.[26]  The Draft International Standard ballot procedures completed in mid-August 2014.[27]
After C++14, a major revision C++17, informally known as C++1z, was completed by the ISO C++ Committee in mid July 2017 and was approved and published in December 2017.[28]
As part of the standardization process, ISO also publishes technical reports and specifications:
More technical specifications are in development and pending approval, including concurrency library extensions, a networking standard library, ranges, and modules.[38]
The C++ language has two main components: a direct mapping of hardware features provided primarily by the C subset, and zero-overhead abstractions based on those mappings. Stroustrup describes C++ as "a light-weight abstraction programming language [designed] for building and using efficient and elegant abstractions";[6] and "offering both hardware access and abstraction is the basis of C++. Doing it efficiently is what distinguishes it from other languages".[39]
C++ inherits most of C's syntax. The following is Bjarne Stroustrup's version of the Hello world program that uses the C++ Standard Library stream facility to write a message to standard output:[40][41]
As in C, C++ supports four types of memory management: static storage duration objects, thread storage duration objects, automatic storage duration objects, and dynamic storage duration objects.[42]
Static storage duration objects are created before main() is entered (see exceptions below) and destroyed in reverse order of creation after main() exits. The exact order of creation is not specified by the standard (though there are some rules defined below) to allow implementations some freedom in how to organize their implementation. More formally, objects of this type have a lifespan that "shall last for the duration of the program".[43]
Static storage duration objects are initialized in two phases. First, "static initialization" is performed, and only after all static initialization is performed, "dynamic initialization" is performed.  In static initialization, all objects are first initialized with zeros; after that, all objects that have a constant initialization phase are initialized with the constant expression (i.e. variables initialized with a literal or constexpr). Though it is not specified in the standard, the static initialization phase can be completed at compile time and saved in the data partition of the executable.  Dynamic initialization involves all object initialization done via a constructor or function call (unless the function is marked with constexpr, in C++11). The dynamic initialization order is defined as the order of declaration within the compilation unit (i.e. the same file). No guarantees are provided about the order of initialization between compilation units.
Variables of this type are very similar to static storage duration objects. The main difference is the creation time is just prior to thread creation and destruction is done after the thread has been joined.[44]
The most common variable types in C++ are local variables inside a function or block, and temporary variables.[45] The common feature about automatic variables is that they have a lifetime that is limited to the scope of the variable. They are created and potentially initialized at the point of declaration (see below for details) and destroyed in the reverse order of creation when the scope is left. This is implemented by allocation on the stack.
Local variables are created as the point of execution passes the declaration point. If the variable has a constructor or initializer this is used to define the initial state of the object. Local variables are destroyed when the local block or function that they are declared in is closed. C++ destructors for local variables are called at the end of the object lifetime, allowing a discipline for automatic resource management termed RAII, which is widely used in C++.
Member variables are created when the parent object is created. Array members are initialized from 0 to the last member of the array in order. Member variables are destroyed when the parent object is destroyed in the reverse order of creation. i.e. If the parent is an "automatic object" then it will be destroyed when it goes out of scope which triggers the destruction of all its members.
Temporary variables are created as the result of expression evaluation and are destroyed when the statement containing the expression has been fully evaluated (usually at the ; at the end of a statement).
These objects have a dynamic lifespan and are created with a call to new and destroyed explicitly with a call to delete.[46]
C++ templates enable generic programming. C++ supports function, class, alias and variable templates. Templates may be parameterized by types, compile-time constants, and other templates. Templates are implemented by instantiation at compile-time. To instantiate a template, compilers substitute specific arguments for a template's parameters to generate a concrete function or class instance. Some substitutions are not possible; these are eliminated by an overload resolution policy described by the phrase "Substitution failure is not an error" (SFINAE). Templates are a powerful tool that can be used for generic programming, template metaprogramming, and code optimization, but this power implies a cost. Template use may increase code size, because each template instantiation produces a copy of the template code: one for each set of template arguments, however, this is the same or smaller amount of code that would be generated if the code was written by hand.[47] This is in contrast to run-time generics seen in other languages (e.g., Java) where at compile-time the type is erased and a single template body is preserved.
Templates are different from macros: while both of these compile-time language features enable conditional compilation, templates are not restricted to lexical substitution. Templates are aware of the semantics and type system of their companion language, as well as all compile-time type definitions, and can perform high-level operations including programmatic flow control based on evaluation of strictly type-checked parameters. Macros are capable of conditional control over compilation based on predetermined criteria, but cannot instantiate new types, recurse, or perform type evaluation and in effect are limited to pre-compilation text-substitution and text-inclusion/exclusion. In other words, macros can control compilation flow based on pre-defined symbols but cannot, unlike templates, independently instantiate new symbols. Templates are a tool for static polymorphism (see below) and generic programming.
In addition, templates are a compile time mechanism in C++ that is Turing-complete, meaning that any computation expressible by a computer program can be computed, in some form, by a template metaprogram prior to runtime.
In summary, a template is a compile-time parameterized function or class written without knowledge of the specific arguments used to instantiate it. After instantiation, the resulting code is equivalent to code written specifically for the passed arguments. In this manner, templates provide a way to decouple generic, broadly applicable aspects of functions and classes (encoded in templates) from specific aspects (encoded in template parameters) without sacrificing performance due to abstraction.
C++ introduces object-oriented programming (OOP) features to C. It offers classes, which provide the four features commonly present in OOP (and some non-OOP) languages: abstraction, encapsulation, inheritance, and polymorphism. One distinguishing feature of C++ classes compared to classes in other programming languages is support for deterministic destructors, which in turn provide support for the Resource Acquisition is Initialization (RAII) concept.
Encapsulation is the hiding of information to ensure that data structures and operators are used as intended and to make the usage model more obvious to the developer. C++ provides the ability to define classes and functions as its primary encapsulation mechanisms. Within a class, members can be declared as either public, protected, or private to explicitly enforce encapsulation. A public member of the class is accessible to any function. A private member is accessible only to functions that are members of that class and to functions and classes explicitly granted access permission by the class ("friends"). A protected member is accessible to members of classes that inherit from the class in addition to the class itself and any friends.
The OO principle is that all of the functions (and only the functions) that access the internal representation of a type should be encapsulated within the type definition. C++ supports this (via member functions and friend functions), but does not enforce it: the programmer can declare parts or all of the representation of a type to be public, and is allowed to make public entities that are not part of the representation of the type. Therefore, C++ supports not just OO programming, but other decomposition paradigms, like modular programming.
It is generally considered good practice to make all data private or protected, and to make public only those functions that are part of a minimal interface for users of the class. This can hide the details of data implementation, allowing the designer to later fundamentally change the implementation without changing the interface in any way.[48][49]
Inheritance allows one data type to acquire properties of other data types. Inheritance from a base class may be declared as public, protected, or private. This access specifier determines whether unrelated and derived classes can access the inherited public and protected members of the base class. Only public inheritance corresponds to what is usually meant by "inheritance". The other two forms are much less frequently used. If the access specifier is omitted, a "class" inherits privately, while a "struct" inherits publicly. Base classes may be declared as virtual; this is called virtual inheritance. Virtual inheritance ensures that only one instance of a base class exists in the inheritance graph, avoiding some of the ambiguity problems of multiple inheritance.
Multiple inheritance is a C++ feature not found in most other languages, allowing a class to be derived from more than one base class; this allows for more elaborate inheritance relationships. For example, a "Flying Cat" class can inherit from both "Cat" and "Flying Mammal". Some other languages, such as C# or Java, accomplish something similar (although more limited) by allowing inheritance of multiple interfaces while restricting the number of base classes to one (interfaces, unlike classes, provide only declarations of member functions, no implementation or member data). An interface as in C# and Java can be defined in C++ as a class containing only pure virtual functions, often known as an abstract base class or "ABC". The member functions of such an abstract base class are normally explicitly defined in the derived class, not inherited implicitly. C++ virtual inheritance exhibits an ambiguity resolution feature called dominance.
C++ provides more than 35 operators, covering basic arithmetic, bit manipulation, indirection, comparisons, logical operations and others. Almost all operators can be overloaded for user-defined types, with a few notable exceptions such as member access (. and .*) as well as the conditional operator. The rich set of overloadable operators is central to making user-defined types in C++ seem like built-in types.
Overloadable operators are also an essential part of many advanced C++ programming techniques, such as smart pointers. Overloading an operator does not change the precedence of calculations involving the operator, nor does it change the number of operands that the operator uses (any operand may however be ignored by the operator, though it will be evaluated prior to execution). Overloaded "&&" and "||" operators lose their short-circuit evaluation property.
Polymorphism enables one common interface for many implementations, and for objects to act differently under different circumstances.
C++ supports several kinds of static (resolved at compile-time) and dynamic (resolved at run-time) polymorphisms, supported by the language features described above. Compile-time polymorphism does not allow for certain run-time decisions, while runtime polymorphism typically incurs a performance penalty.
Function overloading allows programs to declare multiple functions having the same name but with different arguments (i.e. ad hoc polymorphism). The functions are distinguished by the number or types of their formal parameters. Thus, the same function name can refer to different functions depending on the context in which it is used. The type returned by the function is not used to distinguish overloaded functions and would result in a compile-time error message.
When declaring a function, a programmer can specify for one or more parameters a default value. Doing so allows the parameters with defaults to optionally be omitted when the function is called, in which case the default arguments will be used. When a function is called with fewer arguments than there are declared parameters, explicit arguments are matched to parameters in left-to-right order, with any unmatched parameters at the end of the parameter list being assigned their default arguments. In many cases, specifying default arguments in a single function declaration is preferable to providing overloaded function definitions with different numbers of parameters.
Templates in C++ provide a sophisticated mechanism for writing generic, polymorphic code (i.e. parametric polymorphism). In particular, through the curiously recurring template pattern, it's possible to implement a form of static polymorphism that closely mimics the syntax for overriding virtual functions. Because C++ templates are type-aware and Turing-complete, they can also be used to let the compiler resolve recursive conditionals and generate substantial programs through template metaprogramming. Contrary to some opinion, template code will not generate a bulk code after compilation with the proper compiler settings.[47]
Variable pointers and references to a base class type in C++ can also refer to objects of any derived classes of that type. This allows arrays and other kinds of containers to hold pointers to objects of differing types (references cannot be directly held in containers). This enables dynamic (run-time) polymorphism, where the referred objects can behave differently depending on their (actual, derived) types.
C++ also provides the dynamic_cast operator, which allows code to safely attempt conversion of an object, via a base reference/pointer, to a more derived type: downcasting. The attempt is necessary as often one does not know which derived type is referenced. (Upcasting, conversion to a more general type, can always be checked/performed at compile-time via static_cast, as ancestral classes are specified in the derived class's interface, visible to all callers.) dynamic_cast relies on run-time type information (RTTI), metadata in the program that enables differentiating types and their relationships. If a dynamic_cast to a pointer fails, the result is the nullptr constant, whereas if the destination is a reference (which cannot be null), the cast throws an exception. Objects known to be of a certain derived type can be cast to that with static_cast, bypassing RTTI and the safe runtime type-checking of dynamic_cast, so this should be used only if the programmer is very confident the cast is, and will always be, valid.
Ordinarily, when a function in a derived class overrides a function in a base class, the function to call is determined by the type of the object. A given function is overridden when there exists no difference in the number or type of parameters between two or more definitions of that function. Hence, at compile time, it may not be possible to determine the type of the object and therefore the correct function to call, given only a base class pointer; the decision is therefore put off until runtime. This is called dynamic dispatch. Virtual member functions or methods[50] allow the most specific implementation of the function to be called, according to the actual run-time type of the object. In C++ implementations, this is commonly done using virtual function tables. If the object type is known, this may be bypassed by prepending a fully qualified class name before the function call, but in general calls to virtual functions are resolved at run time.
In addition to standard member functions, operator overloads and destructors can be virtual. As a rule of thumb, if any function in the class is virtual, the destructor should be as well. As the type of an object at its creation is known at compile time, constructors, and by extension copy constructors, cannot be virtual. Nonetheless a situation may arise where a copy of an object needs to be created when a pointer to a derived object is passed as a pointer to a base object. In such a case, a common solution is to create a clone() (or similar) virtual function that creates and returns a copy of the derived class when called.
A member function can also be made "pure virtual" by appending it with = 0 after the closing parenthesis and before the semicolon. A class containing a pure virtual function is called an abstract class. Objects cannot be created from an abstract class; they can only be derived from. Any derived class inherits the virtual function as pure and must provide a non-pure definition of it (and all other pure virtual functions) before objects of the derived class can be created. A program that attempts to create an object of a class with a pure virtual member function or inherited pure virtual member function is ill-formed.
C++ provides support for anonymous functions, also known as lambda expressions, with the following form:
The [capture] list supports the definition of closures. Such lambda expressions are defined in the standard as syntactic sugar for an unnamed function object. An example lambda function may be defined as follows:
Exception handling is used to communicate the existence of a runtime problem or error from where it was detected to where the issue can be handled.[51] It permits this to be done in a uniform manner and separately from the main code, while detecting all errors.[52] Should an error occur, an exception is thrown (raised), which is then caught by the nearest suitable exception handler. The exception causes the current scope to be exited, and also each outer scope (propagation) until a suitable handler is found, calling in turn the destructors of any objects in these exited scopes.[53] At the same time, an exception is presented as an object carrying the data about the detected problem.[54]
Note that many C++ "styles", like Google's[55], forbid usage of exceptions in C++ programs, restricting the language thusly.
The exception-causing code is placed inside a try block. The exceptions are handled in separate catch blocks (the handlers); each try block can have multiple exception handlers, as it is visible in the example below.[56]
It is also possible to raise exceptions purposefully, using the throw keyword; these exceptions are handled in the usual way. In some cases, exceptions cannot be used due to technical reasons. One such example is a critical component of an embedded system, where every operation must be guaranteed to complete within a specified amount of time. This cannot be determined with exceptions as no tools exist to determine the maximum time required for an exception to be handled.[57]
The C++ standard consists of two parts: the core language and the standard library. C++ programmers expect the latter on every major implementation of C++; it includes aggregate types (vectors, lists, maps, sets, queues, stacks, arrays, tuples), algorithms (find, for_each, binary_search, random_shuffle, etc.), input/output facilities (iostream, for reading from and writing to the console and files), filesystem library, localisation support, smart pointers for automatic memory management, regular expression support, multi-threading library, atomics support (allowing a variable to be read or written to by at most one thread at a time without any external synchronisation), time utilities (measurement, getting current time, etc.), a system for converting error reporting that doesn't use C++ exceptions into C++ exceptions, a random number generator and a slightly modified version of the C standard library (to make it comply with the C++ type system).
A large part of the C++ library is based on the Standard Template Library (STL). Useful tools provided by the STL include containers as the collections of objects (such as vectors and lists), iterators that provide array-like access to containers, and algorithms that perform operations such as searching and sorting.
Furthermore, (multi)maps (associative arrays) and (multi)sets are provided, all of which export compatible interfaces. Therefore, using templates it is possible to write generic algorithms that work with any container or on any sequence defined by iterators. As in C, the features of the library are accessed by using the #include directive to include a standard header. The C++ Standard Library provides 105 standard headers, of which 27 are deprecated.
The standard incorporates the STL that was originally designed by Alexander Stepanov, who experimented with generic algorithms and containers for many years. When he started with C++, he finally found a language where it was possible to create generic algorithms (e.g., STL sort) that perform even better than, for example, the C standard library qsort, thanks to C++ features like using inlining and compile-time binding instead of function pointers. The standard does not refer to it as "STL", as it is merely a part of the standard library, but the term is still widely used to distinguish it from the rest of the standard library (input/output streams, internationalization, diagnostics, the C library subset, etc.).[58]
Most C++ compilers, and all major ones, provide a standards-conforming implementation of the C++ standard library.
To give compiler vendors greater freedom, the C++ standards committee decided not to dictate the implementation of name mangling, exception handling, and other implementation-specific features. The downside of this decision is that object code produced by different compilers is expected to be incompatible. There were, however, attempts to standardize compilers for particular machines or operating systems (for example C++ ABI),[59] though they seem to be largely abandoned now.
C++ is often considered to be a superset of C, but this is not strictly true.[60] Most C code can easily be made to compile correctly in C++, but there are a few differences that cause some valid C code to be invalid or behave differently in C++. For example, C allows implicit conversion from void* to other pointer types, but C++ does not (for type safety reasons). Also, C++ defines many new keywords, such as new and class, which may be used as identifiers (for example, variable names) in a C program.
Some incompatibilities have been removed by the 1999 revision of the C standard (C99), which now supports C++ features such as line comments (//), and declarations mixed with code. On the other hand, C99 introduced a number of new features that C++ did not support, were incompatible or redundant in C++, such as variable-length arrays, native complex-number types (however, the std::complex class in the C++ standard library provides similar functionality, although not code-compatible), designated initializers, compound literals, and the restrict keyword.[61] Some of the C99-introduced features were included in the subsequent version of the C++ standard, C++11 (out of those which were not redundant).[62][63][64] However, the C++11 standard introduces new incompatibilities, such as disallowing assignment of a string literal to a character pointer, which remains valid C.
To intermix C and C++ code, any function declaration or definition that is to be called from/used both in C and C++ must be declared with C linkage by placing it within an extern "C" {/*...*/} block. Such a function may not rely on features depending on name mangling (i.e., function overloading).
Despite its widespread adoption, some notable programmers have criticized the C++ language, including Linus Torvalds,[65] Richard Stallman,[66] Joshua Bloch, Ken Thompson[67][68][69], and Donald Knuth[70][71].

One of the most often criticised points of C++ is its perceived complexity as a language, with the criticism that a large number of non-orthogonal features in practice necessitates restricting code to subset of C++, thus eschewing the readability benefits of common style and idioms. As expressed by Joshua Bloch:  I think C++ was pushed well beyond its complexity threshold and yet there are a lot of people programming it. But what you do is you force people to subset it. So almost every shop that I know of that uses C++ says, “Yes, we’re using C++ but we’re not doing multiple-implementation inheritance and we’re not using operator overloading.” There are just a bunch of features that you’re not going to use because the complexity of the resulting code is too high. And I don’t think it’s good when you have to start doing that. You lose this programmer portability where everyone can read everyone else’s code, which I think is such a good thing. Donald Knuth (1993, commenting on pre-standardized C++), who said of Edsger Dijkstra that "to think of programming in C++" "would make him physically ill"[70], [71]:  The problem that I have with them today is that... C++ is too complicated. At the moment, it's impossible for me to write portable code that I believe would work on lots of different systems, unless I avoid all exotic features. Whenever the C++ language designers had two competing ideas as to how they should solve some problem, they said "OK, we'll do them both". So the language is too baroque for my taste. Ken Thompson, whose colleague Stroustrup was at Bell Labs, gives his assessment [68][69]:  It certainly has its good points. But by and large I think it’s a bad language. It does a lot of things half well and it’s just a garbage heap of ideas that are mutually exclusive. Everybody I know, whether it’s personal or corporate, selects a subset and these subsets are different. So it’s not a good language to transport an algorithm—to say, “I wrote it; here, take it.” It’s way too big, way too complex. And it’s obviously built by a committee. 
Stroustrup campaigned for years and years and years, way beyond any sort of technical contributions he made to the language, to get it adopted and used. And he sort of ran all the standards committees with a whip and a chair. And he said “no” to no one. He put every feature in that language that ever existed. It wasn’t cleanly designed—it was just the union of everything that came along. And I think it suffered drastically from that. 
However Brian Kernighan, also a colleague at Bell Labs, disputes this assessment[72]: C++ has been enormously influential. ... Lots of people say C++ is too big and too complicated etc. etc. but in fact it is a very powerful language and pretty much everything that is in there is there for a really sound reason: it is not somebody doing random invention, it is actually people trying to solve real world problems. Now a lot of the programs that we take for granted today, that we just use, are C++ programs. Stroustrup himself comments that: "within C++, there is a much smaller and cleaner language struggling to get out"[73]
Other complaints may include a lack of reflection or garbage collection, slow compilation times, perceived feature creep,[74] and verbose error messages, particularly from template metaprogramming.[75]



Modula-2 - Wikipedia
Modula-2 is a computer programming language designed and developed between 1977 and 1985 by Niklaus Wirth at the Swiss Federal Institute of Technology in Zurich (ETH Zurich) as a revision of Pascal to serve as the sole programming language for the operating system and application software for the personal workstation Lilith.[1] The principal concepts were:
Modula-2 was viewed by Niklaus Wirth as a successor to his earlier programming languages Pascal and Modula.[2][3] The language design was also influenced by the Mesa language and the new programming possibilities of the early personal computer Xerox Alto, both from Xerox, that Wirth saw during his 1976 sabbatical year at Xerox PARC.[4] The computer magazine BYTE devoted the August 1984 issue to the language and its surrounding environment.[5]
Modula-2 is a general purpose procedural language, sufficiently flexible to do systems programming, but with much broader application. In particular, it was designed to support separate compilation and data abstraction in a straightforward way. Much of the syntax is based on Wirth's earlier and better-known language, Pascal. Modula-2 was designed to be broadly similar to Pascal, with some elements and syntactic ambiguities removed and the important addition of the module concept, and direct language support for multiprogramming.
Here is an example of the source code for the "Hello world" program:
The Modula-2 module may be used to encapsulate a set of related subprograms and data structures, and restrict their visibility from other portions of the program. The module design implemented the data abstraction feature of Modula-2 in a very clean way.  Modula-2 programs are composed of modules, each of which is made up of two parts: a definition module, the interface portion, which contains only those parts of the subsystem that are exported (visible to other modules), and an implementation module, which contains the working code that is internal to the module.
The language has strict scope control. In particular the scope of a module can be considered as an impenetrable wall: Except for standard identifiers no object from the outer world is visible inside a module unless explicitly imported; no internal module object is visible from the outside unless explicitly exported.
Suppose module M1 exports objects a, b, c, and P by enumerating its identifiers in an explicit export list
Then the objects a, b,c, and P from module M1 become now known outside module M1 as M1.a, M1.b, M1.c, and M1.P. They are exported in a qualified manner to the universe (assumed module M1 is global). The exporting module's name, i.e. M1, is used as a qualifier followed by the object's name.
Suppose module M2 contains the following IMPORT declaration
Then this means that the objects exported by module M1 to the universe of its enclosing program can now be used inside module M2. They are referenced in a qualified manner like this: M1.a, M1.b, M1.c, and M1.P. Example:
Qualified export avoids name clashes: For instance, if another module M3 would also export an object called P, then we can still distinguish the two objects, since M1.P differs from M3.P. Thanks to the qualified export it does not matter that both objects are called P inside their exporting modules M1 and M3.
There is an alternative technique available, which is in wide use by Modula-2 programmers. Suppose module M4 is formulated as this
Then this means that objects exported by module M1 to the universe can again be used inside module M4, but now by mere references to the exported identifiers in an "unqualified" manner like this: a, b, c, and P. Example:
This technique of unqualifying import allows use of variables and other objects outside their exporting module in exactly the same simple, i.e. unqualified, manner as inside the exporting module. The walls surrounding all modules have now become irrelevant for all those objects for which this has been explicitly allowed. Of course unqualifying import is only usable if there are no name clashes.
These export and import rules may seem unnecessarily restrictive and verbose. But they do not only safeguard objects against unwanted access, but also have the pleasant side-effect of providing automatic cross-referencing of the definition of every identifier in a program: if the identifier is qualified by a module name, then the definition comes from that module. Otherwise if it occurs unqualified, simply search backwards, and you will either encounter a declaration of that identifier, or its occurrence in an IMPORT statement which names the module it comes from. This property becomes very useful when trying to understand large programs containing many modules.
The language provides for (limited) single-processor concurrency (monitors, coroutines and explicit transfer of control) and for hardware access (absolute addresses, bit manipulation, and interrupts). It uses a nominal type system.
There are two major dialects of Modula-2.  The first is PIM, named after the book
"Programming in Modula-2" by Niklaus Wirth.  There were three major editions of PIM,
the second, third (corrected) and fourth editions, each describing slight variants of the
language.  The second major dialect is ISO, from the standardization effort by the
International Organization for Standardization.  Here are a few of the differences amongst them.
There are several supersets of Modula-2 with language extensions for specific application domains:
There are several derivative languages that resemble Modula-2 very closely but are new languages in their own right. Most are different languages with different purposes and with strengths and weaknesses of their own:
Many other current programming languages have adopted features of Modula-2.
PIM [2,3,4] defines the following 40 reserved words:
PIM [3,4] defines the following 29 pervasive (built-in) identifiers:
Cambridge Modula-2 by Cambridge Microprocessor Systems is based on a subset of PIM4 with language extensions for embedded development. The compiler runs on DOS and it generates code for M68k based embedded microcontrollers running the MINOS operating system.
Mod51 by Mandeno Granville Electronics is based on ISO Modula-2 with language extensions for embedded development following IEC1131, an industry standard for programmable logic controllers (PLC) closely related to Modula-2. The Mod51 compiler generates standalone code for 80C51 based microcontrollers.
Delco Electronics, then a subsidiary of GM Hughes Electronics, developed a version of Modula-2 for embedded control systems starting in 1985. Delco named it Modula-GM. It was the first high level language used to replace machine language code for embedded systems in Delco's engine control units (ECUs). This was significant because Delco was producing over 28,000 ECUs per day in 1988 for GM; this was then the world's largest producer of ECUs.[16] The first experimental use of Modula-GM in an embedded controller was in the 1985 Antilock Braking System Controller which was based on the Motorola 68xxx microprocessor, and in 1993 Gen-4 ECU used by the CART (Championship Auto Racing Teams) and IRL (Indy Racing League) teams.[17] The first production use of Modula-GM was its use in GM trucks starting with the 1990 model year VCM (Vehicle Control Module) used to manage GM Powertrain's Vortec engines. Modula-GM was also used on all ECUs for GM's 90° Buick V6 family 3800 Series II used in the 1997-2005 model year Buick Park Avenue. The Modula-GM compilers and associated software management tools were sourced by Delco from Intermetrics.
Modula-2 was selected as the basis for Delco's high level language because of its many strengths over other alternative language choices in 1986. After Delco Electronics was spun off from GM (with other component divisions) to form Delphi in 1997, global sourcing required that a non-proprietary high-level software language be used.  ECU embedded software now developed at Delphi is compiled with commercial C compilers.
Source for all entries: Modula2.net[18]
This article is based on material taken from  the Free On-line Dictionary of Computing  prior to 1 November 2008 and incorporated under the "relicensing" terms of the GFDL, version 1.3 or later.



Internet - Wikipedia

The Internet is the global system of interconnected computer networks that use the Internet protocol suite (TCP/IP) to link devices worldwide. It is a network of networks that consists of private, public, academic, business, and government networks of local to global scope, linked by a broad array of electronic, wireless, and optical networking technologies. The Internet carries a vast range of information resources and services, such as the inter-linked hypertext documents and applications of the World Wide Web (WWW), electronic mail, telephony, and file sharing.
The origins of the Internet date back to research commissioned by the federal government of the United States in the 1960s to build robust, fault-tolerant communication with computer networks.[1] The primary precursor network, the ARPANET, initially served as a backbone for interconnection of regional academic and military networks in the 1980s. The funding of the National Science Foundation Network as a new backbone in the 1980s, as well as private funding for other commercial extensions, led to worldwide participation in the development of new networking technologies, and the merger of many networks.[2] The linking of commercial networks and enterprises by the early 1990s marked the beginning of the transition to the modern Internet,[3] and generated a sustained exponential growth as generations of institutional, personal, and mobile computers were connected to the network. Although the Internet was widely used by academia since the 1980s, the commercialization incorporated its services and technologies into virtually every aspect of modern life.
Most traditional communications media, including telephony, radio, television, paper mail and newspapers are reshaped, redefined, or even bypassed by the Internet, giving birth to new services such as email, Internet telephony, Internet television, online music, digital newspapers, and video streaming websites. Newspaper, book, and other print publishing are adapting to website technology, or are reshaped into blogging, web feeds and online news aggregators. The Internet has enabled and accelerated new forms of personal interactions through instant messaging, Internet forums, and social networking. Online shopping has grown exponentially both for major retailers and small businesses and entrepreneurs, as it enables firms to extend their "brick and mortar" presence to serve a larger market or even sell goods and services entirely online.  Business-to-business and financial services on the Internet affect supply chains across entire industries.
The Internet has no centralized governance in either technological implementation or policies for access and usage; each constituent network sets its own policies.[4] Only the overreaching definitions of the two principal name spaces in the Internet, the Internet Protocol address (IP address) space and the Domain Name System (DNS), are directed by a maintainer organization, the Internet Corporation for Assigned Names and Numbers (ICANN). The technical underpinning and standardization of the core protocols is an activity of the Internet Engineering Task Force (IETF), a non-profit organization of loosely affiliated international participants that anyone may associate with by contributing technical expertise.[5] In November 2006, the internet was included on USA Today's list of New Seven Wonders.[6]
When the term Internet is used to refer to the specific global system of interconnected Internet Protocol (IP) networks, the word is a proper noun[7] that should be written with an initial capital letter. In common use and the media, it is often erroneously not capitalized, viz. the internet. Some guides specify that the word should be capitalized when used as a noun, but not capitalized when used as an adjective.[8] The Internet is also often referred to as the Net, as a short form of network. Historically, as early as 1849, the word internetted was used uncapitalized as an adjective, meaning interconnected or interwoven.[9] The designers of early computer networks used internet both as a noun and as a verb in shorthand form of internetwork or internetworking, meaning interconnecting computer networks.[10]
The terms Internet and World Wide Web are often used interchangeably in everyday speech; it is common to speak of "going on the Internet" when using a web browser to view web pages. However, the World Wide Web or the Web is only one of a large number of Internet services. The Web is a collection of interconnected documents (web pages) and other web resources, linked by hyperlinks and URLs.[11] As another point of comparison, Hypertext Transfer Protocol, or HTTP, is the language used on the Web for information transfer, yet it is just one of many languages or protocols that can be used for communication on the Internet.[12] The term Interweb is a portmanteau of Internet and World Wide Web typically used sarcastically to parody a technically unsavvy user.
Research into packet switching, one of the fundamental Internet technologies started in the early 1960s in the work of Paul Baran,[13] and packet switched networks such as the NPL network by Donald Davies,[14] ARPANET, Tymnet, the Merit Network,[15] Telenet, and CYCLADES,[16][17] were developed in the late 1960s and 1970s using a variety of protocols.[18] The ARPANET project led to the development of protocols for internetworking, by which multiple separate networks could be joined into a network of networks.[19] ARPANET development began with two network nodes which were interconnected between the Network Measurement Center at the University of California, Los Angeles (UCLA) Henry Samueli School of Engineering and Applied Science directed by Leonard Kleinrock, and the NLS system at SRI International (SRI) by Douglas Engelbart in Menlo Park, California, on 29 October 1969.[20] The third site was the Culler-Fried Interactive Mathematics Center at the University of California, Santa Barbara, followed by the University of Utah Graphics Department. In an early sign of future growth, fifteen sites were connected to the young ARPANET by the end of 1971.[21][22] These early years were documented in the 1972 film Computer Networks: The Heralds of Resource Sharing.
Early international collaborations on the ARPANET were rare. European developers were concerned with developing the X.25 networks.[23] Notable exceptions were the Norwegian Seismic Array (NORSAR) in June 1973, followed in 1973 by Sweden with satellite links to the Tanum Earth Station and Peter T. Kirstein's research group in the United Kingdom, initially at the Institute of Computer Science, University of London and later at University College London.[24][25][26] In December 1974, .mw-parser-output cite.citation{font-style:inherit}.mw-parser-output q{quotes:"\"""\"""'""'"}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-lock-limited a,.mw-parser-output .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}RFC 675 (Specification of Internet Transmission Control Program), by Vinton Cerf, Yogen Dalal, and Carl Sunshine, used the term internet as a shorthand for internetworking and later RFCs repeated this use.[27]  Access to the ARPANET was expanded in 1981 when the National Science Foundation (NSF) funded the Computer Science Network (CSNET).  In 1982, the Internet Protocol Suite (TCP/IP) was standardized, which permitted worldwide proliferation of interconnected networks.
TCP/IP network access expanded again in 1986 when the National Science Foundation Network (NSFNet) provided access to supercomputer sites in the United States for researchers, first at speeds of 56 kbit/s and later at 1.5 Mbit/s and 45 Mbit/s.[28] Commercial Internet service providers (ISPs) emerged in the late 1980s and early 1990s. The ARPANET was decommissioned in 1990. By 1995, the Internet was fully commercialized in the U.S. when the NSFNet was decommissioned, removing the last restrictions on use of the Internet to carry commercial traffic.[29] The Internet rapidly expanded in Europe and Australia in the mid to late 1980s[30][31] and to Asia in the late 1980s and early 1990s.[32] The beginning of dedicated transatlantic communication between the NSFNET and networks in Europe was established with a low-speed satellite relay between Princeton University and Stockholm, Sweden in December 1988.[33] Although other network protocols such as UUCP had global reach well before this time, this marked the beginning of the Internet as an intercontinental network.
Public commercial use of the Internet began in mid-1989 with the connection of MCI Mail and Compuserve's email capabilities to the 500,000 users of the Internet.[34] Just months later on 1 January 1990, PSInet launched an alternate Internet backbone for commercial use; one of the networks that would grow into the commercial Internet we know today. In March 1990, the first high-speed T1 (1.5 Mbit/s) link between the NSFNET and Europe was installed between Cornell University and CERN, allowing much more robust communications than were capable with satellites.[35] Six months later Tim Berners-Lee would begin writing WorldWideWeb, the first web browser after two years of lobbying CERN management. By Christmas 1990, Berners-Lee had built all the tools necessary for a working Web: the HyperText Transfer Protocol (HTTP) 0.9,[36] the HyperText Markup Language (HTML), the first Web browser (which was also a HTML editor and could access Usenet newsgroups and FTP files), the first HTTP server software (later known as CERN httpd), the first web server,[37] and the first Web pages that described the project itself. In 1991 the Commercial Internet eXchange was founded, allowing PSInet to communicate with the other commercial networks CERFnet and Alternet. Since 1995 the Internet has tremendously impacted culture and commerce, including the rise of near instant communication by email, instant messaging, telephony (Voice over Internet Protocol or VoIP), two-way interactive video calls, and the World Wide Web[38] with its discussion forums, blogs, social networking, and online shopping sites. Increasing amounts of data are transmitted at higher and higher speeds over fiber optic networks operating at 1-Gbit/s, 10-Gbit/s, or more.
The Internet continues to grow, driven by ever greater amounts of online information and knowledge, commerce, entertainment and social networking.[41] During the late 1990s, it was estimated that traffic on the public Internet grew by 100 percent per year, while the mean annual growth in the number of Internet users was thought to be between 20% and 50%.[42] This growth is often attributed to the lack of central administration, which allows organic growth of the network, as well as the non-proprietary nature of the Internet protocols, which encourages vendor interoperability and prevents any one company from exerting too much control over the network.[43] As of 31 March 2011, the estimated total number of Internet users was 2.095 billion (30.2% of world population).[44] It is estimated that in 1993 the Internet carried only 1% of the information flowing through two-way telecommunication, by 2000 this figure had grown to 51%, and by 2007 more than 97% of all telecommunicated information was carried over the Internet.[45]
The Internet is a global network that comprises many voluntarily interconnected autonomous networks. It operates without a central governing body. The technical underpinning and standardization of the core protocols (IPv4 and IPv6) is an activity of the Internet Engineering Task Force (IETF), a non-profit organization of loosely affiliated international participants that anyone may associate with by contributing technical expertise. To maintain interoperability, the principal name spaces of the Internet are administered by the Internet Corporation for Assigned Names and Numbers (ICANN). ICANN is governed by an international board of directors drawn from across the Internet technical, business, academic, and other non-commercial communities. ICANN coordinates the assignment of unique identifiers for use on the Internet, including domain names, Internet Protocol (IP) addresses, application port numbers in the transport protocols, and many other parameters. Globally unified name spaces are essential for maintaining the global reach of the Internet.  This role of ICANN distinguishes it as perhaps the only central coordinating body for the global Internet.[46]
Regional Internet Registries (RIRs) allocate IP addresses:
The National Telecommunications and Information Administration, an agency of the United States Department of Commerce, had final approval over changes to the DNS root zone until the IANA stewardship transition on 1 October 2016.[47][48][49][50] The Internet Society (ISOC) was founded in 1992 with a mission to "assure the open development, evolution and use of the Internet for the benefit of all people throughout the world".[51] Its members include individuals (anyone may join) as well as corporations, organizations, governments, and universities. Among other activities ISOC provides an administrative home for a number of less formally organized groups that are involved in developing and managing the Internet, including: the Internet Engineering Task Force (IETF), Internet Architecture Board (IAB), Internet Engineering Steering Group (IESG), Internet Research Task Force (IRTF), and Internet Research Steering Group (IRSG). On 16 November 2005, the United Nations-sponsored World Summit on the Information Society in Tunis established the Internet Governance Forum (IGF) to discuss Internet-related issues.
The communications infrastructure of the Internet consists of its hardware components and a system of software layers that control various aspects of the architecture.
Internet service providers establish the worldwide connectivity between individual networks at various levels of scope. End-users who only access the Internet when needed to perform a function or obtain information, represent the bottom of the routing hierarchy. At the top of the routing hierarchy are the tier 1 networks, large telecommunication companies that exchange traffic directly with each other via very high speed fibre optic cables and governed by peering agreements. Tier 2 and lower level networks buy Internet transit from other providers to reach at least some parties on the global Internet, though they may also engage in peering. An ISP may use a single upstream provider for connectivity, or implement multihoming to achieve redundancy and load balancing. Internet exchange points are major traffic exchanges with physical connections to multiple ISPs. Large organizations, such as academic institutions, large enterprises, and governments, may perform the same function as ISPs, engaging in peering and purchasing transit on behalf of their internal networks. Research networks tend to interconnect with large subnetworks such as GEANT, GLORIAD, Internet2, and the UK's national research and education network, JANET. Both the Internet IP routing structure and hypertext links of the World Wide Web are examples of scale-free networks.[52] Computers and routers use routing tables in their operating system to direct IP packets to the next-hop router or destination. Routing tables are maintained by manual configuration or automatically by routing protocols. End-nodes typically use a default route that points toward an ISP providing transit, while ISP routers use the Border Gateway Protocol to establish the most efficient routing across the complex connections of the global Internet.
An estimated 70 percent of the world's Internet traffic passes through Ashburn, Virginia.[53][54][55][56]
Common methods of Internet access by users include dial-up with a computer modem via telephone circuits, broadband over coaxial cable, fiber optics or copper wires, Wi-Fi, satellite, and cellular telephone technology (e.g. 3G, 4G). The Internet may often be accessed from computers in libraries and Internet cafes. Internet access points exist in many public places such as airport halls and coffee shops. Various terms are used, such as public Internet kiosk, public access terminal, and Web payphone. Many hotels also have public terminals that are usually fee-based. These terminals are widely accessed for various usages, such as ticket booking, bank deposit, or online payment. Wi-Fi provides wireless access to the Internet via local computer networks. Hotspots providing such access include Wi-Fi cafes, where users need to bring their own wireless devices such as a laptop or PDA. These services may be free to all, free to customers only, or fee-based.
Grassroots efforts have led to wireless community networks. Commercial Wi-Fi services covering large city areas are in many cities, such as New York, London, Vienna, Toronto, San Francisco, Philadelphia, Chicago and Pittsburgh. The Internet can then be accessed from places, such as a park bench.[57] Apart from Wi-Fi, there have been experiments with proprietary mobile wireless networks like Ricochet, various high-speed data services over cellular phone networks, and fixed wireless services. High-end mobile phones such as smartphones in general come with Internet access through the phone network. Web browsers such as Opera are available on these advanced handsets, which can also run a wide variety of other Internet software. More mobile phones have Internet access than PCs, although this is not as widely used.[58] An Internet access provider and protocol matrix differentiates the methods used to get online.
According to the International Telecommunication Union (ITU), by the end of 2017, an estimated 48 per cent of individuals regularly connect to the internet, up from 34 per cent in 2012.[59] Mobile internet connectivity has played an important role in expanding access in recent years especially in Asia and the Pacific and in Africa.[60] The number of unique mobile cellular subscriptions increased from 3.89 billion in 2012 to 4.83 billion in 2016, two-thirds of the world’s population, with more than half of subscriptions located in Asia and the Pacific. The number of subscriptions is predicted to rise to 5.69 billion users in 2020.[61] As of 2016, almost 60 per cent of the world’s population had access to a 4G broadband cellular network, up from almost 50 per cent in 2015 and 11 per cent in 2012.[61] The limits that users face on accessing information via mobile applications coincide with a broader process of fragmentation of the internet. Fragmentation restricts access to media content and tends to affect poorest users the most.[60]
Zero-rating, the practice of internet providers allowing users free connectivity to access specific content or applications for free, has offered some opportunities for individuals to surmount economic hurdles, but has also been accused by its critics as creating a ‘two-tiered’ internet. To address the issues with zero-rating, an alternative model has emerged in the concept of ‘equal rating’ and is being tested in experiments by Mozilla and Orange in Africa. Equal rating prevents prioritization of one type of content and zero-rates all content up to a specified data cap. A study published by Chatham House, 15 out of 19 countries researched in Latin America had some kind of hybrid or zero-rated product offered. Some countries in the region had a handful of plans to choose from (across all mobile network operators) while others, such as Colombia, offered as many as 30 pre-paid and 34 post-paid plans.[62]
A study of eight countries in the Global South found that zero-rated data plans exist in every country, although there is a great range in the frequency with which they are offered and actually used in each.[63] Across the 181 plans examined, 13 per cent were offering zero-rated services. Another study, covering Ghana, Kenya, Nigeria and South Africa, found Facebook’s Free Basics and Wikipedia Zero to be the most commonly zero-rated content.[64]
While the hardware components in the Internet infrastructure can often be used to support other software systems, it is the design and the standardization process of the software that characterizes the Internet and provides the foundation for its scalability and success. The responsibility for the architectural design of the Internet software systems has been assumed by the Internet Engineering Task Force (IETF).[65] The IETF conducts standard-setting work groups, open to any individual, about the various aspects of Internet architecture. Resulting contributions and standards are published as Request for Comments (RFC) documents on the IETF web site. The principal methods of networking that enable the Internet are contained in specially designated RFCs that constitute the Internet Standards. Other less rigorous documents are simply informative, experimental, or historical, or document the best current practices (BCP) when implementing Internet technologies.
The Internet standards describe a framework known as the Internet protocol suite. This is a model architecture that divides methods into a layered system of protocols, originally documented in RFC 1122 and RFC 1123. The layers correspond to the environment or scope in which their services operate. At the top is the application layer, space for the application-specific networking methods used in software applications. For example, a web browser program uses the client-server application model and a specific protocol of interaction between servers and clients, while many file-sharing systems use a peer-to-peer paradigm. Below this top layer, the transport layer connects applications on different hosts with a logical channel through the network with appropriate data exchange methods.
Underlying these layers are the networking technologies that interconnect networks at their borders and exchange traffic across them. The Internet layer enables computers to identify and locate each other via Internet Protocol (IP) addresses, and routes their traffic via intermediate (transit) networks. Last, at the bottom of the architecture is the link layer, which provides logical connectivity between hosts on the same network link, such as a local area network (LAN) or a dial-up connection. The model, also known as TCP/IP, is designed to be independent of the underlying hardware used for the physical connections, which the model does not concern itself with in any detail. Other models have been developed, such as the OSI model, that attempt to be comprehensive in every aspect of communications. While many similarities exist between the models, they are not compatible in the details of description or implementation. Yet, TCP/IP protocols are usually included in the discussion of OSI networking.
The most prominent component of the Internet model is the Internet Protocol (IP), which provides addressing systems, including IP addresses, for computers on the network. IP enables internetworking and, in essence, establishes the Internet itself. Internet Protocol Version 4 (IPv4) is the initial version used on the first generation of the Internet and is still in dominant use. It was designed to address up to ~4.3 billion (109) hosts. However, the explosive growth of the Internet has led to IPv4 address exhaustion, which entered its final stage in 2011,[66] when the global address allocation pool was exhausted.  A new protocol version, IPv6, was developed in the mid-1990s, which provides vastly larger addressing capabilities and more efficient routing of Internet traffic. IPv6 is currently in growing deployment around the world, since Internet address registries (RIRs) began to urge all resource managers to plan rapid adoption and conversion.[67]
IPv6 is not directly interoperable by design with IPv4. In essence, it establishes a parallel version of the Internet not directly accessible with IPv4 software. Thus, translation facilities must exist for internetworking or nodes must have duplicate networking software for both networks. Essentially all modern computer operating systems support both versions of the Internet Protocol. Network infrastructure, however, has been lagging in this development. Aside from the complex array of physical connections that make up its infrastructure, the Internet is facilitated by bi- or multi-lateral commercial contracts, e.g., peering agreements, and by technical specifications or protocols that describe the exchange of data over the network. Indeed, the Internet is defined by its interconnections and routing policies.
The Internet carries many network services, most prominently mobile apps such as social media apps, the World Wide Web, electronic mail, multiplayer online games, Internet telephony, and file sharing services.
Many people use, erroneously, the terms Internet and World Wide Web, or just the Web, interchangeably, but the two terms are not synonymous. The World Wide Web is the primary application program that billions of people use on the Internet, and it has changed their lives immeasurably.[68][69] However, the Internet provides many other services. The Web is a global set of documents, images and other resources, logically interrelated by hyperlinks and referenced with Uniform Resource Identifiers (URIs). URIs symbolically identify services, servers, and other databases, and the documents and resources that they can provide. Hypertext Transfer Protocol (HTTP) is the main access protocol of the World Wide Web. Web services also use HTTP to allow software systems to communicate in order to share and exchange business logic and data.
World Wide Web browser software, such as Microsoft's Internet Explorer/Edge, Mozilla Firefox, Opera, Apple's Safari, and Google Chrome, lets users navigate from one web page to another via hyperlinks embedded in the documents. These documents may also contain any combination of computer data, including graphics, sounds, text, video, multimedia and interactive content that runs while the user is interacting with the page. Client-side software can include animations, games, office applications and scientific demonstrations. Through keyword-driven Internet research using search engines like Yahoo!, Bing and Google, users worldwide have easy, instant access to a vast and diverse amount of online information. Compared to printed media, books, encyclopedias and traditional libraries, the World Wide Web has enabled the decentralization of information on a large scale.
The Web has also enabled individuals and organizations to publish ideas and information to a potentially large audience online at greatly reduced expense and time delay. Publishing a web page, a blog, or building a website involves little initial cost and many cost-free services are available. However, publishing and maintaining large, professional web sites with attractive, diverse and up-to-date information is still a difficult and expensive proposition. Many individuals and some companies and groups use web logs or blogs, which are largely used as easily updatable online diaries. Some commercial organizations encourage staff to communicate advice in their areas of specialization in the hope that visitors will be impressed by the expert knowledge and free information, and be attracted to the corporation as a result.
Advertising on popular web pages can be lucrative, and e-commerce, which is the sale of products and services directly via the Web, continues to grow. Online advertising is a form of marketing and advertising which uses the Internet to deliver promotional marketing messages to consumers.  It includes email marketing, search engine marketing (SEM), social media marketing, many types of display advertising (including web banner advertising), and mobile advertising. In 2011, Internet advertising revenues in the United States surpassed those of cable television and nearly exceeded those of broadcast television.[70]:19 Many common online advertising practices are controversial and increasingly subject to regulation.
When the Web developed in the 1990s, a typical web page was stored in completed form on a web server, formatted in HTML, complete for transmission to a web browser in response to a request. Over time, the process of creating and serving web pages has become dynamic, creating a flexible design, layout, and content. Websites are often created using content management software with, initially, very little content. Contributors to these systems, who may be paid staff, members of an organization or the public, fill underlying databases with content using editing pages designed for that purpose while casual visitors view and read this content in HTML form. There may or may not be editorial, approval and security systems built into the process of taking newly entered content and making it available to the target visitors.
Email is an important communications service available on the Internet. The concept of sending electronic text messages between parties in a way analogous to mailing letters or memos predates the creation of the Internet. Pictures, documents, and other files are sent as email attachments. Emails can be cc-ed to multiple email addresses.
Internet telephony is another common communications service made possible by the creation of the Internet. VoIP stands for Voice-over-Internet Protocol, referring to the protocol that underlies all Internet communication. The idea began in the early 1990s with walkie-talkie-like voice applications for personal computers. In recent years many VoIP systems have become as easy to use and as convenient as a normal telephone. The benefit is that, as the Internet carries the voice traffic, VoIP can be free or cost much less than a traditional telephone call, especially over long distances and especially for those with always-on Internet connections such as cable or ADSL. VoIP is maturing into a competitive alternative to traditional telephone service. Interoperability between different providers has improved and the ability to call or receive a call from a traditional telephone is available. Simple, inexpensive VoIP network adapters are available that eliminate the need for a personal computer.
Voice quality can still vary from call to call, but is often equal to and can even exceed that of traditional calls. Remaining problems for VoIP include emergency telephone number dialing and reliability. Currently, a few VoIP providers provide an emergency service, but it is not universally available. Older traditional phones with no "extra features" may be line-powered only and operate during a power failure; VoIP can never do so without a backup power source for the phone equipment and the Internet access devices. VoIP has also become increasingly popular for gaming applications, as a form of communication between players. Popular VoIP clients for gaming include Ventrilo and Teamspeak. Modern video game consoles also offer VoIP chat features.
File sharing is an example of transferring large amounts of data across the Internet. A computer file can be emailed to customers, colleagues and friends as an attachment. It can be uploaded to a website or File Transfer Protocol (FTP) server for easy download by others. It can be put into a "shared location" or onto a file server for instant use by colleagues. The load of bulk downloads to many users can be eased by the use of "mirror" servers or peer-to-peer networks. In any of these cases, access to the file may be controlled by user authentication, the transit of the file over the Internet may be obscured by encryption, and money may change hands for access to the file. The price can be paid by the remote charging of funds from, for example, a credit card whose details are also passed – usually fully encrypted – across the Internet. The origin and authenticity of the file received may be checked by digital signatures or by MD5 or other message digests. These simple features of the Internet, over a worldwide basis, are changing the production, sale, and distribution of anything that can be reduced to a computer file for transmission. This includes all manner of print publications, software products, news, music, film, video, photography, graphics and the other arts. This in turn has caused seismic shifts in each of the existing industries that previously controlled the production and distribution of these products.
Streaming media is the real-time delivery of digital media for the immediate consumption or enjoyment by end users. Many radio and television broadcasters provide Internet feeds of their live audio and video productions. They may also allow time-shift viewing or listening such as Preview, Classic Clips and Listen Again features. These providers have been joined by a range of pure Internet "broadcasters" who never had on-air licenses. This means that an Internet-connected device, such as a computer or something more specific, can be used to access on-line media in much the same way as was previously possible only with a television or radio receiver. The range of available types of content is much wider, from specialized technical webcasts to on-demand popular multimedia services. Podcasting is a variation on this theme, where – usually audio – material is downloaded and played back on a computer or shifted to a portable media player to be listened to on the move. These techniques using simple equipment allow anybody, with little censorship or licensing control, to broadcast audio-visual material worldwide.
Digital media streaming increases the demand for network bandwidth.  For example, standard image quality needs 1 Mbit/s link speed for SD 480p, HD 720p quality requires 2.5 Mbit/s, and the top-of-the-line HDX quality needs 4.5 Mbit/s for 1080p.[71]
Webcams are a low-cost extension of this phenomenon. While some webcams can give full-frame-rate video, the picture either is usually small or updates slowly. Internet users can watch animals around an African waterhole, ships in the Panama Canal, traffic at a local roundabout or monitor their own premises, live and in real time. Video chat rooms and video conferencing are also popular with many uses being found for personal webcams, with and without two-way sound. YouTube was founded on 15 February 2005 and is now the leading website for free streaming video with a vast number of users. It uses a HTML5 based web player by default to stream and show video files.[72] Registered users may upload an unlimited amount of video and build their own personal profile. YouTube claims that its users watch hundreds of millions, and upload hundreds of thousands of videos daily.
The Internet has enabled new forms of social interaction, activities, and social associations. This phenomenon has given rise to the scholarly study of the sociology of the Internet.
Internet usage has seen tremendous growth. From 2000 to 2009, the number of Internet users globally rose from 394 million to 1.858 billion.[77] By 2010, 22 percent of the world's population had access to computers with 1 billion Google searches every day, 300 million Internet users reading blogs, and 2 billion videos viewed daily on YouTube.[78] In 2014 the world's Internet users surpassed 3 billion or 43.6 percent of world population, but two-thirds of the users came from richest countries, with 78.0 percent of Europe countries population using the Internet, followed by 57.4 percent of the Americas.[79]
The prevalent language for communication on the Internet has been English. This may be a result of the origin of the Internet, as well as the language's role as a lingua franca. Early computer systems were limited to the characters in the American Standard Code for Information Interchange (ASCII), a subset of the Latin alphabet.
After English (27%), the most requested languages on the World Wide Web are Chinese (25%), Spanish (8%), Japanese (5%), Portuguese and German (4% each), Arabic, French and Russian (3% each), and Korean (2%).[75] By region, 42% of the world's Internet users are based in Asia, 24% in Europe, 14% in North America, 10% in Latin America and the Caribbean taken together, 6% in Africa, 3% in the Middle East and 1% in Australia/Oceania.[80]  The Internet's technologies have developed enough in recent years, especially in the use of Unicode, that good facilities are available for development and communication in the world's widely used languages. However, some glitches such as mojibake (incorrect display of some languages' characters) still remain.
In an American study in 2005, the percentage of men using the Internet was very slightly ahead of the percentage of women, although this difference reversed in those under 30. Men logged on more often, spent more time online, and were more likely to be broadband users, whereas women tended to make more use of opportunities to communicate (such as email). Men were more likely to use the Internet to pay bills, participate in auctions, and for recreation such as downloading music and videos. Men and women were equally likely to use the Internet for shopping and banking.[81]
More recent studies indicate that in 2008, women significantly outnumbered men on most social networking sites, such as Facebook and Myspace, although the ratios varied with age.[82] In addition, women watched more streaming content, whereas men downloaded more.[83] In terms of blogs, men were more likely to blog in the first place; among those who blog, men were more likely to have a professional blog, whereas women were more likely to have a personal blog.[84]
Forecasts predict that 44% of the world's population will be users of the Internet by 2020.[85] Splitting by country, in 2012 Iceland, Norway, Sweden, the Netherlands, and Denmark had the highest Internet penetration by the number of users, with 93% or more of the population with access.[86]
Several neologisms exist that refer to Internet users: Netizen (as in "citizen of the net")[87] refers to those actively involved in improving online communities, the Internet in general or surrounding political affairs and rights such as free speech,[88][89] Internaut refers to operators or technically highly capable users of the Internet,[90][91] digital citizen refers to a person using the Internet in order to engage in society, politics, and government participation.[92]
The Internet allows greater flexibility in working hours and location, especially with the spread of unmetered high-speed connections. The Internet can be accessed almost anywhere by numerous means, including through mobile Internet devices. Mobile phones, datacards, handheld game consoles and cellular routers allow users to connect to the Internet wirelessly. Within the limitations imposed by small screens and other limited facilities of such pocket-sized devices, the services of the Internet, including email and the web, may be available. Service providers may restrict the services offered and mobile data charges may be significantly higher than other access methods.
Educational material at all levels from pre-school to post-doctoral is available from websites. Examples range from CBeebies, through school and high-school revision guides and virtual universities, to access to top-end scholarly literature through the likes of Google Scholar. For distance education, help with homework and other assignments, self-guided learning, whiling away spare time, or just looking up more detail on an interesting fact, it has never been easier for people to access educational information at any level from anywhere. The Internet in general and the World Wide Web in particular are important enablers of both formal and informal education. Further, the Internet allows universities, in particular, researchers from the social and behavioral sciences, to conduct research remotely via virtual laboratories, with profound changes in reach and generalizability of findings as well as in communication between scientists and in the publication of results.[93]
The low cost and nearly instantaneous sharing of ideas, knowledge, and skills have made collaborative work dramatically easier, with the help of collaborative software. Not only can a group cheaply communicate and share ideas but the wide reach of the Internet allows such groups more easily to form. An example of this is the free software movement, which has produced, among other things, Linux, Mozilla Firefox, and OpenOffice.org (later forked into LibreOffice). Internet chat, whether using an IRC chat room, an instant messaging system, or a social networking website, allows colleagues to stay in touch in a very convenient way while working at their computers during the day.  Messages can be exchanged even more quickly and conveniently than via email. These systems may allow files to be exchanged, drawings and images to be shared, or voice and video contact between team members.
Content management systems allow collaborating teams to work on shared sets of documents simultaneously without accidentally destroying each other's work. Business and project teams can share calendars as well as documents and other information. Such collaboration occurs in a wide variety of areas including scientific research, software development, conference planning, political activism and creative writing. Social and political collaboration is also becoming more widespread as both Internet access and computer literacy spread.
The Internet allows computer users to remotely access other computers and information stores easily from any access point. Access may be with computer security, i.e. authentication and encryption technologies, depending on the requirements. This is encouraging new ways of working from home, collaboration and information sharing in many industries. An accountant sitting at home can audit the books of a company based in another country, on a server situated in a third country that is remotely maintained by IT specialists in a fourth. These accounts could have been created by home-working bookkeepers, in other remote locations, based on information emailed to them from offices all over the world. Some of these things were possible before the widespread use of the Internet, but the cost of private leased lines would have made many of them infeasible in practice. An office worker away from their desk, perhaps on the other side of the world on a business trip or a holiday, can access their emails, access their data using cloud computing, or open a remote desktop session into their office PC using a secure virtual private network (VPN) connection on the Internet. This can give the worker complete access to all of their normal files and data, including email and other applications, while away from the office. It has been referred to among system administrators as the Virtual Private Nightmare,[94] because it extends the secure perimeter of a corporate network into remote locations and its employees' homes.
Many people use the World Wide Web to access news, weather and sports reports, to plan and book vacations and to pursue their personal interests. People use chat, messaging and email to make and stay in touch with friends worldwide, sometimes in the same way as some previously had pen pals. Social networking websites such as Facebook, Twitter, and Myspace have created new ways to socialize and interact. Users of these sites are able to add a wide variety of information to pages, to pursue common interests, and to connect with others. It is also possible to find existing acquaintances, to allow communication among existing groups of people. Sites like LinkedIn foster commercial and business connections. YouTube and Flickr specialize in users' videos and photographs. While social networking sites were initially for individuals only, today they are widely used by businesses and other organizations to promote their brands, to market to their customers and to encourage posts to "go viral". "Black hat" social media techniques are also employed by some organizations, such as spam accounts and astroturfing.
A risk for both individuals and organizations writing posts (especially public posts) on social networking websites, is that especially foolish or controversial posts occasionally lead to an unexpected and possibly large-scale backlash on social media from other Internet users. This is also a risk in relation to controversial offline behavior, if it is widely made known. The nature of this backlash can range widely from counter-arguments and public mockery, through insults and hate speech, to, in extreme cases, rape and death threats. The online disinhibition effect describes the tendency of many individuals to behave more stridently or offensively online than they would in person. A significant number of feminist women have been the target of various forms of harassment in response to posts they have made on social media, and Twitter in particular has been criticised in the past for not doing enough to aid victims of online abuse.[95]
For organizations, such a backlash can cause overall brand damage, especially if reported by the media. However, this is not always the case, as any brand damage in the eyes of people with an opposing opinion to that presented by the organization could sometimes be outweighed by strengthening the brand in the eyes of others. Furthermore, if an organization or individual gives in to demands that others perceive as wrong-headed, that can then provoke a counter-backlash.
Some websites, such as Reddit, have rules forbidding the posting of personal information of individuals (also known as doxxing), due to concerns about such postings leading to mobs of large numbers of Internet users directing harassment at the specific individuals thereby identified. In particular, the Reddit rule forbidding the posting of personal information is widely understood to imply that all identifying photos and names must be censored in Facebook screenshots posted to Reddit. However, the interpretation of this rule in relation to public Twitter posts is less clear, and in any case, like-minded people online have many other ways they can use to direct each other's attention to public social media posts they disagree with.
Children also face dangers online such as cyberbullying and approaches by sexual predators, who sometimes pose as children themselves. Children may also encounter material which they may find upsetting, or material which their parents consider to be not age-appropriate. Due to naivety, they may also post personal information about themselves online, which could put them or their families at risk unless warned not to do so. Many parents choose to enable Internet filtering, and/or supervise their children's online activities, in an attempt to protect their children from inappropriate material on the Internet. The most popular social networking websites, such as Facebook and Twitter, commonly forbid users under the age of 13. However, these policies are typically trivial to circumvent by registering an account with a false birth date, and a significant number of children aged under 13 join such sites anyway. Social networking sites for younger children, which claim to provide better levels of protection for children, also exist.[96]
The Internet has been a major outlet for leisure activity since its inception, with entertaining social experiments such as MUDs and MOOs being conducted on university servers, and humor-related Usenet groups receiving much traffic.[citation needed] Many Internet forums have sections devoted to games and funny videos.[citation needed] The Internet pornography and online gambling industries have taken advantage of the World Wide Web, and often provide a significant source of advertising revenue for other websites.[97] Although many governments have attempted to restrict both industries' use of the Internet, in general, this has failed to stop their widespread popularity.[98]
Another area of leisure activity on the Internet is multiplayer gaming.[99] This form of recreation creates communities, where people of all ages and origins enjoy the fast-paced world of multiplayer games. These range from MMORPG to first-person shooters, from role-playing video games to online gambling. While online gaming has been around since the 1970s, modern modes of online gaming began with subscription services such as GameSpy and MPlayer.[100] Non-subscribers were limited to certain types of game play or certain games. Many people use the Internet to access and download music, movies and other works for their enjoyment and relaxation. Free and fee-based services exist for all of these activities, using centralized servers and distributed peer-to-peer technologies. Some of these sources exercise more care with respect to the original artists' copyrights than others.
Internet usage has been correlated to users' loneliness.[101]  Lonely people tend to use the Internet as an outlet for their feelings and to share their stories with others, such as in the "I am lonely will anyone speak to me" thread.
Cybersectarianism is a new organizational form which involves: "highly dispersed small groups of practitioners that may remain largely anonymous within the larger social context and operate in relative secrecy, while still linked remotely to a larger network of believers who share a set of practices and texts, and often a common devotion to a particular leader.  Overseas supporters provide funding and support; domestic practitioners distribute tracts, participate in acts of resistance, and share information on the internal situation with outsiders.  Collectively, members and practitioners of such sects construct viable virtual communities of faith, exchanging personal testimonies and engaging in the collective study via email, on-line chat rooms, and web-based message boards."[102] In particular, the British government has raised concerns about the prospect of young British Muslims being indoctrinated into Islamic extremism by material on the Internet, being persuaded to join terrorist groups such as the so-called "Islamic State", and then potentially committing acts of terrorism on returning to Britain after fighting in Syria or Iraq.
Cyberslacking can become a drain on corporate resources; the average UK employee spent 57 minutes a day surfing the Web while at work, according to a 2003 study by Peninsula Business Services.[103] Internet addiction disorder is excessive computer use that interferes with daily life. Nicholas G. Carr believes that Internet use has other effects on individuals, for instance improving skills of scan-reading and interfering with the deep thinking that leads to true creativity.[104]
Electronic business (e-business) encompasses business processes spanning the entire value chain: purchasing, supply chain management, marketing, sales, customer service, and business relationship. E-commerce seeks to add revenue streams using the Internet to build and enhance relationships with clients and partners. According to International Data Corporation, the size of worldwide e-commerce, when global business-to-business and -consumer transactions are combined, equate to $16 trillion for 2013. A report by Oxford Economics adds those two together to estimate the total size of the digital economy at $20.4 trillion, equivalent to roughly 13.8% of global sales.[105]
While much has been written of the economic advantages of Internet-enabled commerce, there is also evidence that some aspects of the Internet such as maps and location-aware services may serve to reinforce economic inequality and the digital divide.[106] Electronic commerce may be responsible for consolidation and the decline of mom-and-pop, brick and mortar businesses resulting in increases in income inequality.[107][108][109]
Author Andrew Keen, a long-time critic of the social transformations caused by the Internet, has recently focused on the economic effects of consolidation from Internet businesses. Keen cites a 2013 Institute for Local Self-Reliance report saying brick-and-mortar retailers employ 47 people for every $10 million in sales while Amazon employs only 14. Similarly, the 700-employee room rental start-up Airbnb was valued at $10 billion in 2014, about half as much as Hilton Hotels, which employs 152,000 people. And car-sharing Internet startup Uber employs 1,000 full-time employees and is valued at $18.2 billion, about the same valuation as Avis and Hertz combined, which together employ almost 60,000 people.[110]
Telecommuting is the performance within a traditional worker and employer relationship when it is facilitated by tools such as groupware, virtual private networks, conference calling, videoconferencing, and voice over IP (VOIP) so that work may be performed from any location, most conveniently the worker's home. It can be efficient and useful for companies as it allows workers to communicate over long distances, saving significant amounts of travel time and cost. As broadband Internet connections become commonplace, more workers have adequate bandwidth at home to use these tools to link their home to their corporate intranet and internal communication networks.
Wikis have also been used in the academic community for sharing and dissemination of information across institutional and international boundaries.[111] In those settings, they have been found useful for collaboration on grant writing, strategic planning, departmental documentation, and committee work.[112] The United States Patent and Trademark Office uses a wiki to allow the public to collaborate on finding prior art relevant to examination of pending patent applications. Queens, New York has used a wiki to allow citizens to collaborate on the design and planning of a local park.[113] The English Wikipedia has the largest user base among wikis on the World Wide Web[114] and ranks in the top 10 among all Web sites in terms of traffic.[115]
The Internet has achieved new relevance as a political tool. The presidential campaign of Howard Dean in 2004 in the United States was notable for its success in soliciting donation via the Internet. Many political groups use the Internet to achieve a new method of organizing for carrying out their mission, having given rise to Internet activism, most notably practiced by rebels in the Arab Spring.[116][117] The New York Times suggested that social media websites, such as Facebook and Twitter, helped people organize the political revolutions in Egypt, by helping activists organize protests, communicate grievances, and disseminate information.[118]
Many have understood the Internet as an extension of the Habermasian notion of the public sphere, observing how network communication technologies provide something like a global civic forum. However, incidents of politically motivated Internet censorship have now been recorded in many countries, including western democracies.[citation needed]
The spread of low-cost Internet access in developing countries has opened up new possibilities for peer-to-peer charities, which allow individuals to contribute small amounts to charitable projects for other individuals.  Websites, such as DonorsChoose and GlobalGiving, allow small-scale donors to direct funds to individual projects of their choice. A popular twist on Internet-based philanthropy is the use of peer-to-peer lending for charitable purposes. Kiva pioneered this concept in 2005, offering the first web-based service to publish individual loan profiles for funding. Kiva raises funds for local intermediary microfinance organizations which post stories and updates on behalf of the borrowers. Lenders can contribute as little as $25 to loans of their choice, and receive their money back as borrowers repay. Kiva falls short of being a pure peer-to-peer charity, in that loans are disbursed before being funded by lenders and borrowers do not communicate with lenders themselves.[119][120]
However, the recent spread of low-cost Internet access in developing countries has made genuine international person-to-person  philanthropy increasingly feasible. In 2009, the US-based nonprofit Zidisha tapped into this trend to offer the first person-to-person microfinance platform to link lenders and borrowers across international borders without intermediaries. Members can fund loans for as little as a dollar, which the borrowers then use to develop business activities that improve their families' incomes while repaying loans to the members with interest. Borrowers access the Internet via public cybercafes, donated laptops in village schools, and even smart phones, then create their own profile pages through which they share photos and information about themselves and their businesses. As they repay their loans, borrowers continue to share updates and dialogue with lenders via their profile pages. This direct web-based connection allows members themselves to take on many of the communication and recording tasks traditionally performed by local organizations, bypassing geographic barriers and dramatically reducing the cost of microfinance services to the entrepreneurs.[121]
Internet resources, hardware, and software components are the target of criminal or malicious attempts to gain unauthorized control to cause interruptions, commit fraud, engage in blackmail or access private information.
Malicious software used and spread on the Internet includes computer viruses which copy with the help of humans, computer worms which copy themselves automatically, software for denial of service attacks, ransomware, botnets, and spyware that reports on the activity and typing of users. Usually, these activities constitute cybercrime. Defense theorists have also speculated about the possibilities of cyber warfare using similar methods on a large scale.[citation needed]
The vast majority of computer surveillance involves the monitoring of data and traffic on the Internet.[122] In the United States for example, under the Communications Assistance For Law Enforcement Act, all phone calls and broadband Internet traffic (emails, web traffic, instant messaging, etc.) are required to be available for unimpeded real-time monitoring by Federal law enforcement agencies.[123][124][125] Packet capture is the monitoring of data traffic on a computer network. Computers communicate over the Internet by breaking up messages (emails, images, videos, web pages, files, etc.) into small chunks called "packets", which are routed through a network of computers, until they reach their destination, where they are assembled back into a complete "message" again. Packet Capture Appliance intercepts these packets as they are traveling through the network, in order to examine their contents using other programs. A packet capture is an information gathering tool, but not an analysis tool. That is it gathers "messages" but it does not analyze them and figure out what they mean. Other programs are needed to perform traffic analysis and sift through intercepted data looking for important/useful information. Under the Communications Assistance For Law Enforcement Act all U.S. telecommunications providers are required to install packet sniffing technology to allow Federal law enforcement and intelligence agencies to intercept all of their customers' broadband Internet and voice over Internet protocol (VoIP) traffic.[126]
The large amount of data gathered from packet capturing requires surveillance software that filters and reports relevant information, such as the use of certain words or phrases, the access of certain types of web sites, or communicating via email or chat with certain parties.[127] Agencies, such as the Information Awareness Office, NSA, GCHQ and the FBI, spend billions of dollars per year to develop, purchase, implement, and operate systems for interception and analysis of data.[128] Similar systems are operated by Iranian secret police to identify and suppress dissidents. The required hardware and software was allegedly installed by German Siemens AG and Finnish Nokia.[129]
Some governments, such as those of Burma, Iran, North Korea, the Mainland China, Saudi Arabia and the United Arab Emirates restrict access to content on the Internet within their territories, especially to political and religious content, with domain name and keyword filters.[135]
In Norway, Denmark, Finland, and Sweden, major Internet service providers have voluntarily agreed to restrict access to sites listed by authorities. While this list of forbidden resources is supposed to contain only known child pornography sites, the content of the list is secret.[136] Many countries, including the United States, have enacted laws against the possession or distribution of certain material, such as child pornography, via the Internet, but do not mandate filter software. Many free or commercially available software programs, called content-control software are available to users to block offensive websites on individual computers or networks, in order to limit access by children to pornographic material or depiction of violence.
As the Internet is a heterogeneous network, the physical characteristics, including for example the data transfer rates of connections, vary widely. It exhibits emergent phenomena that depend on its large-scale organization.[citation needed]
An Internet blackout or outage can be caused by local signalling interruptions. Disruptions of submarine communications cables may cause blackouts or slowdowns to large areas, such as in the 2008 submarine cable disruption.  Less-developed countries are more vulnerable due to a small number of high-capacity links.  Land cables are also vulnerable, as in 2011 when a woman digging for scrap metal severed most connectivity for the nation of Armenia.[137] Internet blackouts affecting almost entire countries can be achieved by governments as a form of Internet censorship, as in the blockage of the Internet in Egypt, whereby approximately 93%[138] of networks were without access in 2011 in an attempt to stop mobilization for anti-government protests.[139]
In 2011, researchers estimated the energy used by the Internet to be between 170 and 307 GW, less than two percent of the energy used by humanity. This estimate included the energy needed to build, operate, and periodically replace the estimated 750 million laptops, a billion smart phones and 100 million servers worldwide as well as the energy that routers, cell towers, optical switches, Wi-Fi transmitters and cloud storage devices use when transmitting Internet traffic.[140][141]
 This article incorporates text from a free content work.  License statement: World Trends in Freedom of Expression and Media Development Global Report 2017/2018, 202,  UNESCO. To learn how to add open license text to Wikipedia articles, please see Wikipedia:Adding open license text to Wikipedia. For information on reusing text from Wikipedia, please see the terms of use.




Mixin - Wikipedia
In object-oriented programming languages, a Mixin is a class that contains methods for use by other classes without having to be the parent class of those other classes. How those other classes gain access to the mixin's methods depends on the language. Mixins are sometimes described as being "included" rather than "inherited". 
Mixins encourage code reuse and can be used to avoid the inheritance ambiguity that multiple inheritance can cause[1] (the "diamond problem"), or to work around lack of support for multiple inheritance in a language.  A mixin can also be viewed as an interface with implemented methods. This pattern is an example of enforcing the dependency inversion principle.
Mixins first appeared in the Symbolics's object-oriented Flavors system (developed by Howard Cannon), which was an approach to object-orientation used in Lisp Machine Lisp. The name was inspired by Steve's Ice Cream Parlor in Somerville, Massachusetts:[2] The owner of the ice cream shop offered a basic flavor of ice cream (vanilla, chocolate, etc.) and blended in a combination of extra items (nuts, cookies, fudge, etc.) and called the item a "mix-in", his own trademarked term at the time.[3]
Mixins are a language concept that allows a programmer to inject some code into a class. Mixin programming is a style of software development, in which units of functionality are created in a class and then mixed in with other classes.[4]
A mixin class acts as the parent class, containing the desired functionality. A subclass can then inherit or simply reuse this functionality, but not as a means of specialization. Typically, the mixin will export the desired functionality to a child class, without creating a rigid, single "is a" relationship. Here lies the important difference between the concepts of mixins and inheritance, in that the child class can still inherit all the features of the parent class, but, the semantics about the child "being a kind of" the parent need not be necessarily applied.
In Simula, classes are defined in a block in which attributes, methods and class initialization are all defined together; thus all the methods that can be invoked on a class are defined together, and the definition of the class is complete.
In Flavors, a Mixin is a class from which another class can inherit slot definitions and methods. The Mixin usually does not have direct instances. Since a Flavor can inherit from more than one other Flavor, it can inherit from one or more Mixins. Note that the original Flavors did not use generic functions.
In New Flavors (a successor of Flavors) and CLOS, methods are organized in "generic functions". These generic functions are functions that are defined in multiple cases (methods) by class dispatch and method combinations.
CLOS and Flavors allow mixin methods to add behavior to existing methods: :before and :after daemons, whoppers and wrappers in Flavors. CLOS added :around methods and the ability to call shadowed methods via CALL-NEXT-METHOD. So, for example, a stream-lock-mixin can add locking around existing methods of a stream class. In Flavors one would write a wrapper or a whopper and in CLOS one would use an :around method. Both CLOS and Flavors allow the computed reuse via method combinations. :before, :after and :around methods are a feature of the standard method combination. Other method combinations are provided.
An example is the + method combination, where the resulting values of each of the applicable methods of a generic function are arithmetically added to compute the return value. This is used, for example, with the border-mixin for graphical objects. A graphical object may have a generic width function. The border-mixin would add a border around an object and has a method computing its width. A new class bordered-button (that is both a graphical object and uses the border mixin) would compute its width by calling all applicable width methods—via the + method combination. All return values are added and create the combined width of the object.
In an OOPSLA 90 paper,[8] Gilad Bracha and William Cook reinterpret different inheritance mechanisms found in Smalltalk, Beta and CLOS as special forms of a mixin inheritance.
Other than Flavors and CLOS (a part of Common Lisp), some languages that use mixins are:
Some languages do not support mixins on the language level, but can easily mimic them by copying methods from one object to another at runtime, thereby "borrowing" the mixin's methods. This is also possible with statically typed languages, but it requires constructing a new object with the extended set of methods.
Other languages that do not support mixins can support them in a round-about way via other language constructs. C# and Visual Basic .NET support the addition of extension methods on interfaces, meaning any class implementing an interface with extension methods defined will have the extension methods available as pseudo-members.
Common Lisp provides mixins in CLOS (Common Lisp Object System) similar to Flavors.
object-width is a generic function with one argument that uses the + method combination. This combination determines that all applicable methods for a generic function will be called and the results will be added.
button is a class with one slot for the button text.
There is a method for objects of class button that computes the width based on the length of the button text. + is the method qualifier for the method combination of the same name.
A border-mixin class. The naming is just a convention. There are no superclasses, and no slots.
There is a method computing the width of the border. Here it is just 4.
bordered-button is a class inheriting from both border-mixin and button.
We can now compute the width of a button. Calling object-width computes 80. The result is the result of the single applicable method: the method object-width for the class button.
We can also compute the width of a bordered-button. Calling object-width computes 84. The result is the sum of the results of the two applicable methods: the method object-width for the class button and the method object-width for the class border-mixin.
In Python, the SocketServer module[12] has both a UDPServer class and a TCPServer class. They act as servers for UDP and TCP socket servers, respectively. Additionally, there are two mixin classes: ForkingMixIn and ThreadingMixIn. Normally, all new connections are handled within the same process. By extending TCPServer with the ThreadingMixIn as follows:
the ThreadingMixIn class adds functionality to the TCP server such that each new connection creates a new thread. Alternatively, using the ForkingMixIn would cause the process to be forked for each new connection. Clearly, the functionality to create a new thread or fork a process is not terribly useful as a stand-alone class.
In this usage example, the mixins provide alternative underlying functionality without affecting the functionality as a socket server.
Most of the Ruby world is based around mixins via Modules. The concept of mixins is implemented in Ruby by the keyword include to which we pass the name of the module as parameter.
Example:
The Object-Literal and extend Approach
It is technically possible to add behavior to an object by binding functions to keys in the object. However, this lack of separation between state and behavior has drawbacks:
An extend function is used to mix the behavior in:[14]
Mixin with using Object.assign()
The pure function and delegation based Flight-Mixin Approach
Even though the firstly described approach is mostly widespread the next one is closer to what JavaScript's language core fundamentally offers - Delegation.
Two function object based patterns already do the trick without the need of a third party's implementation of extend.
In the Curl web-content language, multiple inheritance is used as classes with no instances may implement methods. Common mixins include all skinnable ControlUIs inheriting from SkinnableControlUI, user interface delegate objects that require dropdown menus inheriting from StandardBaseDropdownUI and such explicitly named mixin classes as FontGraphicMixin, FontVisualMixin and NumericAxisMixin-of class. Version 7.0 added library access so that mixins do not need to be in the same package or be public abstract. Curl constructors are factories that facilitates using multiple-inheritance without explicit declaration of either interfaces or mixins.[citation needed]
Java 8 introduces a new feature in the form of default methods for interfaces.[15] Basically it allows a method to be defined in an interface with application in the scenario when a new method is to be added to an interface after the interface class programming setup is done. To add a new function to the interface means to implement the method at every class which uses the interface. Default methods help in this case where they can be introduced to an interface any time and have an implemented structure which is then used by the associated classes. Hence default methods adds a possibility of applying the concept in a mixin sort of a way.
Interfaces combined with aspect-oriented programming can also produce full-fledged mixins in languages that support such features, such as C# or Java. Additionally, through the use of the  marker interface pattern, generic programming, and extension methods, C# 3.0 has the ability to mimic mixins. With C# 3.0 came the introduction of Extension Methods[2] and they can be applied, not only to classes but, also, to interfaces. Extension Methods provide additional functionality on an existing class without modifying the class. It then becomes possible to create a static helper class for specific functionality that defines the extension methods. Because the classes implement the interface (even if the actual interface doesn’t contain any methods or properties to implement) it will pick up all the extension methods also.[16][17][18]
ECMAScript (in most cases implemented as JavaScript) does not need to mimic object composition by stepwise copying fields from one object to another. It natively[19] supports Trait and Mixin[20][21] based object composition via function objects that implement additional behavior and then are delegated via call or apply to objects that are in need of such new functionality.
Scala has a rich type system and Traits are a part of it which helps implement mixin behaviour. As their name reveals, Traits are usually used to represent a distinct feature or aspect that is normally orthogonal to the responsibility of a concrete type or at least of a certain instance.[22]
For example, the ability to sing is modeled as such an orthogonal feature: it could be applied to Birds, Persons, etc.
Here, Bird has mixed in all methods of the trait into its own definition as if class Bird had defined method sing() on its own.
As extends is also used to inherit from a super class, in case of a trait extends is used if no super class is inherited and only for mixin in the first trait. All following traits are mixed in using keyword with.
Scala allows mixing in a trait (creating an anonymous type) when creating a new instance of a class. In the case of a Person class instance, not all instances can sing. This feature comes use then:
Mixin can be achieved in Swift by using a language feature called Default implementation in Protocol Extension. 



Fourth-generation programming language - Wikipedia
A 4th-generation programming language (4GL) or (procedural language) is any computer programming language that belongs to a class of languages envisioned as an advancement upon third-generation programming languages (3GL). Each of the programming language generations aims to provide a higher level of abstraction of the internal computer hardware details, making the language more programmer-friendly, powerful and versatile.  While the definition of 4GL has changed over time, it can be typified by operating more with large collections of information at once rather than focusing on just bits and bytes.  Languages claimed to be 4GL may include support for database management, report generation, mathematical optimization, GUI development, or web development. Some researchers state that 4GLs are a subset of domain-specific languages.[1][2]
The concept of 4GL was developed from the 1970s through the 1990s, overlapping most of the development of 3GL.  While 3GLs like C, C++, C#, Java, and JavaScript remain popular for a wide variety of uses, 4GLs as originally defined found narrower uses.[citation needed] Some advanced 3GLs like Python, Ruby, and Perl combine some 4GL abilities within a general-purpose 3GL environment. Also, libraries with 4GL-like features have been developed as add-ons for most popular 3GLs.  This has blurred the distinction of 4GL and 3GL.
In the 1980s and 1990s, there were efforts to develop fifth-generation programming languages (5GL).
Though used earlier in papers and discussions, the term 4GL was first used formally by James Martin in his 1981 book Applications Development Without Programmers[3]  to refer to non-procedural, high-level specification languages. In some primitive way, early 4GLs were included in the Informatics MARK-IV (1967) product and Sperry's MAPPER (1969 internal use, 1979 release).
The motivations for the '4GL' inception and continued interest are several. The term can apply to a large set of software products. It can also apply to an approach that looks for greater semantic properties and implementation power. Just as the 3GL offered greater power to the programmer, so too did the 4GL open up the development environment to a wider population.
The early input scheme for the 4GL supported entry of data within the 72-character limit of the punched card (8 bytes used for sequencing) where a card's tag would identify the type or function. With judicious use of a few cards, the 4GL deck could offer a wide variety of processing and reporting capability whereas the equivalent functionality coded in a 3GL could subsume, perhaps, a whole box or more of cards.[4]
The 72-character metaphor continued for a while as hardware progressed to larger memory and terminal interfaces. Even with its limitations, this approach supported highly sophisticated applications.
As interfaces improved and allowed longer statement lengths and grammar-driven input handling, greater power ensued. An example of this is described on the Nomad page.
The development of the 4GL was influenced by several factors, with the hardware and operating system constraints having a large weight. When the 4GL was first introduced, a disparate mix of hardware and operating systems mandated custom application development support that was specific to the system in order to ensure sales. One example is the MAPPER system developed by Sperry. Though it has roots back to the beginning, the system has proven successful in many applications and has been ported to modern platforms. The latest variant is embedded in the BIS[5]  offering of Unisys. MARK-IV is now known as VISION:BUILDER and is offered by Computer Associates.
Santa Fe railroad used MAPPER to develop a system, in a project that was an early example of 4GL, rapid prototyping, and programming by users.[6] The idea was that it was easier to teach railroad experts to use MAPPER than to teach programmers the "intricacies of railroad operations".[7]
One of the early (and portable) languages that had 4GL properties was Ramis developed by Gerald C. Cohen at Mathematica, a mathematical software company.  Cohen left Mathematica and founded Information Builders to create a similar reporting-oriented 4GL, called FOCUS.
Later 4GL types are tied to a database system and are far different from the earlier types in their use of techniques and resources that have resulted from the general improvement of computing with time.
An interesting twist to the 4GL scene is realization that graphical interfaces and the related reasoning done by the user form a 'language' that is poorly understood.
A number of different types of 4GLs exist:
Some 4GLs have integrated tools which allow for the easy specification of all the required information:
In the twenty-first century, 4GL systems have emerged as "low code" environments or platforms for the problem of rapid application development in short periods of time. Vendors often provide sample systems such as CRM, contract management, bug tracking from which development can occur with little programming.[8]
Extract data from files or database to create reports in a wide range of formats is done by the report generator tools.



Language primitive - Wikipedia
In computing, language primitives[citation needed] are the simplest elements available in a programming language. A primitive is the smallest 'unit of processing' available to a programmer of a given machine, or can be an atomic element of an expression in a language.
Primitives are units with a meaning, i.e., a semantic value in the language. Thus they are different from tokens in a parser, which are the minimal elements of syntax.
A machine instruction, usually generated by an assembler program, is often considered the smallest unit of processing although this is not always the case. It typically performs what is perceived to be one operation such as copying a byte or string of bytes from one computer memory location to another or adding one processor register to another.
Many of today's computers, however, actually embody an even lower unit of processing known as microcode which interprets the machine code and it is then that the microcode instructions would be the genuine primitives. These instructions would typically be available for modification only by the hardware vendor's programmers.
A high-level programming language (HLL) program is composed of discrete statements and primitive data types that may also be perceived to perform a single operation or represent a single data item, but at a higher semantic level than those provided by the machine. Copying a data item from one location to another may actually involve many machine instructions that, for instance, 
before finally
Some HLL statements, particularly those involving loops, can generate thousands or even millions of primitives in a low-level programming language (LLL), which comprise the genuine instruction path length the processor has to execute at the lowest level. This perception has been referred to as the abstraction penalty.[1][2][3]
An interpreted language statement has similarities to the HLL primitives, but with a further added layer. Before the statement can be executed in a manner very similar to an HLL statement: it must first be processed by an interpreter, a process that may involve many primitives in the target machine language.
Fourth-generation programming languages (4GL) and fifth-generation programming languages (5GL) do not have a simple one-to-many correspondence from high-to-low level primitives. There are some elements of interpreted language primitives embodied in 4GL and 5GL specifications, but the approach to the original problem is less a procedural language construct and are more oriented toward problem solving and systems engineering.



Syntax (programming languages) - Wikipedia
In computer science, the syntax of a computer language is the set of rules that defines the combinations of symbols that are considered to be a correctly structured document or fragment in that language. This applies both to programming languages, where the document represents source code, and markup languages, where the document represents data. The syntax of a language defines its surface form.[1] Text-based computer languages are based on sequences of characters, while visual programming languages are based on the spatial layout and connections between symbols (which may be textual or graphical). Documents that are syntactically invalid are said to have a syntax error.
Syntax – the form – is contrasted with semantics – the meaning. In processing computer languages, semantic processing generally comes after syntactic processing, but in some cases semantic processing is necessary for complete syntactic analysis, and these are done together or concurrently. In a compiler, the syntactic analysis comprises the frontend, while semantic analysis comprises the backend (and middle end, if this phase is distinguished).
Computer language syntax is generally distinguished into three levels:
Distinguishing in this way yields modularity, allowing each level to be described and processed separately, and often independently. First a lexer turns the linear sequence of characters into a linear sequence of tokens; this is known as "lexical analysis" or "lexing". Second the parser turns the linear sequence of tokens into a hierarchical syntax tree; this is known as "parsing" narrowly speaking. Thirdly the contextual analysis resolves names and checks types. This modularity is sometimes possible, but in many real-world languages an earlier step depends on a later step – for example, the lexer hack in C is because tokenization depends on context. Even in these cases, syntactical analysis is often seen as approximating this ideal model.
The parsing stage itself can be divided into two parts: the parse tree or "concrete syntax tree" which is determined by the grammar, but is generally far too detailed for practical use, and the abstract syntax tree (AST), which simplifies this into a usable form. The AST and contextual analysis steps can be considered a form of semantic analysis, as they are adding meaning and interpretation to the syntax, or alternatively as informal, manual implementations of syntactical rules that would be difficult or awkward to describe or implement formally.
The levels generally correspond to levels in the Chomsky hierarchy. Words are in a regular language, specified in the lexical grammar, which is a Type-3 grammar, generally given as regular expressions. Phrases are in a context-free language (CFL), generally a deterministic context-free language (DCFL), specified in a phrase structure grammar, which is a Type-2 grammar, generally given as production rules in Backus–Naur form (BNF). Phrase grammars are often specified in much more constrained grammars than full context-free grammars, in order to make them easier to parse; while the LR parser can parse any DCFL in linear time, the simple LALR parser and even simpler LL parser are more efficient, but can only parse grammars whose production rules are constrained. In principle, contextual structure can be described by a context-sensitive grammar, and automatically analyzed by means such as attribute grammars, though in general this step is done manually, via name resolution rules and type checking, and implemented via a symbol table which stores names and types for each scope.
Tools have been written that automatically generate a lexer from a lexical specification written in regular expressions and a parser from the phrase grammar written in BNF: this allows one to use declarative programming, rather than need to have procedural or functional programming. A notable example is the lex-yacc pair. These automatically produce a concrete syntax tree; the parser writer must then manually write code describing how this is converted to an abstract syntax tree. Contextual analysis is also generally implemented manually. Despite the existence of these automatic tools, parsing is often implemented manually, for various reasons – perhaps the phrase structure is not context-free, or an alternative implementation improves performance or error-reporting, or allows the grammar to be changed more easily. Parsers are often written in functional languages, such as Haskell, or in scripting languages, such as Python or Perl, or in C or C++.
As an example, (add 1 1) is a syntactically valid Lisp program (assuming the 'add' function exists, else name resolution fails), adding 1 and 1. However, the following are invalid:
Note that the lexer is unable to identify the first error – all it knows is that, after producing the token LEFT_PAREN, '(' the remainder of the program is invalid, since no word rule begins with '_'. The second error is detected at the parsing stage: The parser has identified the "list" production rule due to the '(' token (as the only match), and thus can give an error message; in general it may be ambiguous. 
Type errors and undeclared variable errors are sometimes considered to be syntax errors when they are detected at compile-time (which is usually the case when compiling strongly-typed languages), though it is common to classify these kinds of error as semantic errors instead.[2][3][4]
As an example, the Python code 
contains a type error because it adds a string literal to an integer literal. Type errors of this kind can be detected at compile-time: They can be detected during parsing (phrase analysis) if the compiler uses separate rules that allow "integerLiteral + integerLiteral" but not "stringLiteral + integerLiteral", though it is more likely that the compiler will use a parsing rule that allows all expressions of the form "LiteralOrIdentifier + LiteralOrIdentifier" and then the error will be detected during contextual analysis (when type checking occurs). In some cases this validation is not done by the compiler, and these errors are only detected at runtime.
In a dynamically typed language, where type can only be determined at runtime, many type errors can only be detected at runtime. For example, the Python code
is syntactically valid at the phrase level, but the correctness of the types of a and b can only be determined at runtime, as variables do not have types in Python, only values do. Whereas there is disagreement about whether a type error detected by the compiler should be called a syntax error (rather than a static semantic error), type errors which can only be detected at program execution time are always regarded as semantic rather than syntax errors.
The syntax of textual programming languages is usually defined using a combination of regular expressions (for lexical structure) and Backus–Naur form (for grammatical structure) to inductively specify syntactic categories (nonterminals) and terminal symbols. Syntactic categories are defined by rules called productions, which specify the values that belong to a particular syntactic category.[1] Terminal symbols are the concrete characters or strings of characters (for example keywords such as define, if, let, or void) from which syntactically valid programs are constructed. 
A language can have different equivalent grammars, such as equivalent regular expressions (at the lexical levels), or different phrase rules which generate the same language. Using a broader category of grammars, such as LR grammars, can allow shorter or simpler grammars compared with more restricted categories, such as LL grammar, which may require longer grammars with more rules. Different but equivalent phrase grammars yield different parse trees, though the underlying language (set of valid documents) is the same.
Below is a simple grammar, defined using the notation of regular expressions and Extended Backus–Naur form. It describes the syntax of S-expressions, a data syntax of the programming language Lisp, which defines productions for the syntactic categories expression, atom, number, symbol, and list:
This grammar specifies the following:
Here the decimal digits, upper- and lower-case characters, and parentheses are terminal symbols.
The following are examples of well-formed token sequences in this grammar: '12345', '()', '(a b c232 (1))'
The grammar needed to specify a programming language can be classified by its position in the Chomsky hierarchy. The phrase grammar of most programming languages can be specified using a Type-2 grammar, i.e., they are context-free grammars,[5] though the overall syntax is context-sensitive (due to variable declarations and nested scopes), hence Type-1. However, there are exceptions, and for some languages the phrase grammar is Type-0 (Turing-complete).
In some languages like Perl and Lisp the specification (or implementation) of the language allows constructs that execute during the parsing phase. Furthermore, these languages have constructs that allow the programmer to alter the behavior of the parser. This combination effectively blurs the distinction between parsing and execution, and makes syntax analysis an undecidable problem in these languages, meaning that the parsing phase may not finish. For example, in Perl it is possible to execute code during parsing using a BEGIN statement, and Perl function prototypes may alter the syntactic interpretation, and possibly even the syntactic validity of the remaining code.[6] Colloquially this is referred to as "only Perl can parse Perl" (because code must be executed during parsing, and can modify the grammar), or more strongly "even Perl cannot parse Perl" (because it is undecidable). Similarly, Lisp macros introduced by the defmacro syntax also execute during parsing, meaning that a Lisp compiler must have an entire Lisp run-time system present. In contrast, C macros are merely string replacements, and do not require code execution.[7][8]
The syntax of a language describes the form of a valid program, but does not provide any information about the meaning of the program or the results of executing that program. The meaning given to a combination of symbols is handled by semantics (either formal or hard-coded in a reference implementation). Not all syntactically correct programs are semantically correct. Many syntactically correct programs are nonetheless ill-formed, per the language's rules; and may (depending on the language specification and the soundness of the implementation) result in an error on translation or execution. In some cases, such programs may exhibit undefined behavior. Even when a program is well-defined within a language, it may still have a meaning that is not intended by the person who wrote it.
Using natural language as an example, it may not be possible to assign a meaning to a grammatically correct sentence or the sentence may be false:
The following C language fragment is syntactically correct, but performs an operation that is not semantically defined (because p is a null pointer, the operations p->real and p->im have no meaning):
As a simpler example,
is syntactically valid, but not semantically defined, as it uses an uninitialized variable. Even though compilers for some programming languages (e.g., Java and C#) would detect uninitialized variable errors of this kind, they should be regarded as semantic errors rather than syntax errors.[4][9]
To quickly compare syntax of various programming languages, take a look at the list of "Hello, World!" program examples:



Semantics (computer science) - Wikipedia
In programming language theory, semantics is the field concerned with the rigorous mathematical study of the meaning of programming languages. It does so by evaluating the meaning of syntactically valid strings defined by a specific programming language, showing the computation involved. In such a case that the evaluation would be of syntactically invalid strings, the result would be non-computation. Semantics describes the processes a computer follows when executing a program in that specific language. This can be shown by describing the relationship between the input and output of a program, or an explanation of how the program will be executed on a certain platform, hence creating a model of computation.
Formal semantics, for instance, helps to write compilers, better understand what a program is doing and to prove, e.g., that the following if statement 
has the same effect as S1 alone.
The field of formal semantics encompasses all of the following:
It has close links with other areas of computer science such as programming language design, type theory, compilers and interpreters, program verification and model checking.
There are many approaches to formal semantics; these belong to three major classes:
The distinctions between the three broad classes of approaches can sometimes be vague, but all known approaches to formal semantics use the above techniques, or some combination thereof.
Apart from the choice between denotational, operational, or axiomatic approaches, most variation in formal semantic systems arises from the choice of supporting mathematical formalism.
Some variations of formal semantics include the following:
For a variety of reasons, one might wish to describe the relationships between different formal semantics.  For example:
It is also possible to relate multiple semantics through abstractions via the theory of abstract interpretation.
Robert W. Floyd is credited with founding the field of programming language semantics in Floyd (1967).[1]



Regular expression - Wikipedia
A regular expression, regex or regexp[1] (sometimes called a rational expression)[2][3] is, in theoretical computer science and formal language theory, a sequence of characters that define a search pattern. Usually this pattern is then used by string searching algorithms for "find" or "find and replace" operations on strings, or for input validation.
The concept arose in the 1950s when the American mathematician Stephen Cole Kleene formalized the description of a regular language. The concept came into common use with Unix text-processing utilities. Since the 1980s, different syntaxes for writing regular expressions exist, one being the POSIX standard and another, widely used, being the Perl syntax.
Regular expressions are used in search engines, search and replace dialogs of word processors and text editors, in text processing utilities such as sed and AWK and in lexical analysis. Many programming languages provide regex capabilities, built-in or via libraries.
The phrase regular expressions, and consequently, regexes, is often used to mean the specific, standard textual syntax (distinct from the mathematical notation described below) for representing patterns for matching text. Each character in a regular expression (that is, each character in the string describing its pattern) is either a metacharacter, having a special meaning, or a regular character that has a literal meaning. For example, in the regex a., a is a literal character which matches just 'a', while  '.' is a meta character that matches every character except a newline. Therefore, this regex matches, for example, 'a ', or 'ax', or 'a0'. Together, metacharacters and literal characters can be used to identify text of a given pattern, or process a number of instances of it. Pattern matches may vary from a precise equality to a very general similarity, as controlled by the metacharacters. For example, . is a very general pattern, [a-z] (match all lower case letters from 'a' to 'z') is less general and a is a precise pattern (matches just 'a'). The metacharacter syntax is designed specifically to represent prescribed targets in a concise and flexible way to direct the automation of text processing of a variety of input data, in a form easy to type using a standard ASCII keyboard.
A very simple case of a regular expression in this syntax is to locate a word spelled two different ways in a text editor, the regular expression seriali[sz]e matches both "serialise" and "serialize". Wildcards also achieve this, but are more limited in what they can pattern, as they have fewer metacharacters and a simple language-base.
The usual context of wildcard characters is in globbing similar names in a list of files, whereas regexes are usually employed in applications that pattern-match text strings in general. For example, the regex ^[ \t]+|[ \t]+$ matches excess whitespace at the beginning or end of a line. An advanced regular expression that matches any numeral is [+-]?(\d+(\.\d+)?|\.\d+)([eE][+-]?\d+)?.
A regex processor translates a regular expression in the above syntax into an internal representation which can be executed and matched against a string representing the text being searched in. One possible approach is the Thompson's construction algorithm to construct a nondeterministic finite automaton (NFA), which is then made deterministic and the resulting deterministic finite automaton (DFA) is run on the target text string to recognize substrings that match the regular expression.
The picture shows the NFA scheme N(s*) obtained from the regular expression s*, where s denotes a simpler regular expression in turn, which has already been recursively translated to the NFA N(s).
Regular expressions originated in 1951, when mathematician Stephen Cole Kleene described regular languages using his mathematical notation called regular sets.[4] These arose in theoretical computer science, in the subfields of automata theory (models of computation) and the description and classification of formal languages. Other early implementations of pattern matching include the SNOBOL language, which did not use regular expressions, but instead its own pattern matching constructs.
Regular expressions entered popular use from 1968 in two uses: pattern matching in a text editor[5] and lexical analysis in a compiler.[6] Among the first appearances of regular expressions in program form was when Ken Thompson built Kleene's notation into the editor QED as a means to match patterns in text files.[5][7][8][9] For speed, Thompson implemented regular expression matching by just-in-time compilation (JIT) to IBM 7094 code on the Compatible Time-Sharing System, an important early example of JIT compilation.[10] He later added this capability to the Unix editor ed, which eventually led to the popular search tool grep's use of regular expressions ("grep" is a word derived from the command for regular expression searching in the ed editor: g/re/p meaning "Global search for Regular Expression and Print matching lines"[11]). Around the same time when Thompson developed QED, a group of researchers including Douglas T. Ross implemented a tool based on regular expressions that is used for lexical analysis in compiler design.[6]
Many variations of these original forms of regular expressions were used in Unix[9] programs at Bell Labs in the 1970s, including vi, lex, sed, AWK, and expr, and in other programs such as Emacs. Regexes were subsequently adopted by a wide range of programs, with these early forms standardized in the POSIX.2 standard in 1992.
In the 1980s the more complicated regexes arose in Perl, which originally derived from a regex library written by Henry Spencer (1986), who later wrote an implementation of Advanced Regular Expressions for Tcl.[12] The Tcl library is a hybrid NFA/DFA implementation with improved performance characteristics. Software projects that have adopted Spencer's Tcl regular expression implementation include PostgreSQL.[13] Perl later expanded on Spencer's original library to add many new features,[14] but has not yet caught up with Spencer's Advanced Regular Expressions implementation in terms of performance or Unicode handling.[15][16] Part of the effort in the design of Perl 6 is to improve Perl's regex integration, and to increase their scope and capabilities to allow the definition of parsing expression grammars.[17] The result is a mini-language called Perl 6 rules, which are used to define Perl 6 grammar as well as provide a tool to programmers in the language. These rules maintain existing features of Perl 5.x regexes, but also allow BNF-style definition of a recursive descent parser via sub-rules.
The use of regexes in structured information standards for document and database modeling started in the 1960s and expanded in the 1980s when industry standards like ISO SGML (precursored by ANSI "GCA 101-1983") consolidated. The kernel of the structure specification language standards consists of regexes. Its use is evident in the DTD element group syntax.
Starting in 1997, Philip Hazel developed PCRE (Perl Compatible Regular Expressions), which attempts to closely mimic Perl's regex functionality and is used by many modern tools including PHP and Apache HTTP Server.
Today regexes are widely supported in programming languages, text processing programs (particular lexers), advanced text editors, and some other programs. Regex support is part of the standard library of many programming languages, including Java and Python, and is built into the syntax of others, including Perl and ECMAScript. Implementations of regex functionality is often called a regex engine, and a number of libraries are available for reuse.
A regular expression, often called a pattern, is an expression used to specify a set of strings required for a particular purpose. A simple way to specify a finite set of strings is to list its elements or members. However, there are often more concise ways to specify the desired set of strings. For example, the set containing the three strings "Handel", "Händel", and "Haendel" can be specified by the pattern H(ä|ae?)ndel; we say that this pattern matches each of the three strings. In most formalisms, if there exists at least one regular expression that matches a particular set then there exists an infinite number of other regular expression that also match it—the specification is not unique. Most formalisms provide the following operations to construct regular expressions.
The wildcard . matches any character. For example, a.b matches any string that contains an "a", then any other character and then a "b", a.*b matches any string that contains an "a" and a "b" at some later point.
These constructions can be combined to form arbitrarily complex expressions, much like one can construct arithmetical expressions from numbers and the operations +, −, ×, and ÷. For example, H(ae?|ä)ndel and H(a|ae|ä)ndel are both valid patterns which match the same strings as the earlier example, H(ä|ae?)ndel.
The precise syntax for regular expressions varies among tools and with context; more detail is given in the Syntax section.
Regular expressions describe regular languages in formal language theory. They have the same expressive power as regular grammars.
Regular expressions consist of constants, which denote sets of strings, and operator symbols, which denote operations over these sets. The following definition is standard, and found as such in most textbooks on formal language theory.[19][20] Given a finite alphabet Σ, the following constants are defined
as regular expressions:
Given regular expressions R and S, the following operations over them are defined
to produce regular expressions:
To avoid parentheses it is assumed that the Kleene star has the highest priority, then concatenation and then alternation. If there is no ambiguity then parentheses may be omitted. For example, (ab)c can be written as abc, and a|(b(c*)) can be written as a|bc*.
Many textbooks use the symbols ∪, +, or ∨ for alternation instead of the vertical bar.
Examples:
The formal definition of regular expressions is purposely parsimonious and avoids defining the redundant quantifiers ? and +, which can be expressed as follows: a+ = aa*, and a? = (a|ε). Sometimes the complement operator is added, to give a generalized regular expression; here Rc matches all strings over Σ* that do not match R. In principle, the complement operator is redundant, as it can always be circumscribed by using the other operators. However, the process for computing such a representation is complex, and the result may require expressions of a size that is double exponentially larger.[21][22]
Regular expressions in this sense can express the regular languages, exactly the class of languages accepted by deterministic finite automata. There is, however, a significant difference in compactness. Some classes of regular languages can only be described by deterministic finite automata whose size grows exponentially in the size of the shortest equivalent regular expressions. The standard example here is the languages
Lk consisting of all strings over the alphabet {a,b} whose kth-from-last letter equals a. On one hand, a regular expression describing L4 is given by




(
a
∣
b

)

∗


a
(
a
∣
b
)
(
a
∣
b
)
(
a
∣
b
)


{\displaystyle (a\mid b)^{*}a(a\mid b)(a\mid b)(a\mid b)}

.
Generalizing this pattern to Lk gives the expression:




(
a
∣
b

)

∗


a




(
a
∣
b
)
(
a
∣
b
)
⋯
(
a
∣
b
)

⏟



k
−
1

 times



.



{\displaystyle (a\mid b)^{*}a\underbrace {(a\mid b)(a\mid b)\cdots (a\mid b)} _{k-1{\text{ times}}}.\,}


On the other hand, it is known that every deterministic finite automaton accepting the language Lk must have at least 2k states. Luckily, there is a simple mapping from regular expressions to the more general nondeterministic finite automata (NFAs) that does not lead to such a blowup in size; for this reason NFAs are often used as alternative representations of regular languages. NFAs are a simple variation of the type-3 grammars of the Chomsky hierarchy.[19]
In the opposite direction, there are many languages easily described by a DFA that are not easily described a regular expression. For instance, determining the validity of a given ISBN number requires computing the modulus of the integer base 11, and can be easily implemented with an 11-state DFA. However, a regular expression to answer the same problem of divisibility by 11 is at least multiple megabytes in length.[citation needed]
Given a regular expression, Thompson's construction algorithm computes an equivalent nondeterministic finite automaton. A conversion in the opposite direction is achieved by Kleene's algorithm.
Finally, it is worth noting that many real-world "regular expression" engines implement features that cannot be described by the regular expressions in the sense of formal language theory; rather, they implement regexes. See below for more on this.
As seen in many of the examples above, there is more than one way to construct a regular expression to achieve the same results.
It is possible to write an algorithm that, for two given regular expressions, decides whether the described languages are equal; the algorithm reduces each expression to a minimal deterministic finite state machine, and determines whether they are isomorphic (equivalent).
Algebraic laws for regular expressions can be obtained using a method by Gischer which is best explained along an example: In order to check whether (X+Y)* and (X* Y*)* denote the same regular language, for all regular expressions X, Y, it is necessary and sufficient to check whether the particular regular expressions (a+b)* and (a* b*)* denote the same language over the alphabet Σ={a,b}. More generally, an equation E=F between regular-expression terms with variables holds if, and only if, its instantiation with different variables replaced by different symbol constants holds.[23][24]
The redundancy can be eliminated by using Kleene star and set union to find an interesting subset of regular expressions that is still fully expressive, but perhaps their use can be restricted.[clarification needed] This is a surprisingly difficult problem. As simple as the regular expressions are, there is no method to systematically rewrite them to some normal form. The lack of axiom in the past led to the star height problem. In 1991, Dexter Kozen axiomatized regular expressions as a Kleene algebra, using equational and Horn clause axioms.[25]
Already in 1964, Redko had proved that no finite set of purely equational axioms can characterize the algebra of regular languages.[26]
A regex pattern matches a target string. The pattern is composed of a sequence of atoms. An atom is a single point within the regex pattern which it tries to match to the target string. The simplest atom is a literal, but grouping parts of the pattern to match an atom will require using ( ) as metacharacters. Metacharacters help form: atoms; quantifiers telling how many atoms (and whether it is a greedy quantifier or not); a logical OR character, which offers a set of alternatives, and a logical NOT character, which negates an atom's existence; and backreferences to refer to previous atoms of a completing pattern of atoms. A match is made, not when all the atoms of the string are matched, but rather when all the pattern atoms in the regex have matched. The idea is to make a small pattern of characters stand for a large number of possible strings, rather than compiling a large list of all the literal possibilities.
Depending on the regex processor there are about fourteen metacharacters, characters that may or may not have their literal character meaning, depending on context, or whether they are "escaped", i.e. preceded by an escape sequence, in this case, the backslash \. Modern and POSIX extended regexes use metacharacters more often than their literal meaning, so to avoid "backslash-osis" or leaning toothpick syndrome it makes sense to have a metacharacter escape to a literal mode; but starting out, it makes more sense to have the four bracketing metacharacters ( ) and { } be primarily literal, and "escape" this usual meaning to become metacharacters. Common standards implement both. The usual metacharacters are  {}[]()^$.|*+? and \. The usual characters that become metacharacters when escaped are dswDSW and N.
When entering a regex in a programming language, they may be represented as a usual string literal, hence usually quoted; this is common in C, Java, and Python for instance, where the regex re is entered as "re". However, they are often written with slashes as delimiters, as in /re/ for the regex re. This originates in ed, where / is the editor command for searching, and an expression /re/ can be used to specify a range of lines (matching the pattern), which can be combined with other commands on either side, most famously g/re/p as in grep ("global regex print"), which is included in most Unix-based operating systems, such as Linux distributions. A similar convention is used in sed, where search and replace is given by s/re/replacement/ and patterns can be joined with a comma to specify a range of lines as in /re1/,/re2/. This notation is particularly well-known due to its use in Perl, where it forms part of the syntax distinct from normal string literals. In some cases, such as sed and Perl, alternative delimiters can be used to avoid collision with contents, and to avoid having to escape occurrences of the delimiter character in the contents. For example, in sed the command s,/,X, will replace a / with an X, using commas as delimiters.
The IEEE POSIX standard has three sets of compliance: BRE (Basic Regular Expressions),[27] ERE (Extended Regular Expressions), and SRE (Simple Regular Expressions). SRE is deprecated,[28] in favor of BRE, as both provide backward compatibility. The subsection below covering the character classes applies to both BRE and ERE.
BRE and ERE work together. ERE adds ?, +, and |, and it removes the need to escape the metacharacters ( ) and { }, which are required in BRE. Furthermore, as long as the POSIX standard syntax for regexes is adhered to, there can be, and often is, additional syntax to serve specific (yet POSIX compliant) applications. Although POSIX.2 leaves some implementation specifics undefined, BRE and ERE provide a "standard" which has since been adopted as the default syntax of many tools, where the choice of BRE or ERE modes is usually a supported option. For example, GNU grep has the following options: "grep -E" for ERE, and "grep -G" for BRE (the default), and "grep -P" for Perl regexes.
Perl regexes have become a de facto standard, having a rich and powerful set of atomic expressions. Perl has no "basic" or "extended" levels. As in POSIX EREs, ( ) and { } are treated as metacharacters unless escaped; other metacharacters are known to be literal or symbolic based on context alone. Additional functionality includes lazy matching, backtracking, named capture groups, and recursive patterns.
In the POSIX standard, Basic Regular Syntax (BRE) requires that the metacharacters ( ) and { } be designated \(\) and \{\}, whereas Extended Regular Syntax (ERE) does not.
The - character is treated as a literal character if it is the last or the first (after the ^, if present) character within the brackets: [abc-], [-abc]. Note that backslash escapes are not allowed. The ] character can be included in a bracket expression if it is the first (after the ^) character: []abc].
Examples:
The meaning of metacharacters escaped with a backslash is reversed for some characters in the POSIX Extended Regular Expression (ERE) syntax. With this syntax, a backslash causes the metacharacter to be treated as a literal character. So, for example, \( \) is now ( ) and \{ \} is now { }. Additionally, support is removed for \n backreferences and the following metacharacters are added:
Examples:
POSIX Extended Regular Expressions can often be used with modern Unix utilities by including the command line flag -E.
The character class is the most basic regex concept after a literal match. It makes one small sequence of characters match a larger set of characters. For example, [A-Z] could stand for the uppercase alphabet, and \d could mean any digit. Character classes apply to both POSIX levels.
When specifying a range of characters, such as [a-Z] (i.e. lowercase a to uppercase z), the computer's locale settings determine the contents by the numeric ordering of the character encoding. They could store digits in that sequence, or the ordering could be abc…zABC…Z, or aAbBcC…zZ. So the POSIX standard defines a character class, which will be known by the regex processor installed. Those definitions are in the following table:
POSIX character classes can only be used within bracket expressions. For example, [[:upper:]ab] matches the uppercase letters and lowercase "a" and "b".
An additional non-POSIX class understood by some tools is [:word:], which is usually defined as [:alnum:] plus underscore. This reflects the fact that in many programming languages these are the characters that may be used in identifiers. The editor Vim further distinguishes word and word-head classes (using the notation \w and \h) since in many programming languages the characters that can begin an identifier are not the same as those that can occur in other positions.
Note that what the POSIX regex standards call character classes are commonly referred to as POSIX character classes in other regex flavors which support them. With most other regex flavors, the term character class is used to describe what POSIX calls bracket expressions.
Because of its expressive power and (relative) ease of reading, many other utilities and programming languages have adopted syntax similar to Perl's—for example, Java, JavaScript, Python, Ruby, Qt, Microsoft's .NET Framework, and XML Schema. Some languages and tools such as Boost and PHP support multiple regex flavors. Perl-derivative regex implementations are not identical and usually implement a subset of features found in Perl 5.0, released in 1994. Perl sometimes does incorporate features initially found in other languages, for example, Perl 5.10 implements syntactic extensions originally developed in PCRE and Python.[30]
In Python and some other implementations (e.g. Java), the three common quantifiers (*, + and ?) are greedy by default because they match as many characters as possible.[31] The regex ".+" applied to the string
matches the entire line instead of matching only the first character, ". The aforementioned quantifiers may, however, be made lazy or minimal or reluctant, matching as few characters as possible, by appending a question mark: ".+?" matches only "Ganymede,".[31]
However, this does not ensure that not the whole sentence is matched in some contexts.  The question-mark operator does not change the meaning of the dot operator, so this still can match the quotes in the input.  A pattern like ".*?" EOF will still match the whole input if this is the string
To ensure that the quotes cannot be part of the match, the dot has to be replaced, e. g. like this: "[^"]*"  This will match a quoted text part without additional quotes in it.
In Java, quantifiers may be made possessive by appending a plus sign, which disables backing off, even if doing so would allow the overall match to succeed:[32] While the regex ".*" applied to the string
matches the entire line, the regex ".*+" does not match at all, because .*+ consumes the entire input, including the final ". Thus, possessive quantifiers are most useful with negated character classes, e.g. "[^"]*+", which matches "Ganymede," when applied to the same string.
Possessive quantifiers are easier to implement than greedy and lazy quantifiers, and are typically more efficient at runtime.[32]
Many features found in virtually all modern regular expression libraries provide an expressive power that far exceeds the regular languages. For example, many implementations allow grouping subexpressions with parentheses and recalling the value they match in the same expression (backreferences). This means that, among other things, a pattern can match strings of repeated words like "papa" or "WikiWiki", called squares in formal language theory. The pattern for these strings is (.+)\1.
The language of squares is not regular, nor is it context-free, due to the pumping lemma. However, pattern matching with an unbounded number of backreferences, as supported by numerous modern tools, is still context sensitive.[33]
However, many tools, libraries, and engines that provide such constructions still use the term regular expression for their patterns. This has led to a nomenclature where the term regular expression has different meanings in formal language theory and pattern matching. For this reason, some people have taken to using the term regex, regexp, or simply pattern to describe the latter. Larry Wall, author of the Perl programming language, writes in an essay about the design of Perl 6:
"Regular expressions" […] are only marginally related to real regular expressions. Nevertheless, the term has grown with the capabilities of our pattern matching engines, so I'm not going to try to fight linguistic necessity here. I will, however, generally call them "regexes" (or "regexen", when I'm in an Anglo-Saxon mood).[17]
There are at least three different algorithms that decide whether and how a given regex matches a string.
The oldest and fastest relies on a result in formal language theory that allows every nondeterministic finite automaton (NFA) to be transformed into a deterministic finite automaton (DFA). The DFA can be constructed explicitly and then run on the resulting input string one symbol at a time. Constructing the DFA for a regular expression of size m has the time and memory cost of O(2m), but it can be run on a string of size n in time O(n).
An alternative approach is to simulate the NFA directly, essentially building each DFA state on demand and then discarding it at the next step. This keeps the DFA implicit and avoids the exponential construction cost, but running cost rises to O(mn). The explicit approach is called the DFA algorithm and the implicit approach the NFA algorithm. Adding caching to the NFA algorithm is often called the "lazy DFA" algorithm, or just the DFA algorithm without making a distinction. These algorithms are fast, but using them for recalling grouped subexpressions, lazy quantification, and similar features is tricky.[34][35]
The third algorithm is to match the pattern against the input string by backtracking. This algorithm is commonly called NFA, but this terminology can be confusing. Its running time can be exponential, which simple implementations exhibit when matching against expressions like (a|aa)*b that contain both alternation and unbounded quantification and force the algorithm to consider an exponentially increasing number of sub-cases. This behavior can cause a security problem called Regular expression Denial of Service.
Although backtracking implementations only give an exponential guarantee in the worst case, they provide much greater flexibility and expressive power. For example, any implementation which allows the use of backreferences, or implements the various extensions introduced by Perl, must include some kind of backtracking. Some implementations[which?] try to provide the best of both algorithms by first running a fast DFA algorithm, and revert to a potentially slower backtracking algorithm only when a backreference is encountered during the match.
In theoretical terms, any token set can be matched by regular expressions as long as it is pre-defined. In terms of historical implementations, regexes were originally written to use ASCII characters as their token set though regex libraries have supported numerous other character sets. Many modern regex engines offer at least some support for Unicode. In most respects it makes no difference what the character set is, but some issues do arise when extending regexes to support Unicode.
Regexes are useful in a wide variety of text processing tasks, and more generally string processing, where the data need not be textual. Common applications include data validation, data scraping (especially web scraping), data wrangling, simple parsing, the production of syntax highlighting systems, and many other tasks.
While regexes would be useful on Internet search engines, processing them across the entire database could consume excessive computer resources depending on the complexity and design of the regex. Although in many cases system administrators can run regex-based queries internally, most search engines do not offer regex support to the public. Notable exceptions: Google Code Search, Exalead. Google Code Search has been shut down as of January 2012.[38]
It used a trigram index to speed queries.[39]
The specific syntax rules vary depending on the specific implementation, programming language, or library in use. Additionally, the functionality of regex implementations can vary between versions.
Because regexes can be difficult to both explain and understand without examples, interactive web sites for testing regexes are a useful resource for learning regexes by experimentation.
This section provides a basic description of some of the properties of regexes by way of illustration.
The following conventions are used in the examples.[40]
Also worth noting is that these regexes are all Perl-like syntax. Standard POSIX regular expressions are different.
Unless otherwise indicated, the following examples conform to the Perl programming language, release 5.8.8, January 31, 2006. This means that other implementations may lack support for some parts of the syntax shown here (e.g. basic vs. extended regex, \( \) vs. (), or lack of \d instead of POSIX [:digit:]).
The syntax and conventions used in these examples coincide with that of other programming environments as well.[41]
Output:
Output:
Output:
Output:
Output:
Output:
Output:
Output:
Output:
(^\w|\w$|\W\w|\w\W).
Output:
in Unicode,[37] where the Alphabetic property contains more than Latin letters, and the Decimal_Number property contains more than Arab digits.
Output:
in Unicode.
Output:
Output:
Output:
Output:
Output:
Output:
Output:
Output:
Output:
Output:
Regular expressions can often be created ("induced" or "learned") based on a set of example strings. This is known as the induction of regular languages, and is part of the general problem of grammar induction in computational learning theory. Formally, given examples of strings in a regular language, and perhaps also given examples of strings not in that regular language, it is possible to induce a grammar for the language, i.e., a regular expression that generates that language. Not all regular languages can be induced in this way (see language identification in the limit), but many can. For example, the set of examples {1, 10, 100}, and negative set (of counterexamples) {11, 1001, 101, 0} can be used to induce the regular expression 1⋅0* (1 followed by zero or more 0s).



Undefined behavior - Wikipedia
In computer programming, undefined behavior (UB) is the result of executing computer code whose behavior is not prescribed by the language specification to which the code adheres, for the current state of the program. This happens when the translator of the source code makes certain assumptions, but these assumptions are not satisfied during execution.
The behavior of some programming languages—most famously C and C++—is undefined in some cases.[1] In the standards for these languages the semantics of certain operations is described as undefined. These cases typically represent unambiguous bugs in the code, for example indexing an array outside of its bounds. An implementation is allowed to assume that such operations never occur in correct standard-conforming program code. In the case of C/C++, the compiler is allowed to give a compile-time diagnostic in these cases, but is not required to: the implementation will be considered correct whatever it does in such cases, analogous to don't-care terms in digital logic. It is the responsibility of the programmer to write code that never invokes undefined behavior, although compiler implementations are allowed to issue diagnostics when this happens. This assumption can make various program transformations valid or simplify their proof of correctness, giving flexibility to the implementation. As a result, the compiler can often make more optimizations. It also allows more compile-time checks by both compilers and static program analysis. 
In the C community, undefined behavior may be humorously referred to as "nasal demons", after a comp.std.c post that explained undefined behavior as allowing the compiler to do anything it chooses, even "to make demons fly out of your nose".[2] Under some circumstances there can be specific restrictions on undefined behavior. For example, the instruction set specifications of a CPU might leave the behavior of some forms of an instruction undefined, but if the CPU supports memory protection then the specification will probably include a blanket rule stating that no user-accessible instruction may cause a hole in the operating system's security; so an actual CPU would be permitted to corrupt user registers in response to such an instruction, but would not be allowed to, for example, switch into supervisor mode.
Documenting an operation as undefined behavior allows compilers to assume that this operation will never happen in a conforming program. This gives the compiler more information about the code and this information can lead to more optimization opportunities.
An example for the C language:
The value of x cannot be negative and, given that signed integer overflow is undefined behavior in C, the compiler can assume that at the line of the if check value >= 2147483600. Thus the if and the call to the function bar can be ignored by the compiler since the if has no side effects and its condition will never be satisfied. The code above is therefore semantically equivalent to:
Had the compiler been forced to assume that signed integer overflow has wraparound behavior, then the transformation above would not have been legal.
Such optimizations become hard to spot by humans when the code is more complex and other optimizations, like inlining, take place.
Another benefit from allowing signed integer overflow to be undefined is that it makes it possible to store and manipulate a variable's value in a processor register that is larger than the size of the variable in the source code. For example, if the type of a variable as specified in the source code is narrower than the native register width (such as "int" on a 64-bit machine, a common scenario), then the compiler can safely use a signed 64-bit integer for the variable in the machine code it produces, without changing the defined behavior of the code. If the behavior of a 32-bit integer under overflow conditions was depended upon by the program, then a compiler would have to insert additional logic when compiling for a 64-bit machine, because the overflow behavior of most machine code instructions depends on the register width.[3]
A further important benefit of undefined signed integer overflow is that it enables, though does not require, such erroneous overflows to be detected at compile-time or by static program analysis, or by run-time checks such as the  Clang and GCC sanitizers  and valgrind; if such overflow was defined with a valid semantics such as wrap-around then compile-time checks would not be possible.
C and C++ standards have several forms of undefined behavior throughout, which offers increased liberty in compiler implementations and compile-time checks at the expense of undefined run-time behavior if present. In particular, there is an appendix section dedicated to a non-exhaustive listing of common sources of undefined behavior in C.[4] Moreover, compilers are not required to diagnose code that relies on undefined behavior, due to current static analysis limitations. Hence, it is common for programmers, even experienced ones, to unintentionally rely on undefined behavior either by mistake, or simply because they are not well-versed in the rules of the language that can span over hundreds of pages. This can result in bugs that are exposed when optimizations are enabled on the compiler, or when a compiler of a different vendor or version is used. Testing or fuzzing with dynamic undefined behavior checks enabled, e.g. the Clang sanitizers, can help to catch undefined behavior not diagnosed by the compiler or static analyzers[5].
In scenarios where security is critical, undefined behavior can lead to security vulnerabilities in software. When GCC's developers changed their compiler in 2008 such that it omitted certain overflow checks that relied on undefined behavior, CERT issued a warning against the newer versions of the compiler.[6] Linux Weekly News pointed out that the same behavior was observed in PathScale C, Microsoft Visual C++ 2005 and several other compilers;[7] the warning was later amended to warn about various compilers.[8]
The major forms of undefined behavior in C can be broadly classified as [9]: spatial memory safety violations, temporal memory safety violations, integer overflow, strict aliasing violations, alignment violations, unsequenced modifications, data races, and loops that neither perform I/O nor terminate.
In C the use of any automatic variable before it has been initialized yields undefined behavior, as does integer division by zero, signed integer overflow, indexing an array outside of its defined bounds (see buffer overflow), or null pointer dereferencing. In general, any instance of undefined behavior leaves the abstract execution machine in an unknown state, and causes the behavior of the entire program to be undefined. 
Attempting to modify a string literal causes undefined behavior:[10]
Integer division by zero results in undefined behavior:[11]
Certain pointer operations may result in undefined behavior:[12]
In C and C++, the comparison of pointers to objects is only strictly defined if the pointers point to members of the same object, or elements of the same array.[13] Example:
Reaching the end of a value-returning function (other than main()) without a return statement results in undefined behavior if the value of the function call is used by the caller:[14]
Modifying an object between two sequence points more than once produces undefined behavior.[15] It is worth mentioning that there are considerable changes in what causes undefined behavior in relation to sequence points as of C++11.[16] The following example will however cause undefined behavior in both C++ and C. 
When modifying an object between two sequence points, reading the value of the object for any other purpose than determining the value to be stored is also undefined behavior.[17]



Natural language - Wikipedia

In neuropsychology, linguistics, and the philosophy of language, a natural language or ordinary language is any language that has evolved naturally in humans through use and repetition without conscious planning or premeditation. Natural languages can take different forms, such as speech or signing. They are distinguished from constructed and formal languages such as those used to program computers or to study logic.[1]
Though the exact definition varies between scholars, natural language can broadly be defined in contrast to artificial or constructed languages (such as computer programming languages and international auxiliary languages) and to other communication systems in nature. Such examples include bees' waggle dance[2] and whale song, to which researchers have found or applied the linguistic cognates of dialect and even syntax. 
All language varieties of world languages are natural languages, although some varieties are subject to greater degrees of published prescriptivism or language regulation than others. Thus nonstandard dialects can be viewed as a wild type in comparison with standard languages. But even an official language with a regulating academy, such as Standard French with the French Academy, is classified as a natural language (for example, in the field of natural language processing), as its prescriptive points do not make it either constructed enough to be classified as a constructed language or controlled enough to be classified as a controlled natural language.
Controlled natural languages are subsets of natural languages whose grammars and dictionaries have been restricted in order to reduce or eliminate both ambiguity and complexity (for instance, by cutting down on rarely used superlative or adverbial forms or irregular verbs). The purpose behind the development and implementation of a controlled natural language typically is to aid non-native speakers of a natural language in understanding it, or to ease computer processing of a natural language. An example of a widely used controlled natural language is Simplified English, which was originally developed for aerospace industry maintenance manuals.
Constructed international auxiliary languages such as Esperanto and Interlingua (even those that have native speakers) are not generally considered natural languages.[3] Natural languages have been used to communicate and have evolved in a natural way, whereas Esperanto was designed by L.L. Zamenhof selecting elements from natural languages, not grown from natural fluctuations in vocabulary and syntax. Some natural languages have become naturally "standardized" by children's natural tendency to correct for illogical grammatical structures in their parents' speech, which can be seen in the development of pidgin languages into creole languages (as explained by Steven Pinker in The Language Instinct), but this is not the case in many languages, including constructed languages such as Esperanto, where strict rules are in place as an attempt to consciously remove such irregularities. The possible exception to this are true native speakers of such languages.[4] More substantive basis for this designation is that the vocabulary, grammar, and orthography of Interlingua are natural; they have been standardized and presented by a linguistic research body, but they predated it and are not themselves considered a product of human invention.[5] Most experts, however, consider Interlingua to be naturalistic rather than natural.[3] Latino Sine Flexione, a second naturalistic auxiliary language, is also naturalistic in content but is no longer widely spoken.[6]



C (programming language) - Wikipedia

C (/siː/, as in the letter c) is a general-purpose, imperative computer programming language, supporting structured programming, lexical variable scope and recursion, while a static type system prevents many unintended operations. By design, C provides constructs that map efficiently to typical machine instructions, and therefore it has found lasting use in applications that had formerly been coded in assembly language, including operating systems, as well as various application software for computers ranging from supercomputers to embedded systems.
C was originally developed by Dennis Ritchie between 1969 and 1973 at Bell Labs,[6] and used to re-implement the Unix operating system.[7]  It has since become one of the most widely used programming languages of all time,[8][9] with C compilers from various vendors available for the majority of existing computer architectures and operating systems.  C has been standardized by the American National Standards Institute (ANSI) since 1989 (see ANSI C) and subsequently by the International Organization for Standardization (ISO).
C is an imperative procedural language.  It was designed to be compiled using a relatively straightforward compiler, to provide low-level access to memory, to provide language constructs that map efficiently to machine instructions, and to require minimal run-time support.  Despite its low-level capabilities, the language was designed to encourage cross-platform programming.  A standards-compliant C program that is written with portability in mind can be compiled for a very wide variety of computer platforms and operating systems with few changes to its source code.  The language has become available on a very wide range of platforms, from embedded microcontrollers to supercomputers.
Like most imperative languages in the ALGOL tradition, C has facilities for structured programming and allows lexical variable scope and recursion, while a static type system prevents many unintended operations.  In C, all executable code is contained within subroutines, which are called "functions" (although not in the strict sense of functional programming). Function parameters are always passed by value. Pass-by-reference is simulated in C by explicitly passing pointer values. C program source text is free-format, using the semicolon as a statement terminator and curly braces for grouping blocks of statements.
The C language also exhibits the following characteristics:
While C does not include some features found in some other languages, such as object orientation or garbage collection, such features can be implemented or emulated in C, often by way of external libraries (e.g., the Boehm garbage collector or the GLib Object System).
Many later languages have borrowed directly or indirectly from C, including C++, C#, Unix's C shell, D, Go, Java, JavaScript, Limbo, LPC, Objective-C, Perl, PHP, Python, Rust, Swift, Verilog and SystemVerilog (hardware description languages).[5]  These languages have drawn many of their control structures and other basic features from C.  Most of them (with Python being the most dramatic exception) are also very syntactically similar to C in general, and they tend to combine the recognizable expression and statement syntax of C with underlying type systems, data models, and semantics that can be radically different.
The origin of C is closely tied to the development of the Unix operating system, originally implemented in assembly language on a PDP-7 by Dennis Ritchie and Ken Thompson, incorporating several ideas from colleagues.  Eventually, they decided to port the operating system to a PDP-11. The original PDP-11 version of Unix was developed in assembly language. The developers were considering rewriting the system using the B language, Thompson's simplified version of BCPL.[11] However B's inability to take advantage of some of the PDP-11's features, notably byte addressability, led to C. The name of C was chosen simply as the next after B.[12]
The development of C started in 1972 on the PDP-11 Unix system[13] and first appeared in Version 2 Unix.[14] The language was not initially designed with portability in mind, but soon ran on different platforms as well: a compiler for the Honeywell 6000 was written within the first year of C's history, while an IBM System/370 port followed soon.[1][13]
Also in 1972, a large part of Unix was rewritten in C.[15] By 1973, with the addition of struct types, the C language had become powerful enough that most of the Unix kernel was now in C.
Unix was one of the first operating system kernels implemented in a language other than assembly. Earlier instances include the Multics system (which was written in PL/I) and Master Control Program (MCP) for the Burroughs B5000 (which was written in ALGOL) in 1961. In around  1977, Ritchie and Stephen C. Johnson made further changes to the language to facilitate portability of the Unix operating system.  Johnson's Portable C Compiler served as the basis for several implementations of C on new platforms.[13]
In 1978, Brian Kernighan and Dennis Ritchie published the first edition of The C Programming Language.[1] This book, known to C programmers as "K&R", served for many years as an informal specification of the language. The version of C that it describes is commonly referred to as K&R C. The second edition of the book[16] covers the later ANSI C standard, described below.
K&R introduced several language features:
Even after the publication of the 1989 ANSI standard, for many years K&R C was still considered the "lowest common denominator" to which C programmers restricted themselves when maximum portability was desired, since many older compilers were still in use, and because carefully written K&R C code can be legal Standard C as well.
In early versions of C, only functions that return types other than int must be declared if used before the function definition; functions used without prior declaration were presumed to return type int.
For example:
The int type specifiers which are commented out could be omitted in K&R C, but are required in later standards.
Since K&R function declarations did not include any information about function arguments, function parameter type checks were not performed, although some compilers would issue a warning message if a local function was called with the wrong number of arguments, or if multiple calls to an external function used different numbers or types of arguments.  Separate tools such as Unix's lint utility were developed that (among other things) could check for consistency of function use across multiple source files.
In the years following the publication of K&R C, several features were added to the language, supported by compilers from AT&T (in particular PCC[17]) and some other vendors. These included:
The large number of extensions and lack of agreement on a standard library, together with the language popularity and the fact that not even the Unix compilers precisely implemented the K&R specification, led to the necessity of standardization.
During the late 1970s and 1980s, versions of C were implemented for a wide variety of mainframe computers, minicomputers, and microcomputers, including the IBM PC, as its popularity began to increase significantly.
In 1983, the American National Standards Institute (ANSI) formed a committee, X3J11, to establish a standard specification of C. X3J11 based the C standard on the Unix implementation; however, the non-portable portion of the Unix C library was handed off to the IEEE working group 1003 to become the basis for the 1988 POSIX standard. In 1989, the C standard was ratified as ANSI X3.159-1989 "Programming Language C".  This version of the language is often referred to as ANSI C, Standard C, or sometimes C89.
In 1990, the ANSI C standard (with formatting changes) was adopted by the International Organization for Standardization (ISO) as ISO/IEC 9899:1990, which is sometimes called C90. Therefore, the terms "C89" and "C90" refer to the same programming language.
ANSI, like other national standards bodies, no longer develops the C standard independently, but defers to the international C standard, maintained by the working group ISO/IEC JTC1/SC22/WG14.  National adoption of an update to the international standard typically occurs within a year of ISO publication.
One of the aims of the C standardization process was to produce a superset of K&R C, incorporating many of the subsequently introduced unofficial features. The standards committee also included several additional features such as function prototypes (borrowed from C++), void pointers, support for international character sets and locales, and preprocessor enhancements. Although the syntax for parameter declarations was augmented to include the style used in C++, the K&R interface continued to be permitted, for compatibility with existing source code.
C89 is supported by current C compilers, and most C code being written today is based on it. Any program written only in Standard C and without any hardware-dependent assumptions will run correctly on any platform with a conforming C implementation, within its resource limits.  Without such precautions, programs may compile only on a certain platform or with a particular compiler, due, for example, to the use of non-standard libraries, such as GUI libraries, or to a reliance on compiler- or platform-specific attributes such as the exact size of data types and byte endianness.
In cases where code must be compilable by either standard-conforming or K&R C-based compilers, the __STDC__ macro can be used to split the code into Standard and K&R sections to prevent the use on a K&R C-based compiler of features available only in Standard C.
After the ANSI/ISO standardization process, the C language specification remained relatively static for several years. In 1995, Normative Amendment 1 to the 1990 C standard (ISO/IEC 9899/AMD1:1995, known informally as C95) was published, to correct some details and to add more extensive support for international character sets.[18]
The C standard was further revised in the late 1990s, leading to the publication of ISO/IEC 9899:1999 in 1999, which is commonly referred to as "C99". It has since been amended three times by Technical Corrigenda.[19]
C99 introduced several new features, including inline functions, several new data types (including long long int and a complex type to represent complex numbers), variable-length arrays and flexible array members, improved support for IEEE 754 floating point, support for variadic macros (macros of variable arity), and support for one-line comments beginning with //, as in BCPL or C++. Many of these had already been implemented as extensions in several C compilers.
C99 is for the most part backward compatible with C90, but is stricter in some ways; in particular, a declaration that lacks a type specifier no longer has int implicitly assumed. A standard macro __STDC_VERSION__ is defined with value 199901L to indicate that C99 support is available. GCC, Solaris Studio, and other C compilers now support many or all of the new features of C99. The C compiler in Microsoft Visual C++, however, implements the C89 standard and those parts of C99 that are required for compatibility with C++11.[20]
In 2007, work began on another revision of the C standard, informally called "C1X" until its official publication on 2011-12-08. The C standards committee adopted guidelines to limit the adoption of new features that had not been tested by existing implementations.
The C11 standard adds numerous new features to C and the library, including type generic macros, anonymous structures, improved Unicode support, atomic operations, multi-threading, and bounds-checked functions.  It also makes some portions of the existing C99 library optional, and improves compatibility with C++. The standard macro __STDC_VERSION__ is defined as 201112L to indicate that C11 support is available.
Published in June 2018, C18 is the current standard for the C programming language. It introduces no new language features, only technical corrections and clarifications to defects in C11. The standard macro __STDC_VERSION__ is defined as 201710L.
Historically, embedded C programming requires nonstandard extensions to the C language in order to support exotic features such as fixed-point arithmetic, multiple distinct memory banks, and basic I/O operations.
In 2008, the C Standards Committee published a technical report extending the C language[21] to address these issues by providing a common standard for all implementations to adhere to. It includes a number of features not available in normal C, such as fixed-point arithmetic, named address spaces, and basic I/O hardware addressing.
C has a formal grammar specified by the C standard.[22] Line endings are generally not significant in C; however, line boundaries do have significance during the preprocessing phase. Comments may appear either between the delimiters /* and */, or (since C99)  following // until the end of the line. Comments delimited by /* and */ do not nest, and these sequences of characters are not interpreted as comment delimiters if they appear inside string or character literals.[23]
C source files contain declarations and function definitions. Function definitions, in turn, contain declarations and statements. Declarations either define new types using keywords such as struct, union, and enum, or assign types to and perhaps reserve storage for new variables, usually by writing the type followed by the variable name. Keywords such as char and int specify built-in types. Sections of code are enclosed in braces ({ and }, sometimes called "curly brackets") to limit the scope of declarations and to act as a single statement for control structures.
As an imperative language, C uses statements to specify actions. The most common statement is an expression statement, consisting of an expression to be evaluated, followed by a semicolon; as a side effect of the evaluation, functions may be called and variables may be assigned new values. To modify the normal sequential execution of statements, C provides several control-flow statements identified by reserved keywords. Structured programming is supported by if(-else) conditional execution and by do-while, while, and for iterative execution (looping). The for statement has separate initialization, testing, and reinitialization expressions, any or all of which can be omitted. break and continue can be used to leave the innermost enclosing loop statement or skip to its reinitialization. There is also a non-structured goto statement which branches directly to the designated label within the function. switch selects a case to be executed based on the value of an integer expression.
Expressions can use a variety of built-in operators and may contain function calls. The order in which arguments to functions and operands to most operators are evaluated is unspecified. The evaluations may even be interleaved. However, all side effects (including storage to variables) will occur before the next "sequence point"; sequence points include the end of each expression statement, and the entry to and return from each function call.  Sequence points also occur during evaluation of expressions containing certain operators (&&, ||, ?: and the comma operator). This permits a high degree of object code optimization by the compiler, but requires C programmers to take more care to obtain reliable results than is needed for other programming languages.
Kernighan and Ritchie say in the Introduction of The C Programming Language: "C, like any other language, has its blemishes. Some of the operators have the wrong precedence; some parts of the syntax could be better."[24] The C standard did not attempt to correct many of these blemishes, because of the impact of such changes on already existing software.
The basic C source character set includes the following characters:
Newline indicates the end of a text line; it need not correspond to an actual single character, although for convenience C treats it as one.
Additional multi-byte encoded characters may be used in string literals, but they are not entirely portable.  The latest C standard (C11) allows multi-national Unicode characters to be embedded portably within C source text by using \uXXXX or \UXXXXXXXX encoding (where the X denotes a hexadecimal character), although this feature is not yet widely implemented.
The basic C execution character set contains the same characters, along with representations for alert, backspace, and carriage return. Run-time support for extended character sets has increased with each revision of the C standard.
C89 has 32 reserved words, also known as keywords, which are the words that cannot be used for any purposes other than those for which they are predefined:




C99 reserved five more words:



C11 reserved seven more words:[25]




Most of the recently reserved words begin with an underscore followed by a capital letter, because identifiers of that form were previously reserved by the C standard for use only by implementations.  Since existing program source code should not have been using these identifiers, it would not be affected when C implementations started supporting these extensions to the programming language.  Some standard headers do define more convenient synonyms for underscored identifiers. The language previously included a reserved word called entry, but this was seldom implemented, and has now been removed as a reserved word.[26]
C supports a rich set of operators, which are symbols used within an expression to specify the manipulations to be performed while evaluating that expression. C has operators for:
C uses the operator = (used in mathematics to express equality) to indicate assignment, following the precedent of Fortran and PL/I, but unlike ALGOL and its derivatives. C uses the operator == to test for equality.  The similarity between these two operators (assignment and equality) may result in the accidental use of one in place of the other, and in many cases, the mistake does not produce an error message (although some compilers produce warnings). For example, the conditional expression  if(a==b+1) might mistakenly be written as if(a=b+1), which will be evaluated as true if a is not zero after the assignment.[27]
The C operator precedence is not always intuitive.  For example, the operator == binds more tightly than (is executed prior to) the operators & (bitwise AND) and | (bitwise OR) in expressions such as x & 1 == 0, which must be written as (x & 1) == 0 if that is the coder's intent.[28]
The "hello, world" example, which appeared in the first edition of K&R, has become the model for an introductory program in most programming textbooks, regardless of programming language. The program prints "hello, world" to the standard output, which is usually a terminal or screen display.
The original version was:[29]
A standard-conforming "hello, world" program is:[a]
The first line of the program contains a preprocessing directive, indicated by #include.  This causes the compiler to replace that line with the entire text of the stdio.h standard header, which contains declarations for standard input and output functions such as printf. The angle brackets surrounding stdio.h indicate that stdio.h is located using a search strategy that prefers headers provided with the compiler to other headers having the same name, as opposed to double quotes which typically include local or project-specific header files.
The next line indicates that a function named main is being defined. The main function serves a special purpose in C programs; the run-time environment calls the main function to begin program execution. The type specifier int indicates that the value that is returned to the invoker (in this case the run-time environment) as a result of evaluating the main function, is an integer. The keyword void as a parameter list indicates that this function takes no arguments.[b]
The opening curly brace indicates the beginning of the definition of the main function.
The next line calls (diverts execution to) a function named printf, which in this case is supplied from a system library.  In this call, the printf function is passed (provided with) a single argument, the address of the first character in the string literal "hello, world\n". The string literal is an unnamed array with elements of type char, set up automatically by the compiler with a final 0-valued character to mark the end of the array (printf needs to know this). The \n is an escape sequence that C translates to a newline character, which on output signifies the end of the current line.  The return value of the printf function is of type int, but it is silently discarded since it is not used. (A more careful program might test the return value to determine whether or not the printf function succeeded.) The semicolon ; terminates the statement.
The closing curly brace indicates the end of the code for the main function. According to the C99 specification and newer, the main function, unlike any other function, will implicitly return a value of 0 upon reaching the } that terminates the function. (Formerly an explicit return 0; statement was required.) This is interpreted by the run-time system as an exit code indicating successful execution.[30]
The type system in C is static and weakly typed, which makes it similar to the type system of ALGOL descendants such as Pascal.[31]  There are built-in types for integers of various sizes, both signed and unsigned, floating-point numbers, and enumerated types (enum).  Integer type char is often used for single-byte characters.  C99 added a boolean datatype.  There are also derived types including arrays, pointers, records (struct), and unions (union).
C is often used in low-level systems programming where escapes from the type system may be necessary.  The compiler attempts to ensure type correctness of most expressions, but the programmer can override the checks in various ways, either by using a type cast to explicitly convert a value from one type to another, or by using pointers or unions to reinterpret the underlying bits of a data object in some other way.
Some find C's declaration syntax unintuitive, particularly for function pointers. (Ritchie's idea was to declare identifiers in contexts resembling their use: "declaration reflects use".)[32]
C's usual arithmetic conversions allow for efficient code to be generated, but can sometimes produce unexpected results.  For example, a comparison of signed and unsigned integers of equal width requires a conversion of the signed value to unsigned.  This can generate unexpected results if the signed value is negative.
C supports the use of pointers, a type of reference that records the address or location of an object or function in memory.  Pointers can be dereferenced to access data stored at the address pointed to, or to invoke a pointed-to function.  Pointers can be manipulated using assignment or pointer arithmetic.  The run-time representation of a pointer value is typically a raw memory address (perhaps augmented by an offset-within-word field), but since a pointer's type includes the type of the thing pointed to, expressions including pointers can be type-checked at compile time.  Pointer arithmetic is automatically scaled by the size of the pointed-to data type. Pointers are used for many purposes in C.  Text strings are commonly manipulated using pointers into arrays of characters.  Dynamic memory allocation is performed using pointers.  Many data types, such as trees, are commonly implemented as dynamically allocated struct objects linked together using pointers.  Pointers to functions are useful for passing functions as arguments to higher-order functions (such as qsort or bsearch) or as callbacks to be invoked by event handlers.[30]
A null pointer value explicitly points to no valid location.  Dereferencing a null pointer value is undefined, often resulting in a segmentation fault.  Null pointer values are useful for indicating special cases such as no "next" pointer in the final node of a linked list, or as an error indication from functions returning pointers.  In appropriate contexts in source code, such as for assigning to a pointer variable, a null pointer constant can be written as 0, with or without explicit casting to a pointer type, or as the NULL macro defined by several standard headers.  In conditional contexts, null pointer values evaluate to false, while all other pointer values evaluate to true.
Void pointers (void *) point to objects of unspecified type, and can therefore be used as "generic" data pointers. Since the size and type of the pointed-to object is not known, void pointers cannot be dereferenced, nor is pointer arithmetic on them allowed, although they can easily be (and in many contexts implicitly are) converted to and from any other object pointer type.[30]
Careless use of pointers is potentially dangerous.  Because they are typically unchecked, a pointer variable can be made to point to any arbitrary location, which can cause undesirable effects.  Although properly used pointers point to safe places, they can be made to point to unsafe places by using invalid pointer arithmetic; the objects they point to may continue to be used after deallocation (dangling pointers); they may be used without having been initialized (wild pointers); or they may be directly assigned an unsafe value using a cast, union, or through another corrupt pointer.  In general, C is permissive in allowing manipulation of and conversion between pointer types, although compilers typically provide options for various levels of checking. Some other programming languages address these problems by using more restrictive reference types.
Array types in C are traditionally of a fixed, static size specified at compile time.  (The more recent C99 standard also allows a form of variable-length arrays.)  However, it is also possible to allocate a block of memory (of arbitrary size) at run-time, using the standard library's malloc function, and treat it as an array.  C's unification of arrays and pointers means that declared arrays and these dynamically allocated simulated arrays are virtually interchangeable.
Since arrays are always accessed (in effect) via pointers, array accesses are typically not checked against the underlying array size, although some compilers may provide bounds checking as an option.[33][34]  Array bounds violations are therefore possible and rather common in carelessly written code, and can lead to various repercussions, including illegal memory accesses, corruption of data, buffer overruns, and run-time exceptions.  If bounds checking is desired, it must be done manually.
C does not have a special provision for declaring multi-dimensional arrays, but rather relies on recursion within the type system to declare arrays of arrays, which effectively accomplishes the same thing.  The index values of the resulting "multi-dimensional array" can be thought of as increasing in row-major order.
Multi-dimensional arrays are commonly used in numerical algorithms (mainly from applied linear algebra) to store matrices. The structure of the C array is well suited to this particular task. However, since arrays are passed merely as pointers, the bounds of the array must be known fixed values or else explicitly passed to any subroutine that requires them, and dynamically sized arrays of arrays cannot be accessed using double indexing. (A workaround for this is to allocate the array with an additional "row vector" of pointers to the columns.) 
C99 introduced "variable-length arrays" which address some, but not all, of the issues with ordinary C arrays.
The subscript notation x[i] (where x designates a pointer) is syntactic sugar for *(x+i).[35] Taking advantage of the compiler's knowledge of the pointer type, the address that x + i points to is not the base address (pointed to by x) incremented by i bytes, but rather is defined to be the base address incremented by i multiplied by the size of an element that x points to.  Thus, x[i] designates the i+1th element of the array.
Furthermore, in most expression contexts (a notable exception is as operand of sizeof), the name of an array is automatically converted to a pointer to the array's first element. This implies that an array is never copied as a whole when named as an argument to a function, but rather only the address of its first element is passed. Therefore, although function calls in C use pass-by-value semantics, arrays are in effect passed by reference.
The size of an element can be determined by applying the operator sizeof to any dereferenced element of x, as in n = sizeof *x or n = sizeof x[0], and the number of elements in a declared array A can be determined as sizeof A / sizeof A[0]. The latter only applies to array names: variables declared with subscripts (int A[20]). Due to the semantics of C, it is not possible to determine the entire size of arrays through pointers to arrays or those created by dynamic allocation (malloc); code such as sizeof arr / sizeof arr[0] (where arr designates a pointer) will not work since the compiler assumes the size of the pointer itself is being requested.[36][37] Since array name arguments to sizeof are not converted to pointers, they do not exhibit such ambiguity. However, arrays created by dynamic allocation are accessed by pointers rather than true array variables, so they suffer from the same sizeof issues as array pointers.
Thus, despite this apparent equivalence between array and pointer variables, there is still a distinction to be made between them. Even though the name of an array is, in most expression contexts, converted into a pointer (to its first element), this pointer does not itself occupy any storage; the array name is not an l-value, and its address is a constant, unlike a pointer variable. Consequently, what an array "points to" cannot be changed, and it is impossible to assign a new address to an array name. Array contents may be copied, however, by using the memcpy function, or by accessing the individual elements.
One of the most important functions of a programming language is to provide facilities for managing memory and the objects that are stored in memory. C provides three distinct ways to allocate memory for objects:[30]
These three approaches are appropriate in different situations and have various trade-offs. For example, static memory allocation has little allocation overhead, automatic allocation may involve slightly more overhead, and dynamic memory allocation can potentially have a great deal of overhead for both allocation and deallocation. The persistent nature of static objects is useful for maintaining state information across function calls, automatic allocation is easy to use but stack space is typically much more limited and transient than either static memory or heap space, and dynamic memory allocation allows convenient allocation of objects whose size is known only at run-time. Most C programs make extensive use of all three.
Where possible, automatic or static allocation is usually simplest because the storage is managed by the compiler, freeing the programmer of the potentially error-prone chore of manually allocating and releasing storage. However, many data structures can change in size at runtime, and since static allocations (and automatic allocations before C99) must have a fixed size at compile-time, there are many situations in which dynamic allocation is necessary.[30]  Prior to the C99 standard, variable-sized arrays were a common example of this. (See the article on malloc for an example of dynamically allocated arrays.) Unlike automatic allocation, which can fail at run time with uncontrolled consequences, the dynamic allocation functions return an indication (in the form of a null pointer value) when the required storage cannot be allocated.  (Static allocation that is too large is usually detected by the linker or loader, before the program can even begin execution.)
Unless otherwise specified, static objects contain zero or null pointer values upon program startup. Automatically and dynamically allocated objects are initialized only if an initial value is explicitly specified; otherwise they initially have indeterminate values (typically, whatever bit pattern happens to be present in the storage, which might not even represent a valid value for that type). If the program attempts to access an uninitialized value, the results are undefined. Many modern compilers try to detect and warn about this problem, but both false positives and false negatives can occur.
Another issue is that heap memory allocation has to be synchronized with its actual usage in any program in order for it to be reused as much as possible.  For example, if the only pointer to a heap memory allocation goes out of scope or has its value overwritten before free() is called, then that memory cannot be recovered for later reuse and is essentially lost to the program, a phenomenon known as a memory leak. Conversely, it is possible for memory to be freed but continue to be referenced, leading to unpredictable results. Typically, the symptoms will appear in a portion of the program far removed from the actual error, making it difficult to track down the problem. (Such issues are ameliorated in languages with automatic garbage collection.)
The C programming language uses libraries as its primary method of extension. In C, a library is a set of functions contained within a single "archive" file.  Each library typically has a header file, which contains the prototypes of the functions contained within the library that may be used by a program, and declarations of special data types and macro symbols used with these functions. In order for a program to use a library, it must include the library's header file, and the library must be linked with the program, which in many cases requires compiler flags (e.g., -lm, shorthand for "link the math library").[30]
The most common C library is the C standard library, which is specified by the ISO and ANSI C standards and comes with every C implementation (implementations which target limited environments such as embedded systems may provide only a subset of the standard library). This library supports stream input and output, memory allocation, mathematics, character strings, and time values.  Several separate standard headers (for example, stdio.h) specify the interfaces for these and other standard library facilities.
Another common set of C library functions are those used by applications specifically targeted for Unix and Unix-like systems, especially functions which provide an interface to the kernel. These functions are detailed in various standards such as POSIX and the Single UNIX Specification.
Since many programs have been written in C, there are a wide variety of other libraries available. Libraries are often written in C because C compilers generate efficient object code; programmers then create interfaces to the library so that the routines can be used from higher-level languages like Java, Perl, and Python.[30]
A number of tools have been developed to help C programmers find and fix statements with undefined behavior or possibly erroneous expressions, with greater rigor than that provided by the compiler.  The tool lint was the first such, leading to many others.
Automated source code checking and auditing are beneficial in any language, and for C many such tools exist, such as Lint. A common practice is to use Lint to detect questionable code when a program is first written. Once a program passes Lint, it is then compiled using the C compiler. Also, many compilers can optionally warn about syntactically valid constructs that are likely to actually be errors. MISRA C is a proprietary set of guidelines to avoid such questionable code, developed for embedded systems.[38]
There are also compilers, libraries, and operating system level mechanisms for performing actions that are not a standard part of C, such as bounds checking for arrays, detection of buffer overflow, serialization, dynamic memory tracking, and automatic garbage collection.
Tools such as Purify or Valgrind and linking with libraries containing special versions of the memory allocation functions can help uncover runtime errors in memory usage.
C is widely used for system programming in implementing operating systems and embedded system applications,[40] because C code, when written for portability, can be used for most purposes, yet when needed, system-specific code can be used to access specific hardware addresses and to perform type punning to match externally imposed interface requirements, with a low run-time demand on system resources.
C can also be used for website programming using CGI as a "gateway" for information between the Web application, the server, and the browser.[41] C is often chosen over interpreted languages because of its speed, stability, and near-universal availability.[42]
One consequence of C's wide availability and efficiency is that compilers, libraries and interpreters of other programming languages are often implemented in C. The reference implementations of Python, Perl and PHP, for example, are all written in C.
Because the layer of abstraction is thin and the overhead is low, C enables programmers to create efficient implementations of algorithms and data structures, useful for computationally intense programs. For example, the GNU Multiple Precision Arithmetic Library, the GNU Scientific Library, Mathematica, and MATLAB are completely or partially written in C.
C is sometimes used as an intermediate language by implementations of other languages. This approach may be used for portability or convenience; by using C as an intermediate language, additional machine-specific code generators are not necessary.  C has some features, such as line-number preprocessor directives and optional superfluous commas at the end of initializer lists, that support compilation of generated code. However, some of C's shortcomings have prompted the development of other C-based languages specifically designed for use as intermediate languages, such as C--.
C has also been widely used to implement end-user applications. However, such applications can also be written in newer, higher-level languages.
C has both directly and indirectly influenced many later languages such as C#, D, Go, Java, JavaScript, Limbo, LPC, Perl, PHP, Python, and Unix's C shell.[43] The most pervasive influence has been syntactical, all of the languages mentioned combine the statement and (more or less recognizably) expression syntax of C with type systems, data models and/or large-scale program structures that differ from those of C, sometimes radically.
Several C or near-C interpreters exist, including Ch and CINT, which can also be used for scripting.
When object-oriented languages became popular, C++ and Objective-C were two different extensions of C that provided object-oriented capabilities. Both languages were originally implemented as source-to-source compilers; source code was translated into C, and then compiled with a C compiler.[44]
The C++ programming language was devised by Bjarne Stroustrup as an approach to providing object-oriented functionality with a C-like syntax.[45] C++ adds greater typing strength, scoping, and other tools useful in object-oriented programming, and permits generic programming via templates. Nearly a superset of C, C++ now supports most of C, with a few exceptions.
Objective-C was originally a very "thin" layer on top of C, and remains a strict superset of C that permits object-oriented programming using a hybrid dynamic/static typing paradigm. Objective-C derives its syntax from both C and Smalltalk: syntax that involves preprocessing, expressions, function declarations, and function calls is inherited from C, while the syntax for object-oriented features was originally taken from Smalltalk.
In addition to C++ and Objective-C, Ch, Cilk and Unified Parallel C are nearly supersets of C.



Declaration (computer programming) - Wikipedia
In computer programming, a declaration is a language construct that specifies properties of an identifier: it declares what a word (identifier) "means".[1] Declarations are most commonly used for functions, variables, constants, and classes, but can also be used for other entities such as enumerations and type definitions.[1] Beyond the name (the identifier itself) and the kind of entity (function, variable, etc.), declarations typically specify the data type (for variables and constants), or the type signature (for functions); types may also include dimensions, such as for arrays. A declaration is used to announce the existence of the entity to the compiler; this is important in those strongly typed languages that require functions, variables, and constants, and their types to be specified with a declaration before use, and is used in forward declaration.[2] The term "declaration" is frequently contrasted with the term "definition",[1] but meaning and usage varies significantly between languages; see below.
Declarations are particularly prominent in languages in the ALGOL tradition, including the BCPL family, most prominently C and C++, and also Pascal. Java uses the term "declaration", though Java does not have separate declarations and definitions.
A basic dichotomy is whether a declaration contains a definition or not: for example, whether a declaration of a constant or variable specifies the value of the constant (respectively, initial value of a variable), or only its type; and similarly whether a declaration of a function specifies the body (implementation) of the function, or only its type signature.[1] Not all languages make this distinction: in many languages, declarations always include a definition, and may be referred to as either "declarations" or "definitions", depending on the language.[a] However, these concepts are distinguished in languages that require declaration before use (for which forward declarations are used), and in languages where interface and implementation are separated: the interface contains declarations, the implementation contains definitions.[b]
In informal usage, a "declaration" refers only to a pure declaration (types only, no value or body), while a "definition" refers to a declaration that includes a value or body. However, in formal usage (in language specifications), "declaration" includes both of these senses, with finer distinctions by language: in C and C++, a declaration of a function that does not include a body is called a function prototype, while a declaration of a function that does include a body is called a "function definition". By contrast in Java declarations always include the body, and the word "definition" has no technical meaning in Java.
In the C-family of programming languages, declarations are often collected into header files, which are included in other source files that reference and use these declarations, but don't have access to the definition. The information in the header file provides the interface between code that uses the declaration and that which defines it, a form of information hiding. A declaration is often used in order to access functions or variables defined in different source files, or in a library. A mismatch between the definition type and the declaration type generates a compiler error.
For variables, definitions assign values to an area of memory that was reserved during the declaration phase. For functions, definitions supply the function body. While a variable or function may be declared many times, it is typically defined once (in C++, this is known as the One Definition Rule or ODR).
Dynamic languages such as JavaScript or Python generally allow functions to be redefined, that is, re-bound; a function is a variable much like any other, with a name and a value (the definition).
Here are some examples of declarations that are not definitions, in C:
Here are some examples of declarations that are definitions, again in C:
In some programming languages, an implicit declaration is provided the first time such a variable is encountered at compile time.  In other languages, such a usage is considered to be an error, which may resulting in a diagnostic message. Some languages have started out with the implicit declaration behavior, but as they matured they provided an option to disable it (e.g. Perl's "use strict" or Visual Basic's "Option Explicit").



Chomsky hierarchy - Wikipedia
In the formal languages of computer science and linguistics, the Chomsky hierarchy (occasionally referred to as Chomsky–Schützenberger hierarchy) is a containment hierarchy of classes of formal grammars.
This hierarchy of grammars was described by Noam Chomsky in 1956.[1] It is also named after Marcel-Paul Schützenberger, who played a crucial role in the development of the theory of formal languages.
A formal grammar of this type consists of a finite set of production rules (left-hand side → right-hand side), where each side consists of a finite sequence of the following symbols:
A formal grammar provides an axiom schema for (or generates) a formal language, which is a (usually infinite) set of finite-length sequences of symbols  that may be constructed by applying production rules to another sequence of symbols (which initially contains just the start symbol).  A rule may be applied by replacing an occurrence of the symbols on its left-hand side with those that appear on its right-hand side.  A sequence of rule applications is called a derivation.  Such a grammar defines the formal language: all words consisting solely of terminal symbols which can be reached by a derivation from the start symbol.
Nonterminals are often represented by uppercase letters, terminals by lowercase letters, and the start symbol by S.  For example, the grammar with terminals {a, b}, nonterminals {S, A, B}, production rules
and start symbol S, defines the language of all words of the form 




a

n



b

n




{\displaystyle a^{n}b^{n}}

 (i.e. n copies of a followed by n copies of b).
The following is a simpler grammar that defines the same language: 
Terminals {a, b}, Nonterminals {S}, Start symbol S, Production rules
As another example, a grammar for a toy subset of English language is given by:
and start symbol SENTENCE. An example derivation is
Other sequences that can be derived from this grammar are: "ideas hate great linguists", and "ideas generate". While these sentences are nonsensical, they are syntactically correct. A syntactically incorrect sentence ( e.g. "ideas ideas great hate") cannot be derived from this grammar. See "Colorless green ideas sleep furiously" for a similar example given by Chomsky in 1957; see Phrase structure grammar and Phrase structure rules for more natural language examples and the problems of formal grammar in that area.
The following table summarizes each of Chomsky's four types of grammars, the class of language it generates, the type of automaton that recognizes it, and the form its rules must have.





a



{\displaystyle {\text{a}}}

 = terminal




α


{\displaystyle \alpha }

 = string of terminals, non-terminals, or empty




β


{\displaystyle \beta }

 = string of terminals, non-terminals, or empty




γ


{\displaystyle \gamma }

 = string of terminals, non-terminals, never empty




A


{\displaystyle A}

 = non-terminal




B


{\displaystyle B}

 = non-terminal
Note that the set of grammars corresponding to recursive languages is not a member of this hierarchy; these would be properly between Type-0 and Type-1.
Every regular language is context-free, every context-free language is context-sensitive, every context-sensitive language is recursive and every recursive language is recursively enumerable. These are all proper inclusions, meaning that there exist recursively enumerable languages that are not context-sensitive, context-sensitive languages that are not context-free and context-free languages that are not regular.
Type-0 grammars include all formal grammars. They generate exactly all languages that can be recognized by a Turing machine. These languages are also known as the recursively enumerable or Turing-recognizable languages.[2]  Note that this is different from the recursive languages, which can be decided by an always-halting Turing machine.
Type-1 grammars generate context-sensitive languages.  These grammars have rules of the form 



α
A
β
→
α
γ
β


{\displaystyle \alpha A\beta \rightarrow \alpha \gamma \beta }

 with 



A


{\displaystyle A}

 a nonterminal and 



α


{\displaystyle \alpha }

, 



β


{\displaystyle \beta }

 and 



γ


{\displaystyle \gamma }

 strings of terminals and/or nonterminals. The strings 



α


{\displaystyle \alpha }

 and 



β


{\displaystyle \beta }

 may be empty, but 



γ


{\displaystyle \gamma }

 must be nonempty.  The rule 



S
→
ϵ


{\displaystyle S\rightarrow \epsilon }

 is allowed if 



S


{\displaystyle S}

 does not appear on the right side of any rule.  The languages described by these grammars are exactly all languages that can be recognized by a linear bounded automaton (a nondeterministic Turing machine whose tape is bounded by a constant times the length of the input.)
Type-2 grammars generate the context-free languages. These are defined by rules of the form 



A
→
γ


{\displaystyle A\rightarrow \gamma }

 with 



A


{\displaystyle A}

 being a nonterminal and 



γ


{\displaystyle \gamma }

 being a string of terminals and/or nonterminals. These languages are exactly all languages that can be recognized by a non-deterministic pushdown automaton. Context-free languages—or rather its subset of deterministic context-free language—are the theoretical basis for the phrase structure of most programming languages, though their syntax also includes context-sensitive name resolution due to declarations and scope. Often a subset of grammars is used to make parsing easier, such as by an LL parser.
Type-3 grammars generate the regular languages.  Such a grammar restricts its rules to a single nonterminal on the left-hand side and a right-hand side consisting of a single terminal, possibly followed by a single nonterminal (right regular). Alternatively, the right-hand side of the grammar can consist of a single terminal, possibly preceded by a single nonterminal (left regular). These generate the same languages. However, if left-regular rules and right-regular rules are combined, the language need no longer be regular. The rule 



S
→
ϵ


{\displaystyle S\rightarrow \epsilon }

 is also allowed here if 



S


{\displaystyle S}

 does not appear on the right side of any rule.  These languages are exactly all languages that can be decided by a finite state automaton. Additionally, this family of formal languages can be obtained by regular expressions. Regular languages are commonly used to define search patterns and the lexical structure of programming languages.



Semantics - Wikipedia
Semantics (from Ancient Greek: σημαντικός sēmantikós, "significant")[1][2] is the linguistic and philosophical study of meaning, in language, programming languages, formal logics, and semiotics. It is concerned with the relationship between signifiers—like words, phrases, signs, and symbols—and what they stand for, their denotation.
In international scientific vocabulary semantics is also called semasiology. The word semantics was first used by Michel Bréal, a French philologist.[3] It denotes a range of ideas—from the popular to the highly technical. It is often used in ordinary language for denoting a problem of understanding that comes down to word selection or connotation. This problem of understanding has been the subject of many formal enquiries, over a long period of time, especially in the field of formal semantics. In linguistics, it is the study of the interpretation of signs or symbols used in agents or communities within particular circumstances and contexts.[4] Within this view, sounds, facial expressions, body language, and proxemics have semantic (meaningful) content, and each comprises several branches of study. In written language, things like paragraph structure and punctuation bear semantic content; other forms of language bear other semantic content.[4]
The formal study of semantics intersects with many other fields of inquiry, including lexicology, syntax, pragmatics, etymology and others. Independently, semantics is also a well-defined field in its own right, often with synthetic properties.[5] In the philosophy of language, semantics and reference are closely connected. Further related fields include philology, communication, and semiotics. The formal study of semantics can therefore be manifold and complex.
Semantics contrasts with syntax, the study of the combinatorics of units of a language (without reference to their meaning), and pragmatics, the study of the relationships between the symbols of a language, their meaning, and the users of the language.[6] Semantics as a field of study also has significant ties to various representational theories of meaning including truth theories of meaning, coherence theories of meaning, and correspondence theories of meaning. Each of these is related to the general philosophical study of reality and the representation of meaning. In 1960s psychosemantic studies became popular after Osgood's massive cross-cultural studies using his semantic differential (SD) method that used thousands of nouns and adjective bipolar scales. A specific form of the SD, Projective Semantics method[7] uses only most common and neutral nouns that correspond to the 7 groups (factors) of adjective-scales most consistently found in cross-cultural studies (Evaluation, Potency, Activity as found by Osgood, and Reality, Organization, Complexity, Limitation as found in other studies). In this method, seven groups of bipolar adjective scales corresponded to seven types of nouns so the method was thought to have the object-scale symmetry (OSS) between the scales and nouns for evaluation using these scales. For example, the nouns corresponding to the listed 7 factors would be: Beauty, Power, Motion, Life, Work, Chaos, Law. Beauty was expected to be assessed unequivocally as “very good” on adjectives of Evaluation-related scales, Life as “very real” on Reality-related scales, etc. However, deviations in this symmetric and very basic matrix might show underlying biases of two types: scales-related bias and objects-related bias. This OSS design meant to increase the sensitivity of the SD method to any semantic biases in responses of people within the same culture and educational background.[8][9]
In linguistics, semantics is the subfield that is devoted to the study of meaning, as inherent at the levels of words, phrases, sentences, and larger units of discourse (termed texts, or narratives). The study of semantics is also closely linked to the subjects of representation, reference and denotation. The basic study of semantics is oriented to the examination of the meaning of signs, and the study of relations between different linguistic units and compounds: homonymy, synonymy, antonymy, hypernymy, hyponymy, meronymy, metonymy, holonymy, paronyms. A key concern is how meaning attaches to larger chunks of text, possibly as a result of the composition from smaller units of meaning. Traditionally, semantics has included the study of sense and denotative reference, truth conditions, argument structure, thematic roles, discourse analysis, and the linkage of all of these to syntax.
In the late 1960s, Richard Montague proposed a system for defining semantic entries in the lexicon in terms of the lambda calculus. In these terms, the syntactic parse of the sentence John ate every bagel would consist of a subject (John) and a predicate (ate every bagel); Montague demonstrated that the meaning of the sentence altogether could be decomposed into the meanings of its parts and in relatively few rules of combination. The logical predicate thus obtained would be elaborated further, e.g. using truth theory models, which ultimately relate meanings to a set of Tarskian universals, which may lie outside the logic. The notion of such meaning atoms or primitives is basic to the language of thought hypothesis from the 1970s.
Despite its elegance, Montague grammar was limited by the context-dependent variability in word sense, and led to several attempts at incorporating context, such as:
In Chomskyan linguistics there was no mechanism for the learning of semantic relations, and the nativist view considered all semantic notions as inborn. Thus, even novel concepts were proposed to have been dormant in some sense. This view was also thought unable to address many issues such as metaphor or associative meanings, and semantic change, where meanings within a linguistic community change over time, and qualia or subjective experience. Another issue not addressed by the nativist model was how perceptual cues are combined in thought, e.g. in mental rotation.[10]
This view of semantics, as an innate finite meaning inherent in a lexical unit that can be composed to generate meanings for larger chunks of discourse, is now being fiercely debated in the emerging domain of cognitive linguistics[11]
and also in the non-Fodorian camp in philosophy of language.[12]
The main challenge is motivated by:
A concrete example of the latter phenomenon is semantic underspecification – meanings are not complete without some elements of context. To take an example of one word, red, its meaning in a phrase such as red book is similar to many other usages, and can be viewed as compositional.[13] However, the colours implied in phrases such as red wine (very dark), and red hair (coppery), or red soil, or red skin are very different. Indeed, these colours by themselves would not be called red by native speakers. These instances are contrastive, so red wine is so called only in comparison with the other kind of wine (which also is not white for the same reasons). This view goes back to de Saussure:
Each of a set of synonyms like redouter ('to dread'), craindre ('to fear'), avoir peur ('to be afraid') has its particular value only because they stand in contrast with one another. No word has a value that can be identified independently of what else is in its vicinity.[14]
and may go back to earlier Indian views on language, especially the Nyaya view of words as indicators and not carriers of meaning.[15]
An attempt to defend a system based on propositional meaning for semantic underspecification can be found in the generative lexicon model of James Pustejovsky, who extends contextual operations (based on type shifting) into the lexicon. Thus meanings are generated "on the fly" (as you go), based on finite context.
Another set of concepts related to fuzziness in semantics is based on prototypes. The work of Eleanor Rosch in the 1970s led to a view that natural categories are not characterizable in terms of necessary and sufficient conditions, but are graded (fuzzy at their boundaries) and inconsistent as to the status of their constituent members. One may compare it with Jung's archetype, though the concept of archetype sticks to static concept. Some post-structuralists are against the fixed or static meaning of the words. Derrida, following Nietzsche, talked about slippages in fixed meanings.
Systems of categories are not objectively out there in the world but are rooted in people's experience. These categories evolve as learned concepts of the world – meaning is not an objective truth, but a subjective construct, learned from experience, and language arises out of the "grounding of our conceptual systems in shared embodiment and bodily experience".[16]
A corollary of this is that the conceptual categories (i.e. the lexicon) will not be identical for different cultures, or indeed, for every individual in the same culture. This leads to another debate (see the Sapir–Whorf hypothesis or Eskimo words for snow).
Originates from Montague's work (see above). A highly formalized theory of natural language semantics in which expressions are assigned denotations (meanings) such as individuals, truth values, or functions from one of these to another. The truth of a sentence, and its logical relation to other sentences, is then evaluated relative to a model.
Pioneered by the philosopher Donald Davidson, another formalized theory, which aims to associate each natural language sentence with a meta-language description of the conditions under which it is true, for example: 'Snow is white' is true if and only if snow is white. The challenge is to arrive at the truth conditions for any sentences from fixed meanings assigned to the individual words and fixed rules for how to combine them. In practice, truth-conditional semantics is similar to model-theoretic semantics; conceptually, however, they differ in that truth-conditional semantics seeks to connect language with statements about the real world (in the form of meta-language statements), rather than with abstract models.
This theory is an effort to explain properties of argument structure. The assumption behind this theory is that syntactic properties of phrases reflect the meanings of the words that head them.[17] With this theory, linguists can better deal with the fact that subtle differences in word meaning correlate with other differences in the syntactic structure that the word appears in.[17] The way this is gone about is by looking at the internal structure of words.[18] These small parts that make up the internal structure of words are termed semantic primitives.[18]
A linguistic theory that investigates word meaning. This theory understands that the meaning of a word is fully reflected by its context. Here, the meaning of a word is constituted by its contextual relations.[19] Therefore, a distinction between degrees of participation as well as modes of participation are made.[19] In order to accomplish this distinction any part of a sentence that bears a meaning and combines with the meanings of other constituents is labeled as a semantic constituent. Semantic constituents that cannot be broken down into more elementary constituents are labeled minimal semantic constituents.[19]
Computational semantics is focused on the processing of linguistic meaning. In order to do this concrete algorithms and architectures are described. Within this framework the algorithms and architectures are also analyzed in terms of decidability, time/space complexity, data structures that they require and communication protocols.[20]
In computer science, the term semantics refers to the meaning of language constructs, as opposed to their form (syntax). According to Euzenat, semantics "provides the rules for interpreting the syntax which do not provide the meaning directly but constrains the possible interpretations of what is declared."[21] In ontology engineering, the term semantics refers to the meaning of concepts, properties, and relationships that formally represent real-world entities, events, and scenes in a logical underpinning, such as a description logic, and typically implemented in the Web Ontology Language. The meaning of description logic concepts and roles is defined by their model-theoretic semantics, which are based on interpretations.[22] The concepts, properties, and relationships defined in OWL ontologies can be deployed directly in the web site markup as RDFa, HTML5 Microdata, or JSON-LD, in graph databases as RDF triples or quads, and dereferenced in LOD datasets.
The semantics of programming languages and other languages is an important issue and area of study in computer science. Like the syntax of a language, its semantics can be defined exactly.
For instance, the following statements use different syntaxes, but cause the same instructions to be executed, namely, perform an arithmetical addition of 'y' to 'x' and store the result in a variable called 'x':
Various ways have been developed to describe the semantics of programming languages formally, building on mathematical logic:[23]
The Semantic Web refers to the extension of the World Wide Web via embedding added semantic metadata, using semantic data modeling techniques such as Resource Description Framework (RDF) and Web Ontology Language (OWL).
On the Semantic Web, terms such as semantic network and semantic data model are used to describe particular types of data model characterized by the use of directed graphs in which the vertices denote concepts or entities in the world and their properties, and the arcs denote relationships between them. These can formally be described as description logic concepts and roles, which correspond to OWL classes and properties.[22]
In psychology, semantic memory is memory for meaning – in other words, the aspect of memory that preserves only the gist, the general significance, of remembered experience – while episodic memory is memory for the ephemeral details – the individual features, or the unique particulars of experience. The term 'episodic memory' was introduced by Tulving and Schacter in the context of 'declarative memory' which involved simple association of factual or objective information concerning its object. Word meaning is measured by the company they keep, i.e. the relationships among words themselves in a semantic network. The memories may be transferred intergenerationally or isolated in one generation due to a cultural disruption. Different generations may have different experiences at similar points in their own time-lines. This may then create a vertically heterogeneous semantic net for certain words in an otherwise homogeneous culture.[24] In a network created by people analyzing their understanding of the word (such as Wordnet) the links and decomposition structures of the network are few in number and kind, and include part of, kind of, and similar links. In automated ontologies the links are computed vectors without explicit meaning. Various automated technologies are being developed to compute the meaning of words: latent semantic indexing and support vector machines as well as natural language processing, artificial neural networks and predicate calculus techniques.
Ideasthesia is a psychological phenomenon in which activation of concepts evokes sensory experiences. For example, in synesthesia, activation of a concept of a letter (e.g., that of the letter A) evokes sensory-like experiences (e.g., of red color).



Evaluation strategy - Wikipedia
Evaluation strategies are used by programming languages to determine when to evaluate the argument(s) of a function call (for function, also read: operation, method, or relation) and what kind of value to pass to the function. For example, call by value/call by reference specifies that a function application evaluates the argument before it proceeds to the evaluation of the function's body and that it passes two capabilities to the function, namely, the ability to look up the current value of the argument and to modify it via an assignment statement.[1] The notion of reduction strategy in lambda calculus is similar but distinct.
In practical terms, many modern programming languages have converged on a call-by-value/call-by-reference[clarification needed] evaluation strategy for function calls (C#, Java). Some languages, especially lower-level languages such as C++, combine several notions of parameter passing. Historically, call by value and call by name date back to ALGOL 60, a language designed in the late 1950s. Call by reference is used by PL/I and some Fortran systems.[2] Purely functional languages like Haskell, as well as non-purely functional languages like R, use call by need.
The evaluation strategy is specified by the programming language definition, and is not a function of any specific implementation.
In strict evaluation, the arguments to a function are always evaluated completely before the function is applied.
Under Church encoding, eager evaluation of operators maps to strict evaluation of functions; for this reason, strict evaluation is sometimes called "eager". Most existing programming languages use strict evaluation for functions.
Applicative order (or leftmost innermost[3][4]) evaluation refers to an evaluation strategy in which the arguments of a function are evaluated from left to right in a post-order traversal of reducible expressions (redexes). Applicative order is a call-by-value evaluation.
Call by value (also referred to as pass by value) is the most common evaluation strategy, used in languages as different as C and Scheme. In call by value, the argument expression is evaluated, and the resulting value is bound to the corresponding variable in the function (frequently by copying the value into a new memory region). If the function or procedure is able to assign values to its parameters, only its local variable is assigned—that is, anything passed into a function call is unchanged in the caller's scope when the function returns.
Call by value is not a single evaluation strategy, but rather the family of evaluation strategies in which a function's argument is evaluated before being passed to the function. While many programming languages (such as Common Lisp, Eiffel and Java) that use call by value evaluate function arguments left-to-right, some evaluate functions and their arguments right-to-left, and others (such as Scheme, OCaml and C) leave the order unspecified.
In some cases, the term "call by value" is problematic, as the value which is passed is not the value of the variable as understood by the ordinary meaning of value, but an implementation-specific reference to the value. The effect is that what syntactically looks like call by value may end up rather behaving like call by reference or call by sharing, often depending on very subtle aspects of the language semantics.
The reason for passing a reference is often that the language technically does not provide a value representation of complicated data, but instead represents them as a data structure while preserving some semblance of value appearance in the source code. Exactly where the boundary is drawn between proper values and data structures masquerading as such is often hard to predict. In C, an array (of which strings are special cases) is a data structure but the name of an array is treated as (has as value) the reference to the first element of the array, while a struct variable's name refers to a value even if it has fields that are vectors. In Maple, a vector is a special case of a table and therefore a data structure, but a list (which gets rendered and can be indexed in exactly the same way) is a value. In Tcl, values are "dual-ported" such that the value representation is used at the script level, and the language itself manages the corresponding data structure, if one is required. Modifications made via the data structure are reflected back to the value representation, and vice versa.
The description "call by value where the value is a reference" is common (but should not be understood as being call by reference); another term is call by sharing. Thus the behaviour of call by value Java or Visual Basic and call by value C or Pascal are significantly different: in C or Pascal, calling a function with a large structure as an argument will cause the entire structure to be copied (except if it's actually a reference to a structure), potentially causing serious performance degradation, and mutations to the structure are invisible to the caller. However, in Java or Visual Basic only the reference to the structure is copied, which is fast, and mutations to the structure are visible to the caller.
Call by reference (also referred to as pass by reference) is an evaluation strategy where a function receives an implicit reference to a variable used as argument, rather than a copy of its value.
This typically means that the function can modify (i.e. assign to) the variable used as argument—something that will be seen by its caller. Call by reference can therefore be used to provide an additional channel of communication between the called function and the calling function. A call-by-reference language makes it more difficult for a programmer to track the effects of a function call, and may introduce subtle bugs.
Many languages support call by reference in some form or another, but comparatively few use it as a default. FORTRAN II is an early example of a call-by-reference language. A few languages, such as C++, PHP, Visual Basic .NET, C# and REALbasic, default to call by value, but offer special syntax for call-by-reference parameters. C++ additionally offers call by reference to const. Rust also offers call by reference, but defaults to immutable (const) references[5]. Mutable references have a similar syntax to immutable references.

Call by reference can be simulated in languages that use call by value and don't exactly support call by reference, by making use of references (objects that refer to other objects), such as pointers (objects representing the memory addresses of other objects). Languages such as C and ML use this technique. It is not a separate evaluation strategy—the language calls by value—but sometimes it is referred to as call by address (also referred to as pass by address). In ML references are type- and memory- safe.
A similar effect is achieved by call by sharing (passing an object, which can then be mutated), used in languages like Java, Python and Ruby.
In purely functional languages there is typically no semantic difference between the two strategies (since their data structures are immutable, so there is no possibility for a function to modify any of its arguments), so they are typically described as call by value even though implementations frequently use call by reference internally for the efficiency benefits.
Following is an example that demonstrates call by reference in E:
Following is an example of call by address that simulates call by reference in C:
Call by sharing (also referred to as call by object or call by object-sharing) is an evaluation strategy first named by Barbara Liskov et al. for the language CLU in 1974.[6] It is used by languages such as Python,[7] Iota,[8] Java (for object references), Ruby, JavaScript, Scheme, OCaml, AppleScript, and many others. However, the term "call by sharing" is not in common use; the terminology is inconsistent across different sources. For example, in the Java community, they say that Java is call by value.[9] Call by sharing implies that values in the language are based on objects rather than primitive types, i.e. that all values are "boxed".
The semantics of call by sharing differ from call by reference: "In particular it is not call by value because mutations of arguments performed by the called routine will be visible to the caller. And it is not call by reference because access is not given to the variables of the caller, but merely to certain objects"[10]. So e.g. if a variable was passed, it is not possible to simulate an assignment on that variable in the callee's scope[11]. However, since the function has access to the same object as the caller (no copy is made), mutations to those objects, if the objects are mutable, within the function are visible to the caller, which may appear to differ from call by value semantics. Mutations of a mutable object within the function are visible to the caller because the object is not copied or cloned — it is shared. For example, in Python, lists are mutable, so:
outputs [1] because the append method modifies the object on which it is called.
Assignments within a function are not noticeable to the caller, because, in these languages, passing the variable only means passing (access to) the actual object referred to by the variable, not access to the original (caller's) variable. Since the rebound variable only exists within the scope of the function, the counterpart in the caller retains its original binding. Compare the Python mutation above with this code that binds the formal argument to a new object:
outputs [], because the statement l = [1] reassigns a new list to the variable rather than to the location it references.
For immutable objects, there is no real difference between call by sharing and call by value, except if object identity is visible in the language. The use of call by sharing with mutable objects is an alternative to input/output parameters:[12] the parameter is not assigned to (the argument is not overwritten and object identity is not changed), but the object (argument) is mutated.
Although this term has widespread usage in the Python community, identical semantics in other languages such as Java and Visual Basic are often described as call by value, where the value is implied to be a reference to the object.[citation needed]
Call by copy-restore (also referred to as copy-in copy-out, call by value result or call by value return—as termed in the Fortran community) is a special case of call by reference where the provided reference is unique to the caller. This variant has gained attention in multiprocessing contexts and Remote procedure call[13]: if a parameter to a function call is a reference that might be accessible by another thread of execution, its contents may be copied to a new reference that is not; when the function call returns, the updated contents of this new reference are copied back to the original reference ("restored"). 
The semantics of call by copy-restore also differ from those of call by reference where two or more function arguments alias one another; that is, point to the same variable in the caller's environment. Under call by reference, writing to one will affect the other; call by copy-restore avoids this by giving the function distinct copies, but leaves the result in the caller's environment undefined depending on which of the aliased arguments is copied back first—will the copies be made in left-to-right order both on entry and on return?
When the reference is passed to the callee uninitialized, this evaluation strategy may be called call by result.
In partial evaluation, evaluation may continue into the body of a function that has not been applied. Any sub-expressions that do not contain unbound variables are evaluated, and function applications whose argument values are known may be reduced. In the presence of side-effects, complete partial evaluation may produce unintended results; for this reason, systems that support partial evaluation tend to do so only for "pure" expressions (expressions without side-effects) within functions.
In non-strict evaluation, arguments to a function are not evaluated unless they are actually used in the evaluation of the function body.
Under Church encoding, lazy evaluation of operators maps to non-strict evaluation of functions; for this reason, non-strict evaluation is often referred to as "lazy". Boolean expressions in many languages use a form of non-strict evaluation called short-circuit evaluation, where evaluation returns as soon as it can be determined that an unambiguous Boolean will result—for example, in a disjunctive expression where true is encountered, or in a conjunctive expression where false is encountered, and so forth. Conditional expressions also usually use lazy evaluation, where evaluation returns as soon as an unambiguous branch will result.
Normal-order (or leftmost outermost) evaluation is the evaluation strategy where the outermost redex is always reduced, applying functions before evaluating function arguments.
In contrast, call by name does not evaluate inside the body of an unapplied function.
Call by name is an evaluation strategy where the arguments to a function are not evaluated before the function is called—rather, they are substituted directly into the function body (using capture-avoiding substitution) and then left to be evaluated whenever they appear in the function. If an argument is not used in the function body, the argument is never evaluated; if it is used several times, it is re-evaluated each time it appears. (See Jensen's Device.)
Call-by-name evaluation is occasionally preferable to call-by-value evaluation. If a function's argument is not used in the function, call by name will save time by not evaluating the argument, whereas call by value will evaluate it regardless. If the argument is a non-terminating computation, the advantage is enormous. However, when the function argument is used, call by name is often slower, requiring a mechanism such as a thunk.
An early use was ALGOL 60. Today's .NET languages can simulate call by name using delegates or Expression<T> parameters. The latter results in an abstract syntax tree being given to the function. Eiffel provides agents, which represent an operation to be evaluated when needed. Seed7 provides call by name with function parameters. Java programs can accomplish similar lazy evaluation using lambda expressions and the java.util.function.Supplier<T> interface.
Call by need is a memoized variant of call by name where, if the function argument is evaluated, that value is stored for subsequent uses. If the argument is side-effect free, this produces the same results as call by name, saving the cost of recomputing the argument.
Haskell is a well-known language that uses call-by-need evaluation. Because evaluation of expressions may happen arbitrarily far into a computation, Haskell only supports side-effects (such as mutation) via the use of monads. This eliminates any unexpected behavior from variables whose values change prior to their delayed evaluation.
In R, all arguments are passed by call-by-need. R allows arbitrary side-effects in call-by-need arguments.
Lazy evaluation is the most commonly used implementation strategy for call-by-need semantics, but variations exist—for instance optimistic evaluation.
.NET languages implement call by need using the type Lazy<T>.
Call by macro expansion is similar to call by name, but uses textual substitution rather than capture-avoiding substitution. With uncautious use, macro substitution may result in variable capture and lead to undesired behavior. Hygienic macros avoid this problem by checking for and replacing shadowed variables that are not parameters.
Under full β-reduction, any function application may be reduced (substituting the function's argument into the function using capture-avoiding substitution) at any time. This may be done even within the body of an unapplied function.
Call by future (also referred to as parallel call by name) is a concurrent evaluation strategy where the value of a future expression is computed concurrently with the flow of the rest of the program by one or more promises. When the value of the future is needed, the main program blocks until the future has a value (the promise or one of the promises finishes computing, if it has not already completed by then).
This strategy is non-deterministic, as the evaluation can occur at any time between creation of the future (i.e., when the expression is given) and use of the future's value. It is similar to call by need in that the value is only computed once, and computation may be deferred until the value is needed, but it may be started before. Further, if the value of a future is not needed, such as if it is a local variable in a function that returns, the computation may be terminated part-way through.
If implemented with processes or threads, creating a future will spawn one or more new processes or threads (for the promises), accessing the value will synchronize these with the main thread, and terminating the computation of the future corresponds to killing the promises computing its value.
If implemented with a coroutine, as in .NET async/await, creating a future calls a coroutine (an async function), which may yield to the caller, and in turn be yielded back to when the value is used, cooperatively multitasking.
Optimistic evaluation[14] is another variant of call by need in which the function's argument is partially evaluated for some amount of time (which may be adjusted at runtime), after which evaluation is aborted and the function is applied using call by need. This approach avoids some of the runtime expense of call by need, while still retaining the desired termination characteristics.



Decidability (logic) - Wikipedia
In logic, the term decidable refers to the decision problem, the question of the existence of an effective method for determining membership in a set of formulas, or, more precisely, an algorithm that can and will return a boolean true or false value that is correct (instead of looping indefinitely, crashing, returning "don't know" or returning a wrong answer). Logical systems such as propositional logic are decidable if membership in their set of logically valid formulas (or theorems) can be effectively determined. A theory (set of sentences closed under logical consequence) in a fixed logical system is decidable if there is an effective method for determining whether arbitrary formulas are included in the theory. Many important problems are undecidable, that is, it has been proven that no effective method for determining membership (returning a correct answer after finite, though possibly very long, time in all cases) can exist for them.
As with the concept of a decidable set, the definition of a decidable theory or logical system can be given either in terms of effective methods or in terms of computable functions. These are generally considered equivalent per Church's thesis. Indeed, the proof that a logical system or theory is undecidable will use the formal definition of computability to show that an appropriate set is not a decidable set, and then invoke Church's thesis to show that the theory or logical system is not decidable by any effective method (Enderton 2001, pp. 206ff.).
Each logical system comes with both a syntactic component, which among other things determines the notion of provability, and a semantic component, which determines the notion of logical validity. The logically valid formulas of a system are sometimes called the theorems of the system, especially in the context of first-order logic where Gödel's completeness theorem establishes the equivalence of semantic and syntactic consequence. In other settings, such as linear logic, the syntactic consequence (provability) relation may be used to define the theorems of a system.
A logical system is decidable if there is an effective method for determining whether arbitrary formulas are theorems of the logical system. For example, propositional logic is decidable, because the truth-table method can be used to determine whether an arbitrary propositional formula is logically valid.
First-order logic is not decidable in general; in particular, the set of logical validities in any signature that includes equality and at least one other predicate with two or more arguments is not decidable.[1] Logical systems extending first-order logic, such as second-order logic and type theory, are also undecidable.
The validities of monadic predicate calculus with identity are decidable, however. This system is first-order logic restricted to signatures that have no function symbols and whose relation symbols other than equality never take more than one argument.
Some logical systems are not adequately represented by the set of theorems alone. (For example, Kleene's logic has no theorems at all.) In such cases, alternative definitions of decidability of a logical system are often used, which ask for an effective method for determining something more general than just validity of formulas; for instance, validity of sequents, or the consequence relation {(Г, A) | Г ⊧ A} of the logic.
A theory is a set of formulas, which here is assumed to be closed under logical consequence. The question of decidability for a theory is whether there is an effective procedure that, given an arbitrary formula in the signature of the theory, decides whether the formula is a member of the theory or not. This problem arises naturally when a theory is defined as the set of logical consequences of a fixed set of axioms. Examples of decidable first-order theories include the theory of real closed fields, and Presburger arithmetic, while the theory of groups and Robinson arithmetic are examples of undecidable theories.
There are several basic results about decidability of theories. Every inconsistent theory is decidable, as every formula in the signature of the theory will be a logical consequence of, and thus a member of, the theory. Every complete recursively enumerable first-order theory is decidable. An extension of a decidable theory may not be decidable. For example, there are undecidable theories in propositional logic, although the set of validities (the smallest theory) is decidable.
A consistent theory that has the property that every consistent extension is undecidable is said to be essentially undecidable. In fact, every consistent extension will be essentially undecidable. The theory of fields is undecidable but not essentially undecidable. Robinson arithmetic is known to be essentially undecidable, and thus every consistent theory that includes or interprets Robinson arithmetic is also (essentially) undecidable.
Some decidable theories include (Monk 1976, p. 234):[2]
Methods used to establish decidability include quantifier elimination, model completeness, and Vaught's test.
Some games have been classified as to their decidability:
Some undecidable theories include (Monk 1976, p. 279):[2]
The interpretability method is often used to establish undecidability of theories. If an essentially undecidable theory T is interpretable in a consistent theory S, then S is also essentially undecidable. This is closely related to the concept of a many-one reduction in computability theory.
A property of a theory or logical system weaker than decidability is semidecidability.  A theory is semidecidable if there is an effective method which, given an arbitrary formula, will always tell correctly when the formula is in the theory, but may give either a negative answer or no answer at all when the formula is not in the theory. A logical system is semidecidable if there is an effective method for generating theorems (and only theorems) such that every theorem will eventually be generated. This is different from decidability because in a semidecidable system there may be no effective procedure for checking that a formula is not a theorem.
Every decidable theory or logical system is semidecidable, but in general the converse is not true; a theory is decidable if and only if both it and its complement are semi-decidable. For example, the  set of logical validities V of first-order logic is semi-decidable, but not decidable. In this case, it is because there is no effective method for determining for an arbitrary formula A whether A is not in V. Similarly, the set of logical consequences of any recursively enumerable set of first-order axioms is semidecidable. Many of the examples of undecidable first-order theories given above are of this form.
Decidability should not be confused with completeness. For example, the theory of algebraically closed fields is decidable but incomplete, whereas the set of all true first-order statements about nonnegative integers in the language with + and × is complete but undecidable. 
Unfortunately, as a terminological ambiguity, the term "undecidable statement" is sometimes used as a synonym for independent statement.



Rexx - Wikipedia
Rexx (Restructured Extended Executor) is an interpreted programming language developed at IBM by Mike Cowlishaw.[2][3] It is a structured, high-level programming language designed for ease of learning and reading. Proprietary and open source Rexx interpreters exist for a wide range of computing platforms; compilers exist for IBM mainframe computers.[4]
Rexx is used as a scripting and macro language, and is often used for processing data and text and generating reports; these similarities with Perl mean that Rexx works well in Common Gateway Interface (CGI) programming and it is indeed used for this purpose. Rexx is the primary scripting language in some operating systems, e.g. OS/2, MVS, VM, AmigaOS, and is also used as an internal macro language in some other software, such as SPFPC, KEDIT, THE and the ZOC terminal emulator. Additionally, the Rexx language can be used for scripting and macros in any program that uses Windows Scripting Host ActiveX scripting engines languages (e.g. VBScript and JScript) if one of the Rexx engines is installed.
Rexx is supplied with VM/SP on up, TSO/E Version 2 on up, OS/2 (1.3 and later, where it is officially named Procedures Language/2), AmigaOS Version 2 on up, PC DOS (7.0 or 2000), and Windows NT 4.0 (Resource Kit: Regina). REXX scripts for OS/2 share the filename extension .cmd with other scripting languages, and the first line of the script specifies the interpreter to be used. REXX macros for REXX-aware applications use extensions determined by the application. In the late 1980s Rexx became the common scripting language for IBM Systems Application Architecture, where it was renamed "SAA Procedure Language REXX".
A Rexx script or command is sometimes referred to as an EXEC[5] in a nod to Rexx's role as a replacement for the older EXEC command language on CP/CMS and VM/370 and EXEC 2 command language on VM/SP.
Rexx has the following characteristics and features:
Rexx has just twenty-three, largely self-evident, instructions (such as call, parse, and select) with minimal punctuation and formatting requirements. It is essentially an almost free-form language with only one data-type, the character string; this philosophy means that all data are visible (symbolic) and debugging and tracing are simplified.
Rexx's syntax looks similar to PL/I, but has fewer notations; this makes it harder to parse (by program) but easier to use, except for cases where PL/I habits may lead to surprises. One of the REXX design goals was the principle of least astonishment.[6]
Rexx was designed and first implemented, in assembly language, as an 'own-time' project between 20 March 1979 and mid-1982 by Mike Cowlishaw of IBM, originally as a scripting programming language to replace the languages EXEC and EXEC 2.[2] It was designed to be a macro or scripting language for any system. As such, Rexx is considered a precursor to Tcl and Python. Rexx was also intended by its creator to be a simplified and easier to learn version of the PL/I programming language. However, some differences from PL/I may trip up the unwary.
It was first described in public at the SHARE 56 conference in Houston, Texas, in 1981,[7] where customer reaction, championed by Ted Johnston of SLAC, led to it being shipped as an IBM product in 1982.
Over the years IBM included Rexx in almost all of its operating systems (VM/CMS, MVS TSO/E, AS/400, VSE/ESA, AIX, PC DOS, and OS/2), and has made versions available for Novell NetWare, Windows, Java, and Linux.
The first non-IBM version was written for PC DOS by Charles Daney in 1984/5[3] and marketed by the Mansfield Software Group (founded by Kevin J. Kearney in 1986).[2] The first compiler version appeared in 1987, written for CMS by Lundin and Woodruff.[8] Other versions have also been developed for Atari, AmigaOS, Unix (many variants), Solaris, DEC, Windows, Windows CE, Pocket PC, DOS, Palm OS, QNX, OS/2, Linux, BeOS, EPOC32/Symbian, AtheOS, OpenVMS, Apple Macintosh, and Mac OS X.[9]
The Amiga version of Rexx, called ARexx, was included with AmigaOS 2 onwards and was popular for scripting as well as application control. Many Amiga applications have an "ARexx port" built into them which allows control of the application from Rexx. One single Rexx script could even switch between different Rexx ports in order to control several running applications.
In 1990, Cathie Dager of SLAC organized the first independent Rexx symposium, which led to the forming of the REXX Language Association. Symposia are held annually.
Several freeware versions of Rexx are available. In 1992, the two most widely used open-source ports appeared: Ian Collier's REXX/imc for Unix and Anders Christensen's Regina[1] (later adopted by Mark Hessling) for Windows and Unix. BREXX is well known for WinCE and Pocket PC platforms, and has been "back-ported" to VM/370 and MVS.
OS/2 had a visual development system from Watcom VX-REXX another dialect was VisPro REXX from Hockware. 
Portable Rexx by Kilowatt and Personal Rexx by Quercus are two Rexx interpreters designed for DOS and can be run under Windows as well using a command prompt. Since the mid-1990s, two newer variants of Rexx have appeared:
In 1996 American National Standards Institute (ANSI) published a standard for Rexx: ANSI X3.274–1996 "Information Technology – Programming Language REXX".[10] More than two dozen books on Rexx have been published since 1985.
Rexx marked its 25th anniversary on 20 March 2004, which was celebrated at the REXX Language Association's 15th International REXX Symposium in Böblingen, Germany, in May 2004.
On October 12, 2004, IBM announced their plan to release their Object REXX implementation's sources under the Common Public License. Recent releases of Object REXX contain an ActiveX WSH scripting engine implementing this version of the Rexx language.
On February 22, 2005, the first public release of Open Object Rexx (ooRexx) was announced. This product contains a WSH scripting engine which allows for programming of the Windows operating system and applications with Rexx in the same fashion in which Visual Basic and JScript are implemented by the default WSH installation and Perl, Tcl, Python third-party scripting engines. 
As of  January 2017[update] REXX was listed in the TIOBE index as one of the fifty languages in its top 100 not belonging to the top 50.[11]
Rexx/Tk, a toolkit for graphics to be used in Rexx programmes in the same fashion as Tcl/Tk is widely available.
A Rexx IDE, RxxxEd, has been developed for Windows.[12] RxSock for network communication as well as other add-ons to and implementations of Regina Rexx have been developed, and a Rexx interpreter for the Windows command line is supplied in most Resource Kits for various versions of Windows and works under all of them as well as DOS.
Originally the language was called Rex (Reformed Executor); the extra "X" was added to avoid collisions with other products' names. REX was originally all uppercase because the mainframe code was uppercase oriented. The style in those days was to have all-caps names, partly because almost all code was still all-caps then. For the product it became REXX, and both editions of Mike Cowlishaw's book use all-caps. The expansion to REstructured eXtended eXecutor was used for the system product in 1984.[6]
The loop control structure in Rexx begins with a DO and ends with an END but comes in several varieties. NetRexx uses the keyword LOOP instead of DO for looping, while ooRexx treats LOOP and DO as equivalent when looping.
Rexx supports a variety of traditional structured-programming loops while testing a condition either before (do while) or after (do until) the list of instructions are executed:
Like most languages, Rexx can loop while incrementing an index variable and stop when a limit is reached:
The increment may be omitted and defaults to 1. The limit can also be omitted, which makes the loop continue forever. 
Rexx permits counted loops, where an expression is computed at the start of the loop and the instructions within the loop are executed that many times:
Rexx can even loop until the program is terminated:
A program can break out of the current loop with the leave instruction, which is the normal way to exit a do forever loop, or can short-circuit it with the iterate instruction.
Most unusually, Rexx allows both conditional and repetitive elements to be combined in the same loop:[13]
Testing conditions with IF:
The ELSE clause is optional.
For single instructions, DO and END can also be omitted:
Indentation is optional, but it helps improve the readability.
SELECT is Rexx's CASE structure, like many other constructs derived from PL/I.  Like some implementations of CASE constructs in other dynamic languages, Rexx's WHEN clauses specify full conditions, which need not be related to each other.  In that, they are more like cascaded sets of IF-THEN-ELSEIF-THEN-...-ELSE code than they are like the C or Java switch statement.
The NOP instruction performs "no operation", and is used when the programmer wishes to do nothing in a place where one or more instructions would be required.
The OTHERWISE clause is optional. If omitted and no WHEN conditions are met, then the SYNTAX condition is raised.
Variables in Rexx are typeless, and initially are evaluated as their names, in upper case. Thus a variable's type can vary with its use in the program:
Unlike many other programming languages, classic Rexx has no direct support for arrays of variables addressed by a numerical index. Instead it provides compound variables.[14] A compound variable consists of a stem followed by a tail. A . (dot) is used to join the stem to the tail. If the tails used are numeric, it is easy to produce the same effect as an array.
Afterwards the following variables with the following values exist: stem.1 = 9, stem.2 = 8, stem.3 = 7...
Unlike arrays, the index for a stem variable is not required to have an integer value. For example, the following code is valid:
In Rexx it is also possible to set a default value for a stem.
After these assignments the term stem.3 would produce 'Unknown'.
The whole stem can also be erased with the DROP statement.
This also has the effect of removing any default value set previously.
By convention (and not as part of the language) the compound stem.0 is often used to keep track of how many items are in a stem, for example a procedure to add a word to a list might be coded like this:
It is also possible to have multiple elements in the tail of a compound variable. For example:
Multiple numerical tail elements can be used to provide the effect of a multi-dimensional array.
Features similar to Rexx compound variables are found in many other languages (including associative arrays in AWK, hashes in Perl and Hashtables in Java). Most of these languages provide an instruction to iterate over all the keys (or tails in Rexx terms) of such a construct, but this is lacking in classic Rexx. Instead it is necessary to keep auxiliary lists of tail values as appropriate. For example, in a program to count words the following procedure might be used to record each occurrence of a word.
and then later:
At the cost of some clarity it is possible to combine these techniques into a single stem:
and later:
Rexx provides no safety net here, so if one of the words happens to be a whole number less than dictionary.0 this technique will fail mysteriously.
Recent implementations of Rexx, including IBM's Object REXX and the open source implementations like ooRexx include a new language construct to simplify iteration over the value of a stem, or over another collection object such as an array, table or list.
The PARSE instruction is particularly powerful; it combines some useful string-handling functions. Its syntax is:
where origin specifies the source:
and template can be:
upper is optional; if specified, data will be converted to upper case before parsing.
Examples:
Using a list of variables as template
displays the following:
Using a delimiter as template:
also displays the following:
Using column number delimiters:
displays the following:
A template can use a combination of variables, literal delimiters, and column number delimiters.
The INTERPRET instruction evaluates its argument and treats its value as a Rexx statement. Sometimes INTERPRET is the clearest way to perform a task, but it is often used where clearer code is possible using, e.g., value().
The other reasons being Rexx's (decimal) arbitrary precision arithmetic (including fuzzy comparisons), use of the PARSE statement with programmatic templates, stemmed arrays, and sparse arrays.
This displays 16 and exits. Because variable contents in Rexx are strings, including rational numbers with exponents and even entire programs, Rexx offers to interpret strings as evaluated expressions.
This feature could be used to pass functions as function parameters, such as passing SIN or COS to a procedure to calculate integrals.
Rexx offers only basic math functions like ABS, DIGITS, MAX, MIN, SIGN, RANDOM, and a complete set of hex plus binary conversions with bit operations. More complex functions like SIN had to be implemented from scratch or obtained from third party external libraries. Some external libraries, typically those implemented in traditional languages, did not support extended precision.
Later versions (non-classic) support CALL variable constructs. Together with the built-in function VALUE, CALL can be used in place of many cases of INTERPRET. This is a classic program:
A slightly more sophisticated "Rexx calculator":
PULL is shorthand for parse upper pull. Likewise, ARG is shorthand for parse upper arg.
The power of the INTERPRET instruction had other uses. The Valour software package relied upon Rexx's interpretive ability to implement an OOP environment. Another use was found in an unreleased Westinghouse product called Time Machine that was able to fully recover following a fatal error.
The SIGNAL instruction is intended for abnormal changes in the flow of control (see the next section). However, it can be misused and treated like the GOTO statement found in other languages (although it is not strictly equivalent, because it terminates loops and other constructs). This can produce difficult-to-read code.
It is possible in Rexx to intercept and deal with errors and other exceptions, using the SIGNAL instruction. There are seven system conditions: ERROR, FAILURE, HALT, NOVALUE, NOTREADY, LOSTDIGITS and SYNTAX. Handling of each can be switched on and off in the source code as desired.
The following program will run until terminated by the user:
A signal on novalue statement intercepts uses of undefined variables, which would otherwise get their own (upper case) name as their value. Regardless of the state of the NOVALUE condition, the status of a variable can always be checked with the built-in function SYMBOL returning VAR for defined variables.
The VALUE function can be used to get the value of variables without triggering a NOVALUE condition, but its main purpose is to read and set environment variables, similar to POSIX getenv and putenv.
When a condition is handled by SIGNAL ON, the SIGL and RC system variables can be analyzed to understand the situation. RC contains the Rexx error code and SIGL contains the line number where the error arose.
Beginning with Rexx version 4 conditions can get names, and there's also a CALL ON construct. That's handy if external functions do not necessarily exist:



Assembly language - Wikipedia
An assembly (or assembler) language,[1] often abbreviated asm, is any low-level programming language in which there is a very strong correspondence between the program's statements and the architecture's machine code instructions.[2]
Each assembly language is specific to a particular computer architecture and operating system.[3] In contrast, most high-level programming languages are generally portable across multiple architectures but require interpreting or compiling. Assembly language may also be called symbolic machine code.[4][5]
Assembly language usually has one statement per machine instruction, but assembler directives,[6] macros[7][1] and symbolic labels of program and memory locations are often also supported.
Assembly code is converted into executable machine code by a utility program referred to as an assembler. The conversion process is referred to as assembly, or assembling the source code.
Assembly language uses a mnemonic to represent each low-level machine instruction or opcode, typically also each architectural register, flag, etc. Many operations require one or more operands in order to form a complete instruction. Most assemblers permit named constants, registers, and labels for program and memory locations, and can calculate expressions for operands. Thus, the programmers are freed from tedious repetitive calculations and assembler programs are much more readable than machine code. Depending on the architecture, these elements may also be combined for specific instructions or addressing modes using offsets or other data as well as fixed addresses. Many assemblers offer additional mechanisms to facilitate program development, to control the assembly process, and to aid debugging.

An assembler program creates object code by translating combinations of mnemonics and syntax for operations and addressing modes into their numerical equivalents. This representation typically includes an operation code ("opcode") as well as other control bits and data. The assembler also calculates constant expressions and resolves symbolic names for memory locations and other entities.[9] The use of symbolic references is a key feature of assemblers, saving tedious calculations and manual address updates after program modifications. Most assemblers also include macro facilities for performing textual substitution – e.g., to generate common short sequences of instructions as inline, instead of called subroutines.
Some assemblers may also be able to perform some simple types of instruction set-specific optimizations. One concrete example of this may be the ubiquitous x86 assemblers from various vendors. Most of them are able to perform jump-instruction replacements (long jumps replaced by short or relative jumps) in any number of passes, on request. Others may even do simple rearrangement or insertion of instructions, such as some assemblers for RISC architectures that can help optimize a sensible instruction scheduling to exploit the CPU pipeline as efficiently as possible.[citation needed]
Like early programming languages such as Fortran, Algol, Cobol and Lisp, assemblers have been available since the 1950s and the first generations of text based computer interfaces. However, assemblers came first as they are far simpler to write than compilers for high-level languages. This is because each mnemonic along with the addressing modes and operands of an instruction translates rather directly into the numeric representations of that particular instruction, without much context or analysis. There have also been several classes of translators and semi automatic code generators with properties similar to both assembly and high level languages, with speedcode as perhaps one of the better known examples.
There may be several assemblers with different syntax for a particular CPU or instruction set architecture. For instance, an instruction to add memory data to a register in a x86-family processor might be add eax,[ebx], in original Intel syntax, whereas this would be written addl (%ebx),%eax in the AT&T syntax used by the GNU Assembler. Despite different appearances, different syntactic forms generally generate the same numeric machine code, see further below. A single assembler may also have different modes in order to support variations in syntactic forms as well as their exact semantic interpretations (such as FASM-syntax, TASM-syntax, ideal mode etc., in the special case of x86 assembly programming).
There are two types of assemblers based on how many passes through the source are needed  (how many times the assembler reads the source) to produce the object file.
In both cases, the assembler must be able to determine the size of each instruction on the initial passes in order to calculate the addresses of subsequent symbols. This means that if the size of an operation referring to an operand defined later depends on the type or distance of the operand, the assembler will make a pessimistic estimate when first encountering the operation, and if necessary pad it with one or more
"no-operation" instructions in a later pass or the errata. In an assembler with peephole optimization, addresses may be recalculated between passes to allow replacing pessimistic code with code tailored to the exact distance from the target.
The original reason for the use of one-pass assemblers was speed of assembly – often a second pass would require rewinding and rereading the program source on tape or rereading a deck of cards or punched paper tape. Later computers with much larger memories (especially disc storage), had the space to perform all necessary processing without such re-reading. The advantage of the multi-pass assembler is that the absence of errata makes the linking process (or the program load if the assembler directly produces executable code) faster.[10]
Example: in the following code snippet a one-pass assembler would be able to determine the address of the backward reference BKWD when assembling statement S2, but would not be able to determine the address of the forward reference FWD when assembling the branch statement S1; indeed FWD may be undefined. A two-pass assembler would determine both addresses in pass 1, so they would be known when generating code in pass 2,
More sophisticated high-level assemblers provide language abstractions such as:
See Language design below for more details.
A program written in assembly language consists of a series of mnemonic processor instructions and meta-statements (known variously as directives, pseudo-instructions and pseudo-ops), comments and data. Assembly language instructions usually consist of an opcode mnemonic followed by a list of data, arguments or parameters.[12] These are translated by an assembler into machine language instructions that can be loaded into memory and executed.
For example, the instruction below tells an x86/IA-32 processor to move an immediate 8-bit value into a register. The binary code for this instruction is 10110 followed by a 3-bit identifier for which register to use. The identifier for the AL register is 000, so the following machine code loads the AL register with the data 01100001.[13]
This binary computer code can be made more human-readable by expressing it in hexadecimal as follows.
Here, B0 means 'Move a copy of the following value into AL', and 61 is a hexadecimal representation of the value 01100001, which is 97 in decimal. Assembly language for the 8086 family provides the mnemonic MOV (an abbreviation of move) for instructions such as this, so the machine code above can be written as follows in assembly language, complete with an explanatory comment if required, after the semicolon. This is much easier to read and to remember.
In some assembly languages the same mnemonic such as MOV may be used for a family of related instructions for loading, copying and moving data, whether these are immediate values, values in registers, or memory locations pointed to by values in registers.  Other assemblers may use separate opcode mnemonics such as L for "move memory to register", ST for "move register to memory", LR for "move register to register", MVI for "move immediate operand to memory", etc.
The x86 opcode 10110000 (B0) copies an 8-bit value into the AL register, while 10110001 (B1) moves it into CL and 10110010 (B2) does so into DL. Assembly language examples for these follow.[13]
The syntax of MOV can also be more complex as the following examples show.[14]
In each case, the MOV mnemonic is translated directly into an opcode in the ranges 88-8E, A0-A3, B0-B8, C6 or C7 by an assembler, and the programmer does not have to know or remember which.[13]
Transforming assembly language into machine code is the job of an assembler, and the reverse can at least partially be achieved by a disassembler. Unlike high-level languages, there is a one-to-one correspondence between many simple assembly statements and machine language instructions. However, in some cases, an assembler may provide pseudoinstructions (essentially macros) which expand into several machine language instructions to provide commonly needed functionality. For example, for a machine that lacks a "branch if greater or equal" instruction, an assembler may provide a pseudoinstruction that expands to the machine's "set if less than" and "branch if zero (on the result of the set instruction)". Most full-featured assemblers also provide a rich macro language (discussed below) which is used by vendors and programmers to generate more complex code and data sequences.
Each computer architecture has its own machine language.  Computers differ in the number and type of operations they support, in the different sizes and numbers of registers, and in the representations of data in storage. While most general-purpose computers are able to carry out essentially the same functionality, the ways they do so differ; the corresponding assembly languages reflect these differences.
Multiple sets of mnemonics or assembly-language syntax may exist for a single instruction set, typically instantiated in different assembler programs. In these cases, the most popular one is usually that supplied by the manufacturer and used in its documentation.
There is a large degree of diversity in the way the authors of assemblers categorize statements and in the nomenclature that they use. In particular, some describe anything other than a machine mnemonic or extended mnemonic as a pseudo-operation (pseudo-op). A typical assembly language consists of 3 types of instruction statements that are used to define program operations:
Instructions (statements) in assembly language are generally very simple, unlike those in high-level languages. Generally, a mnemonic is a symbolic name for a single executable machine language instruction (an opcode), and there is at least one opcode mnemonic defined for each machine language instruction. Each instruction typically consists of an operation or opcode plus zero or more operands. Most instructions refer to a single value, or a pair of values.  Operands can be immediate (value coded in the instruction itself), registers specified in the instruction or implied, or the addresses of data located elsewhere in storage. This is determined by the underlying processor architecture: the assembler merely reflects how this architecture works. Extended mnemonics are often used to specify a combination of an opcode with a specific operand, e.g., the System/360 assemblers use B as an extended mnemonic for BC with a mask of 15 and NOP ("NO OPeration" – do nothing for one step) for BC with a mask of 0.
Extended mnemonics are often used to support specialized uses of instructions, often for purposes not obvious from the instruction name. For example, many CPU's do not have an explicit NOP instruction, but do have instructions that can be used for the purpose. In 8086 CPUs the instruction xchg ax,ax is used for nop, with nop being a pseudo-opcode to encode the instruction xchg ax,ax. Some disassemblers recognize this and will decode the xchg ax,ax instruction as nop. Similarly, IBM assemblers for System/360 and System/370 use the extended mnemonics NOP and NOPR for BC and BCR with zero masks.  For the SPARC architecture, these are known as synthetic instructions.[15]
Some assemblers also support simple built-in macro-instructions that generate two or more machine instructions. For instance, with some Z80 assemblers the instruction ld hl,bc is recognized to generate ld l,c followed by ld h,b.[16] These are sometimes known as pseudo-opcodes.
Mnemonics are arbitrary symbols; in 1985 the IEEE published Standard 694 for a uniform set of mnemonics to be used by all assemblers. The standard has since been withdrawn.
There are instructions used to define data elements to hold data and variables.  They define the type of data, the length and the alignment of data. These instructions can also define whether the data is available to outside programs (programs assembled separately) or only to the program in which the data section is defined. Some assemblers classify these as pseudo-ops.
Assembly directives, also called pseudo-opcodes, pseudo-operations or pseudo-ops, are commands given to an assembler "directing it to perform operations other than assembling instructions.".[9] Directives affect how the assembler operates and "may affect the object code, the symbol table, the listing file, and the values of internal assembler parameters."  Sometimes the term pseudo-opcode is reserved for directives that generate object code, such as those that generate data.[17]
The names of pseudo-ops often start with a dot to distinguish them from machine instructions.  Pseudo-ops can make the assembly of the program dependent on parameters input by a programmer, so that one program can be assembled different ways, perhaps for different applications. Or, a pseudo-op can be used to manipulate presentation of a program to make it easier to read and maintain. Another common use of pseudo-ops is to reserve storage areas for run-time data and optionally initialize their contents to known values.
Symbolic assemblers let programmers associate arbitrary names (labels or symbols) with memory locations and various constants. Usually, every constant and variable is given a name so instructions can reference those locations by name, thus promoting self-documenting code. In executable code, the name of each subroutine is associated with its entry point, so any calls to a subroutine can use its name. Inside subroutines, GOTO destinations are given labels. Some assemblers support local symbols which are lexically distinct from normal symbols (e.g., the use of "10$" as a GOTO destination).
Some assemblers, such as NASM, provide flexible symbol management, letting programmers manage different namespaces, automatically calculate offsets within data structures, and assign labels that refer to literal values or the result of simple computations performed by the assembler. Labels can also be used to initialize constants and variables with relocatable addresses.
Assembly languages, like most other computer languages, allow comments to be added to program source code that will be ignored during assembly. Judicious commenting is essential in assembly language programs, as the meaning and purpose of a sequence of binary machine instructions can be difficult to determine. The "raw" (uncommented) assembly language generated by compilers or disassemblers is quite difficult to read when changes must be made.
Many assemblers support predefined macros, and others support programmer-defined (and repeatedly re-definable) macros involving sequences of text lines in which variables and constants are embedded. The macro definition is most commonly[a] a mixture of assembler statements, e.g., directives, symbolic machine instructions, and templates for assembler statements. This sequence of text lines may include opcodes or directives. Once a macro has been defined its name may be used in place of a mnemonic. When the assembler processes such a statement, it replaces the statement with the text lines associated with that macro, then processes them as if they existed in the source code file (including, in some assemblers, expansion of any macros existing in the replacement text). Macros in this sense date to IBM autocoders of the 1950s.[18]
[19]
In assembly language, the term "macro" represents a more comprehensive concept than it does in some other contexts, such as in the C programming language, where its #define directive typically is used to create short single line macros. Assembler macro instructions, like macros in PL/I and some other languages, can be lengthy "programs" by themselves, executed by interpretation by the assembler during assembly.
Since macros can have 'short' names but expand to several or indeed many lines of code, they can be used to make assembly language programs appear to be far shorter, requiring fewer lines of source code, as with higher level languages. They can also be used to add higher levels of structure to assembly programs, optionally introduce embedded debugging code via parameters and other similar features.
Macro assemblers often allow macros to take parameters. Some assemblers include quite sophisticated macro languages, incorporating such high-level language elements as optional parameters, symbolic variables, conditionals, string manipulation, and arithmetic operations, all usable during the execution of a given macro, and allowing macros to save context or exchange information. Thus a macro might generate numerous assembly language instructions or data definitions, based on the macro arguments. This could be used to generate record-style data structures or "unrolled" loops, for example, or could generate entire algorithms based on complex parameters. For instance, a "sort" macro could accept the specification of a complex sort key and generate code crafted for that specific key, not needing the run-time tests that would be required  for a general procedure interpreting the specification. An organization using assembly language that has been heavily extended using such a macro suite can be considered to be working in a higher-level language, since such programmers are not working with a computer's lowest-level conceptual elements. Underlining this point, macros were used to implement an early virtual machine in SNOBOL4 (1967), which was written in the SNOBOL Implementation Language (SIL), an assembly language for a virtual machine. The target machine would translate this to its native code using a macro assembler.[20] This allowed a high degree of portability for the time.
Macros were used to customize large scale software systems for specific customers in the mainframe era and were also used by customer personnel to satisfy their employers' needs by making specific versions of manufacturer operating systems. This was done, for example, by systems programmers working with IBM's Conversational Monitor System / Virtual Machine (VM/CMS) and with IBM's "real time transaction processing" add-ons, Customer Information Control System CICS, and ACP/TPF, the airline/financial system that began in the 1970s and still runs many large computer reservation systems (CRS) and credit card systems today.
It is also possible to use solely the macro processing abilities of an assembler to generate code written in completely different languages, for example, to generate a version of a program in COBOL using a pure macro assembler program containing lines of COBOL code inside assembly time operators instructing the assembler to generate arbitrary code. IBM OS/360 uses macros to perform system generation. The user specifies options by coding a series of assembler macros.  Assembling these macros generates a job stream to build the system, including job control language and utility control statements.
This is because, as was realized in the 1960s, the concept of "macro processing" is independent of the concept of "assembly", the former being in modern terms more word processing, text processing, than generating object code. The concept of macro processing appeared, and appears, in the C programming language, which supports "preprocessor instructions" to set variables, and make conditional tests on their values. Note that unlike certain previous macro processors inside assemblers, the C preprocessor is not Turing-complete because it lacks the ability to either loop or "go to", the latter allowing programs to loop.
Despite the power of macro processing, it fell into disuse in many high level languages (major exceptions being C, C++ and PL/I) while remaining a perennial for assemblers.
Macro parameter substitution is strictly by name: at macro processing time, the value of a parameter is textually substituted for its name. The most famous class of bugs resulting was the use of a parameter that itself was an expression and not a simple name when the macro writer expected a name. In the macro:
the intention was that the caller would provide the name of a variable, and the "global" variable or constant b would be used to multiply "a". If foo is called with the parameter a-c, the macro expansion of load a-c*b occurs.  To avoid any possible ambiguity, users of macro processors can parenthesize formal parameters inside macro definitions, or callers can parenthesize the input parameters.[21]
Some assemblers have incorporated structured programming elements to encode execution flow. The earliest example of this approach was in the Concept-14 macro set, originally proposed by Dr. Harlan Mills (March 1970), and implemented by Marvin Kessler at IBM's Federal Systems Division, which extended the S/360 macro assembler with IF/ELSE/ENDIF and similar control flow blocks.[22] This was a way to reduce or eliminate the use of GOTO operations in assembly code, one of the main factors causing spaghetti code in assembly language. This approach was widely accepted in the early '80s (the latter days of large-scale assembly language use).
A curious design was A-natural, a "stream-oriented" assembler for 8080/Z80 processors[citation needed] from Whitesmiths Ltd. (developers of the Unix-like Idris operating system, and what was reported to be the first commercial C compiler). The language was classified as an assembler, because it worked with raw machine elements such as opcodes, registers, and memory references; but it incorporated an expression syntax to indicate execution order. Parentheses and other special symbols, along with block-oriented structured programming constructs, controlled the sequence of the generated instructions. A-natural was built as the object language of a C compiler, rather than for hand-coding, but its logical syntax won some fans.
There has been little apparent demand for more sophisticated assemblers since the decline of large-scale assembly language development.[23] In spite of that, they are still being developed and applied in cases where resource constraints or peculiarities in the target system's architecture prevent the effective use of higher-level languages.[24]
Assemblers with a strong macro engine allow structured programming via macros, such as the switch macro provided with the Masm32 package (note this code is a complete program):
Assembly languages date to the introduction of the stored-program computer. The first assembly language was developed in 1947 by Kathleen Booth for the ARC2 at Birkbeck, University of London following work with John von Neumann and Herman Goldstine at the Institute for Advanced Study.[25][26] The Electronic Delay Storage Automatic Calculator (EDSAC) had an assembler (named "initial orders") integrated into its bootstrap program that used one-letter mnemonics in late 1948 developed by David Wheeler and credited by the IEEE Computer Society as the creator of the first "assembler."[27][28] [29] Reports on the EDSAC introduced the term "assembly" for the process of combining fields into an instruction word.[30] SOAP (Symbolic Optimal Assembly Program) was an assembly language for the IBM 650 computer written by Stan Poley in 1955.[31]
Assembly languages eliminate much of the error-prone, tedious, and time-consuming first-generation programming needed with the earliest computers, freeing programmers from tedium such as remembering numeric codes and calculating addresses. They were once widely used for all sorts of programming. However, by the 1980s (1990s on microcomputers), their use had largely been supplanted by higher-level languages, in the search for improved programming productivity. Today assembly language is still used for direct hardware manipulation, access to specialized processor instructions, or to address critical performance issues. Typical uses are device drivers, low-level embedded systems, and real-time systems.
Historically, numerous programs have been written entirely in assembly language. The Burroughs MCP (1961) was the first computer for which an operating system was not developed entirely in assembly language; it was written in Executive Systems Problem Oriented Language (ESPOL), an Algol dialect. Many commercial applications were written in assembly language as well, including a large amount of the IBM mainframe software written by large corporations. COBOL, FORTRAN and some PL/I eventually displaced much of this work, although a number of large organizations retained assembly-language application infrastructures well into the 1990s.
Most early microcomputers relied on hand-coded assembly language, including most operating systems and large applications. This was because these systems had severe resource constraints, imposed idiosyncratic memory and display architectures, and provided limited, buggy system services. Perhaps more important was the lack of first-class high-level language compilers suitable for microcomputer use. A psychological factor may have also played a role: the first generation of microcomputer programmers retained a hobbyist, "wires and pliers" attitude.
In a more commercial context, the biggest reasons for using assembly language were minimal bloat (size), minimal overhead, greater speed, and reliability.
Typical examples of large assembly language programs from this time are IBM PC DOS operating systems, the Turbo Pascal compiler and early applications such as the spreadsheet program Lotus 1-2-3. According to some[who?] industry insiders, the assembly language was the best computer language to use to get the best performance out of the Sega Saturn, a console that was notoriously challenging to develop and program games for.[32] The 1993 arcade game NBA Jam is another example.
Assembly language has long been the primary development language for many popular home computers of the 1980s and 1990s (such as the MSX,  Sinclair ZX Spectrum, Commodore 64, Commodore Amiga, and Atari ST). This was in large part because interpreted BASIC dialects on these systems offered insufficient execution speed, as well as insufficient facilities to take full advantage of the available hardware on these systems. Some systems even have an integrated development environment (IDE) with highly advanced debugging and macro facilities. Some compilers available for the Radio Shack TRS-80 and its successors had the capability to combine inline assembly source with high-level program statements. Upon compilation a built-in assembler produced inline machine code.
There have always been debates over the usefulness and performance of assembly language relative to high-level languages. Assembly language has specific niche uses where it is important; see below. As of  July 2017[update], the TIOBE index of programming language popularity ranks assembly language at 11, ahead of Visual Basic, for example.[33] Assembler can be used to optimize for speed or optimize for size. In the case of speed optimization, modern optimizing compilers are claimed[34]  to render high-level languages into code that can run as fast as hand-written assembly, despite the counter-examples that can be found.[35][36][37] The complexity of modern processors and memory sub-systems makes effective optimization increasingly difficult for compilers, as well as assembly programmers.[38][39] Moreover, increasing processor performance has meant that most CPUs sit idle most of the time,[40] with delays caused by predictable bottlenecks such as cache misses, I/O operations and paging. This has made raw code execution speed a non-issue for many programmers.
There are some situations in which developers might choose to use assembly language:
Assembly language is still taught in most computer science and electronic engineering programs. Although few programmers today regularly work with assembly language as a tool, the underlying concepts remain very important. Such fundamental topics as binary arithmetic, memory allocation, stack processing, character set encoding, interrupt processing, and compiler design would be hard to study in detail without a grasp of how a computer operates at the hardware level. Since a computer's behavior is fundamentally defined by its instruction set, the logical way to learn such concepts is to study an assembly language. Most modern computers have similar instruction sets. Therefore, studying a single assembly language is sufficient to learn: I) the basic concepts; II) to recognize situations where the use of assembly language might be appropriate; and III) to see how efficient executable code can be created from high-level languages.[45] This is analogous to children needing to learn the basic arithmetic operations (e.g., long division), although calculators are widely used for all except the most trivial calculations.




Type theory - Wikipedia
In mathematics, logic, and computer science, a type theory is any of a class of formal systems, some of which can serve as alternatives to set theory as a foundation for all mathematics. In type theory, every "term" has a "type" and operations are restricted to terms of a certain type.
Type theory is closely related to (and in some cases overlaps with) type systems, which are a programming language feature used to reduce bugs. Type theory was created to avoid paradoxes in a variety of formal logics and rewrite systems.
Two well-known type theories that can serve as mathematical foundations are Alonzo Church's typed λ-calculus and Per Martin-Löf's intuitionistic type theory.
Between 1902 and 1908 Bertrand Russell proposed various "theories of type" in response to his discovery that Gottlob Frege's version of naive set theory was afflicted with Russell's paradox. By 1908 Russell arrived at a "ramified" theory of types together with an "axiom of reducibility" both of which featured prominently in Whitehead and Russell's Principia Mathematica published between 1910 and 1913. They attempt to avoid Russell's paradox by first creating a hierarchy of types, then assigning each mathematical (and possibly other) entity to a type. Objects of a given type are built exclusively from objects of preceding types (those lower in the hierarchy), thus preventing cycles. In the 1920s, Leon Chwistek and Frank P. Ramsey proposed a simpler theory, now known as the "theory of simple types" or "simple type theory", that collapsed the complicated hierarchy of the "ramified theory" and did not require the axiom of reducibility.
The common usage of "type theory" is when those types are used with a term rewrite system. The most famous early example is Alonzo Church's simply typed lambda calculus. Church's theory of types[1] helped the formal system avoid the Kleene–Rosser paradox that afflicted the original untyped lambda calculus. Church demonstrated that it could serve as a foundation of mathematics and it was referred to as a higher-order logic.
Some other type theories include Per Martin-Löf's intuitionistic type theory, which has been the foundation used in some areas of constructive mathematics and for the proof assistant Agda. Thierry Coquand's calculus of constructions and its derivatives are the foundation used by Coq and others. The field is an area of active research, as demonstrated by homotopy type theory.
In a system of type theory, each term has a type.  For example, 



4


{\displaystyle 4}

, 



2
+
2


{\displaystyle 2+2}

, and 



2
⋅
2


{\displaystyle 2\cdot 2}

 are all separate terms with the type 




n
a
t



{\displaystyle \mathrm {nat} }

 for natural numbers.  Traditionally, the term is followed by a colon and its type, such as 



2
:

n
a
t



{\displaystyle 2:\mathrm {nat} }

.
Type theories have explicit computation and it is encoded in rules for rewriting terms. These are called conversion rules or, if the rule only works in one direction, a reduction rule. For example, 



2
+
2


{\displaystyle 2+2}

 and 



4


{\displaystyle 4}

 are syntactically different terms, but the former reduces to the latter. This reduction is written 



2
+
2
↠
4


{\displaystyle 2+2\twoheadrightarrow 4}

.
Functions in type theory have a special reduction rule: the argument of the function call gets substituted for every occurrence of the parameter in the function definition.  Let's say the function 




d
o
u
b
l
e



{\displaystyle \mathrm {double} }

 is defined as 



λ
x
.
x
+
x


{\displaystyle \lambda x.x+x}

 (using Church's lambda notation) or 



x
↦
x
+
x


{\displaystyle x\mapsto x+x}

 (using a more modern notation).  Then, the function call 




d
o
u
b
l
e

 
2


{\displaystyle \mathrm {double} \ 2}

 would be reduced by substituting 



2


{\displaystyle 2}

 for every copy of 



x


{\displaystyle x}

 in the body of the function definition.  Thus, 




d
o
u
b
l
e

 
2
↠
2
+
2


{\displaystyle \mathrm {double} \ 2\twoheadrightarrow 2+2}

.
The type of a function is denoted with an arrow 



→


{\displaystyle \to }

 from the parameter type to the function's resulting type. Thus, 




d
o
u
b
l
e

:

n
a
t

→

n
a
t



{\displaystyle \mathrm {double} :\mathrm {nat} \to \mathrm {nat} }

.   Calling or "applying" a function to an argument may be written with or without parentheses, so 




d
o
u
b
l
e

(
2
)


{\displaystyle \mathrm {double} (2)}

 or 




d
o
u
b
l
e

 
2


{\displaystyle \mathrm {double} \ 2}

. Not using parentheses is more common, because multiple argument functions can be defined using currying.
There are many different set theories and many different systems of type theory, so what follows are generalizations.
The term 



2
+
1


{\displaystyle 2+1}

 reduces to 



3


{\displaystyle 3}

. Since 



3


{\displaystyle 3}

 cannot be reduced further, it is called a normal form. A system of type theory is said to be strongly normalizing if all terms have a normal form and any order of reductions reaches it. Weakly normalizing systems have a normal form but some orders of reductions may loop forever and never reach it.
For a normalizing system, some borrow the word element from set theory and use it to refer to all closed terms that can reduce to the same normal form. A closed term is one without parameters. (A term like 



x
+
1


{\displaystyle x+1}

 with its parameter 



x


{\displaystyle x}

 is called an open term.) Thus, 



2
+
1


{\displaystyle 2+1}

 and 



3
+
0


{\displaystyle 3+0}

 may be different terms but they are both from the element 



3


{\displaystyle 3}

.
A similar idea that works for open and closed terms is convertibility. Two terms are convertible if there exists a term that they both reduce to. For example, 



2
+
1


{\displaystyle 2+1}

 and 



1
+
2


{\displaystyle 1+2}

 are convertible. As are 



x
+
(
1
+
1
)


{\displaystyle x+(1+1)}

 and 



x
+
2


{\displaystyle x+2}

. However, 



x
+
1


{\displaystyle x+1}

 and 



1
+
x


{\displaystyle 1+x}

 (where 



x


{\displaystyle x}

 is a free variable) are not because both are in normal form and they are not the same. Confluent and weakly normalizing systems can test if two terms are convertible by checking if they both reduce to the same normal form.
A dependent type is a type that depends on a term or on another type. Thus, the type returned by a function may depend upon the argument to the function.
For example, a list of 




n
a
t



{\displaystyle \mathrm {nat} }

s of length 4 may be a different type than a list of 




n
a
t



{\displaystyle \mathrm {nat} }

s of length 5. In a type theory with dependent types, it is possible to define a function that takes a parameter "n" and returns a list containing "n" zeros. Calling the function with 4 would produce a term with a different type than if the function was called with 5.
Dependent types play a central role in intuitionistic type theory and in the design of functional programming languages like Idris, ATS, Agda and Epigram.

Many systems of type theory have a type that represents equality of types and of terms. This type is different from convertibility, and is often denoted propositional equality.
In intuitionistic type theory, the equality type (also called the identity type) is known as 



I


{\displaystyle I}

 for identity. There is a type 



I
 
A
 
a
 
b


{\displaystyle I\ A\ a\ b}

 when 



A


{\displaystyle A}

 is a type and 



a


{\displaystyle a}

 and 



b


{\displaystyle b}

 are both terms of type 



A


{\displaystyle A}

. A term of type 



I
 
A
 
a
 
b


{\displaystyle I\ A\ a\ b}

 is interpreted as meaning that 



a


{\displaystyle a}

 is equal to 



b


{\displaystyle b}

.
In practice, it is possible to build a type 



I
 

n
a
t

 
3
 
4


{\displaystyle I\ \mathrm {nat} \ 3\ 4}

 but there will not exist a term of that type. In intuitionistic type theory, new terms of equality start with reflexivity. If 



3


{\displaystyle 3}

 is a term of type 




n
a
t



{\displaystyle \mathrm {nat} }

, then there exists a term of type 



I
 

n
a
t

 
3
 
3


{\displaystyle I\ \mathrm {nat} \ 3\ 3}

. More complicated equalities can be created by creating a reflexive term and then doing a reduction on one side. So if 



2
+
1


{\displaystyle 2+1}

 is a term of type 




n
a
t



{\displaystyle \mathrm {nat} }

, then there is a term of type 



I
 

n
a
t

 
(
2
+
1
)
 
(
2
+
1
)


{\displaystyle I\ \mathrm {nat} \ (2+1)\ (2+1)}

 and, by reduction, generate a term of type 



I
 

n
a
t

 
(
2
+
1
)
 
3


{\displaystyle I\ \mathrm {nat} \ (2+1)\ 3}

. Thus, in this system, the equality type denotes that two values of the same type are convertible by reductions.
Having a type for equality is important because it can be manipulated inside the system. There is usually no judgement to say two terms are not equal; instead, as in the Brouwer–Heyting–Kolmogorov interpretation, we map 



¬
(
a
=
b
)


{\displaystyle \neg (a=b)}

 to 



(
a
=
b
)
→
⊥


{\displaystyle (a=b)\to \bot }

, where 



⊥


{\displaystyle \bot }

 is the bottom type having no values. There exists a term with type 



(
I
 

n
a
t

 
3
 
4
)
→
⊥


{\displaystyle (I\ \mathrm {nat} \ 3\ 4)\to \bot }

, but not one of type 



(
I
 

n
a
t

 
3
 
3
)
→
⊥


{\displaystyle (I\ \mathrm {nat} \ 3\ 3)\to \bot }

.
Homotopy type theory differs from intuitionistic type theory mostly by its handling of the equality type.
A system of type theory requires some basic terms and types to operate on. Some systems build them out of functions using Church encoding. Other systems have inductive types: a set of base types and a set of type constructors that generate types with well-behaved properties. For example, certain recursive functions called on inductive types are guaranteed to terminate.
Coinductive type are infinite data types created by giving a function that generates the next element(s). See Coinduction and Corecursion.
Induction-induction  is a feature for declaring an inductive type and a family of types that depends on the inductive type.
Induction recursion allows a wider range of well-behaved types, allowing the type and recursive functions operating on it to be defined at the same time.
Types were created to prevent paradoxes, such as Russell's paradox. However, the motives that lead to those paradoxes—being able to say things about all types—still exist. So, many type theories have a "universe type", which contains all other types (and not itself).
In systems where you might want to say something about universe types, there is a hierarchy of universe types, each containing the one below it in the hierarchy. The hierarchy is defined as being infinite, but statements must only refer to a finite number of universe levels.
Type universes are particularly tricky in type theory. The initial proposal of intuitionistic type theory suffered from Girard's paradox.
Many systems of type theory, such as the simply-typed lambda calculus, intuitionistic type theory, and the calculus of constructions, are also programming languages. That is, they are said to have a "computational component". The computation is the reduction of terms of the language using rewriting rules.
A system of type theory that has a well-behaved computational component also has a simple connection to constructive mathematics through the BHK interpretation.
Non-constructive mathematics in these systems is possible by adding operators on continuations such as call with current continuation. However, these operators tend to break desirable properties such as canonicity and parametricity.
There is extensive overlap and interaction between the fields of type theory and type systems. Type systems are a programming language feature designed to identify bugs. Any static program analysis, such as the type checking algorithms in the semantic analysis phase of compiler, has a connection to type theory.
A prime example is Agda, a programming language which uses intuitionistic type theory for its type system. The programming language ML was developed for manipulating type theories (see LCF) and its own type system was heavily influenced by them.
The first computer proof assistant, called Automath, used type theory to encode mathematics on a computer. Martin-Löf specifically developed intuitionistic type theory to encode all mathematics to serve as a new foundation for mathematics. There is current research into mathematical foundations using homotopy type theory.
Mathematicians working in category theory already had difficulty working with the widely accepted foundation of Zermelo–Fraenkel set theory. This led to proposals such as Lawvere's Elementary Theory of the Category of Sets (ETCS).[2] Homotopy type theory continues in this line using type theory. Researchers are exploring connections between dependent types (especially the identity type) and algebraic topology (specifically homotopy).
Much of the current research into type theory is driven by proof checkers, interactive proof assistants, and automated theorem provers. Most of these systems use a type theory as the mathematical foundation for encoding proofs, which is not surprising, given the close connection between type theory and programming languages:
Multiple type theories are supported by LEGO and Isabelle. Isabelle also supports foundations besides type theories, such as ZFC. Mizar is an example of a proof system that only supports set theory.
Type theory is also widely in use in formal theories of semantics of natural languages, especially Montague grammar and its descendants. In particular, categorial grammars and pregroup grammars make extensive use of type constructors to define the types (noun, verb, etc.) of words.
The most common construction takes the basic types 



e


{\displaystyle e}

 and 



t


{\displaystyle t}

 for individuals and truth-values, respectively, and defines the set of types recursively as follows:
A complex type 



⟨
a
,
b
⟩


{\displaystyle \langle a,b\rangle }

 is the type of functions from entities of type 



a


{\displaystyle a}

 to entities of type 



b


{\displaystyle b}

. Thus one has types like 



⟨
e
,
t
⟩


{\displaystyle \langle e,t\rangle }

 which are interpreted as elements of the set of functions from entities to truth-values, i.e. indicator functions of sets of entities. An expression of type 



⟨
⟨
e
,
t
⟩
,
t
⟩


{\displaystyle \langle \langle e,t\rangle ,t\rangle }

 is a function from sets of entities to truth-values, i.e. a (indicator function of a) set of sets. This latter type is standardly taken to be the type of natural language quantifiers, like  everybody or  nobody (Montague 1973, Barwise and Cooper 1981).
Gregory Bateson introduced a theory of logical types into the social sciences; his notions of double bind and logical levels are based on Russell's theory of types.
Although the initial motivation for category theory was far removed from foundationalism, the two fields turned out to have deep connections. As John Lane Bell writes: "In fact categories can themselves be viewed as type theories of a certain kind; this fact alone indicates that type theory is much more closely related to category theory than it is to set theory." In brief, a category can be viewed as a type theory by regarding its objects as types (or sorts), i.e. "Roughly speaking, a category may be thought of as a type theory shorn of its syntax." A number of significant results follow in this way:[3]
The interplay, known as categorical logic, has been a subject of active research since then; see the monograph of Jacobs (1999) for instance.



Type system - Wikipedia
In programming languages, a type system is a set of rules that assigns a property called type to the various constructs of a computer program, such as variables, expressions, functions or modules.[1]  These types formalize and enforce the otherwise implicit categories the programmer uses for algebraic data types, data structures, or other components (e.g. "string", "array of float", "function returning boolean"). The main purpose of a type system is to reduce possibilities for bugs in computer programs[2] by defining interfaces between different parts of a computer program, and then checking that the parts have been connected in a consistent way. This checking can happen statically (at compile time), dynamically (at run time), or as a combination of static and dynamic checking. Type systems have other purposes as well, such as expressing business rules, enabling certain compiler optimizations, allowing for multiple dispatch, providing a form of documentation, etc.
A type system associates a type with each computed value and, by examining the flow of these values, attempts to ensure or prove that no type errors can occur. The given type system in question determines exactly what constitutes a type error, but in general the aim is to prevent operations expecting a certain kind of value from being used with values for which that operation does not make sense (logic errors). Type systems are often specified as part of programming languages, and built into the interpreters and compilers for them; although the type system of a language can be extended by optional tools that perform added kinds of checks using the language's original type syntax and grammar.
An example of a simple type system is that of the C language. The portions of a C program are the function definitions.  One function is invoked by another function.  The interface of a function states the name of the function and a list of values that are passed to the function's code.  The code of an invoking function states the name of the invoked, along with the names of variables that hold values to pass to it.  During execution, the values are placed into temporary storage, then execution jumps to the code of the invoked function.  The invoked function's code accesses the values and makes use of them.  If the instructions inside the function are written with the assumption of receiving an integer value, but the calling code passed a floating-point value, then the wrong result will be computed by the invoked function.  The C compiler checks the type declared for each variable sent, against the type declared for each variable in the interface of the invoked function.  If the types do not match, the compiler throws a compile-time error.
A compiler may also use the static type of a value to optimize the storage it needs and the choice of algorithms for operations on the value. In many C compilers the float data type, for example, is represented in 32 bits, in accord with the IEEE specification for single-precision floating point numbers. They will thus use floating-point-specific microprocessor operations on those values (floating-point addition, multiplication, etc.).
The depth of type constraints and the manner of their evaluation affect the typing of the language. A programming language may further associate an operation with various resolutions for each type, in the case of type polymorphism. Type theory is the study of type systems. The concrete types of some programming languages, such as integers and strings, depend on practical issues of computer architecture, compiler implementation, and language design.
Formally, type theory studies type systems. A programming language must have occurrence to type check using the type system whether at compile time or runtime, manually annotated or automatically inferred.  As Mark Manasse concisely put it:[3]
The fundamental problem addressed by a type theory is to ensure that programs have meaning. The fundamental problem caused by a type theory is that meaningful programs may not have meanings ascribed to them. The quest for richer type systems results from this tension.
Assigning a data type, termed typing, gives meaning to a sequence of bits such as a value in memory or some object such as a variable. The hardware of a general purpose computer is unable to discriminate between for example a memory address and an instruction code, or between a character, an integer, or a floating-point number, because it makes no intrinsic distinction between any of the possible values that a sequence of bits might mean.[note 1] Associating a sequence of bits with a type conveys that meaning to the programmable hardware to form a symbolic system composed of that hardware and some program.
A program associates each value with at least one specific type, but it also can occur that one value is associated with many subtypes.  Other entities, such as objects, modules, communication channels, and dependencies can become associated with a type.  Even a type can become associated with a type.  An implementation of a type system could in theory associate identifications called data type (a type of a value), class (a type of an object), and kind (a type of a type, or metatype). These are the abstractions that typing can go through, on a hierarchy of levels contained in a system.
When a programming language evolves a more elaborate type system, it gains a more finely grained rule set than basic type checking, but this comes at a price when the type inferences (and other properties) become undecidable, and when more attention must be paid by the programmer to annotate code or to consider computer-related operations and functioning. It is challenging to find a sufficiently expressive type system that satisfies all programming practices in a type safe manner.
The more type restrictions that are imposed by the compiler, the more strongly typed a programming language is.  Strongly typed languages often require the programmer to make explicit conversions in contexts where an implicit conversion would cause no harm.  Pascal's type system has been described as "too strong" because, for example, the size of an array or string is part of its type, making some programming tasks difficult.[4][5] Haskell is also strongly typed but its types are automatically inferred so that explicit conversions are often (but not always) unnecessary.
A programming language compiler can also implement a dependent type or an effect system, which enables even more program specifications to be verified by a type checker. Beyond simple value-type pairs, a virtual "region" of code is associated with an "effect" component describing what is being done with what, and enabling for example to "throw" an error report.  Thus the symbolic system may be a type and effect system, which endows it with more safety checking than type checking alone.
Whether automated by the compiler or specified by a programmer, a type system makes program behavior illegal if outside the type-system rules. Advantages provided by programmer-specified type systems include:
Advantages provided by compiler-specified type systems include:
A type error is an unintended condition which might manifest in multiple stages of a program's development. Thus a facility for detection of the error is needed in the type system. In some languages, such as Haskell, for which type inference is automated, lint might be available to its compiler to aid in the detection of error.
Type safety contributes to program correctness, but might only guarantee correctness at the cost of making the type checking itself an undecidable problem.[citation needed]  In a type system with automated type checking a program may prove to run incorrectly yet be safely typed, and produce no compiler errors. Division by zero is an unsafe and incorrect operation, but a type checker running at compile time only does not scan for division by zero in most languages, and then it is left as a runtime error. To prove the absence of these more-general-than-types defects, other kinds of formal methods, collectively known as program analyses, are in common use. Alternatively, a sufficiently expressive type system, such as in dependently typed languages, can prevent these kinds of errors (for example, expressing the type of non-zero numbers).  In addition software testing is an empirical method for finding errors that the type checker cannot detect.
The process of verifying and enforcing the constraints of types—type checking—may occur either at compile-time (a static check) or at run-time. If a language specification requires its typing rules strongly (i.e., more or less allowing only those automatic type conversions that do not lose information), one can refer to the process as strongly typed, if not, as weakly typed. The terms are not usually used in a strict sense.

Static type checking is the process of verifying the type safety of a program based on analysis of a program's text (source code). If a program passes a static type checker, then the program is guaranteed to satisfy some set of type safety properties for all possible inputs.
Static type checking can be considered a limited form of program verification (see type safety), and in a type-safe language, can be considered also an optimization. If a compiler can prove that a program is well-typed, then it does not need to emit dynamic safety checks, allowing the resulting compiled binary to run faster and to be smaller.
Static type checking for Turing-complete languages is inherently conservative. That is, if a type system is both sound (meaning that it rejects all incorrect programs) and decidable (meaning that it is possible to write an algorithm that determines whether a program is well-typed), then it must be incomplete (meaning there are correct programs, which are also rejected, even though they do not encounter runtime errors).[6] For example, consider a program containing the code:
if <complex test> then <do something> else <signal that there is a type error>
Even if the expression <complex test> always evaluates to true at run-time, most type checkers will reject the program as ill-typed, because it is difficult (if not impossible) for a static analyzer to determine that the else branch will not be taken.[7] Conversely, a static type checker will quickly detect type errors in rarely used code paths. Without static type checking, even code coverage tests with 100% coverage may be unable to find such type errors. The tests may fail to detect such type errors, because the combination of all places where values are created and all places where a certain value is used must be taken into account.
A number of useful and common programming language features cannot be checked statically, such as downcasting. Thus, many languages will have both static and dynamic type checking; the static type checker verifies what it can, and dynamic checks verify the rest.
Many languages with static type checking provide a way to bypass the type checker. Some languages allow programmers to choose between static and dynamic type safety. For example, C# distinguishes between statically-typed and dynamically-typed variables. Uses of the former are checked statically, whereas uses of the latter are checked dynamically. Other languages allow writing code that is not type-safe; for example, in C, programmers can freely cast a value between any two types that have the same size, effectively subverting the type concept.
For a list of languages with static type checking, see the category for statically typed languages.

Dynamic type checking is the process of verifying the type safety of a program at runtime. Implementations of dynamically type-checked languages generally associate each runtime object with a type tag (i.e., a reference to a type) containing its type information. This runtime type information (RTTI) can also be used to implement dynamic dispatch, late binding, downcasting, reflection, and similar features.
Most type-safe languages include some form of dynamic type checking, even if they also have a static type checker.[citation needed] The reason for this is that many useful features or properties are difficult or impossible to verify statically. For example, suppose that a program defines two types, A and B, where B is a subtype of A. If the program tries to convert a value of type A to type B, which is known as downcasting, then the operation is legal only if the value being converted is actually a value of type B. Thus, a dynamic check is needed to verify that the operation is safe. This requirement is one of the criticisms of downcasting.
By definition, dynamic type checking may cause a program to fail at runtime. In some programming languages, it is possible to anticipate and recover from these failures. In others, type-checking errors are considered fatal.
Programming languages that include dynamic type checking but not static type checking are often called "dynamically typed programming languages". For a list of such languages, see the category for dynamically typed programming languages.

Some languages allow both static and dynamic typing (type checking), sometimes called soft typing. For example, Java and some other ostensibly statically typed languages support downcasting types to their subtypes, querying an object to discover its dynamic type, and other type operations that depend on runtime type information. More generally, most programming languages include mechanisms for dispatching over different 'kinds' of data, such as disjoint unions, subtype polymorphism, and variant types. Even when not interacting with type annotations or type checking, such mechanisms are materially similar to dynamic typing implementations. See programming language for more discussion of the interactions between static and dynamic typing.
Objects in object-oriented languages are usually accessed by a reference whose static target type (or manifest type) is equal to either the object's run-time type (its latent type) or a supertype thereof. This is conformant with the Liskov substitution principle, which states that all operations performed on an instance of a given type can also be performed on an instance of a subtype. This concept is also known as subsumption. In some languages subtypes may also possess covariant or contravariant return types and argument types respectively.
Certain languages, for example Clojure, Common Lisp, or Cython are dynamically type-checked by default, but allow programs to opt into static type checking by providing optional annotations. One reason to use such hints would be to optimize the performance of critical sections of a program. This is formalized by gradual typing. The programming environment DrRacket, a  pedagogic environment based on Lisp, and a precursor of the language Racket was also soft-typed.
Conversely, as of version 4.0, the C# language provides a way to indicate that a variable should not be statically type-checked. A variable whose type is dynamic will not be subject to static type checking. Instead, the program relies on runtime type information to determine how the variable may be used.[8]
The choice between static and dynamic typing requires certain trade-offs.
Static typing can find type errors reliably at compile time, which should increase the reliability of the delivered program. However, programmers disagree over how commonly type errors occur, resulting in further disagreements over the proportion of those bugs that are coded that would be caught by appropriately representing the designed types in code.[9][10] Static typing advocates[who?] believe programs are more reliable when they have been well type-checked, whereas dynamic-typing advocates[who?] point to distributed code that has proven reliable and to small bug databases.[citation needed] The value of static typing, then, presumably[vague] increases as the strength of the type system is increased. Advocates of dependent typing,[who?] implemented in languages such as Dependent ML and Epigram, have suggested that almost all bugs can be considered type errors, if the types used in a program are properly declared by the programmer or correctly inferred by the compiler.[11]
Static typing usually results in compiled code that executes faster. When the compiler knows the exact data types that are in use (which is necessary for static verification, either through declaration or inference) it can produce optimized machine code. Some dynamically typed languages such as Common Lisp allow optional type declarations for optimization for this reason.
By contrast, dynamic typing may allow compilers to run faster and interpreters to dynamically load new code, because changes to source code in dynamically typed languages may result in less checking to perform and less code to revisit.[clarification needed] This too may reduce the edit-compile-test-debug cycle.
Statically typed languages that lack type inference (such as C and Java) require that programmers declare the types that a method or function must use. This can serve as added program documentation, that is active and dynamic, instead of static. This allows a compiler to prevent it from drifting out of synchrony, and from being ignored by programmers. However, a language can be statically typed without requiring type declarations (examples include Haskell, Scala, OCaml, F#, and to a lesser extent C# and C++), so explicit type declaration is not a necessary requirement for static typing in all languages.
Dynamic typing allows constructs that some static type checking would reject as illegal. For example, eval functions, which execute arbitrary data as code, become possible. An eval function is possible with static typing, but requires advanced uses of algebraic data types. Further, dynamic typing better accommodates transitional code and prototyping, such as allowing a placeholder data structure (mock object) to be transparently used in place of a full data structure (usually for the purposes of experimentation and testing).
Dynamic typing typically allows duck typing (which enables easier code reuse). Many[specify] languages with static typing also feature duck typing or other mechanisms like generic programming that also enable easier code reuse.
Dynamic typing typically makes metaprogramming easier to use. For example, C++ templates are typically more cumbersome to write than the equivalent Ruby or Python code since C++ has stronger rules regarding type definitions (for both functions and variables). This forces a developer to write more boilerplate code for a template than a Python developer would need to. More advanced run-time constructs such as metaclasses and introspection are often harder to use in statically typed languages. In some languages, such features may also be used e.g. to generate new types and behaviors on the fly, based on run-time data. Such advanced constructs are often provided by dynamic programming languages; many of these are dynamically typed, although dynamic typing need not be related to dynamic programming languages.
Languages are often colloquially referred to as strongly typed or weakly typed. In fact, there is no universally accepted definition of what these terms mean. In general, there are more precise terms to represent the differences between type systems that lead people to call them "strong" or "weak".
A third way of categorizing the type system of a programming language uses the safety of typed operations and conversions. Computer scientists consider a language "type-safe" if it does not allow operations or conversions that violate the rules of the type system.
Some observers use the term memory-safe language (or just safe language) to describe languages that do not allow programs to access memory that has not been assigned for their use. For example, a memory-safe language will check array bounds, or else statically guarantee (i.e., at compile time before execution) that array accesses out of the array boundaries will cause compile-time and perhaps runtime errors.
Consider the following program of a language that is both type-safe and memory-safe:[12]
In this example, the variable z will have the value 42. Although this may not be what the programmer anticipated, it is a well-defined result. If y were a different string, one that could not be converted to a number (e.g. "Hello World"), the result would be well-defined as well. Note that a program can be type-safe or memory-safe and still crash on an invalid operation; in fact, if a program encounters an operation that is not type-safe, terminating the program is often the only option.
Now consider a similar example in C:
In this example z will point to a memory address five characters beyond y, equivalent to three characters after the terminating zero character of the string pointed to by y. This is memory that the program is not expected to access. It may contain garbage data, and it certainly doesn't contain anything useful. As this example shows, C is neither a memory-safe nor a type-safe language.
In general, type-safety and memory-safety go hand in hand. For example, a language that supports pointer arithmetic and number-to-pointer conversions (like C) is neither memory-safe nor type-safe, because it allows arbitrary memory to be accessed as if it were valid memory of any type.
For more information, see memory safety.
Some languages allow different levels of checking to apply to different regions of code. Examples include:
Additional tools such as lint and IBM Rational Purify can also be used to achieve a higher level of strictness.
It has been proposed, chiefly by Gilad Bracha, that the choice of type system be made independent of choice of language; that a type system should be a module that can be plugged into a language as needed. He believes this is advantageous, because what he calls mandatory type systems make languages less expressive and code more fragile.[17] The requirement that types do not affect the semantics of the language is difficult to fulfill.
Optional typing is related to gradual typing, but still distinct from it.[18][better source needed]
The term polymorphism refers to the ability of code (especially, functions or classes) to act on values of multiple types, or to the ability of different instances of the same data structure to contain elements of different types. Type systems that allow polymorphism generally do so in order to improve the potential for code re-use: in a language with polymorphism, programmers need only implement a data structure such as a list or an associative array once, rather than once for each type of element with which they plan to use it. For this reason computer scientists sometimes call the use of certain forms of polymorphism generic programming. The type-theoretic foundations of polymorphism are closely related to those of abstraction, modularity and (in some cases) subtyping.
In duck typing,[19] a statement calling a method m on an object does not rely on the declared type of the object; only that the object, of whatever type, must supply an implementation of the method called, when called, at run-time.
Duck typing differs from structural typing in that, if the part (of the whole module structure) needed for a given local computation is present at runtime, the duck type system is satisfied in its type identity analysis. On the other hand, a structural type system would require the analysis of the whole module structure at compile time to determine type identity or type dependence.
Duck typing differs from a nominative type system in a number of aspects. The most prominent ones are that for duck typing, type information is determined at runtime (as contrasted to compile time), and the name of the type is irrelevant to determine type identity or type dependence; only partial structure information is required for that for a given point in the program execution.
Duck typing uses the premise that (referring to a value) "if it walks like a duck, and quacks like a duck, then it is a duck" (this is a reference to the duck test that is attributed to James Whitcomb Riley). The term may have been coined[citation needed] by Alex Martelli in a 2000 message[20] to the comp.lang.python newsgroup (see Python).
While one controlled experiment showed an increase in developer productivity for duck typing in single developer projects,[21] other controlled experiments on API usability show the opposite.[22][23]
Many type systems have been created that are specialized for use in certain environments with certain types of data, or for out-of-band static program analysis. Frequently, these are based on ideas from formal type theory and are only available as part of prototype research systems.
Dependent types are based on the idea of using scalars or values to more precisely describe the type of some other value. For example, 




m
a
t
r
i
x

(
3
,
3
)


{\displaystyle \mathrm {matrix} (3,3)}

 might be the type of a 



3
×
3


{\displaystyle 3\times 3}

 matrix. We can then define typing rules such as the following rule for matrix multiplication:






m
a
t
r
i
x



m
u
l
t
i
p
l
y



:

m
a
t
r
i
x

(
k
,
m
)
×

m
a
t
r
i
x

(
m
,
n
)
→

m
a
t
r
i
x

(
k
,
n
)


{\displaystyle \mathrm {matrix} _{\mathrm {multiply} }:\mathrm {matrix} (k,m)\times \mathrm {matrix} (m,n)\to \mathrm {matrix} (k,n)}


where 



k


{\displaystyle k}

, 



m


{\displaystyle m}

, 



n


{\displaystyle n}

 are arbitrary positive integer values. A variant of ML called Dependent ML has been created based on this type system, but because type checking for conventional dependent types is undecidable, not all programs using them can be type-checked without some kind of limits. Dependent ML limits the sort of equality it can decide to Presburger arithmetic.
Other languages such as Epigram make the value of all expressions in the language decidable so that type checking can be decidable. However, in general proof of decidability is undecidable, so many programs require hand-written annotations that may be very non-trivial. As this impedes the development process, many language implementations provide an easy way out in the form of an option to disable this condition. This, however, comes at the cost of making the type-checker run in an infinite loop when fed programs that do not type-check, causing the compilation to fail.
Linear types, based on the theory of linear logic, and closely related to uniqueness types, are types assigned to values having the property that they have one and only one reference to them at all times. These are valuable for describing large immutable values such as files, strings, and so on, because any operation that simultaneously destroys a linear object and creates a similar object (such as 'str= str + "a"') can be optimized "under the hood" into an in-place mutation. Normally this is not possible, as such mutations could cause side effects on parts of the program holding other references to the object, violating referential transparency. They are also used in the prototype operating system Singularity for interprocess communication, statically ensuring that processes cannot share objects in shared memory in order to prevent race conditions. The Clean language (a Haskell-like language) uses this type system in order to gain a lot of speed (compared to performing a deep copy) while remaining safe.
Intersection types are types describing values that belong to both of two other given types with overlapping value sets. For example, in most implementations of C the signed char has range -128 to 127 and the unsigned char has range 0 to 255, so the intersection type of these two types would have range 0 to 127. Such an intersection type could be safely passed into functions expecting either signed or unsigned chars, because it is compatible with both types.
Intersection types are useful for describing overloaded function types: For example, if "int → int" is the type of functions taking an integer argument and returning an integer, and "float → float" is the type of functions taking a float argument and returning a float, then the intersection of these two types can be used to describe functions that do one or the other, based on what type of input they are given. Such a function could be passed into another function expecting an "int → int" function safely; it simply would not use the "float → float" functionality.
In a subclassing hierarchy, the intersection of a type and an ancestor type (such as its parent) is the most derived type. The intersection of sibling types is empty.
The Forsythe language includes a general implementation of intersection types. A restricted form is refinement types.
Union types are types describing values that belong to either of two types. For example, in C, the signed char has a -128 to 127 range, and the unsigned char has a 0 to 255 range, so the union of these two types would have an overall "virtual" range of -128 to 255 that may be used partially depending on which union member is accessed. Any function handling this union type would have to deal with integers in this complete range. More generally, the only valid operations on a union type are operations that are valid on both types being unioned. C's "union" concept is similar to union types, but is not typesafe, as it permits operations that are valid on either type, rather than both. Union types are important in program analysis, where they are used to represent symbolic values whose exact nature (e.g., value or type) is not known.
In a subclassing hierarchy, the union of a type and an ancestor type (such as its parent) is the ancestor type. The union of sibling types is a subtype of their common ancestor (that is, all operations permitted on their common ancestor are permitted on the union type, but they may also have other valid operations in common).
Existential types are frequently used in connection with record types to represent modules and abstract data types, due to their ability to separate implementation from interface. For example, the type "T = ∃X { a: X; f: (X → int); }" describes a module interface that has a data member named a of type X and a function named f that takes a parameter of the same type X and returns an integer. This could be implemented in different ways; for example:
These types are both subtypes of the more general existential type T and correspond to concrete implementation types, so any value of one of these types is a value of type T. Given a value "t" of type "T", we know that "t.f(t.a)" is well-typed, regardless of what the abstract type X is. This gives flexibility for choosing types suited to a particular implementation while clients that use only values of the interface type—the existential type—are isolated from these choices.
In general it's impossible for the typechecker to infer which existential type a given module belongs to. In the above example intT { a: int; f: (int → int); } could also have the type ∃X { a: X; f: (int → int); }. The simplest solution is to annotate every module with its intended type, e.g.:
Although abstract data types and modules had been implemented in programming languages for quite some time, it wasn't until 1988 that John C. Mitchell and Gordon Plotkin established the formal theory under the slogan: "Abstract [data] types have existential type".[24] The theory is a second-order typed lambda calculus similar to System F, but with existential instead of universal quantification.
Gradual typing is a type system in which variables may be typed either at compile-time (which is static typing) or at run-time (which is dynamic typing), allowing software developers to choose either type paradigm as appropriate, from within a single language.[25]  In particular, gradual typing uses a special type named dynamic to represent statically-unknown types, and gradual typing replaces the notion of type equality with a new relation called consistency that relates the dynamic type to every other type. The consistency relation is symmetric but not transitive.[26]
Many static type systems, such as those of C and Java, require type declarations: The programmer must explicitly associate each variable with a specific type. Others, such as Haskell's, perform type inference: The compiler draws conclusions about the types of variables based on how programmers use those variables. For example, given a function f(x, y) that adds x and y together, the compiler can infer that x and y must be numbers – since addition is only defined for numbers. Thus, any call to f elsewhere in the program that specifies a non-numeric type (such as a string or list) as an argument would signal an error.
Numerical and string constants and expressions in code can and often do imply type in a particular context. For example, an expression 3.14 might imply a type of floating-point, while [1, 2, 3] might imply a list of integers – typically an array.
Type inference is in general possible, if it is decidable in the type theory in question. Moreover, even if inference is undecidable in general for a given type theory, inference is often possible for a large subset of real-world programs. Haskell's type system, a version of Hindley–Milner, is a restriction of System Fω to so-called rank-1 polymorphic types, in which type inference is decidable. Most Haskell compilers allow arbitrary-rank polymorphism as an extension, but this makes type inference undecidable. (Type checking is decidable, however, and rank-1 programs still have type inference; higher rank polymorphic programs are rejected unless given explicit type annotations.)
Some languages like Perl 6 or C# have a unified type system.[27] This means that all C# types including primitive types inherit from a single root object. Every type in C# inherits from the Object class. Java has several primitive types that are not objects. Java provides wrapper object types that exist together with the primitive types so developers can use either the wrapper object types or the simpler non-object primitive types.
A type-checker for a statically typed language must verify that the type of any expression is consistent with the type expected by the context in which that expression appears. For example, in an assignment statement of the form x := e,
the inferred type of the expression e must be consistent with the declared or inferred type of the variable x. This notion of consistency, called compatibility, is specific to each programming language.
If the type of e and the type of x are the same, and assignment is allowed for that type, then this is a valid expression. Thus, in the simplest type systems, the question of whether two types are compatible reduces to that of whether they are equal (or equivalent). Different languages, however, have different criteria for when two type expressions are understood to denote the same type. These different equational theories of types vary widely, two extreme cases being structural type systems, in which any two types that describe values with the same structure are equivalent, and nominative type systems, in which no two syntactically distinct type expressions denote the same type (i.e., types must have the same "name" in order to be equal).
In languages with subtyping, the compatibility relation is more complex. In particular, if A is a subtype of B, then a value of type A can be used in a context where one of type B is expected, even if the reverse is not true. Like equivalence, the subtype relation is defined differently for each programming language, with many variations possible. The presence of parametric or ad hoc polymorphism in a language may also have implications for type compatibility.



Manifest typing - Wikipedia
In computer science, manifest typing is explicit identification by the software programmer of the type of each variable being declared. For example: if variable X is going to store integers then its type must be declared as integer. The term "manifest typing" is often used with the term latent typing to describe the difference between the static, compile-time type membership of the object and its run-time type identity.
In contrast, some programming languages use implicit typing (a.k.a. type inference) where the type is deduced from context at compile-time or allow for dynamic typing in which the variable is just declared and may be assigned a value of any type at runtime.
Consider the following example written in the C programming language:
Note that the variables s, x, and y were declared as a character array, floating point number, and an integer, respectively. The type system rejects, at compile-time, such fallacies as trying to add s and x.
In contrast, in Standard ML, the types do not need to be explicitly declared. Instead, the type is determined by the type of the assigned expression.
There are no manifest types in this program, but the compiler still infers the types string, real and int for them, and would reject the expression s+x as a compile-time error.



Type system - Wikipedia
In programming languages, a type system is a set of rules that assigns a property called type to the various constructs of a computer program, such as variables, expressions, functions or modules.[1]  These types formalize and enforce the otherwise implicit categories the programmer uses for algebraic data types, data structures, or other components (e.g. "string", "array of float", "function returning boolean"). The main purpose of a type system is to reduce possibilities for bugs in computer programs[2] by defining interfaces between different parts of a computer program, and then checking that the parts have been connected in a consistent way. This checking can happen statically (at compile time), dynamically (at run time), or as a combination of static and dynamic checking. Type systems have other purposes as well, such as expressing business rules, enabling certain compiler optimizations, allowing for multiple dispatch, providing a form of documentation, etc.
A type system associates a type with each computed value and, by examining the flow of these values, attempts to ensure or prove that no type errors can occur. The given type system in question determines exactly what constitutes a type error, but in general the aim is to prevent operations expecting a certain kind of value from being used with values for which that operation does not make sense (logic errors). Type systems are often specified as part of programming languages, and built into the interpreters and compilers for them; although the type system of a language can be extended by optional tools that perform added kinds of checks using the language's original type syntax and grammar.
An example of a simple type system is that of the C language. The portions of a C program are the function definitions.  One function is invoked by another function.  The interface of a function states the name of the function and a list of values that are passed to the function's code.  The code of an invoking function states the name of the invoked, along with the names of variables that hold values to pass to it.  During execution, the values are placed into temporary storage, then execution jumps to the code of the invoked function.  The invoked function's code accesses the values and makes use of them.  If the instructions inside the function are written with the assumption of receiving an integer value, but the calling code passed a floating-point value, then the wrong result will be computed by the invoked function.  The C compiler checks the type declared for each variable sent, against the type declared for each variable in the interface of the invoked function.  If the types do not match, the compiler throws a compile-time error.
A compiler may also use the static type of a value to optimize the storage it needs and the choice of algorithms for operations on the value. In many C compilers the float data type, for example, is represented in 32 bits, in accord with the IEEE specification for single-precision floating point numbers. They will thus use floating-point-specific microprocessor operations on those values (floating-point addition, multiplication, etc.).
The depth of type constraints and the manner of their evaluation affect the typing of the language. A programming language may further associate an operation with various resolutions for each type, in the case of type polymorphism. Type theory is the study of type systems. The concrete types of some programming languages, such as integers and strings, depend on practical issues of computer architecture, compiler implementation, and language design.
Formally, type theory studies type systems. A programming language must have occurrence to type check using the type system whether at compile time or runtime, manually annotated or automatically inferred.  As Mark Manasse concisely put it:[3]
The fundamental problem addressed by a type theory is to ensure that programs have meaning. The fundamental problem caused by a type theory is that meaningful programs may not have meanings ascribed to them. The quest for richer type systems results from this tension.
Assigning a data type, termed typing, gives meaning to a sequence of bits such as a value in memory or some object such as a variable. The hardware of a general purpose computer is unable to discriminate between for example a memory address and an instruction code, or between a character, an integer, or a floating-point number, because it makes no intrinsic distinction between any of the possible values that a sequence of bits might mean.[note 1] Associating a sequence of bits with a type conveys that meaning to the programmable hardware to form a symbolic system composed of that hardware and some program.
A program associates each value with at least one specific type, but it also can occur that one value is associated with many subtypes.  Other entities, such as objects, modules, communication channels, and dependencies can become associated with a type.  Even a type can become associated with a type.  An implementation of a type system could in theory associate identifications called data type (a type of a value), class (a type of an object), and kind (a type of a type, or metatype). These are the abstractions that typing can go through, on a hierarchy of levels contained in a system.
When a programming language evolves a more elaborate type system, it gains a more finely grained rule set than basic type checking, but this comes at a price when the type inferences (and other properties) become undecidable, and when more attention must be paid by the programmer to annotate code or to consider computer-related operations and functioning. It is challenging to find a sufficiently expressive type system that satisfies all programming practices in a type safe manner.
The more type restrictions that are imposed by the compiler, the more strongly typed a programming language is.  Strongly typed languages often require the programmer to make explicit conversions in contexts where an implicit conversion would cause no harm.  Pascal's type system has been described as "too strong" because, for example, the size of an array or string is part of its type, making some programming tasks difficult.[4][5] Haskell is also strongly typed but its types are automatically inferred so that explicit conversions are often (but not always) unnecessary.
A programming language compiler can also implement a dependent type or an effect system, which enables even more program specifications to be verified by a type checker. Beyond simple value-type pairs, a virtual "region" of code is associated with an "effect" component describing what is being done with what, and enabling for example to "throw" an error report.  Thus the symbolic system may be a type and effect system, which endows it with more safety checking than type checking alone.
Whether automated by the compiler or specified by a programmer, a type system makes program behavior illegal if outside the type-system rules. Advantages provided by programmer-specified type systems include:
Advantages provided by compiler-specified type systems include:
A type error is an unintended condition which might manifest in multiple stages of a program's development. Thus a facility for detection of the error is needed in the type system. In some languages, such as Haskell, for which type inference is automated, lint might be available to its compiler to aid in the detection of error.
Type safety contributes to program correctness, but might only guarantee correctness at the cost of making the type checking itself an undecidable problem.[citation needed]  In a type system with automated type checking a program may prove to run incorrectly yet be safely typed, and produce no compiler errors. Division by zero is an unsafe and incorrect operation, but a type checker running at compile time only does not scan for division by zero in most languages, and then it is left as a runtime error. To prove the absence of these more-general-than-types defects, other kinds of formal methods, collectively known as program analyses, are in common use. Alternatively, a sufficiently expressive type system, such as in dependently typed languages, can prevent these kinds of errors (for example, expressing the type of non-zero numbers).  In addition software testing is an empirical method for finding errors that the type checker cannot detect.
The process of verifying and enforcing the constraints of types—type checking—may occur either at compile-time (a static check) or at run-time. If a language specification requires its typing rules strongly (i.e., more or less allowing only those automatic type conversions that do not lose information), one can refer to the process as strongly typed, if not, as weakly typed. The terms are not usually used in a strict sense.

Static type checking is the process of verifying the type safety of a program based on analysis of a program's text (source code). If a program passes a static type checker, then the program is guaranteed to satisfy some set of type safety properties for all possible inputs.
Static type checking can be considered a limited form of program verification (see type safety), and in a type-safe language, can be considered also an optimization. If a compiler can prove that a program is well-typed, then it does not need to emit dynamic safety checks, allowing the resulting compiled binary to run faster and to be smaller.
Static type checking for Turing-complete languages is inherently conservative. That is, if a type system is both sound (meaning that it rejects all incorrect programs) and decidable (meaning that it is possible to write an algorithm that determines whether a program is well-typed), then it must be incomplete (meaning there are correct programs, which are also rejected, even though they do not encounter runtime errors).[6] For example, consider a program containing the code:
if <complex test> then <do something> else <signal that there is a type error>
Even if the expression <complex test> always evaluates to true at run-time, most type checkers will reject the program as ill-typed, because it is difficult (if not impossible) for a static analyzer to determine that the else branch will not be taken.[7] Conversely, a static type checker will quickly detect type errors in rarely used code paths. Without static type checking, even code coverage tests with 100% coverage may be unable to find such type errors. The tests may fail to detect such type errors, because the combination of all places where values are created and all places where a certain value is used must be taken into account.
A number of useful and common programming language features cannot be checked statically, such as downcasting. Thus, many languages will have both static and dynamic type checking; the static type checker verifies what it can, and dynamic checks verify the rest.
Many languages with static type checking provide a way to bypass the type checker. Some languages allow programmers to choose between static and dynamic type safety. For example, C# distinguishes between statically-typed and dynamically-typed variables. Uses of the former are checked statically, whereas uses of the latter are checked dynamically. Other languages allow writing code that is not type-safe; for example, in C, programmers can freely cast a value between any two types that have the same size, effectively subverting the type concept.
For a list of languages with static type checking, see the category for statically typed languages.

Dynamic type checking is the process of verifying the type safety of a program at runtime. Implementations of dynamically type-checked languages generally associate each runtime object with a type tag (i.e., a reference to a type) containing its type information. This runtime type information (RTTI) can also be used to implement dynamic dispatch, late binding, downcasting, reflection, and similar features.
Most type-safe languages include some form of dynamic type checking, even if they also have a static type checker.[citation needed] The reason for this is that many useful features or properties are difficult or impossible to verify statically. For example, suppose that a program defines two types, A and B, where B is a subtype of A. If the program tries to convert a value of type A to type B, which is known as downcasting, then the operation is legal only if the value being converted is actually a value of type B. Thus, a dynamic check is needed to verify that the operation is safe. This requirement is one of the criticisms of downcasting.
By definition, dynamic type checking may cause a program to fail at runtime. In some programming languages, it is possible to anticipate and recover from these failures. In others, type-checking errors are considered fatal.
Programming languages that include dynamic type checking but not static type checking are often called "dynamically typed programming languages". For a list of such languages, see the category for dynamically typed programming languages.

Some languages allow both static and dynamic typing (type checking), sometimes called soft typing. For example, Java and some other ostensibly statically typed languages support downcasting types to their subtypes, querying an object to discover its dynamic type, and other type operations that depend on runtime type information. More generally, most programming languages include mechanisms for dispatching over different 'kinds' of data, such as disjoint unions, subtype polymorphism, and variant types. Even when not interacting with type annotations or type checking, such mechanisms are materially similar to dynamic typing implementations. See programming language for more discussion of the interactions between static and dynamic typing.
Objects in object-oriented languages are usually accessed by a reference whose static target type (or manifest type) is equal to either the object's run-time type (its latent type) or a supertype thereof. This is conformant with the Liskov substitution principle, which states that all operations performed on an instance of a given type can also be performed on an instance of a subtype. This concept is also known as subsumption. In some languages subtypes may also possess covariant or contravariant return types and argument types respectively.
Certain languages, for example Clojure, Common Lisp, or Cython are dynamically type-checked by default, but allow programs to opt into static type checking by providing optional annotations. One reason to use such hints would be to optimize the performance of critical sections of a program. This is formalized by gradual typing. The programming environment DrRacket, a  pedagogic environment based on Lisp, and a precursor of the language Racket was also soft-typed.
Conversely, as of version 4.0, the C# language provides a way to indicate that a variable should not be statically type-checked. A variable whose type is dynamic will not be subject to static type checking. Instead, the program relies on runtime type information to determine how the variable may be used.[8]
The choice between static and dynamic typing requires certain trade-offs.
Static typing can find type errors reliably at compile time, which should increase the reliability of the delivered program. However, programmers disagree over how commonly type errors occur, resulting in further disagreements over the proportion of those bugs that are coded that would be caught by appropriately representing the designed types in code.[9][10] Static typing advocates[who?] believe programs are more reliable when they have been well type-checked, whereas dynamic-typing advocates[who?] point to distributed code that has proven reliable and to small bug databases.[citation needed] The value of static typing, then, presumably[vague] increases as the strength of the type system is increased. Advocates of dependent typing,[who?] implemented in languages such as Dependent ML and Epigram, have suggested that almost all bugs can be considered type errors, if the types used in a program are properly declared by the programmer or correctly inferred by the compiler.[11]
Static typing usually results in compiled code that executes faster. When the compiler knows the exact data types that are in use (which is necessary for static verification, either through declaration or inference) it can produce optimized machine code. Some dynamically typed languages such as Common Lisp allow optional type declarations for optimization for this reason.
By contrast, dynamic typing may allow compilers to run faster and interpreters to dynamically load new code, because changes to source code in dynamically typed languages may result in less checking to perform and less code to revisit.[clarification needed] This too may reduce the edit-compile-test-debug cycle.
Statically typed languages that lack type inference (such as C and Java) require that programmers declare the types that a method or function must use. This can serve as added program documentation, that is active and dynamic, instead of static. This allows a compiler to prevent it from drifting out of synchrony, and from being ignored by programmers. However, a language can be statically typed without requiring type declarations (examples include Haskell, Scala, OCaml, F#, and to a lesser extent C# and C++), so explicit type declaration is not a necessary requirement for static typing in all languages.
Dynamic typing allows constructs that some static type checking would reject as illegal. For example, eval functions, which execute arbitrary data as code, become possible. An eval function is possible with static typing, but requires advanced uses of algebraic data types. Further, dynamic typing better accommodates transitional code and prototyping, such as allowing a placeholder data structure (mock object) to be transparently used in place of a full data structure (usually for the purposes of experimentation and testing).
Dynamic typing typically allows duck typing (which enables easier code reuse). Many[specify] languages with static typing also feature duck typing or other mechanisms like generic programming that also enable easier code reuse.
Dynamic typing typically makes metaprogramming easier to use. For example, C++ templates are typically more cumbersome to write than the equivalent Ruby or Python code since C++ has stronger rules regarding type definitions (for both functions and variables). This forces a developer to write more boilerplate code for a template than a Python developer would need to. More advanced run-time constructs such as metaclasses and introspection are often harder to use in statically typed languages. In some languages, such features may also be used e.g. to generate new types and behaviors on the fly, based on run-time data. Such advanced constructs are often provided by dynamic programming languages; many of these are dynamically typed, although dynamic typing need not be related to dynamic programming languages.
Languages are often colloquially referred to as strongly typed or weakly typed. In fact, there is no universally accepted definition of what these terms mean. In general, there are more precise terms to represent the differences between type systems that lead people to call them "strong" or "weak".
A third way of categorizing the type system of a programming language uses the safety of typed operations and conversions. Computer scientists consider a language "type-safe" if it does not allow operations or conversions that violate the rules of the type system.
Some observers use the term memory-safe language (or just safe language) to describe languages that do not allow programs to access memory that has not been assigned for their use. For example, a memory-safe language will check array bounds, or else statically guarantee (i.e., at compile time before execution) that array accesses out of the array boundaries will cause compile-time and perhaps runtime errors.
Consider the following program of a language that is both type-safe and memory-safe:[12]
In this example, the variable z will have the value 42. Although this may not be what the programmer anticipated, it is a well-defined result. If y were a different string, one that could not be converted to a number (e.g. "Hello World"), the result would be well-defined as well. Note that a program can be type-safe or memory-safe and still crash on an invalid operation; in fact, if a program encounters an operation that is not type-safe, terminating the program is often the only option.
Now consider a similar example in C:
In this example z will point to a memory address five characters beyond y, equivalent to three characters after the terminating zero character of the string pointed to by y. This is memory that the program is not expected to access. It may contain garbage data, and it certainly doesn't contain anything useful. As this example shows, C is neither a memory-safe nor a type-safe language.
In general, type-safety and memory-safety go hand in hand. For example, a language that supports pointer arithmetic and number-to-pointer conversions (like C) is neither memory-safe nor type-safe, because it allows arbitrary memory to be accessed as if it were valid memory of any type.
For more information, see memory safety.
Some languages allow different levels of checking to apply to different regions of code. Examples include:
Additional tools such as lint and IBM Rational Purify can also be used to achieve a higher level of strictness.
It has been proposed, chiefly by Gilad Bracha, that the choice of type system be made independent of choice of language; that a type system should be a module that can be plugged into a language as needed. He believes this is advantageous, because what he calls mandatory type systems make languages less expressive and code more fragile.[17] The requirement that types do not affect the semantics of the language is difficult to fulfill.
Optional typing is related to gradual typing, but still distinct from it.[18][better source needed]
The term polymorphism refers to the ability of code (especially, functions or classes) to act on values of multiple types, or to the ability of different instances of the same data structure to contain elements of different types. Type systems that allow polymorphism generally do so in order to improve the potential for code re-use: in a language with polymorphism, programmers need only implement a data structure such as a list or an associative array once, rather than once for each type of element with which they plan to use it. For this reason computer scientists sometimes call the use of certain forms of polymorphism generic programming. The type-theoretic foundations of polymorphism are closely related to those of abstraction, modularity and (in some cases) subtyping.
In duck typing,[19] a statement calling a method m on an object does not rely on the declared type of the object; only that the object, of whatever type, must supply an implementation of the method called, when called, at run-time.
Duck typing differs from structural typing in that, if the part (of the whole module structure) needed for a given local computation is present at runtime, the duck type system is satisfied in its type identity analysis. On the other hand, a structural type system would require the analysis of the whole module structure at compile time to determine type identity or type dependence.
Duck typing differs from a nominative type system in a number of aspects. The most prominent ones are that for duck typing, type information is determined at runtime (as contrasted to compile time), and the name of the type is irrelevant to determine type identity or type dependence; only partial structure information is required for that for a given point in the program execution.
Duck typing uses the premise that (referring to a value) "if it walks like a duck, and quacks like a duck, then it is a duck" (this is a reference to the duck test that is attributed to James Whitcomb Riley). The term may have been coined[citation needed] by Alex Martelli in a 2000 message[20] to the comp.lang.python newsgroup (see Python).
While one controlled experiment showed an increase in developer productivity for duck typing in single developer projects,[21] other controlled experiments on API usability show the opposite.[22][23]
Many type systems have been created that are specialized for use in certain environments with certain types of data, or for out-of-band static program analysis. Frequently, these are based on ideas from formal type theory and are only available as part of prototype research systems.
Dependent types are based on the idea of using scalars or values to more precisely describe the type of some other value. For example, 




m
a
t
r
i
x

(
3
,
3
)


{\displaystyle \mathrm {matrix} (3,3)}

 might be the type of a 



3
×
3


{\displaystyle 3\times 3}

 matrix. We can then define typing rules such as the following rule for matrix multiplication:






m
a
t
r
i
x



m
u
l
t
i
p
l
y



:

m
a
t
r
i
x

(
k
,
m
)
×

m
a
t
r
i
x

(
m
,
n
)
→

m
a
t
r
i
x

(
k
,
n
)


{\displaystyle \mathrm {matrix} _{\mathrm {multiply} }:\mathrm {matrix} (k,m)\times \mathrm {matrix} (m,n)\to \mathrm {matrix} (k,n)}


where 



k


{\displaystyle k}

, 



m


{\displaystyle m}

, 



n


{\displaystyle n}

 are arbitrary positive integer values. A variant of ML called Dependent ML has been created based on this type system, but because type checking for conventional dependent types is undecidable, not all programs using them can be type-checked without some kind of limits. Dependent ML limits the sort of equality it can decide to Presburger arithmetic.
Other languages such as Epigram make the value of all expressions in the language decidable so that type checking can be decidable. However, in general proof of decidability is undecidable, so many programs require hand-written annotations that may be very non-trivial. As this impedes the development process, many language implementations provide an easy way out in the form of an option to disable this condition. This, however, comes at the cost of making the type-checker run in an infinite loop when fed programs that do not type-check, causing the compilation to fail.
Linear types, based on the theory of linear logic, and closely related to uniqueness types, are types assigned to values having the property that they have one and only one reference to them at all times. These are valuable for describing large immutable values such as files, strings, and so on, because any operation that simultaneously destroys a linear object and creates a similar object (such as 'str= str + "a"') can be optimized "under the hood" into an in-place mutation. Normally this is not possible, as such mutations could cause side effects on parts of the program holding other references to the object, violating referential transparency. They are also used in the prototype operating system Singularity for interprocess communication, statically ensuring that processes cannot share objects in shared memory in order to prevent race conditions. The Clean language (a Haskell-like language) uses this type system in order to gain a lot of speed (compared to performing a deep copy) while remaining safe.
Intersection types are types describing values that belong to both of two other given types with overlapping value sets. For example, in most implementations of C the signed char has range -128 to 127 and the unsigned char has range 0 to 255, so the intersection type of these two types would have range 0 to 127. Such an intersection type could be safely passed into functions expecting either signed or unsigned chars, because it is compatible with both types.
Intersection types are useful for describing overloaded function types: For example, if "int → int" is the type of functions taking an integer argument and returning an integer, and "float → float" is the type of functions taking a float argument and returning a float, then the intersection of these two types can be used to describe functions that do one or the other, based on what type of input they are given. Such a function could be passed into another function expecting an "int → int" function safely; it simply would not use the "float → float" functionality.
In a subclassing hierarchy, the intersection of a type and an ancestor type (such as its parent) is the most derived type. The intersection of sibling types is empty.
The Forsythe language includes a general implementation of intersection types. A restricted form is refinement types.
Union types are types describing values that belong to either of two types. For example, in C, the signed char has a -128 to 127 range, and the unsigned char has a 0 to 255 range, so the union of these two types would have an overall "virtual" range of -128 to 255 that may be used partially depending on which union member is accessed. Any function handling this union type would have to deal with integers in this complete range. More generally, the only valid operations on a union type are operations that are valid on both types being unioned. C's "union" concept is similar to union types, but is not typesafe, as it permits operations that are valid on either type, rather than both. Union types are important in program analysis, where they are used to represent symbolic values whose exact nature (e.g., value or type) is not known.
In a subclassing hierarchy, the union of a type and an ancestor type (such as its parent) is the ancestor type. The union of sibling types is a subtype of their common ancestor (that is, all operations permitted on their common ancestor are permitted on the union type, but they may also have other valid operations in common).
Existential types are frequently used in connection with record types to represent modules and abstract data types, due to their ability to separate implementation from interface. For example, the type "T = ∃X { a: X; f: (X → int); }" describes a module interface that has a data member named a of type X and a function named f that takes a parameter of the same type X and returns an integer. This could be implemented in different ways; for example:
These types are both subtypes of the more general existential type T and correspond to concrete implementation types, so any value of one of these types is a value of type T. Given a value "t" of type "T", we know that "t.f(t.a)" is well-typed, regardless of what the abstract type X is. This gives flexibility for choosing types suited to a particular implementation while clients that use only values of the interface type—the existential type—are isolated from these choices.
In general it's impossible for the typechecker to infer which existential type a given module belongs to. In the above example intT { a: int; f: (int → int); } could also have the type ∃X { a: X; f: (int → int); }. The simplest solution is to annotate every module with its intended type, e.g.:
Although abstract data types and modules had been implemented in programming languages for quite some time, it wasn't until 1988 that John C. Mitchell and Gordon Plotkin established the formal theory under the slogan: "Abstract [data] types have existential type".[24] The theory is a second-order typed lambda calculus similar to System F, but with existential instead of universal quantification.
Gradual typing is a type system in which variables may be typed either at compile-time (which is static typing) or at run-time (which is dynamic typing), allowing software developers to choose either type paradigm as appropriate, from within a single language.[25]  In particular, gradual typing uses a special type named dynamic to represent statically-unknown types, and gradual typing replaces the notion of type equality with a new relation called consistency that relates the dynamic type to every other type. The consistency relation is symmetric but not transitive.[26]
Many static type systems, such as those of C and Java, require type declarations: The programmer must explicitly associate each variable with a specific type. Others, such as Haskell's, perform type inference: The compiler draws conclusions about the types of variables based on how programmers use those variables. For example, given a function f(x, y) that adds x and y together, the compiler can infer that x and y must be numbers – since addition is only defined for numbers. Thus, any call to f elsewhere in the program that specifies a non-numeric type (such as a string or list) as an argument would signal an error.
Numerical and string constants and expressions in code can and often do imply type in a particular context. For example, an expression 3.14 might imply a type of floating-point, while [1, 2, 3] might imply a list of integers – typically an array.
Type inference is in general possible, if it is decidable in the type theory in question. Moreover, even if inference is undecidable in general for a given type theory, inference is often possible for a large subset of real-world programs. Haskell's type system, a version of Hindley–Milner, is a restriction of System Fω to so-called rank-1 polymorphic types, in which type inference is decidable. Most Haskell compilers allow arbitrary-rank polymorphism as an extension, but this makes type inference undecidable. (Type checking is decidable, however, and rank-1 programs still have type inference; higher rank polymorphic programs are rejected unless given explicit type annotations.)
Some languages like Perl 6 or C# have a unified type system.[27] This means that all C# types including primitive types inherit from a single root object. Every type in C# inherits from the Object class. Java has several primitive types that are not objects. Java provides wrapper object types that exist together with the primitive types so developers can use either the wrapper object types or the simpler non-object primitive types.
A type-checker for a statically typed language must verify that the type of any expression is consistent with the type expected by the context in which that expression appears. For example, in an assignment statement of the form x := e,
the inferred type of the expression e must be consistent with the declared or inferred type of the variable x. This notion of consistency, called compatibility, is specific to each programming language.
If the type of e and the type of x are the same, and assignment is allowed for that type, then this is a valid expression. Thus, in the simplest type systems, the question of whether two types are compatible reduces to that of whether they are equal (or equivalent). Different languages, however, have different criteria for when two type expressions are understood to denote the same type. These different equational theories of types vary widely, two extreme cases being structural type systems, in which any two types that describe values with the same structure are equivalent, and nominative type systems, in which no two syntactically distinct type expressions denote the same type (i.e., types must have the same "name" in order to be equal).
In languages with subtyping, the compatibility relation is more complex. In particular, if A is a subtype of B, then a value of type A can be used in a context where one of type B is expected, even if the reverse is not true. Like equivalence, the subtype relation is defined differently for each programming language, with many variations possible. The presence of parametric or ad hoc polymorphism in a language may also have implications for type compatibility.



Strong and weak typing - Wikipedia
In computer programming, programming languages are often colloquially classified as to whether the language's type system makes it strongly typed or weakly typed (loosely typed). Generally, a strongly typed language has stricter typing rules at compile time, which implies that errors and exceptions are more likely to happen during compilation. Most of these rules affect variable assignment, return values and function calling. On the other hand, a weakly typed language has looser typing rules and may produce unpredictable results or may perform implicit type conversion at runtime.[1] A different but related concept is latent typing.
In 1974, Liskov and Zilles defined a strongly-typed language as one in which "whenever an object is passed from a calling function to a called function, its type must be compatible with the type declared in the called function."[2]
In 1977, Jackson wrote, "In a strongly typed language each data area will have a distinct type and each process will state its communication requirements in terms of these types."[3]
A number of different language design decisions have been referred to as evidence of "strong" or "weak" typing. In fact, many of these are more accurately understood as the presence or absence of type safety, memory safety, static type-checking, or dynamic type-checking.
"Strong typing" generally refers to use of programming language types in order to both capture invariants of the code, and ensure its correctness, and definitely exclude certain classes of programming errors. Thus there are many "strong typing" disciplines used to achieve these goals.
Some programming languages make it easy to use a value of one type as if it were a value of another type. This is sometimes described as "weak typing".
For example, Aahz Maruch opines that "Coercion occurs when you have a statically typed language and you use the syntactic features of the language to force the usage of one type as if it were a different type (consider the common use of void* in C). Coercion is usually a symptom of weak typing. Conversion, on the other hand, creates a brand-new object of the appropriate type."[4]
As another example, GCC describes this as type-punning and warns that it will break strict aliasing. Thiago Macieira discusses several problems that can arise when type-punning causes the compiler to make inappropriate optimizations.[5]
There are many examples of languages that allow implicit type conversions, but in a type-safe manner. For example, both C++ and C# allow programs to define operators to convert a value from one type to another in a semantically meaningful way. When a C++ compiler encounters such a conversion, it treats the operation just like a function call. In contrast, converting a value to the C type .mw-parser-output .monospaced{font-family:monospace,monospace}void* is an unsafe operation that is invisible to the compiler.
Some programming languages expose pointers as if they were numeric values, and allow users to perform arithmetic on them. These languages are sometimes referred to as "weakly typed", since pointer arithmetic can be used to bypass the language's type system.
Some programming languages support untagged unions, which allow a value of one type to be viewed as if it were a value of another type.
In Luca Cardelli's article Typeful Programming,[6] a "strong type system" is described as one in which there is no possibility of an unchecked runtime type error. In other writing, the absence of unchecked run-time errors is referred to as safety or type safety; Tony Hoare's early papers call this property security[7].
In theory, all programming languages have static type-checking. However, some programming languages axiomatise the correctness of the statements written by the programmer at compile-time and instead leave it up to the language runtime to check for semantic correctness of such statements. For example, a variable might store either a number on the "then" branch of an if statement or the Boolean value "false" on the "else" branch, which the type-system would allow at compile-time. This is often called "dynamic" type-checking, but it should be noted that such a programming language is in fact not "dynamically" typed.
Note that some of these definitions are contradictory, others are merely conceptually independent, and still others are special cases (with additional constraints) of other, more "liberal" (less strong) definitions. Because of the wide divergence among these definitions, it is possible to defend claims about most programming languages that they are either strongly or weakly typed. For instance:
For this reason, writers who wish to write unambiguously about type systems often eschew the term "strong typing" in favor of specific expressions such as "type safety".



Strong and weak typing - Wikipedia
In computer programming, programming languages are often colloquially classified as to whether the language's type system makes it strongly typed or weakly typed (loosely typed). Generally, a strongly typed language has stricter typing rules at compile time, which implies that errors and exceptions are more likely to happen during compilation. Most of these rules affect variable assignment, return values and function calling. On the other hand, a weakly typed language has looser typing rules and may produce unpredictable results or may perform implicit type conversion at runtime.[1] A different but related concept is latent typing.
In 1974, Liskov and Zilles defined a strongly-typed language as one in which "whenever an object is passed from a calling function to a called function, its type must be compatible with the type declared in the called function."[2]
In 1977, Jackson wrote, "In a strongly typed language each data area will have a distinct type and each process will state its communication requirements in terms of these types."[3]
A number of different language design decisions have been referred to as evidence of "strong" or "weak" typing. In fact, many of these are more accurately understood as the presence or absence of type safety, memory safety, static type-checking, or dynamic type-checking.
"Strong typing" generally refers to use of programming language types in order to both capture invariants of the code, and ensure its correctness, and definitely exclude certain classes of programming errors. Thus there are many "strong typing" disciplines used to achieve these goals.
Some programming languages make it easy to use a value of one type as if it were a value of another type. This is sometimes described as "weak typing".
For example, Aahz Maruch opines that "Coercion occurs when you have a statically typed language and you use the syntactic features of the language to force the usage of one type as if it were a different type (consider the common use of void* in C). Coercion is usually a symptom of weak typing. Conversion, on the other hand, creates a brand-new object of the appropriate type."[4]
As another example, GCC describes this as type-punning and warns that it will break strict aliasing. Thiago Macieira discusses several problems that can arise when type-punning causes the compiler to make inappropriate optimizations.[5]
There are many examples of languages that allow implicit type conversions, but in a type-safe manner. For example, both C++ and C# allow programs to define operators to convert a value from one type to another in a semantically meaningful way. When a C++ compiler encounters such a conversion, it treats the operation just like a function call. In contrast, converting a value to the C type .mw-parser-output .monospaced{font-family:monospace,monospace}void* is an unsafe operation that is invisible to the compiler.
Some programming languages expose pointers as if they were numeric values, and allow users to perform arithmetic on them. These languages are sometimes referred to as "weakly typed", since pointer arithmetic can be used to bypass the language's type system.
Some programming languages support untagged unions, which allow a value of one type to be viewed as if it were a value of another type.
In Luca Cardelli's article Typeful Programming,[6] a "strong type system" is described as one in which there is no possibility of an unchecked runtime type error. In other writing, the absence of unchecked run-time errors is referred to as safety or type safety; Tony Hoare's early papers call this property security[7].
In theory, all programming languages have static type-checking. However, some programming languages axiomatise the correctness of the statements written by the programmer at compile-time and instead leave it up to the language runtime to check for semantic correctness of such statements. For example, a variable might store either a number on the "then" branch of an if statement or the Boolean value "false" on the "else" branch, which the type-system would allow at compile-time. This is often called "dynamic" type-checking, but it should be noted that such a programming language is in fact not "dynamically" typed.
Note that some of these definitions are contradictory, others are merely conceptually independent, and still others are special cases (with additional constraints) of other, more "liberal" (less strong) definitions. Because of the wide divergence among these definitions, it is possible to defend claims about most programming languages that they are either strongly or weakly typed. For instance:
For this reason, writers who wish to write unambiguously about type systems often eschew the term "strong typing" in favor of specific expressions such as "type safety".



Perl - Wikipedia

5.26.2[2] / April 14, 2018; 5 months ago (2018-04-14)
Perl is a family of two high-level, general-purpose, interpreted, dynamic programming languages, Perl 5 and Perl 6.[7]
Though Perl is not officially an acronym,[8] there are various backronyms in use, including "Practical Extraction and Reporting Language".[9] Perl was originally developed by Larry Wall in 1987 as a general-purpose Unix scripting language to make report processing easier.[10] Since then, it has undergone many changes and revisions. Perl 6, which began as a redesign of Perl 5 in 2000, eventually evolved into a separate language. Both languages continue to be developed independently by different development teams and liberally borrow ideas from one another.
The Perl languages borrow features from other programming languages including C, shell script (sh), AWK, and sed[11]; Wall also alludes to Basic and Lisp in the introduction to Learning Perl (Schwartz & Christiansen) and so on. [12] They provide text processing facilities without the arbitrary data-length limits of many contemporary Unix commandline tools,[13] facilitating manipulation of text files. Perl 5 gained widespread popularity in the late 1990s as a CGI scripting language, in part due to its then unsurpassed regular expression and string parsing abilities.[14][15][16][17]
In addition to CGI, Perl 5 is used for system administration, network programming, finance, bioinformatics, and other applications, such as for GUIs. It has been nicknamed "the Swiss Army chainsaw of scripting languages" because of its flexibility and power,[18] and also its ugliness.[19] In 1998, it was also referred to as the "duct tape that holds the Internet together," in reference to both its ubiquitous use as a glue language and its perceived inelegance.[20]
Larry Wall began work on Perl in 1987, while working as a programmer at Unisys,[13] and released version 1.0 to the comp.sources.misc newsgroup on December 18, 1987.[21] The language expanded rapidly over the next few years.
Perl 2, released in 1988, featured a better regular expression engine. Perl 3, released in 1989, added support for binary data streams.
Originally, the only documentation for Perl was a single lengthy man page. In 1991, Programming Perl, known to many Perl programmers as the "Camel Book" because of its cover, was published and became the de facto reference for the language. At the same time, the Perl version number was bumped to 4, not to mark a major change in the language but to identify the version that was well documented by the book.
Perl 4 went through a series of maintenance releases, culminating in Perl 4.036 in 1993. At that point, Wall abandoned Perl 4 to begin work on Perl 5.  Initial design of Perl 5 continued into 1994. The perl5-porters mailing list was established in May 1994 to coordinate work on porting Perl 5 to different platforms. It remains the primary forum for development, maintenance, and porting of Perl 5.[22]
Perl 5.000 was released on October 17, 1994.[23] It was a nearly complete rewrite of the interpreter, and it added many new features to the language, including objects, references, lexical (my) variables, and modules. Importantly, modules provided a mechanism for extending the language without modifying the interpreter. This allowed the core interpreter to stabilize, even as it enabled ordinary Perl programmers to add new language features.  Perl 5 has been in active development since then.
Perl 5.001 was released on March 13, 1995.  Perl 5.002 was released on February 29, 1996 with the new prototypes feature.  This allowed module authors to make subroutines that behaved like Perl builtins.  Perl 5.003 was released June 25, 1996, as a security release.
One of the most important events in Perl 5 history took place outside of the language proper and was a consequence of its module support. On October 26, 1995, the Comprehensive Perl Archive Network (CPAN) was established as a repository for Perl modules and Perl itself; as of May 2017, it carries over 185,178 modules in 35,190 distributions, written by more than 13,071 authors, and is mirrored worldwide at more than 245 locations.[24]
Perl 5.004 was released on May 15, 1997, and included among other things the UNIVERSAL package, giving Perl a base object to which all classes were automatically derived and the ability to require versions of modules. Another significant development was the inclusion of the CGI.pm module,[25] which contributed to Perl's popularity as a CGI scripting language.[26]
Perl is also now supported running under Microsoft Windows and several other operating systems.[25]
Perl 5.005 was released on July 22, 1998.  This release included several enhancements to the regex engine, new hooks into the backend through the B::* modules, the qr// regex quote operator, a large selection of other new core modules, and added support for several more operating systems, including BeOS.[27]
Perl 5.6 was released on March 22, 2000.  Major changes included 64-bit support, Unicode string representation, support for files over 2 GiB, and the "our" keyword.[29][30]  When developing Perl 5.6, the decision was made to switch the versioning scheme to one more similar to other open source projects; after 5.005_63, the next version became 5.5.640, with plans for development versions to have odd numbers and stable versions to have even numbers.
In 2000, Wall put forth a call for suggestions for a new version of Perl from the community. The process resulted in 361 RFC (request for comments) documents that were to be used in guiding development of Perl 6. In 2001,[31] work began on the "Apocalypses" for Perl 6, a series of documents meant to summarize the change requests and present the design of the next generation of Perl. They were presented as a digest of the RFCs, rather than a formal document. At this point, Perl 6 existed only as a description of a language.
Perl 5.8 was first released on July 18, 2002, and had nearly yearly updates since then. Perl 5.8 improved Unicode support, added a new I/O implementation, added a new thread implementation, improved numeric accuracy, and added several new modules.[32] As of 2013 this version still remains the most popular version of Perl and is used by Red Hat 5, Suse 10, Solaris 10, HP-UX 11.31 and AIX 5.
In 2004, work began on the "Synopses" – documents that originally summarized the Apocalypses, but which became the specification for the Perl 6 language. In February 2005, Audrey Tang began work on Pugs, a Perl 6 interpreter written in Haskell.[33] This was the first concerted effort towards making Perl 6 a reality.  This effort stalled in 2006.[34]
On December 18, 2007, the 20th anniversary of Perl 1.0, Perl 5.10.0 was released. Perl 5.10.0 included notable new features, which brought it closer to Perl 6. These included a switch statement (called "given"/"when"), regular expressions updates, and the smart match operator[clarification needed], "~~".[35][36]
Around this same time, development began in earnest on another implementation of Perl 6 known as Rakudo Perl, developed in tandem with the Parrot virtual machine.  As of November 2009, Rakudo Perl has had regular monthly releases and now is the most complete implementation of Perl 6.
A major change in the development process of Perl 5 occurred with Perl 5.11; the development community has switched to a monthly release cycle of development releases, with a yearly schedule of stable releases. By that plan, bugfix point releases will follow the stable releases every three months.
On April 12, 2010, Perl 5.12.0 was released. Notable core enhancements include new package NAME VERSION syntax, the Yada Yada operator (intended to mark placeholder code that is not yet implemented), implicit strictures, full Y2038 compliance, regex conversion overloading, DTrace support, and Unicode 5.2.[37] On January 21, 2011, Perl 5.12.3 was released; it contains updated modules and some documentation changes.[38] Version 5.12.4 was released on June 20, 2011. The latest version of that branch, 5.12.5, was released on November 10, 2012.
On May 14, 2011, Perl 5.14 was released. JSON support is built-in as of 5.14.0.[39] The latest version of that branch, 5.14.4, was released on March 10, 2013.
On May 20, 2012, Perl 5.16 was released. Notable new features include the ability to specify a given version of Perl that one wishes to emulate, allowing users to upgrade their version of Perl, but still run old scripts that would normally be incompatible.[40] Perl 5.16 also updates the core to support Unicode 6.1.[40]
On May 18, 2013, Perl 5.18 was released. Notable new features include the new dtrace hooks, lexical subs, more CORE:: subs, overhaul of the hash for security reasons, support for Unicode 6.2.[41]
On May 27, 2014, Perl 5.20 was released. Notable new features include subroutine signatures, hash slices/new slice syntax, postfix dereferencing (experimental), Unicode 6.3, rand() using consistent random number generator.[42]
Some observers credit the release of Perl 5.10 with the start of the Modern Perl movement.[43] In particular, this phrase describes a style of development that embraces the use of the CPAN, takes advantage of recent developments in the language, and is rigorous about creating high quality code.[44] While the book "Modern Perl"[45] may be the most visible standard-bearer of this idea, other groups such as the Enlightened Perl Organization[46] have taken up the cause.
In late 2012 and 2013, several projects for alternative implementations for Perl 5 started: Perl5 in Perl6 by the Rakudo Perl team,[47] moe by Stevan Little and friends,[48] p2[49] by the Perl11 team under Reini Urban, gperl by goccy,[50] and rperl a kickstarter project led by Will Braswell and affiliated with the Perll11 project.[51]
PONIE is an acronym for Perl On New Internal Engine. The PONIE Project existed from 2003 until 2006 and was to be a bridge between Perl 5 and Perl 6. It was an effort to rewrite the Perl 5 interpreter to run on Parrot, the Perl 6 virtual machine. The goal was to ensure the future of the millions of lines of Perl 5 code at thousands of companies around the world.[52]
The PONIE project ended in 2006 and is no longer being actively developed.  Some of the improvements made to the Perl 5 interpreter as part of PONIE were folded into that project.[53]
Perl was originally named "Pearl". Wall wanted to give the language a short name with positive connotations; he claims that he considered every three- and four-letter word in the dictionary. He also considered naming it after his wife Gloria. Wall discovered the existing PEARL programming language before Perl's official release and changed the spelling of the name.[54]
When referring to the language, the name is normally capitalized (Perl) as a proper noun. When referring to the interpreter program itself, the name is often uncapitalized (perl) because most Unix-like file systems are case-sensitive. Before the release of the first edition of Programming Perl, it was common to refer to the language as perl; Randal L. Schwartz, however, capitalized the language's name in the book to make it stand out better when typeset. This case distinction was subsequently documented as canonical.[55]
The name is occasionally expanded as Practical Extraction and Report Language, but this is a backronym.[56] Other expansions have been suggested as equally canonical, including Wall's own Pathologically Eclectic Rubbish Lister which is in the manual page for perl.[57] Indeed, Wall claims that the name was intended to inspire many different expansions.[58]
Programming Perl, published by O'Reilly Media, features a picture of a dromedary camel on the cover and is commonly called the "Camel Book".[59] This image of a camel has become an unofficial symbol of Perl as well as a general hacker emblem, appearing on T-shirts and other clothing items.
O'Reilly owns the image as a trademark but licenses it for non-commercial use, requiring only an acknowledgement and a link to www.perl.com. Licensing for commercial use is decided on a case by case basis.[60] O'Reilly also provides "Programming Republic of Perl" logos for non-commercial sites and "Powered by Perl" buttons for any site that uses Perl.[60]
The Perl Foundation owns an alternative symbol, an onion, which it licenses to its subsidiaries, Perl Mongers, PerlMonks, Perl.org, and others.[61] The symbol is a visual pun on pearl onion.[62]
Sebastian Riedel, the creator of Mojolicious, has created a logo depicting a Raptor, which is available under a CC-SA License, Version 4.0.[63] The logo is being remixed and used in different places and is symbolising Perl 5. The analogue of the raptor comes from a series of talks given by Matt S Trout beginning in 2010.[64] The talks were aimed at being more Perl 5 community centric, in a period where Perl 6 was a hot topic.
According to Wall, Perl has two slogans. The first is "There's more than one way to do it," commonly known as TMTOWTDI. The second slogan is "Easy things should be easy and hard things should be possible".[13]
The overall structure of Perl derives broadly from C. Perl is procedural in nature, with variables, expressions, assignment statements, brace-delimited blocks, control structures, and subroutines.
Perl also takes features from shell programming. All variables are marked with leading sigils, which allow variables to be interpolated directly into strings. However, unlike the shell, Perl uses sigils on all accesses to variables, and unlike most other programming languages that use sigils, the sigil doesn't denote the type of the variable but the type of the expression. So for example, to access a list of values in a hash, the sigil for an array ("@") is used, not the sigil for a hash ("%").
Perl also has many built-in functions that provide tools often used in shell programming (although many of these tools are implemented by programs external to the shell) such as sorting, and calling operating system facilities.
Perl takes lists from Lisp, hashes ("associative arrays") from AWK, and regular expressions from sed. These simplify and facilitate many parsing, text-handling, and data-management tasks. Also shared with Lisp are the implicit return of the last value in a block, and the fact that all statements have a value, and thus are also expressions and can be used in larger expressions themselves.
Perl 5 added features that support complex data structures, first-class functions (that is, closures as values), and an object-oriented programming model. These include references, packages, class-based method dispatch, and lexically scoped variables, along with compiler directives (for example, the strict pragma). A major additional feature introduced with Perl 5 was the ability to package code as reusable modules. Wall later stated that "The whole intent of Perl 5's module system was to encourage the growth of Perl culture rather than the Perl core."[65]
All versions of Perl do automatic data-typing and automatic memory management. The interpreter knows the type and storage requirements of every data object in the program; it allocates and frees storage for them as necessary using reference counting (so it cannot deallocate circular data structures without manual intervention). Legal type conversions — for example, conversions from number to string — are done automatically at run time; illegal type conversions are fatal errors.
The design of Perl can be understood as a response to three broad trends in the computer industry: falling hardware costs, rising labor costs, and improvements in compiler technology. Many earlier computer languages, such as Fortran and C, aimed to make efficient use of expensive computer hardware. In contrast, Perl was designed so that computer programmers could write programs more quickly and easily.
Perl has many features that ease the task of the programmer at the expense of greater CPU and memory requirements. These include automatic memory management; dynamic typing; strings, lists, and hashes; regular expressions; introspection; and an eval() function. Perl follows the theory of "no built-in limits,"[59] an idea similar to the Zero One Infinity rule.
Wall was trained as a linguist, and the design of Perl is very much informed by linguistic principles. Examples include Huffman coding (common constructions should be short), good end-weighting (the important information should come first), and a large collection of language primitives. Perl favors language constructs that are concise and natural for humans to write, even where they complicate the Perl interpreter.
Perl's syntax reflects the idea that "things that are different should look different."[66] For example, scalars, arrays, and hashes have different leading sigils. Array indices and hash keys use different kinds of braces. Strings and regular expressions have different standard delimiters. This approach can be contrasted with a language such as Lisp, where the same basic syntax, composed of simple and universal symbolic expressions, is used for all purposes.
Perl does not enforce any particular programming paradigm (procedural, object-oriented, functional, or others) or even require the programmer to choose among them.
There is a broad practical bent to both the Perl language and the community and culture that surround it. The preface to Programming Perl begins: "Perl is a language for getting your job done."[13] One consequence of this is that Perl is not a tidy language. It includes many features, tolerates exceptions to its rules, and employs heuristics to resolve syntactical ambiguities. Because of the forgiving nature of the compiler, bugs can sometimes be hard to find. Perl's function documentation remarks on the variant behavior of built-in functions in list and scalar contexts by saying, "In general, they do what you want, unless you want consistency."[67]
No written specification or standard for the Perl language exists for Perl versions through Perl 5, and there are no plans to create one for the current version of Perl. There has been only one implementation of the interpreter, and the language has evolved along with it. That interpreter, together with its functional tests, stands as a de facto specification of the language.  Perl 6, however, started with a specification,[68] and several projects[69] aim to implement some or all of the specification.
Perl has many and varied applications, compounded by the availability of many standard and third-party modules.
Perl has chiefly been used to write CGI scripts: large projects written in Perl include cPanel, Slash, Bugzilla, RT, TWiki, and Movable Type; high-traffic websites that use Perl extensively include Priceline.com, Craigslist,[70] IMDb,[71] LiveJournal, DuckDuckGo,[72][73] Slashdot and Ticketmaster. 
It is also an optional component of the popular LAMP technology stack for Web development, in lieu of PHP or Python.
Perl is often used as a glue language, tying together systems and interfaces that were not specifically designed to interoperate, and for "data munging,"[74] that is, converting or processing large amounts of data for tasks such as creating reports. In fact, these strengths are intimately linked. The combination makes Perl a popular all-purpose language for system administrators, particularly because short programs, often called "one-liner programs," can be entered and run on a single command line.
Perl code can be made portable across Windows and Unix; such code is often used by suppliers of software (both COTS and bespoke) to simplify packaging and maintenance of software build- and deployment-scripts.
Graphical user interfaces (GUIs) may be developed using Perl. For example, Perl/Tk and wxPerl are commonly used to enable user interaction with Perl scripts. Such interaction may be synchronous or asynchronous, using callbacks to update the GUI.
Perl is implemented as a core interpreter, written in C, together with a large collection of modules, written in Perl and C. As of  2010[update], the interpreter is 150,000 lines of C code and compiles to a 1 MB executable on typical machine architectures. Alternatively, the interpreter can be compiled to a link library and embedded in other programs. There are nearly 500 modules in the distribution, comprising 200,000 lines of Perl and an additional 350,000 lines of C code (much of the C code in the modules consists of character encoding tables).
The interpreter has an object-oriented architecture. All of the elements of the Perl language—scalars, arrays, hashes, coderefs, file handles—are represented in the interpreter by C structs. Operations on these structs are defined by a large collection of macros, typedefs, and functions; these constitute the Perl C API. The Perl API can be bewildering to the uninitiated, but its entry points follow a consistent naming scheme, which provides guidance to those who use it.
The life of a Perl interpreter divides broadly into a compile phase and a run phase.[75]  In Perl, the phases are the major stages in the interpreter's life-cycle. Each interpreter goes through each phase only once, and the phases follow in a fixed sequence.
Most of what happens in Perl's compile phase is compilation, and most of what happens in Perl's run phase is execution, but there are significant exceptions. Perl makes important use of its capability to execute Perl code during the compile phase. Perl will also delay compilation into the run phase. The terms that indicate the kind of processing that is actually occurring at any moment are compile time and run time.  Perl is in compile time at most points during the compile phase, but compile time may also be entered during the run phase. The compile time for code in a string argument passed to the eval built-in occurs during the run phase. Perl is often in run time during the compile phase and spends most of the run phase in run time.  Code in BEGIN blocks executes at run time but in the compile phase.
At compile time, the interpreter parses Perl code into a syntax tree. At run time, it executes the program by walking the tree. Text is parsed only once, and the syntax tree is subject to optimization before it is executed, so that execution is relatively efficient. Compile-time optimizations on the syntax tree include constant folding and context propagation, but peephole optimization is also performed.
Perl has a Turing-complete grammar because parsing can be affected by run-time code executed during the compile phase.[76] Therefore, Perl cannot be parsed by a straight Lex/Yacc lexer/parser combination. Instead, the interpreter implements its own lexer, which coordinates with a modified GNU bison parser to resolve ambiguities in the language.
It is often said that "Only perl can parse Perl,"[77] meaning that only the Perl interpreter (perl) can parse the Perl language (Perl), but even this is not, in general, true. Because the Perl interpreter can simulate a Turing machine during its compile phase, it would need to decide the halting problem in order to complete parsing in every case. It is a long-standing result that the halting problem is undecidable, and therefore not even perl can always parse Perl. Perl makes the unusual choice of giving the user access to its full programming power in its own compile phase. The cost in terms of theoretical purity is high, but practical inconvenience seems to be rare.
Other programs that undertake to parse Perl, such as source-code analyzers and auto-indenters, have to contend not only with ambiguous syntactic constructs but also with the undecidability of Perl parsing in the general case. Adam Kennedy's PPI project focused on parsing Perl code as a document (retaining its integrity as a document), instead of parsing Perl as executable code (that not even Perl itself can always do). It was Kennedy who first conjectured that "parsing Perl suffers from the 'halting problem',"[78] which was later proved.[79]
Perl is distributed with over 250,000 functional tests for core Perl language and over 250,000 functional tests for core modules. These run as part of the normal build process and extensively exercise the interpreter and its core modules. Perl developers rely on the functional tests to ensure that changes to the interpreter do not introduce software bugs; additionally, Perl users who see that the interpreter passes its functional tests on their system can have a high degree of confidence that it is working properly.
Perl is dual licensed under both the Artistic License 1.0[4][5] and the GNU General Public License.[6] Distributions are available for most operating systems. It is particularly prevalent on Unix and Unix-like systems, but it has been ported to most modern (and many obsolete) platforms. With only six[citation needed] reported exceptions, Perl can be compiled from source code on all POSIX-compliant, or otherwise-Unix-compatible platforms.[80]
Because of unusual changes required for the classic Mac OS environment, a special port called MacPerl was shipped independently.[81]
The Comprehensive Perl Archive Network carries a complete list of supported platforms with links to the distributions available on each.[82] CPAN is also the source for publicly available Perl modules that are not part of the core Perl distribution.
Users of Microsoft Windows typically install one of the native binary distributions of Perl for Win32, most commonly Strawberry Perl or ActivePerl. Compiling Perl from source code under Windows is possible, but most installations lack the requisite C compiler and build tools. This also makes it difficult to install modules from the CPAN, particularly those that are partially written in C.
ActivePerl is a closed source distribution from ActiveState that has regular releases that track the core Perl releases.[83] The distribution also includes the Perl package manager (PPM),[84] a popular tool for installing, removing, upgrading, and managing the use of common Perl modules.  Included also is PerlScript, a Windows Script Host (WSH) engine implementing the Perl language.  Visual Perl is an ActiveState tool that adds Perl to the Visual Studio .NET development suite.  A VBScript to Perl converter, as well as a Perl compiler for Windows, and converters of awk and sed to Perl have also been produced by this company and included on the ActiveState CD for Windows, which includes all of their distributions plus the Komodo IDE and all but the first on the Unix/Linux/Posix variant thereof in 2002 and subsequently. [85]
Strawberry Perl is an open source distribution for Windows.  It has had regular, quarterly releases since January 2008, including new modules as feedback and requests come in.  Strawberry Perl aims to be able to install modules like standard Perl distributions on other platforms, including compiling XS modules.
The Cygwin emulation layer is another way of running Perl under Windows. Cygwin provides a Unix-like environment on Windows, and both Perl and CPAN are available as standard pre-compiled packages in the Cygwin setup program. Since Cygwin also includes gcc, compiling Perl from source is also possible.
A perl executable is included in several Windows Resource kits in the directory with other scripting tools.
Implementations of Perl come with the MKS Toolkit, Interix (the base of earlier implementations of Windows Services for Unix, and UWIN.
Perl's text-handling capabilities can be used for generating SQL queries; arrays, hashes, and automatic memory management make it easy to collect and process the returned data. For example, in Tim Bunce's Perl DBI application programming interface (API), the arguments to the API can be the text of SQL queries; thus it is possible to program in multiple languages at the same time (e.g., for generating a Web page using HTML, JavaScript, and SQL in a here document). The use of Perl variable interpolation to programmatically customize each of the SQL queries, and the specification of Perl arrays or hashes as the structures to programmatically hold the resulting data sets from each SQL query, allows a high-level mechanism for handling large amounts of data for post-processing by a Perl subprogram.[86]
In early versions of Perl, database interfaces were created by relinking the interpreter with a client-side database library. This was sufficiently difficult that it was done for only a few of the most-important and most widely used databases, and it restricted the resulting perl executable to using just one database interface at a time.
In Perl 5, database interfaces are implemented by Perl DBI modules. The DBI (Database Interface) module presents a single, database-independent interface to Perl applications, while the DBD (Database Driver) modules handle the details of accessing some 50 different databases; there are DBD drivers for most ANSI SQL databases.
DBI provides caching for database handles and queries, which can greatly improve performance in long-lived execution environments such as mod perl,[87] helping high-volume systems avert load spikes as in the Slashdot effect.
In modern Perl applications, especially those written using web frameworks such as Catalyst, the DBI module is often used indirectly via object-relational mappers such as DBIx::Class, Class::DBI or Rose::DB::Object that generate SQL queries and handle data transparently to the application author.
The Computer Language Benchmarks Game compares the performance of implementations of typical programming problems in several programming languages.[88] The submitted Perl implementations typically perform toward the high end of the memory-usage spectrum and give varied speed results. Perl's performance in the benchmarks game is typical for interpreted languages.[89]
Large Perl programs start more slowly than similar programs in compiled languages because perl has to compile the source every time it runs. In a talk at the YAPC::Europe 2005 conference and subsequent article "A Timely Start," Jean-Louis Leroy found that his Perl programs took much longer to run than expected because the perl interpreter spent significant time finding modules within his over-large include path.[90] Unlike Java, Python, and Ruby, Perl has only experimental support for pre-compiling.[91] Therefore, Perl programs pay this overhead penalty on every execution. The run phase of typical programs is long enough that amortized startup time is not substantial, but benchmarks that measure very short execution times are likely to be skewed due to this overhead.
A number of tools have been introduced to improve this situation. The first such tool was Apache's mod perl, which sought to address one of the most-common reasons that small Perl programs were invoked rapidly: CGI Web development. ActivePerl, via Microsoft ISAPI, provides similar performance improvements.
Once Perl code is compiled, there is additional overhead during the execution phase that typically isn't present for programs written in compiled languages such as C or C++. Examples of such overhead include bytecode interpretation, reference-counting memory management, and dynamic type-checking.
Because Perl is an interpreted language, it can give problems when efficiency is critical; in such situations, the most critical routines can be written in other languages (such as C), which can be connected to Perl via simple Inline modules or the more complex but flexible XS mechanism.[92]
At the 2000 Perl Conference, Jon Orwant made a case for a major new language-initiative.[94] This led to a decision to begin work on a redesign of the language, to be called Perl 6. Proposals for new language features were solicited from the Perl community at large, which submitted more than 300 RFCs.
Wall spent the next few years digesting the RFCs and synthesizing them into a coherent framework for Perl 6. He has presented his design for Perl 6 in a series of documents called "apocalypses" – numbered to correspond to chapters in Programming Perl. As of  January 2011[update], the developing specification of Perl 6 is encapsulated in design documents called Synopses – numbered to correspond to Apocalypses.[95]
Thesis work by Bradley M. Kuhn, overseen by Wall, considered the possible use of the Java virtual machine as a runtime for Perl.[96]  Kuhn's thesis showed this approach to be problematic. In 2001, it was decided that Perl 6 would run on a cross-language virtual machine called Parrot. This will mean that other languages targeting the Parrot will gain native access to CPAN, allowing some level of cross-language development.
In 2005, Audrey Tang created the Pugs project, an implementation of Perl 6 in Haskell. This acted as, and continues to act as, a test platform for the Perl 6 language (separate from the development of the actual implementation) – allowing the language designers to explore. The Pugs project spawned an active Perl/Haskell cross-language community centered around the freenode #perl6 IRC channel. Many functional programming influences were absorbed by the Perl 6 design team.
In 2012, Perl 6 development was centered primarily around two compilers:[97]
In 2013, MoarVM (“Metamodel On A Runtime”), a C language-based virtual machine designed primarily for Rakudo was announced.[99]
As of  2017[update], only the Rakudo Perl implementation, MoarVM and support for an other virtual machines, such as the Java Virtual Machine and JavaScript are under active development.[100]
Development of Perl 5 is also continuing. Perl 5.12.0 was released in April 2010 with some new features influenced by the design of Perl 6,[37][101] followed by Perl 5.14.1 (released on June 17, 2011), Perl 5.16.1 (released on August 9, 2012.[102]), and Perl 5.18.0 (released on May 18, 2013). Perl 5 development versions are released on a monthly basis, with major releases coming out once per year.[103]
The relative proportion of Internet searches for 'Perl programming', as compared with similar searches for other programming languages, steadily declined from about 10% in 2005 to about 2% in 2011, and about 1% in 2018.[104]
Perl's culture and community has developed alongside the language itself. Usenet was the first public venue in which Perl was introduced, but over the course of its evolution, Perl's community was shaped by the growth of broadening Internet-based services including the introduction of the World Wide Web. The community that surrounds Perl was, in fact, the topic of Wall's first "State of the Onion" talk.[105]
State of the Onion is the name for Wall’s yearly keynote-style summaries on the progress of Perl and its community.  They are characterized by his hallmark humor, employing references to Perl’s culture, the wider hacker culture, Wall’s linguistic background, sometimes his family life, and occasionally even his Christian background.[106]
Each talk is first given at various Perl conferences and is eventually also published online.
There are a number of IRC channels that offer support for the language and some modules.
There are also many examples of code written purely for entertainment on the CPAN. Lingua::Romana::Perligata, for example, allows writing programs in Latin.[114] Upon execution of such a program, the module translates its source code into regular Perl and runs it.
The Perl community has set aside the "Acme" namespace for modules that are fun in nature (but its scope has widened to include exploratory or experimental code or any other module that is not meant to ever be used in production). Some of the Acme modules are deliberately implemented in amusing ways. This includes Acme::Bleach, one of the first modules in the Acme:: namespace,[115] which allows the program's source code to be "whitened" (i.e., all characters replaced with whitespace) and yet still work.
In older versions of Perl, one would write the Hello World program as:
Here is a more complex Perl program, that counts down the seconds up to a given threshold:
The perl interpreter can also be used for one-off scripts on the command line. The following example (as invoked from an sh-compatible shell, such as Bash) translates the string "Bob" in all files ending with .txt in the current directory to "Robert":
Perl has been referred to as "line noise" by some programmers who claim its syntax makes it a write-only language. The earliest such mention was in the first edition of the book Learning Perl, a Perl 5 tutorial book written by Randal L. Schwartz,[116] in the first chapter of which he states: "Yes, sometimes Perl looks like line noise to the uninitiated, but to the seasoned Perl programmer, it looks like checksummed line noise with a mission in life."[117] He also stated that the accusation that Perl is a write-only language could be avoided by coding with "proper care".[117] The Perl overview document perlintro states that the names of built-in "magic" scalar variables "look like punctuation or line noise".[118] The perlstyle document states that line noise in regular expressions could be mitigated using the /x modifier to add whitespace.[119]
According to the Perl 6 FAQ, Perl 6 was designed to mitigate "the usual suspects" that elicit the "line noise" claim from Perl 5 critics, including the removal of "the majority of the punctuation variables" and the sanitization of the regex syntax.[120] The Perl 6 FAQ also states that what is sometimes referred to as Perl's line noise is "the actual syntax of the language" just as gerunds and prepositions are a part of the English language.[120] In a December 2012 blog posting, despite claiming that "Rakudo Perl 6 has failed and will continue to fail unless it gets some adult supervision," chromatic stated that the design of Perl 6 has a "well-defined grammar" as well as an "improved type system, a unified object system with an intelligent metamodel, metaoperators, and a clearer system of context that provides for such niceties as pervasive laziness".[121] He also stated that "Perl 6 has a coherence and a consistency that Perl 5 lacks."[121]



Library (computing) - Wikipedia
In computer science, a library is a collection of non-volatile resources used by computer programs, often for software development. These may include configuration data, documentation, help data, message templates, pre-written code and subroutines, classes, values or type specifications. In IBM's OS/360 and its successors they are referred to as partitioned data sets.
A library is also a collection of implementations of behavior, written in terms of a language, that has a well-defined interface by which the behavior is invoked. For instance, people who want to write a higher level program can use a library to make system calls instead of implementing those system calls over and over again. In addition, the behavior is provided for reuse by multiple independent programs.  A program invokes the library-provided behavior via a mechanism of the language.  For example, in a simple imperative language such as C, the behavior in a library is invoked by using C's normal function-call.  What distinguishes the call as being to a library function, versus being to another function in the same program, is the way that the code is organized in the system.
Library code is organized in such a way that it can be used by multiple programs that have no connection to each other, while code that is part of a program is organized to be used only within that one program.  This distinction can gain a hierarchical notion when a program grows large, such as a multi-million-line program.  In that case, there may be internal libraries that are reused by independent sub-portions of the large program.  The distinguishing feature is that a library is organized for the purposes of being reused by independent programs or sub-programs, and the user only needs to know the interface, and not the internal details of the library.
The value of a library lies in the reuse of the behavior.  When a program invokes a library, it gains the behavior implemented inside that library without having to implement that behavior itself. Libraries encourage the sharing of code in a modular fashion, and ease the distribution of the code.
The behavior implemented by a library can be connected to the invoking program at different program lifecycle phases.  If the code of the library is accessed during the build of the invoking program, then the library is called a static library.[1]  An alternative is to build the executable of the invoking program and distribute that, independently of the library implementation.  The library behavior is connected after the executable has been invoked to be executed, either as part of the process of starting the execution, or in the middle of execution.  In this case the library is called a dynamic library (loaded at run time).  A dynamic library can be loaded and linked when preparing a program for execution, by the linker.  Alternatively, in the middle of execution, an application may explicitly request that a module be loaded.
Most compiled languages have a standard library although programmers can also create their own custom libraries. Most modern software systems provide libraries that implement the majority of the system services. Such libraries have commoditized the services which a modern application requires. As such, most code used by modern applications is provided in these system libraries.
The earliest programming concepts analogous to libraries were intended to separate data definitions from the program implementation. JOVIAL brought the "COMPOOL" (Communication Pool) concept to popular attention in 1959, although it adopted the idea from the large-system SAGE software. Following the computer science principles of separation of concerns and information hiding, "Comm Pool's purpose was to permit the sharing of System Data among many programs by providing a centralized data description."[2]
COBOL also included "primitive capabilities for a library system" in 1959,[3] but Jean Sammet described them as "inadequate library facilities" in retrospect.[4]
Another major contributor to the modern library concept came in the form of the subprogram innovation of FORTRAN. FORTRAN subprograms can be compiled independently of each other, but the compiler lacked a linker.  So prior to the introduction of modules in Fortran-90, type checking between FORTRAN[NB 1] subprograms was impossible.[5]
Finally, historians of the concept should remember the influential Simula 67. Simula was the first object-oriented programming language, and its classes were nearly identical to the modern concept as used in Java, C++, and C#. The class concept of Simula was also a progenitor of the package in Ada and the module of Modula-2.[6] Even when developed originally in 1965, Simula classes could be included in library files and added at compile time.[7]
Libraries are important in the program linking or binding process, which resolves references known as links or symbols to library modules. The linking process is usually automatically done by a linker or binder program that searches a set of libraries and other modules in a given order. Usually it is not considered an error if a link target can be found multiple times in a given set of libraries. Linking may be done when an executable file is created, or whenever the program is used at run time.
The references being resolved may be addresses for jumps and other routine calls. They may be in the main program, or in one module depending upon another. They are resolved into fixed or relocatable addresses (from a common base) by allocating runtime memory for the memory segments of each module referenced.
Some programming languages may use a feature called smart linking whereby the linker is aware of or integrated with the compiler, such that the linker knows how external references are used, and code in a library that is never actually used, even though internally referenced, can be discarded from the compiled application. For example, a program that only uses integers for arithmetic, or does no arithmetic operations at all, can exclude floating-point library routines. This smart-linking feature can lead to smaller application file sizes and reduced memory usage.
Some references in a program or library module are stored in a relative or symbolic form which cannot be resolved until all code and libraries are assigned final static addresses. Relocation is the process of adjusting these references, and is done either by the linker or the loader. In general, relocation cannot be done to individual libraries themselves because the addresses in memory may vary depending on the program using them and other libraries they are combined with. Position-independent code avoids references to absolute addresses and therefore does not require relocation.
When linking is performed during the creation of an executable or another object file, it is known as static linking or early binding. In this case, the linking is usually done by a linker, but may also be done by the compiler. A static library, also known as an archive, is one intended to be statically linked. Originally, only static libraries existed. Static linking must be performed when any modules are recompiled.
All of the modules required by a program are sometimes statically linked and copied into the executable file. This process, and the resulting stand-alone file, is known as a static build of the program. A static build may not need any further relocation if virtual memory is used and no address space layout randomization is desired.[8]
A shared library or shared object is a file that is intended to be shared by executable files and further shared object files. Modules used by a program are loaded from individual shared objects into memory at load time or run time, rather than being copied by a linker when it creates a single monolithic executable file for the program.
Shared libraries can be statically linked, meaning that references to the library modules are resolved and the modules are allocated memory when the executable file is created. But often linking of shared libraries is postponed until they are loaded.[dubious  – discuss]
Most modern operating systems[NB 2] can have shared library files of the same format as the executable files. This offers two main advantages: first, it requires making only one loader for both of them, rather than two (having the single loader is considered well worth its added complexity). Secondly, it allows the executables also to be used as shared libraries, if they have a symbol table. Typical combined executable and shared library formats are ELF and Mach-O (both in Unix) and PE (Windows).
In some older environments such as 16-bit Windows or MPE for the HP 3000 only stack-based data (local) was allowed in shared-library code, or other significant restrictions were placed on shared-library code.
Library code may be shared in memory by multiple processes, as well as on disk. If virtual memory is used, processes would execute the same physical page of RAM that is mapped into the different address spaces of the processes. This has advantages. For instance, on the OpenStep system, applications were often only a few hundred kilobytes in size and loaded quickly; the majority of their code was located in libraries that had already been loaded for other purposes by the operating system.[citation needed]
Programs can accomplish RAM sharing by using position-independent code, as in Unix, which leads to a complex but flexible architecture, or by using common virtual addresses, as in Windows and OS/2.  These systems make sure, by various tricks like pre-mapping the address space and reserving slots for each shared library, that code has a great probability of being shared. A third alternative is single-level store, as used by the IBM System/38 and its successors.  This allows position-dependent code, but places no significant restrictions on where code can be placed or how it can be shared.
In some cases different versions of shared libraries can cause problems, especially when libraries of different versions have the same file name, and different applications installed on a system each require a specific version. Such a scenario is known as DLL hell, named after the Windows and OS/2 DLL file. Most modern operating systems after 2001 have clean-up methods to eliminate such situations or use application-specific "private" libraries.[9]
Dynamic linking or late binding is linking performed while a program is being loaded (load time) or executed (run time), rather than when the executable file is created. A dynamically linked library (dynamic-link library, or DLL, under Windows and OS/2; dynamic shared object, or DSO, under Unix-like systems) is a library intended for dynamic linking. Only a minimal amount of work is done by the linker when the executable file is created; it only records what library routines the program needs and the index names or numbers of the routines in the library. The majority of the work of linking is done at the time the application is loaded (load time) or during execution (run time). Usually, the necessary linking program, called a "dynamic linker" or "linking loader", is actually part of the underlying operating system.  (However, it is possible, and not exceedingly difficult, to write a program that uses dynamic linking and includes its own dynamic linker, even for an operating system that itself provides no support for dynamic linking.)
Programmers originally developed dynamic linking in the Multics operating system, starting in 1964, and the MTS (Michigan Terminal System), built in the late 1960s.[10]
Since shared libraries on most systems do not change often, systems can compute a likely load address for each shared library on the system before it is needed and store that information in the libraries and executables. If every shared library that is loaded has undergone this process, then each will load at its predetermined address, which speeds up the process of dynamic linking. This optimization is known as prebinding in macOS and prelinking in Linux. Disadvantages of this technique include the time required to precompute these addresses every time the shared libraries change, the inability to use address space layout randomization, and the requirement of sufficient virtual address space for use (a problem that will be alleviated by the adoption of 64-bit architectures, at least for the time being).
Loaders for shared libraries vary widely in functionality. Some depend on the executable storing explicit paths to the libraries. Any change to the library naming or layout of the file system will cause these systems to fail. More commonly, only the name of the library (and not the path) is stored in the executable, with the operating system supplying a method to find the library on disk, based on some algorithm.
If a shared library that an executable depends on is deleted, moved, or renamed, or if an incompatible version of the library is copied to a place that is earlier in the search, the executable would fail to load. This is called dependency hell, existing on many platforms. The (infamous) Windows variant is commonly known as DLL hell.  This problem cannot occur if each version of each library is uniquely identified and each program references libraries only by their full unique identifiers.  The "DLL hell" problems with earlier Windows versions arose from using only the names of libraries, which were not guaranteed to be unique, to resolve dynamic links in programs.  (To avoid "DLL hell", later versions of Windows rely largely on options for programs to install private DLLs—essentially a partial retreat from the use of shared libraries—along with mechanisms to prevent replacement of shared system DLLs with earlier versions of them.)
Microsoft Windows checks the registry to determine the proper place to load DLLs that implement COM objects, but for other DLLs it will check the directories in a defined order. First, Windows checks the directory where it loaded the program (private DLL[9]); any directories set by calling the SetDllDirectory() function; the System32, System, and Windows directories; then the current working directory; and finally the directories specified by the PATH environment variable.[11] Applications written for the .NET Framework framework (since 2002), also check the Global Assembly Cache as the primary store of shared dll files to remove the issue of DLL hell.
OpenStep used a more flexible system, collecting a list of libraries from a number of known locations (similar to the PATH concept) when the system first starts. Moving libraries around causes no problems at all, although users incur a time cost when first starting the system.
Most Unix-like systems have a "search path" specifying file-system directories in which to look for dynamic libraries. Some systems specify the default path in a configuration file, others hard-code it into the dynamic loader. Some executable file formats can specify additional directories in which to search for libraries for a particular program. This can usually be overridden with an environment variable, although it is disabled for setuid and setgid programs, so that a user can't force such a program to run arbitrary code with root permissions. Developers of libraries are encouraged to place their dynamic libraries in places in the default search path. On the downside, this can make installation of new libraries problematic, and these "known" locations quickly become home to an increasing number of library files, making management more complex.
Dynamic loading, a subset of dynamic linking, involves a dynamically linked library loading and unloading at run time on request. Such a request may be made implicitly at compile time or explicitly at run time. Implicit requests are made at compile time when a linker adds library references that include file paths or simply file names. Explicit requests are made when applications make direct calls to an operating system's API at run time.
Most operating systems that support dynamically linked libraries also support dynamically loading such libraries via a run-time linker API. For instance, Microsoft Windows uses the API functions LoadLibrary, LoadLibraryEx, FreeLibrary and GetProcAddress with Microsoft Dynamic Link Libraries; POSIX-based systems, including most UNIX and UNIX-like systems, use dlopen, dlclose and dlsym. Some development systems automate this process.
Although originally pioneered in the 1960s, dynamic linking did not reach operating systems used by consumers until the late 1980s. It was generally available in some form in most operating systems by the early 1990s. During this same period, object-oriented programming (OOP) was becoming a significant part of the programming landscape. OOP with runtime binding requires additional information that traditional libraries don't supply. In addition to the names and entry points of the code located within, they also require a list of the objects they depend on. This is a side-effect of one of OOP's main advantages, inheritance, which means that parts of the complete definition of any method may be in different places. This is more than simply listing that one library requires the services of another: in a true OOP system, the libraries themselves may not be known at compile time, and vary from system to system.
At the same time many developers worked on the idea of multi-tier programs, in which a "display" running on a desktop computer would use the services of a mainframe or minicomputer for data storage or processing. For instance, a program on a GUI-based computer would send messages to a minicomputer to return small samples of a huge dataset for display. Remote procedure calls (RPC) already handled these tasks, but there was no standard RPC system.
Soon the majority of the minicomputer and mainframe vendors instigated projects to combine the two, producing an OOP library format that could be used anywhere. Such systems were known as object libraries, or distributed objects, if they supported remote access (not all did). Microsoft's COM is an example of such a system for local use, DCOM a modified version that supports remote access.
For some time object libraries held the status of the "next big thing" in the programming world. There were a number of efforts to create systems that would run across platforms, and companies competed to try to get developers locked into their own system. Examples include IBM's System Object Model (SOM/DSOM), Sun Microsystems' Distributed Objects Everywhere (DOE), NeXT's Portable Distributed Objects (PDO), Digital's ObjectBroker, Microsoft's Component Object Model (COM/DCOM), and any number of CORBA-based systems.
After the inevitable cooling of marketing hype, object libraries continue to be used in both object-oriented programming and distributed information systems. Class libraries are the rough OOP equivalent of older types of code libraries. They contain classes, which describe characteristics and define actions (methods) that involve objects. Class libraries are used to create instances, or objects with their characteristics set to specific values. In some OOP languages, like Java, the distinction is clear, with the classes often contained in library files (like Java's JAR file format) and the instantiated objects residing only in memory (although potentially able to be made persistent in separate files). In others, like Smalltalk, the class libraries are merely the starting point for a system image that includes the entire state of the environment, classes and all instantiated objects.
Another solution to the library issue comes from using completely separate executables (often in some lightweight form) and calling them using a remote procedure call (RPC) over a network to another computer. This approach maximizes operating system re-use: the code needed to support the library is the same code being used to provide application support and security for every other program. Additionally, such systems do not require the library to exist on the same machine, but can forward the requests over the network.
However, such an approach means that every library call requires a considerable amount of overhead. RPC calls are much more expensive than calling a shared library that has already been loaded on the same machine. This approach is commonly used in a distributed architecture that makes heavy use of such remote calls, notably client-server systems and application servers such as Enterprise JavaBeans.
Code generation libraries are high-level APIs that can generate or transform byte code for Java. They are used by aspect-oriented programming, some data access frameworks, and for testing to generate dynamic proxy objects. They also are used to intercept field access.[12]
The system stores libfoo.a and libfoo.so files in directories such as /lib, /usr/lib or /usr/local/lib. The filenames always start with lib, and end with a suffix of .a (archive, static library) or of .so (shared object, dynamically linked library). Some systems might have multiple names for the dynamically linked library, with most of the names being names for symbolic links to the remaining name; those names might include the major version of the library, or the full version number; for example, on some systems libfoo.so.2 would be the filename for the second major interface revision of the dynamically linked library libfoo.  The .la files sometimes found in the library directories are libtool archives, not usable by the system as such.
The system inherits static library conventions from BSD, with the library stored in a .a file, and can use .so-style dynamically linked libraries (with the .dylib suffix instead). Most libraries in macOS, however, consist of "frameworks", placed inside special directories called "bundles" which wrap the library's required files and metadata. For example, a framework called MyFramework would be implemented in a bundle called MyFramework.framework, with MyFramework.framework/MyFramework being either the dynamically linked library file or being a symlink to the dynamically linked library file in MyFramework.framework/Versions/Current/MyFramework.
Dynamic-link libraries usually have the suffix *.DLL,[13] although other file name extensions may identify specific-purpose dynamically linked libraries, e.g. *.OCX for OLE libraries. The interface revisions are either encoded in the file names, or abstracted away using COM-object interfaces. Depending on how they are compiled, *.LIB files can be either static libraries or representations of dynamically linkable libraries needed only during compilation, known as "import libraries". Unlike in the UNIX world, which uses different file extensions, when linking against .LIB file in Windows one must first know if it is a regular static library or an import library. In the latter case, a .DLL file must be present at run time.




Java (programming language) - Wikipedia

Java is a general-purpose computer-programming language that is concurrent, class-based, object-oriented,[15] and specifically designed to have as few implementation dependencies as possible. It is intended to let application developers "write once, run anywhere" (WORA),[16] meaning that compiled Java code can run on all platforms that support Java without the need for recompilation.[17] Java applications are typically compiled to bytecode that can run on any Java virtual machine (JVM) regardless of computer architecture. As of 2016, Java is one of the most popular programming languages in use,[18][19][20][21] particularly for client-server web applications, with a reported 9 million developers.[22] Java was originally developed by James Gosling at Sun Microsystems (which has since been acquired by Oracle Corporation) and released in 1995 as a core component of Sun Microsystems' Java platform. The language derives much of its syntax from C and C++, but it has fewer low-level facilities than either of them.
The original and reference implementation Java compilers, virtual machines, and class libraries were originally released by Sun under proprietary licenses. As of May 2007, in compliance with the specifications of the Java Community Process, Sun relicensed most of its Java technologies under the GNU General Public License. Others have also developed alternative implementations of these Sun technologies, such as the GNU Compiler for Java (bytecode compiler), GNU Classpath (standard libraries), and IcedTea-Web (browser plugin for applets).
The latest version is Java 11, released on September 25, 2018, which follows Java 10 after only six months[23] in line with the new release schedule. Java 8 is still supported but there will be no more security updates for Java 9.[24] Versions earlier than Java 8 are supported by companies on a commercial basis; e.g. by Oracle back to Java 6 as of October 2017 (while they still "highly recommend that you uninstall"[25] pre-Java 8 from at least Windows computers).
James Gosling, Mike Sheridan, and Patrick Naughton initiated the Java language project in June 1991.[26] Java was originally designed for interactive television, but it was too advanced for the digital cable television industry at the time.[27] The language was initially called Oak after an oak tree that stood outside Gosling's office. Later the project went by the name Green and was finally renamed Java, from Java coffee.[28] Gosling designed Java with a C/C++-style syntax that system and application programmers would find familiar.[29]
Sun Microsystems released the first public implementation as Java 1.0 in 1996.[30] It promised "Write Once, Run Anywhere" (WORA), providing no-cost run-times on popular platforms. Fairly secure and featuring configurable security, it allowed network- and file-access restrictions. Major web browsers soon incorporated the ability to run Java applets within web pages, and Java quickly became popular. The Java 1.0 compiler was re-written in Java by Arthur van Hoff to comply strictly with the Java 1.0 language specification.[31] With the advent of Java 2 (released initially as J2SE 1.2 in December 1998 – 1999), new versions had multiple configurations built for different types of platforms. J2EE included technologies and APIs for enterprise applications typically run in server environments, while J2ME featured APIs optimized for mobile applications. The desktop version was renamed J2SE. In 2006, for marketing purposes, Sun renamed new J2 versions as Java EE, Java ME, and Java SE, respectively.
In 1997, Sun Microsystems approached the ISO/IEC JTC 1 standards body and later the Ecma International to formalize Java, but it soon withdrew from the process.[32][33][34] Java remains a de facto standard, controlled through the Java Community Process.[35] At one time, Sun made most of its Java implementations available without charge, despite their proprietary software status. Sun generated revenue from Java through the selling of licenses for specialized products such as the Java Enterprise System.
On November 13, 2006, Sun released much of its Java virtual machine (JVM) as free and open-source software, (FOSS), under the terms of the GNU General Public License (GPL). On May 8, 2007, Sun finished the process, making all of its JVM's core code available under free software/open-source distribution terms, aside from a small portion of code to which Sun did not hold the copyright.[36]
Sun's vice-president Rich Green said that Sun's ideal role with regard to Java was as an "evangelist".[37] Following Oracle Corporation's acquisition of Sun Microsystems in 2009–10, Oracle has described itself as the "steward of Java technology with a relentless commitment to fostering a community of participation and transparency".[38] This did not prevent Oracle from filing a lawsuit against Google shortly after that for using Java inside the Android SDK (see Google section below). Java software runs on everything from laptops to data centers, game consoles to scientific supercomputers.[39] On April 2, 2010, James Gosling resigned from Oracle.[40]
In January 2016, Oracle announced that Java runtime environments based on JDK 9 will discontinue the browser plugin.[41]
There were five primary goals in the creation of the Java language:[17]
As of  20 March 2018[update], both Java 8 and 11 are officially supported. Major release versions of Java, along with their release dates:
Sun has defined and supports four editions of Java targeting different application environments and segmented many of its APIs so that they belong to one of the platforms. The platforms are:
The classes in the Java APIs are organized into separate groups called packages. Each package contains a set of related interfaces, classes, and exceptions. Refer to the separate platforms for a description of the packages available.[relevant to this section?  – discuss]
Sun also provided an edition called PersonalJava that has been superseded by later, standards-based Java ME configuration-profile pairings.
One design goal of Java is portability, which means that programs written for the Java platform must run similarly on any combination of hardware and operating system with adequate runtime support.
This is achieved by compiling the Java language code to an intermediate representation called Java bytecode, instead of directly to architecture-specific machine code. Java bytecode instructions are analogous to machine code, but they are intended to be executed by a virtual machine (VM) written specifically for the host hardware. End users commonly use a Java Runtime Environment (JRE) installed on their own machine for standalone Java applications, or in a web browser for Java applets.
Standard libraries provide a generic way to access host-specific features such as graphics, threading, and networking.
The use of universal bytecode makes porting simple. However, the overhead of interpreting bytecode into machine instructions made interpreted programs almost always run more slowly than native executables. Just-in-time (JIT) compilers that compile bytecodes to machine code during runtime were introduced from an early stage. Java itself is platform-independent and is adapted to the particular platform it is to run on by a Java virtual machine for it, which translates the Java bytecode into the platform's machine language.[48]
Programs written in Java have a reputation for being slower and requiring more memory than those written in C++.[49][50] However, Java programs' execution speed improved significantly with the introduction of just-in-time compilation in 1997/1998 for Java 1.1,[51] the addition of language features supporting better code analysis (such as inner classes, the StringBuilder class, optional assertions, etc.), and optimizations in the Java virtual machine, such as HotSpot becoming the default for Sun's JVM in 2000. With Java 1.5, the performance was improved with the addition of the java.util.concurrent package, including lock free implementations of the ConcurrentMaps and other multi-core collections, and it was improved further with Java 1.6.
Some platforms offer direct hardware support for Java; there are microcontrollers that can run Java bytecode in hardware instead of a software Java virtual machine,[52] and some ARM based processors could have hardware support for executing Java bytecode through their Jazelle option, though support has mostly been dropped in current implementations of ARM.
Java uses an automatic garbage collector to manage memory in the object lifecycle. The programmer determines when objects are created, and the Java runtime is responsible for recovering the memory once objects are no longer in use. Once no references to an object remain, the unreachable memory becomes eligible to be freed automatically by the garbage collector. Something similar to a memory leak may still occur if a programmer's code holds a reference to an object that is no longer needed, typically when objects that are no longer needed are stored in containers that are still in use. If methods for a nonexistent object are called, a "null pointer exception" is thrown.[53][54]
One of the ideas behind Java's automatic memory management model is that programmers can be spared the burden of having to perform manual memory management. In some languages, memory for the creation of objects is implicitly allocated on the stack or explicitly allocated and deallocated from the heap. In the latter case, the responsibility of managing memory resides with the programmer. If the program does not deallocate an object, a memory leak occurs. If the program attempts to access or deallocate memory that has already been deallocated, the result is undefined and difficult to predict, and the program is likely to become unstable or crash. This can be partially remedied by the use of smart pointers, but these add overhead and complexity. Note that garbage collection does not prevent "logical" memory leaks, i.e., those where the memory is still referenced but never used.
Garbage collection may happen at any time. Ideally, it will occur when a program is idle. It is guaranteed to be triggered if there is insufficient free memory on the heap to allocate a new object; this can cause a program to stall momentarily. Explicit memory management is not possible in Java.
Java does not support C/C++ style pointer arithmetic, where object addresses and unsigned integers (usually long integers) can be used interchangeably. This allows the garbage collector to relocate referenced objects and ensures type safety and security.
As in C++ and some other object-oriented languages, variables of Java's primitive data types are either stored directly in fields (for objects) or on the stack (for methods) rather than on the heap, as is commonly true for non-primitive data types (but see escape analysis). This was a conscious decision by Java's designers for performance reasons.
Java contains multiple types of garbage collectors. By default, HotSpot uses the parallel scavenge garbage collector.[55] However, there are also several other garbage collectors that can be used to manage the heap. For 90% of applications in Java, the Concurrent Mark-Sweep (CMS) garbage collector is sufficient.[56] Oracle aims to replace CMS with the Garbage-First collector (G1).[57]
The syntax of Java is largely influenced by C++. Unlike C++, which combines the syntax for structured, generic, and object-oriented programming, Java was built almost exclusively as an object-oriented language.[17] All code is written inside classes, and every data item is an object, with the exception of the primitive data types, (i.e. integers, floating-point numbers, boolean values, and characters), which are not objects for performance reasons. Java reuses some popular aspects of C++ (such as the printf method).
Unlike C++, Java does not support operator overloading[58] or multiple inheritance for classes, though multiple inheritance is supported for interfaces.[59]
Java uses comments similar to those of C++. There are three different styles of comments: a single line style marked with two slashes (//), a multiple line style opened with /* and closed with */, and the Javadoc commenting style opened with /** and closed with */. The Javadoc style of commenting allows the user to run the Javadoc executable to create documentation for the program and can be read by some integrated development environments (IDEs) such as Eclipse to allow developers to access documentation within the IDE.
The traditional "Hello, world!" program can be written in Java as:[60]
Source files must be named after the public class they contain, appending the suffix .java, for example, HelloWorldApp.java. It must first be compiled into bytecode, using a Java compiler, producing a file named HelloWorldApp.class. Only then can it be executed, or "launched". The Java source file may only contain one public class, but it can contain multiple classes with other than public access modifier and any number of public inner classes. When the source file contains multiple classes, make one class "public" and name the source file with that public class name.
A class that is not declared public may be stored in any .java file. The compiler will generate a class file for each class defined in the source file. The name of the class file is the name of the class, with .class appended. For class file generation, anonymous classes are treated as if their name were the concatenation of the name of their enclosing class, a $, and an integer.
The keyword public denotes that a method can be called from code in other classes, or that a class may be used by classes outside the class hierarchy. The class hierarchy is related to the name of the directory in which the .java file is located. This is called an access level modifier. Other access level modifiers include the keywords private and protected.
The keyword static in front of a method indicates a static method, which is associated only with the class and not with any specific instance of that class. Only static methods can be invoked without a reference to an object. Static methods cannot access any class members that are not also static. Methods that are not designated static are instance methods and require a specific instance of a class to operate.
The keyword void indicates that the main method does not return any value to the caller. If a Java program is to exit with an error code, it must call System.exit() explicitly.
The method name "main" is not a keyword in the Java language. It is simply the name of the method the Java launcher calls to pass control to the program. Java classes that run in managed environments such as applets and Enterprise JavaBeans do not use or need a main() method. A Java program may contain multiple classes that have main methods, which means that the VM needs to be explicitly told which class to launch from.
The main method must accept an array of String objects. By convention, it is referenced as args although any other legal identifier name can be used. Since Java 5, the main method can also use variable arguments, in the form of public static void main(String... args), allowing the main method to be invoked with an arbitrary number of String arguments. The effect of this alternate declaration is semantically identical (to the args parameter which is still an array of String objects), but it allows an alternative syntax for creating and passing the array.
The Java launcher launches Java by loading a given class (specified on the command line or as an attribute in a JAR) and starting its public static void main(String[]) method. Stand-alone programs must declare this method explicitly. The String[] args parameter is an array of String objects containing any arguments passed to the class. The parameters to main are often passed by means of a command line.
Printing is part of a Java standard library: The System class defines a public static field called out. The out object is an instance of the PrintStream class and provides many methods for printing data to standard out, including println(String) which also appends a new line to the passed string.
The string "Hello World!" is automatically converted to a String object by the compiler.
Java applets were programs that were embedded in other applications, typically in a Web page displayed in a web browser.  The Java applet API is now deprecated since Java 9 in 2017.
Java servlet technology provides Web developers with a simple, consistent mechanism for extending the functionality of a Web server and for accessing existing business systems. Servlets are server-side Java EE components that generate responses (typically HTML pages) to requests (typically HTTP requests) from clients.
The Java servlet API has to some extent been superseded by two standard Java technologies for web services:
JavaServer Pages (JSP) are server-side Java EE components that generate responses, typically HTML pages, to HTTP requests from clients. JSPs embed Java code in an HTML page by using the special delimiters <% and %>. A JSP is compiled to a Java servlet, a Java application in its own right, the first time it is accessed. After that, the generated servlet creates the response.
Swing is a graphical user interface library for the Java SE platform. It is possible to specify a different look and feel through the pluggable look and feel system of Swing. Clones of Windows, GTK+, and Motif are supplied by Sun. Apple also provides an Aqua look and feel for macOS. Where prior implementations of these looks and feels may have been considered lacking, Swing in Java SE 6 addresses this problem by using more native GUI widget drawing routines of the underlying platforms.
In 2004, generics were added to the Java language, as part of J2SE 5.0. Prior to the introduction of generics, each variable declaration had to be of a specific type. For container classes, for example, this is a problem because there is no easy way to create a container that accepts only specific types of objects. Either the container operates on all subtypes of a class or interface, usually Object, or a different container class has to be created for each contained class. Generics allow compile-time type checking without having to create many container classes, each containing almost identical code. In addition to enabling more efficient code, certain runtime exceptions are prevented from occurring, by issuing compile-time errors. If Java prevented all runtime type errors (ClassCastException's) from occurring, it would be type safe.
In 2016, the type system of Java was proven unsound.[61]
Criticisms directed at Java include the implementation of generics,[62] speed,[63] the handling of unsigned numbers,[64] the implementation of floating-point arithmetic,[65] and a history of security vulnerabilities in the primary Java VM implementation HotSpot.[66]
The Java Class Library is the standard library, developed to support application development in Java. It is controlled by Sun Microsystems in cooperation with others through the Java Community Process program. Companies or individuals participating in this process can influence the design and development of the APIs. This process has been a subject of controversy.[when?] The class library contains features such as:
Javadoc is a comprehensive documentation system, created by Sun Microsystems, used by many Java developers[by whom?]. It provides developers with an organized system for documenting their code. Javadoc comments have an extra asterisk at the beginning, i.e. the delimiters are /** and */, whereas the normal multi-line comments in Java are set off with the delimiters /* and */.[70]
Oracle Corporation is the current owner of the official implementation of the Java SE platform, following their acquisition of Sun Microsystems on January 27, 2010. This implementation is based on the original implementation of Java by Sun. The Oracle implementation is available for Microsoft Windows (still works for XP, while only later versions are currently officially supported), macOS, Linux, and Solaris. Because Java lacks any formal standardization recognized by Ecma International, ISO/IEC, ANSI, or other third-party standards organization, the Oracle implementation is the de facto standard.
The Oracle implementation is packaged into two different distributions: The Java Runtime Environment (JRE) which contains the parts of the Java SE platform required to run Java programs and is intended for end users, and the Java Development Kit (JDK), which is intended for software developers and includes development tools such as the Java compiler, Javadoc, Jar, and a debugger.
OpenJDK is another notable Java SE implementation that is licensed under the GNU GPL. The implementation started when Sun began releasing the Java source code under the GPL. As of Java SE 7, OpenJDK is the official Java reference implementation.
The goal of Java is to make all implementations of Java compatible. Historically, Sun's trademark license for usage of the Java brand insists that all implementations be "compatible". This resulted in a legal dispute with Microsoft after Sun claimed that the Microsoft implementation did not support RMI or JNI and had added platform-specific features of their own. Sun sued in 1997, and, in 2001, won a settlement of US$20 million, as well as a court order enforcing the terms of the license from Sun.[71] As a result, Microsoft no longer ships Java with Windows.
Platform-independent Java is essential to Java EE, and an even more rigorous validation is required to certify an implementation. This environment enables portable server-side applications.
The Java programming language requires the presence of a software platform in order for compiled programs to be executed.
Oracle supplies the Java platform for use with Java. The Android SDK is an alternative software platform, used primarily for developing Android applications with its own GUI system. The Eclipse IDE platform supports Java, but provides its own GUI system SWT.
The Java language is a key pillar in Android, an open source mobile operating system. Although Android, built on the Linux kernel, is written largely in C, the Android SDK uses the Java language as the basis for Android applications. The bytecode language supported by the Android SDK is incompatible with Java bytecode and runs on its own virtual machine, optimized for low-memory devices such as smartphones and tablet computers.
Depending on the Android version, the bytecode is either interpreted by the Dalvik virtual machine or compiled into native code by the Android Runtime.
Android does not provide the full Java SE standard library, although the Android SDK does include an independent implementation of a large subset of it. It supports Java 6 and some Java 7 features, offering an implementation compatible with the standard library (Apache Harmony).
The use of Java-related technology in Android led to a legal dispute between Oracle and Google. On May 7, 2012, a San Francisco jury found that if APIs could be copyrighted, then Google had infringed Oracle's copyrights by the use of Java in Android devices.[72] District Judge William Haskell Alsup ruled on May 31, 2012, that APIs cannot be copyrighted,[73] but this was reversed by the United States Court of Appeals for the Federal Circuit in May 2014.[74] On May 26, 2016, the district court decided in favor of Google, ruling the copyright infringement of the Java API in Android constitutes fair use.[75]



Abstraction (computer science) - Wikipedia
– John V. Guttag[1]
In software engineering and computer science, abstraction is:
Abstraction, in general, is a fundamental concept to computer science and software development[4]. The process of abstraction can also be referred to as modeling and is closely related to the concepts of theory and design[5]. Models can also be considered types of abstractions per their generalization of aspects of reality.
Abstraction in computer science is also closely related to abstraction in mathematics due to their common focus on building abstractions as objects[2], but is also related to other notions of abstraction used in other fields such as art[3].
Abstractions may also refer to vehicles, features, or rules of computational systems or programming languages that carry or utilize features of or abstraction itself, such as:
Computing mostly operates independently of the concrete world. The hardware implements a model of computation that is interchangeable with others.[citation needed] The software is structured in architectures to enable humans to create the enormous systems by concentrating on a few issues at a time. These architectures are made of specific choices of abstractions. Greenspun's Tenth Rule is an aphorism on how such an architecture is both inevitable and complex.
A central form of abstraction in computing is language abstraction: new artificial languages are developed to express specific aspects of a system. Modeling languages help in planning. Computer languages can be processed with a computer. An example of this abstraction process is the generational development of programming languages from the machine language to the assembly language and the high-level language. Each stage can be used as a stepping stone for the next stage. The language abstraction continues for example in scripting languages and domain-specific programming languages.
Within a programming language, some features let the programmer create new abstractions. These include subroutines, modules, polymorphism, and software components. Some other abstractions such as software design patterns and architectural styles remain invisible to a translator and operate only in the design of a system.
Some abstractions try to limit the range of concepts a programmer needs to be aware of, by completely hiding the abstractions that they in turn are built on. The software engineer and writer Joel Spolsky has criticised these efforts by claiming that all abstractions are leaky — that they can never completely hide the details below;[10] however, this does not negate the usefulness of abstraction.
Some abstractions are designed to inter-operate with other abstractions - for example, a programming language may contain a foreign function interface for making calls to the lower-level language.
In simple terms, abstraction is removing irrelevant data so a program is easier to understand.[citation needed]
Different programming languages provide different types of abstraction, depending on the intended applications for the language. For example:
Analysts have developed various methods to formally specify software systems.  Some known methods include:
Specification languages generally rely on abstractions of one kind or another, since specifications are typically defined earlier in a project, (and at a more abstract level) than an eventual implementation. The UML specification language, for example, allows the definition of abstract classes, which in a waterfall project, remain abstract during the architecture and specification phase of the project.
Programming languages offer control abstraction as one of the main purposes of their use. Computer machines understand operations at the very low level such as moving some bits from one location of the memory to another location and producing the sum of two sequences of bits. Programming languages allow this to be done in the higher level. For example, consider this statement written in a Pascal-like fashion:
To a human, this seems a fairly simple and obvious calculation ("one plus two is three, times five is fifteen"). However, the low-level steps necessary to carry out this evaluation, and return the value "15", and then assign that value to the variable "a", are actually quite subtle and complex. The values need to be converted to binary representation (often a much more complicated task than one would think) and the calculations decomposed (by the compiler or interpreter) into assembly instructions (again, which are much less intuitive to the programmer: operations such as shifting a binary register left, or adding the binary complement of the contents of one register to another, are simply not how humans think about the abstract arithmetical operations of addition or multiplication). Finally, assigning the resulting value of "15" to the variable labeled "a", so that "a" can be used later, involves additional 'behind-the-scenes' steps of looking up a variable's label and the resultant location in physical or virtual memory, storing the binary representation of "15" to that memory location, etc.
Without control abstraction, a programmer would need to specify all the register/binary-level steps each time they simply wanted to add or multiply a couple of numbers and assign the result to a variable. Such duplication of effort has two serious negative consequences:
Structured programming involves the splitting of complex program tasks into smaller pieces with clear flow-control and interfaces between components, with reduction of the complexity potential for side-effects.
In a simple program, this may aim to ensure that loops have single or obvious exit points and (where possible) to have single exit points from functions and procedures.
In a larger system, it may involve breaking down complex tasks into many different modules. Consider a system which handles payroll on ships and at shore offices:
These layers produce the effect of isolating the implementation details of one component and its assorted internal methods from the others. Object-oriented programming embraces and extends this concept.
Data abstraction enforces a clear separation between the abstract properties of a data type and the concrete details of its implementation. The abstract properties are those that are visible to client code that makes use of the data type—the interface to the data type—while the concrete implementation is kept entirely private, and indeed can change, for example to incorporate efficiency improvements over time. The idea is that such changes are not supposed to have any impact on client code, since they involve no difference in the abstract behaviour.
For example, one could define an abstract data type called lookup table which uniquely associates keys with values, and in which values may be retrieved by specifying their corresponding keys. Such a lookup table may be implemented in various ways: as a hash table, a binary search tree, or even a simple linear list of (key:value) pairs. As far as client code is concerned, the abstract properties of the type are the same in each case.
Of course, this all relies on getting the details of the interface right in the first place, since any changes there can have major impacts on client code. As one way to look at this: the interface forms a contract on agreed behaviour between the data type and client code; anything not spelled out in the contract is subject to change without notice.
In object-oriented programming theory, abstraction involves the facility to define objects that represent abstract "actors" that can perform work, report on and change their state, and "communicate" with other objects in the system. The term encapsulation refers to the hiding of state details, but extending the concept of data type from earlier programming languages to associate behavior most strongly with the data, and standardizing the way that different data types interact, is the beginning of abstraction.  When abstraction proceeds into the operations defined, enabling objects of different types to be substituted, it is called polymorphism. When it proceeds in the opposite direction, inside the types or classes, structuring them to simplify a complex set of relationships, it is called delegation or inheritance.
Various object-oriented programming languages offer similar facilities for abstraction, all to support a general strategy of polymorphism in object-oriented programming, which includes the substitution of one type for another in the same or similar role. Although not as generally supported, a configuration or image or package may predetermine a great many of these bindings at compile-time, link-time, or loadtime. This would leave only a minimum of such bindings to change at run-time.
Common Lisp Object System or Self, for example, feature less of a class-instance distinction and more use of delegation for polymorphism. Individual objects and functions are abstracted more flexibly to better fit with a shared functional heritage from Lisp.
C++ exemplifies another extreme: it relies heavily on templates and overloading and other static bindings at compile-time, which in turn has certain flexibility problems.
Although these examples offer alternate strategies for achieving the same abstraction, they do not fundamentally alter the need to support abstract nouns in code - all programming relies on an ability to abstract verbs as functions, nouns as data structures, and either as processes.
Consider for example a sample Java fragment to represent some common farm "animals" to a level of abstraction suitable to model simple aspects of their hunger and feeding. It defines an Animal class to represent both the state of the animal and its functions:
With the above definition, one could create objects of type Animal and call their methods like this:
In the above example, the class Animal is an abstraction used in place of an actual animal, LivingThing is a further abstraction (in this case a generalisation) of Animal.
If one requires a more differentiated hierarchy of animals — to differentiate, say, those who provide milk from those who provide nothing except meat at the end of their lives — that is an intermediary level of abstraction, probably DairyAnimal (cows, goats) who would eat foods suitable to giving good milk, and MeatAnimal (pigs, steers) who would eat foods to give the best meat-quality.
Such an abstraction could remove the need for the application coder to specify the type of food, so s/he could concentrate instead on the feeding schedule. The two classes could be related using inheritance or stand alone, and the programmer could define varying degrees of polymorphism between the two types. These facilities tend to vary drastically between languages, but in general each can achieve anything that is possible with any of the others. A great many operation overloads, data type by data type, can have the same effect at compile-time as any degree of inheritance or other means to achieve polymorphism. The class notation is simply a coder's convenience.
Decisions regarding what to abstract and what to keep under the control of the coder become the major concern of object-oriented design and domain analysis—actually determining the relevant relationships in the real world is the concern of object-oriented analysis or legacy analysis.
In general, to determine appropriate abstraction, one must make many small decisions about scope (domain analysis), determine what other systems one must cooperate with (legacy analysis), then perform a detailed object-oriented analysis which is expressed within project time and budget constraints as an object-oriented design. In our simple example, the domain is the barnyard, the live pigs and cows and their eating habits are the legacy constraints, the detailed analysis is that coders must have the flexibility to feed the animals what is available and thus there is no reason to code the type of food into the class itself, and the design is a single simple Animal class of which pigs and cows are instances with the same functions. A decision to differentiate DairyAnimal would change the detailed analysis but the domain and legacy analysis would be unchanged—thus it is entirely under the control of the programmer, and we refer to abstraction in object-oriented programming as distinct from abstraction in domain or legacy analysis.
When discussing formal semantics of programming languages, formal methods or abstract interpretation, abstraction refers to the act of considering a less detailed, but safe, definition of the observed program behaviors. For instance, one may observe only the final result of program executions instead of considering all the intermediate steps of executions. Abstraction is defined to a concrete (more precise) model of execution.
Abstraction may be exact or faithful with respect to a property if one can answer a question about the property equally well on the concrete or abstract model. For instance, if we wish to know what the result of the evaluation of a mathematical expression involving only integers +, -, ×, is worth modulo n, we need only perform all operations modulo n (a familiar form of this abstraction is casting out nines).
Abstractions, however, though not necessarily exact, should be sound. That is, it should be possible to get sound answers from them—even though the abstraction may simply yield a result of undecidability. For instance, we may abstract the students in a class by their minimal and maximal ages; if one asks whether a certain person belongs to that class, one may simply compare that person's age with the minimal and maximal ages; if his age lies outside the range, one may safely answer that the person does not belong to the class; if it does not, one may only answer "I don't know".
The level of abstraction included in a programming language can influence its overall usability. The Cognitive dimensions framework includes the concept of abstraction gradient in a formalism. This framework allows the designer of a programming language to study the trade-offs between abstraction and other characteristics of the design, and how changes in abstraction influence the language usability.
Abstractions can prove useful when dealing with computer programs, because non-trivial properties of computer programs are essentially undecidable (see Rice's theorem). As a consequence, automatic methods for deriving information on the behavior of computer programs either have to drop termination (on some occasions, they may fail, crash or never yield out a result), soundness (they may provide false information), or precision (they may answer "I don't know" to some questions).
Abstraction is the core concept of abstract interpretation. Model checking generally takes place on abstract versions of the studied systems.
Computer science commonly presents levels (or, less commonly, layers) of abstraction, wherein each level represents a different model of the same information and processes, but with varying amounts of detail. Each level uses a system of expression involving a unique set of objects and compositions that apply only to a particular domain.
[13]
Each relatively abstract, "higher" level builds on a relatively concrete, "lower" level, which tends to provide an increasingly "granular" representation. For example, gates build on electronic circuits, binary on gates, machine language on binary, programming language on machine language, applications and operating systems on programming languages. Each level is embodied, but not determined, by the level beneath it, making it a language of description that is somewhat self-contained.
Since many users of database systems lack in-depth familiarity with computer data-structures, database developers often hide complexity through the following levels:
Physical level: The lowest level of abstraction describes how a system actually stores data. The physical level describes complex low-level data structures in detail.
Logical level: The next higher level of abstraction describes what data the database stores, and what relationships exist among those data. The logical level thus describes an entire database in terms of a small number of relatively simple structures. Although implementation of the simple structures at the logical level may involve complex physical level structures, the user of the logical level does not need to be aware of this complexity. This is referred to as physical data independence. Database administrators, who must decide what information to keep in a database, use the logical level of abstraction.
View level: The highest level of abstraction describes only part of the entire database. Even though the logical level uses simpler structures, complexity remains because of the variety of information stored in a large database. Many users of a database system do not need all this information; instead, they need to access only a part of the database. The view level of abstraction exists to simplify their interaction with the system. The system may provide many views for the same database.
The ability to provide a design of different levels of abstraction can
Systems design and business process design can both use this. Some design processes specifically generate designs that contain various levels of abstraction.
Layered architecture partitions the concerns of the application into stacked groups (layers).
It is a technique used in designing computer software, hardware, and communications in which system or network components are isolated in layers so that changes can be made in one layer without affecting the others.
This article is based on material taken from  the Free On-line Dictionary of Computing  prior to 1 November 2008 and incorporated under the "relicensing" terms of the GFDL, version 1.3 or later.




Natural-language programming - Wikipedia

Natural-language programming (NLP) is an ontology-assisted way of programming  in terms of  natural-language sentences, e.g.  English. A structured document with Content, sections and subsections for explanations of sentences forms a NLP document, which is actually a computer program. Natural languages and natural-language user interfaces include Inform7, a natural programming language for making interactive fiction, Ring[1][2], a general-purpose language, Shakespeare, an esoteric natural programming language in the style of the plays of William Shakespeare, and Wolfram Alpha, a computational knowledge engine, using natural-language input.
The smallest unit of statement in NLP is a sentence.  Each sentence is stated in terms of concepts from the underlying ontology, attributes in that ontology and named objects in capital letters.   In an NLP text every sentence unambiguously compiles into a procedure call in the underlying high-level programming language such as MATLAB, Octave, SciLab, Python, etc.
Symbolic languages such as Mathematica are capable of interpreted processing of queries by  sentences. This can allow interactive  requests such as that implemented in Wolfram Alpha.[3][4] The difference between these and NLP is that the latter builds up a single program or a  library of routines that are programmed through natural language sentences using an ontology that defines the available data structures in a high level programming language.
An example text from an English language natural-language program (in Farsi) is as follows:
 If  U_ is 'smc01-control',  then do the following. Define surface weights Alpha as "[0.5, 0.5]".
 Initialise matrix Phi as a 'unit matrix'. Define J as the 'inertia matrix' of Spc01. Compute
 matrix J2 as the inverse of J .  Compute position velocity error Ve and angular velocity error
 Oe from dynamical state X, guidance reference Xnow .   Define the joint sliding surface G2
 from the position velocity error Ve and angular velocity error Oe using the surface weights
 Alpha. Compute the smoothed sign function SG2 from the joint sliding surface G2 with sign
 threshold 0.01.  Compute special dynamical force F from dynamical state  X and surface
 weights Alpha.  Compute control torque T and control force U from matrix J2, surface weights
 Alpha, special  dynamical force F, smoothed sign function SG2.  Finish conditional actions.
that defines a feedback control scheme using a sliding mode control method.
Natural-language programming is a top-down method of writing software. Its stages are as follows:
A natural-language program is a precise formal  description of some  procedure that its author created. It is human readable and it can also be read by a suitable software agent.  For example, a web page in an NLP format can be read by a software personal assistant agent to a person and she or he can ask the agent to execute some sentences, i.e. carry out some task or answer a question. There is a reader agent available for English interpretation of HTML based NLP documents that a person can run on her personal computer .
An ontology class in a natural-language program that is not a concept in the sense as humans use concepts. Concepts in an NLP are examples (samples) of  generic human concepts. Each sentence in a natural-language program is either (1) stating a relationship in a world model or  (2) carries out an action in the environment or (3) carries out a computational procedure or (4) invokes an answering mechanism in response to a question.
A set of NLP sentences, with associated ontology defined, can also be used as a pseudo code that does not provide the details in any underlying high level programming language. In such an application the sentences used become high level abstractions (conceptualisations) of computing procedures that are computer language and machine independent.



Programmer - Wikipedia

A programmer, developer, dev, coder, or software engineer is a person who creates computer software. The term computer programmer can refer to a specialist in one area of computers or to a generalist who writes code for many kinds of software. One who practices or professes a formal approach to programming may also be known as a programmer analyst.
A programmer's primary computer language (Assembly, COBOL, C, C++, C#, Java, Lisp, Python, etc.) is often prefixed to these titles, and those who work in a web environment often prefix their titles with web.
A range of occupations, including: software developer, web developer, mobile applications developer, embedded firmware developer, software engineer, computer scientist, or software analyst, while they do involve programming, also require a range of other skills. The use of the simple term programmer for these positions is sometimes considered an insulting or derogatory simplification.[1][2][3][4][5]
British countess and mathematician Ada Lovelace is often considered the first computer programmer, as she was the first to publish an algorithm intended for implementation on Charles Babbage's analytical engine, in October 1842, intended for the calculation of Bernoulli numbers.[7]  Because Babbage's machine was never completed to a functioning standard in her time, she never saw this algorithm run.
The first person to run a program on a functioning modern electronically based computer was computer scientist Konrad Zuse, in 1941.
The ENIAC programming team, consisting of Kay McNulty, Betty Jennings, Betty Snyder, Marlyn Wescoff, Fran Bilas and Ruth Lichterman were the first regularly working programmers.[8][9]
International Programmers' Day is celebrated annually on 7 January.[10]  In 2009, the government of Russia decreed a professional annual holiday known as Programmers' Day to be celebrated on 13 September (12 September in leap years). It had also been an unofficial international holiday before that.
The word "software" was first used as early as 1953, but did not appear in print until the 1960s.[11] Before this time, computers were programmed either by customers, or the few commercial computer vendors of the time, such as UNIVAC and IBM. The first company founded to provide software products and services was Computer Usage Company in 1955.[12]
The software industry expanded in the early 1960s, almost immediately after computers were first sold in mass-produced quantities. Universities, government, and business customers created a demand for software. Many of these programs were written in-house by full-time staff programmers. Some were distributed freely between users of a particular machine for no charge. Others were done on a commercial basis, and other firms such as Computer Sciences Corporation (founded in 1959) started to grow. The computer/hardware makers started bundling operating systems, system software and programming environments with their machines.[citation needed]
The industry expanded greatly with the rise of the personal computer ("PC") in the mid-1970s, which brought computing to the desktop of the office worker. In the following years, it also created a growing market for games, applications, and utilities. DOS, Microsoft's first operating system product, was the dominant operating system at the time.[13]
In the early years of the 21st century, another successful business model has arisen for hosted software, called software-as-a-service, or SaaS; this was at least the third time[citation needed] this model had been attempted. From the point of view of producers of some proprietary software, SaaS reduces the concerns about unauthorized copying, since it can only be accessed through the Web, and by definition, no client software is loaded onto the end user's PC.[citation needed]  By 2014, the role of cloud developer had been defined; in this context, one definition of a "developer" in general was published:[14]
Computer programmers write, test, debug, and maintain the detailed instructions, called computer programs, that computers must follow to perform their functions. Programmers also conceive, design, and test logical structures for solving problems by computer. Many technical innovations in programming — advanced computing technologies and sophisticated new languages and programming tools — have redefined the role of a programmer and elevated much of the programming work done today. Job titles and descriptions may vary, depending on the organization.
Programmers work in many settings, including corporate information technology ("IT") departments, big software companies, small service firms and government entities of all sizes. Many professional programmers also work for consulting companies at client sites as contractors. Licensing is not typically required to work as a programmer, although professional certifications are commonly held by programmers. Programming is widely considered a profession (although some[who?] authorities disagree on the grounds that only careers with legal licensing requirements count as a profession).
Programmers' work varies widely depending on the type of business for which they are writing programs. For example, the instructions involved in updating financial records are very different from those required to duplicate conditions on an aircraft for pilots training in a flight simulator. Simple programs can be written in a few hours, more complex ones may require more than a year of work, while others are never considered 'complete' but rather are continuously improved as long as they stay in use. In most cases, several programmers work together as a team under a senior programmer’s supervision.
Programmers write programs according to the specifications determined primarily by more senior programmers and by systems analysts. After the design process is complete, it is the job of the programmer to convert that design into a logical series of instructions that the computer can follow. The programmer codes these instructions in one of many programming languages. Different programming languages are used depending on the purpose of the program. COBOL, for example, is commonly used for business applications that typically run on mainframe and midrange computers, whereas Fortran is used in science and engineering. C++ is widely used for both scientific and business applications. Java, C#, VB  and PHP are popular programming languages for Web and business applications. Programmers generally know more than one programming language and, because many languages are similar, they often can learn new languages relatively easily. In practice, programmers often are referred to by the language they know, e.g. as Java programmers, or by the type of function they perform or environment in which they work: for example, database programmers, mainframe programmers, or Web developers.
When making changes to the source code that programs are made up of, programmers need to make other programmers aware of the task that the routine is to perform. They do this by inserting comments in the source code so that others can understand the program more easily and by documenting their code. To save work, programmers often use libraries of basic code that can be modified or customized for a specific application. This approach yields more reliable and consistent programs and increases programmers' productivity by eliminating some routine steps.
Programmers test a program by running it and looking for bugs (errors). As they are identified, the programmer usually makes the appropriate corrections, then rechecks the program until an acceptably low level and severity of bugs remain. This process is called testing and debugging. These are important parts of every programmer's job. Programmers may continue to fix these problems throughout the life of a program. Updating, repairing, modifying, and expanding existing programs is sometimes called maintenance programming. Programmers may contribute to user guides and online help, or they may work with technical writers to do such work.
Computer programmers often are grouped into two broad types: application programmers and systems programmers. Application programmers write programs to handle a specific job, such as a program to track inventory within an organization. They also may revise existing packaged software or customize generic applications which are frequently purchased from independent software vendors. Systems programmers, in contrast, write programs to maintain and control computer systems software, such as operating systems and database management systems. These workers make changes in the instructions that determine how the network, workstations, and CPU of the system handle the various jobs they have been given and how they communicate with peripheral equipment such as printers and disk drives.
A software developer needs to have deep technical expertise with certain aspects of computing. Some positions will require a degree in a relevant field such as computer science, information technology, engineering, programming, or any other IT related post graduate studies.[15] An ideal software developer is a self-motivated professional carrying a dynamic hands-on experience on key languages of programming such as C++, C#, PHP, Java, C, Javascript, VB, Oracle, UML, Linux, Python, UNIX, XML, HTTP, Smalltalk, or other software testing tools.
According to developer Eric Sink, the differences between system design, software development, and programming are more apparent. Already in the current market place there can be found a segregation between programmers and developers, in that one who implements is not the same as the one who designs the class structure or hierarchy. Even more so that developers become software architects or systems architects, those who design the multi-leveled architecture or component interactions of a large software system.[16]
Programmers in software development companies may work directly with experts from various fields to create software – either programs designed for specific clients or packaged software for general use – ranging from video games to educational software to programs for desktop publishing and financial planning. Programming of packaged software constitutes one of the most rapidly growing segments of the computer services industry. Some companies or organizations – even small ones – have set up their own IT team to ensure the design and development of in-house software to answer to very specific needs from their internal end-users, especially when existing software are not suitable or too expensive. This is for example the case in research laboratories.[citation needed]
In some organizations, particularly small ones, people commonly known as programmer analysts are responsible for both the systems analysis and the actual programming work. The transition from a mainframe environment to one that is based primarily on personal computers (PCs) has blurred the once rigid distinction between the programmer and the user. Increasingly, adept end users are taking over many of the tasks previously performed by programmers. For example, the growing use of packaged software, such as spreadsheet and database management software packages, allows users to write simple programs to access data and perform calculations.[citation needed]
In addition, the rise of the Internet has made web development a huge part of the programming field. Currently more software applications are web applications that can be used by anyone with a web browser.[citation needed] Examples of such applications include the Google search service, the Outlook.com e-mail service, and the Flickr photo-sharing service.
Programming editors, also known as source code editors, are text editors that are specifically designed for programmers or developers for writing the source code of an application or a program. Most of these editors include features useful for programmers, which may include color syntax highlighting, auto indentation, auto-complete, bracket matching, syntax check, and allows plug-ins. These features aid the users during coding, debugging and testing.[17]
According to BBC News, 17% of computer science students could not find work in their field 6 months after graduation in 2009 which was the highest rate of the university subjects surveyed while 0% of medical students were unemployed in the same survey.[18] The UK category system does, however, class such degrees as information technology and game design as 'computer science', industries in which jobs can be extremely difficult to find, somewhat inflating the actual figure.[19]
Computer programming, offshore outsourcing, and Foreign Worker Visas became a controversial topic after the crash of the dot-com bubble left many programmers without work or with lower wages. Programming was even mentioned in the 2004 US Presidential debate on the topic of offshore outsourcing.[20]
Large companies claim there is a skills shortage with regard to programming talent. However, US programmers and unions counter that large companies are exaggerating their case in order to obtain cheaper programmers from developing countries and avoid previously employer paid training using industry specific technologies not covered in most accredited degree programs.[21] Other reasons for employers claiming skill shortages is the result of their own cost saving combining of several disparate skill sets previously held by several specialized programmers into fewer generalized multifaceted  positions that are unlikely to have enough "qualified" candidates with the desired experience.[22]
Enrollment in computer-related degrees in US has dropped recently[when?] due to lack of general interests in science and mathematics and also out of an apparent fear that programming will be subject to the same pressures as manufacturing and agriculture careers.[23] This situation has resulted in confusion about whether the US economy is entering a "post-information age" and the nature of US comparative advantages. Most academic institutions have an Institutional research office that keep past statistics of degrees conferred which show several dips and rises in Computer Science degrees over the past 30 years. The overall trend shows a slightly overall decline in growth (especially when compared to other STEM degree growth) since certain peaks of 1986, 1992, 2002, and 2008 showing periods of flat growth or even declines.[24] In addition the U.S. Bureau of Labor Statistics Occupational Outlook 2016-26 is -7% (a decline in their words) for Computer Programmers because Computer programming can be done from anywhere in the world, so companies sometimes hire programmers in countries where wages are lower.[25]
.



Compiler - Wikipedia

A compiler is computer software that transforms computer code written in one programming language (the source language) into another programming language (the target language). Compilers are a type of translator that support digital devices, primarily computers. The name compiler is primarily used for programs that translate source code from a high-level programming language to a lower level language (e.g., assembly language, object code, or machine code) to create an executable program.[1]
However, there are many different types of compilers. If the compiled program can run on a computer whose CPU or operating system is different from the one on which the compiler runs, the compiler is a cross-compiler. A bootstrap compiler is written in the language that it intends to compile. A program that translates from a low-level language to a higher level one is a decompiler. A program that translates between high-level languages is usually called a source-to-source compiler or transpiler. A language rewriter is usually a program that translates the form of expressions without a change of language. The term compiler-compiler refers to tools used to create parsers that perform syntax analysis.
A compiler is likely to perform many or all of the following operations: preprocessing, lexical analysis, parsing, semantic analysis (syntax-directed translation), conversion of input programs to an intermediate representation, code optimization and code generation. Compilers implement these operations in phases that promote efficient design and correct transformations of source input to target output. Program faults caused by incorrect compiler behavior can be very difficult to track down and work around; therefore, compiler implementers invest significant effort to ensure compiler correctness.[2]
Compilers are not the only translators used to transform source programs. An interpreter is computer software that transforms and then executes the indicated operations. The translation process influences the design of computer languages which leads to a preference of compilation or interpretation. In practice, an interpreter can be implemented for compiled languages and compilers can be implemented for interpreted languages.
Theoretical computing concepts developed by scientists, mathematicians, and engineers formed the basis of digital modern computing development during World War II. Primitive binary languages evolved because digital devices only understand ones and zeros and the circuit patterns in the underlying machine architecture. In the late 1940s, assembly languages were created to offer a more workable abstraction of the computer architectures. Limited memory capacity of early computers led to substantial technical challenges when the first compilers were designed. Therefore, the compilation process needed to be divided into several small programs. The front end programs produce the analysis products used by the back end programs to generate target code. As computer technology provided more resources, compiler designs could align better with the compilation process.
The human mind can design better solutions as the language moves from the machine to a higher level. So the development of high-level languages followed naturally from the capabilities offered by the digital computers. High-level languages are formal languages that are strictly defined by their syntax and semantics which form the high-level language architecture. Elements of these formal languages include:
The sentences in a language may be defined by a set of rules called a grammar.[3]
Backus-Naur form (BNF) describes the syntax of "sentences" of a language and was used for the syntax of Algol 60 by John Backus.[4] The ideas derive from the context-free grammar concepts by Noam Chomsky, a linguist.[5] "BNF and its extensions have become standard tools for describing the syntax of programming notations, and in many cases parts of compilers are generated automatically from a BNF description."[6]
In the 1940s, Konrad Zuse designed an algorithmic programming language called Plankalkül ("Plan Calculus"). While no actual implementation occurred until the 1970s, it presented concepts later seen in APL designed by Ken Iverson in the late 1950s.[7] APL is a language for mathematical computations.
High-level language design during the formative years of digital computing provided useful programming tools for a variety of applications:
Compiler technology evolved from the need for a strictly defined transformation of the high-level source program into a low-level target program for the digital computer. The compiler could be viewed as a front end to deal with analysis of the source code and a back end to synthesize the analysis into the target code. Optimization between the front end and back end could produce more efficient target code.[11]
Some early milestones in the development of compiler technology:
Early operating systems and software were written in assembly language. In the 60s and early 70s, the use of high-level languages for system programming was still controversial due to resource limitations. However, several research and industry efforts began the shift toward high-level systems programming languages, for example, BCPL, BLISS, B, and C.
BCPL (Basic Combined Programming Language) designed in 1966 by Martin Richards at the University of Cambridge was originally developed as a compiler writing tool.[16] Several compilers have been implemented, Richards' book provides insights to the language and its compiler.[17] BCPL was not only an influential systems programming language that is still used in research[18] but also provided a basis for the design of B and C languages.
BLISS (Basic Language for Implementation of System Software) was developed for a Digital Equipment Corporation (DEC) PDP-10 computer by W.A. Wulf's Carnegie Mellon University (CMU) research team. The CMU team went on to develop BLISS-11 compiler one year later in 1970.
Multics (Multiplexed Information and Computing Service), a time-sharing operating system project, involved MIT, Bell Labs, General Electric (later Honeywell) and was led by Fernando Corbató from MIT.[19] Multics was written in the PL/I language developed by IBM and IBM User Group.[20] IBM's goal was to satisfy business, scientific, and systems programming requirements. There were other languages that could have been considered but PL/I offered the most complete solution even though it had not been implemented.[21] For the first few years of the Mulitics project, a subset of the language could be compiled to assembly language with the Early PL/I (EPL) compiler by Doug McIlory and Bob Morris from Bell Labs.[22] EPL supported the project until a boot-strapping compiler for the full PL/I could be developed.[23]
Bell Labs left the Multics project in 1969: "Over time, hope was replaced by frustration as the group effort initially failed to produce an economically useful system."[24] Continued participation would drive up project support costs. So researchers turned to other development efforts. A system programming language B based on BCPL concepts was written by Dennis Ritchie and Ken Thompson. Ritchie created a boot-strapping compiler for B and wrote Unics (Uniplexed Information and Computing Service) operating system for a PDP-7 in B. Unics eventually became spelled Unix.
Bell Labs started development and expansion of C based on B and BCPL. The BCPL compiler had been transported to Multics by Bell Labs and BCPL was a preferred language at Bell Labs.[25] Initially, a front-end program to Bell Labs' B compiler was used while a C compiler was developed. In 1971, a new PDP-11 provided the resource to define extensions to B and rewrite the compiler. By 1973 the design of C language was essentially complete and the Unix kernel for a PDP-11 was rewritten in C. Steve Johnson started development of Portable C Compiler (PCC) to support retargeting of C compilers to new machines.[26][27]
Object-oriented programming (OOP) offered some interesting possibilities for application development and maintenance. OOP concepts go further back but were part of LISP and Simula language science.[28] At Bell Labs, the development of C++ became interested in OOP.[29] C++ was first used in 1980 for systems programming. The initial design leveraged C language systems programming capabilities with Simula concepts. Object-oriented facilities were added in 1983.[30] The Cfront program implemented a C++ front-end for C84 language compiler. In subsequent years several C++ compilers were developed as C++ popularity grew.
In many application domains, the idea of using a higher-level language quickly caught on. Because of the expanding functionality supported by newer programming languages and the increasing complexity of computer architectures, compilers became more complex.
DARPA (Defense Advanced Research Projects Agency) sponsored a compiler project with Wulf's CMU research team in 1970. The Production Quality Compiler-Compiler PQCC design would produce a Production Quality Compiler (PQC) from formal definitions of source language and the target.[31] PQCC tried to extend the term compiler-compiler beyond the traditional meaning as a parser generator (e.g., Yacc) without much success. PQCC might more properly be referred to as a compiler generator.
PQCC research into code generation process sought to build a truly automatic compiler-writing system. The effort discovered and designed the phase structure of the PQC. The BLISS-11 compiler provided the initial structure.[32] The phases included analyses (front end), intermediate translation to virtual machine (middle end), and translation to the target (back end). TCOL was developed for the PQCC research to handle language specific constructs in the intermediate representation.[33] Variations of TCOL supported various languages. The PQCC project investigated techniques of automated compiler construction. The design concepts proved useful in optimizing compilers and compilers for the object-oriented programming language Ada.
The Ada Stoneman Document formalized the program support environment (APSE) along with the kernel (KAPSE) and minimal (MAPSE). An Ada interpreter NYU/ED supported development and standardization efforts with American National Standards Institute (ANSI) and the International Standards Organization (ISO). Initial Ada compiler development by the U.S. Military Services included the compilers in a complete integrated design environment along the lines of the Stoneman Document. Army and Navy worked on the Ada Language System (ALS) project targeted to DEC/VAX architecture while the Air Force started on the Ada Integrated Environment (AIE) targeted to IBM 370 series. While the projects did not provide the desired results, they did contribute to the overal effort on Ada development.[34]
Other Ada compiler efforts got under way in Britain at University of York and in Germany at University of Karlsruhe. In the U. S., Verdix (later acquired by Rational) delivered the Verdix Ada Development System (VADS) to the Army. VADS provided a set of development tools including a compiler. Unix/VADS could be hosted on a variety of Unix platforms such as DEC Ultrix and the Sun 3/60 Solaris targeted to Motorola 68020 in an Army CECOM evaluation.[35] There were soon many Ada compilers available that passed the Ada Validation tests. The Free Software Foundation GNU project developed the GNU Compiler Collection (GCC) which provides a core capability to support multiple languages and targets. The Ada version GNAT is one of the most widely used Ada compilers. GNAT is free but there is also commercial support, for example, AdaCore, was founded in 1994 to provide commercial software solutions for Ada. GNAT Pro includes the GNU GCC based GNAT with a tool suite to provide an integrated development environment.
High-level languages continued to drive compiler research and development. Focus areas included optimization and automatic code generation. Trends in programming languages and development environments influenced compiler technology. More compilers became included in language distributions (PERL, Java Development Kit) and as a component of an IDE (VADS, Eclipse, Ada Pro). The interrelationship and interdependence of technologies grew. The advent of web services promoted growth of web languages and scripting languages. Scripts trace back to the early days of Command Line Interfaces (CLI) where the user could enter commands to be executed by the system. User Shell concepts developed with languages to write shell programs. Early Windows designs offered a simple batch programming capability. The conventional transformation of these language used an interpreter. While not widely used, Bash and Batch compilers have been written. More recently sophisticated interpreted languages became part of the developers tool kit. Modern scripting languages include PHP, Python, Ruby and Lua. (Lua is widely used in game development.) All of these have interpreter and compiler support.[36]
"When the field of compiling began in the late 50s, its focus was limited to the translation of high-level language programs into machine code ... The compiler field is increasingly intertwined with other disciplines including computer architecture, programming languages, formal methods, software engineering, and computer security."[37] The "Compiler Research: The Next 50 Years" article noted the importance of object-oriented languages and Java. Security and parallel computing were cited among the future research targets.
A compiler implements a formal transformation from a high-level source program to a low-level target program. Compiler design can define an end to end solution or tackle a defined subset that interfaces with other compilation tools e.g. preprocessors, assemblers, linkers. Design requirements include rigorously defined interfaces both internally between compiler components and externally between supporting toolsets.
In the early days, the approach taken to compiler design was directly affected by the complexity of the computer language to be processed, the experience of the person(s) designing it, and the resources available. Resource limitations led to the need to pass through the source code more than once.
A compiler for a relatively simple language written by one person might be a single, monolithic piece of software. However, as the source language grows in complexity the design may be split into a number of interdependent phases. Separate phases provide design improvements that focus development on the functions in the compilation process.
Classifying compilers by number of passes has its background in the hardware resource limitations of computers. Compiling involves performing lots of work and early computers did not have enough memory to contain one program that did all of this work. So compilers were split up into smaller programs which each made a pass over the source (or some representation of it) performing some of the required analysis and translations.
The ability to compile in a single pass has classically been seen as a benefit because it simplifies the job of writing a compiler and one-pass compilers generally perform compilations faster than multi-pass compilers. Thus, partly driven by the resource limitations of early systems, many early languages were specifically designed so that they could be compiled in a single pass (e.g., Pascal).
In some cases the design of a language feature may require a compiler to perform more than one pass over the source. For instance, consider a declaration appearing on line 20 of the source which affects the translation of a statement appearing on line 10. In this case, the first pass needs to gather information about declarations appearing after statements that they affect, with the actual translation happening during a subsequent pass.
The disadvantage of compiling in a single pass is that it is not possible to perform many of the sophisticated optimizations needed to generate high quality code. It can be difficult to count exactly how many passes an optimizing compiler makes. For instance, different phases of optimization may analyse one expression many times but only analyse another expression once.
Splitting a compiler up into small programs is a technique used by researchers interested in producing provably correct compilers. Proving the correctness of a set of small programs often requires less effort than proving the correctness of a larger, single, equivalent program.
Regardless of the exact number of phases in the compiler design, the phases can be assigned to one of three stages. The stages include a front end, a middle end, and a back end.
This front/middle/back-end approach makes it possible to combine front ends for different languages with back ends for different CPUs while sharing the optimizations of the middle end.[38] Practical examples of this approach are the GNU Compiler Collection, LLVM,[39] and the Amsterdam Compiler Kit, which have multiple front-ends, shared optimizations and multiple back-ends.
The front end analyzes the source code to build an internal representation of the program, called the intermediate representation (IR). It also manages the symbol table, a data structure mapping each symbol in the source code to associated information such as location, type and scope.
While the frontend can be a single monolithic function or program, as in a scannerless parser, it is more commonly implemented and analyzed as several phases, which may execute sequentially or concurrently. This method is favored due to its modularity and separation of concerns. Most commonly today, the frontend is broken into three phases: lexical analysis (also known as lexing), syntax analysis (also known as scanning or parsing), and semantic analysis. Lexing and parsing comprise the syntactic analysis (word syntax and phrase syntax, respectively), and in simple cases these modules (the lexer and parser) can be automatically generated from a grammar for the language, though in more complex cases these require manual modification. The lexical grammar and phrase grammar are usually context-free grammars, which simplifies analysis significantly, with context-sensitivity handled at the semantic analysis phase. The semantic analysis phase is generally more complex and written by hand, but can be partially or fully automated using attribute grammars. These phases themselves can be further broken down: lexing as scanning and evaluating, and parsing as building a concrete syntax tree (CST, parse tree) and then transforming it into an abstract syntax tree (AST, syntax tree). In some cases additional phases are used, notably line reconstruction and preprocessing, but these are rare.
The main phases of the front end include the following:
The middle end performs optimizations on the intermediate representation in order to improve the performance and the quality of the produced machine code.[43] The middle end contains those optimizations that are independent of the CPU architecture being targeted.
The main phases of the middle end include the following:
Compiler analysis is the prerequisite for any compiler optimization, and they tightly work together. For example, dependence analysis is crucial for loop transformation.
The scope of compiler analysis and optimizations vary greatly, from as small as a basic block to the procedure/function level, or even over the whole program (interprocedural optimization). Obviously,[clarification needed] a compiler can potentially do a better job using a broader view. But that broad view is not free: large scope analysis and optimizations are very costly in terms of compilation time and memory space; this is especially true for interprocedural analysis and optimizations.
Interprocedural analysis and optimizations are common in modern commercial compilers from HP, IBM, SGI, Intel, Microsoft, and Sun Microsystems. The free software GCC was criticized for a long time for lacking powerful interprocedural optimizations, but it is changing in this respect. Another open source compiler with full analysis and optimization infrastructure is Open64, which is used by many organizations for research and commercial purposes.
Due to the extra time and space needed for compiler analysis and optimizations, some compilers skip them by default. Users have to use compilation options to explicitly tell the compiler which optimizations should be enabled.
The back end is responsible for the CPU architecture specific optimizations and for code generation.
The main phases of the back end include the following:
Compiler correctness is the branch of software engineering that deals with trying to show that a compiler behaves according to its language specification.[45][self-published source?][non-primary source needed] Techniques include developing the compiler using formal methods and using rigorous testing (often called compiler validation) on an existing compiler.
Higher-level programming languages usually appear with a type of translation in mind: either designed as compiled language or interpreted language. However, in practice there is rarely anything about a language that requires it to be exclusively compiled or exclusively interpreted, although it is possible to design languages that rely on re-interpretation at run time. The categorization usually reflects the most popular or widespread implementations of a language — for instance, BASIC is sometimes called an interpreted language, and C a compiled one, despite the existence of BASIC compilers and C interpreters.
Interpretation does not replace compilation completely. It only hides it from the user and makes it gradual. Even though an interpreter can itself be interpreted, a directly executed program is needed somewhere at the bottom of the stack (see machine language).
Further, compilers can contain interpreters for optimization reasons. For example, where an expression can be executed during compilation and the results inserted into the output program, then it prevents it having to be recalculated each time the program runs, which can greatly speed up the final program. Modern trends toward just-in-time compilation and bytecode interpretation at times blur the traditional categorizations of compilers and interpreters even further.
Some language specifications spell out that implementations must include a compilation facility; for example, Common Lisp. However, there is nothing inherent in the definition of Common Lisp that stops it from being interpreted. Other languages have features that are very easy to implement in an interpreter, but make writing a compiler much harder; for example, APL, SNOBOL4, and many scripting languages allow programs to construct arbitrary source code at runtime with regular string operations, and then execute that code by passing it to a special evaluation function. To implement these features in a compiled language, programs must usually be shipped with a runtime library that includes a version of the compiler itself.
One classification of compilers is by the platform on which their generated code executes. This is known as the target platform.
A native or hosted compiler is one whose output is intended to directly run on the same type of computer and operating system that the compiler itself runs on. The output of a cross compiler is designed to run on a different platform. Cross compilers are often used when developing software for embedded systems that are not intended to support a software development environment.
The output of a compiler that produces code for a virtual machine (VM) may or may not be executed on the same platform as the compiler that produced it. For this reason such compilers are not usually classified as native or cross compilers.
The lower level language that is the target of a compiler may itself be a high-level programming language. C, often viewed as some sort of portable assembler, can also be the target language of a compiler. E.g.: Cfront, the original compiler for C++ used C as target language. The C created by such a compiler is usually not intended to be read and maintained by humans. So indent style and pretty C intermediate code are irrelevant. Some features of C turn it into a good target language. E.g.: C code with #line directives can be generated to support debugging of the original source.
While a common compiler type outputs machine code, there are many other types:



Compiler - Wikipedia

A compiler is computer software that transforms computer code written in one programming language (the source language) into another programming language (the target language). Compilers are a type of translator that support digital devices, primarily computers. The name compiler is primarily used for programs that translate source code from a high-level programming language to a lower level language (e.g., assembly language, object code, or machine code) to create an executable program.[1]
However, there are many different types of compilers. If the compiled program can run on a computer whose CPU or operating system is different from the one on which the compiler runs, the compiler is a cross-compiler. A bootstrap compiler is written in the language that it intends to compile. A program that translates from a low-level language to a higher level one is a decompiler. A program that translates between high-level languages is usually called a source-to-source compiler or transpiler. A language rewriter is usually a program that translates the form of expressions without a change of language. The term compiler-compiler refers to tools used to create parsers that perform syntax analysis.
A compiler is likely to perform many or all of the following operations: preprocessing, lexical analysis, parsing, semantic analysis (syntax-directed translation), conversion of input programs to an intermediate representation, code optimization and code generation. Compilers implement these operations in phases that promote efficient design and correct transformations of source input to target output. Program faults caused by incorrect compiler behavior can be very difficult to track down and work around; therefore, compiler implementers invest significant effort to ensure compiler correctness.[2]
Compilers are not the only translators used to transform source programs. An interpreter is computer software that transforms and then executes the indicated operations. The translation process influences the design of computer languages which leads to a preference of compilation or interpretation. In practice, an interpreter can be implemented for compiled languages and compilers can be implemented for interpreted languages.
Theoretical computing concepts developed by scientists, mathematicians, and engineers formed the basis of digital modern computing development during World War II. Primitive binary languages evolved because digital devices only understand ones and zeros and the circuit patterns in the underlying machine architecture. In the late 1940s, assembly languages were created to offer a more workable abstraction of the computer architectures. Limited memory capacity of early computers led to substantial technical challenges when the first compilers were designed. Therefore, the compilation process needed to be divided into several small programs. The front end programs produce the analysis products used by the back end programs to generate target code. As computer technology provided more resources, compiler designs could align better with the compilation process.
The human mind can design better solutions as the language moves from the machine to a higher level. So the development of high-level languages followed naturally from the capabilities offered by the digital computers. High-level languages are formal languages that are strictly defined by their syntax and semantics which form the high-level language architecture. Elements of these formal languages include:
The sentences in a language may be defined by a set of rules called a grammar.[3]
Backus-Naur form (BNF) describes the syntax of "sentences" of a language and was used for the syntax of Algol 60 by John Backus.[4] The ideas derive from the context-free grammar concepts by Noam Chomsky, a linguist.[5] "BNF and its extensions have become standard tools for describing the syntax of programming notations, and in many cases parts of compilers are generated automatically from a BNF description."[6]
In the 1940s, Konrad Zuse designed an algorithmic programming language called Plankalkül ("Plan Calculus"). While no actual implementation occurred until the 1970s, it presented concepts later seen in APL designed by Ken Iverson in the late 1950s.[7] APL is a language for mathematical computations.
High-level language design during the formative years of digital computing provided useful programming tools for a variety of applications:
Compiler technology evolved from the need for a strictly defined transformation of the high-level source program into a low-level target program for the digital computer. The compiler could be viewed as a front end to deal with analysis of the source code and a back end to synthesize the analysis into the target code. Optimization between the front end and back end could produce more efficient target code.[11]
Some early milestones in the development of compiler technology:
Early operating systems and software were written in assembly language. In the 60s and early 70s, the use of high-level languages for system programming was still controversial due to resource limitations. However, several research and industry efforts began the shift toward high-level systems programming languages, for example, BCPL, BLISS, B, and C.
BCPL (Basic Combined Programming Language) designed in 1966 by Martin Richards at the University of Cambridge was originally developed as a compiler writing tool.[16] Several compilers have been implemented, Richards' book provides insights to the language and its compiler.[17] BCPL was not only an influential systems programming language that is still used in research[18] but also provided a basis for the design of B and C languages.
BLISS (Basic Language for Implementation of System Software) was developed for a Digital Equipment Corporation (DEC) PDP-10 computer by W.A. Wulf's Carnegie Mellon University (CMU) research team. The CMU team went on to develop BLISS-11 compiler one year later in 1970.
Multics (Multiplexed Information and Computing Service), a time-sharing operating system project, involved MIT, Bell Labs, General Electric (later Honeywell) and was led by Fernando Corbató from MIT.[19] Multics was written in the PL/I language developed by IBM and IBM User Group.[20] IBM's goal was to satisfy business, scientific, and systems programming requirements. There were other languages that could have been considered but PL/I offered the most complete solution even though it had not been implemented.[21] For the first few years of the Mulitics project, a subset of the language could be compiled to assembly language with the Early PL/I (EPL) compiler by Doug McIlory and Bob Morris from Bell Labs.[22] EPL supported the project until a boot-strapping compiler for the full PL/I could be developed.[23]
Bell Labs left the Multics project in 1969: "Over time, hope was replaced by frustration as the group effort initially failed to produce an economically useful system."[24] Continued participation would drive up project support costs. So researchers turned to other development efforts. A system programming language B based on BCPL concepts was written by Dennis Ritchie and Ken Thompson. Ritchie created a boot-strapping compiler for B and wrote Unics (Uniplexed Information and Computing Service) operating system for a PDP-7 in B. Unics eventually became spelled Unix.
Bell Labs started development and expansion of C based on B and BCPL. The BCPL compiler had been transported to Multics by Bell Labs and BCPL was a preferred language at Bell Labs.[25] Initially, a front-end program to Bell Labs' B compiler was used while a C compiler was developed. In 1971, a new PDP-11 provided the resource to define extensions to B and rewrite the compiler. By 1973 the design of C language was essentially complete and the Unix kernel for a PDP-11 was rewritten in C. Steve Johnson started development of Portable C Compiler (PCC) to support retargeting of C compilers to new machines.[26][27]
Object-oriented programming (OOP) offered some interesting possibilities for application development and maintenance. OOP concepts go further back but were part of LISP and Simula language science.[28] At Bell Labs, the development of C++ became interested in OOP.[29] C++ was first used in 1980 for systems programming. The initial design leveraged C language systems programming capabilities with Simula concepts. Object-oriented facilities were added in 1983.[30] The Cfront program implemented a C++ front-end for C84 language compiler. In subsequent years several C++ compilers were developed as C++ popularity grew.
In many application domains, the idea of using a higher-level language quickly caught on. Because of the expanding functionality supported by newer programming languages and the increasing complexity of computer architectures, compilers became more complex.
DARPA (Defense Advanced Research Projects Agency) sponsored a compiler project with Wulf's CMU research team in 1970. The Production Quality Compiler-Compiler PQCC design would produce a Production Quality Compiler (PQC) from formal definitions of source language and the target.[31] PQCC tried to extend the term compiler-compiler beyond the traditional meaning as a parser generator (e.g., Yacc) without much success. PQCC might more properly be referred to as a compiler generator.
PQCC research into code generation process sought to build a truly automatic compiler-writing system. The effort discovered and designed the phase structure of the PQC. The BLISS-11 compiler provided the initial structure.[32] The phases included analyses (front end), intermediate translation to virtual machine (middle end), and translation to the target (back end). TCOL was developed for the PQCC research to handle language specific constructs in the intermediate representation.[33] Variations of TCOL supported various languages. The PQCC project investigated techniques of automated compiler construction. The design concepts proved useful in optimizing compilers and compilers for the object-oriented programming language Ada.
The Ada Stoneman Document formalized the program support environment (APSE) along with the kernel (KAPSE) and minimal (MAPSE). An Ada interpreter NYU/ED supported development and standardization efforts with American National Standards Institute (ANSI) and the International Standards Organization (ISO). Initial Ada compiler development by the U.S. Military Services included the compilers in a complete integrated design environment along the lines of the Stoneman Document. Army and Navy worked on the Ada Language System (ALS) project targeted to DEC/VAX architecture while the Air Force started on the Ada Integrated Environment (AIE) targeted to IBM 370 series. While the projects did not provide the desired results, they did contribute to the overal effort on Ada development.[34]
Other Ada compiler efforts got under way in Britain at University of York and in Germany at University of Karlsruhe. In the U. S., Verdix (later acquired by Rational) delivered the Verdix Ada Development System (VADS) to the Army. VADS provided a set of development tools including a compiler. Unix/VADS could be hosted on a variety of Unix platforms such as DEC Ultrix and the Sun 3/60 Solaris targeted to Motorola 68020 in an Army CECOM evaluation.[35] There were soon many Ada compilers available that passed the Ada Validation tests. The Free Software Foundation GNU project developed the GNU Compiler Collection (GCC) which provides a core capability to support multiple languages and targets. The Ada version GNAT is one of the most widely used Ada compilers. GNAT is free but there is also commercial support, for example, AdaCore, was founded in 1994 to provide commercial software solutions for Ada. GNAT Pro includes the GNU GCC based GNAT with a tool suite to provide an integrated development environment.
High-level languages continued to drive compiler research and development. Focus areas included optimization and automatic code generation. Trends in programming languages and development environments influenced compiler technology. More compilers became included in language distributions (PERL, Java Development Kit) and as a component of an IDE (VADS, Eclipse, Ada Pro). The interrelationship and interdependence of technologies grew. The advent of web services promoted growth of web languages and scripting languages. Scripts trace back to the early days of Command Line Interfaces (CLI) where the user could enter commands to be executed by the system. User Shell concepts developed with languages to write shell programs. Early Windows designs offered a simple batch programming capability. The conventional transformation of these language used an interpreter. While not widely used, Bash and Batch compilers have been written. More recently sophisticated interpreted languages became part of the developers tool kit. Modern scripting languages include PHP, Python, Ruby and Lua. (Lua is widely used in game development.) All of these have interpreter and compiler support.[36]
"When the field of compiling began in the late 50s, its focus was limited to the translation of high-level language programs into machine code ... The compiler field is increasingly intertwined with other disciplines including computer architecture, programming languages, formal methods, software engineering, and computer security."[37] The "Compiler Research: The Next 50 Years" article noted the importance of object-oriented languages and Java. Security and parallel computing were cited among the future research targets.
A compiler implements a formal transformation from a high-level source program to a low-level target program. Compiler design can define an end to end solution or tackle a defined subset that interfaces with other compilation tools e.g. preprocessors, assemblers, linkers. Design requirements include rigorously defined interfaces both internally between compiler components and externally between supporting toolsets.
In the early days, the approach taken to compiler design was directly affected by the complexity of the computer language to be processed, the experience of the person(s) designing it, and the resources available. Resource limitations led to the need to pass through the source code more than once.
A compiler for a relatively simple language written by one person might be a single, monolithic piece of software. However, as the source language grows in complexity the design may be split into a number of interdependent phases. Separate phases provide design improvements that focus development on the functions in the compilation process.
Classifying compilers by number of passes has its background in the hardware resource limitations of computers. Compiling involves performing lots of work and early computers did not have enough memory to contain one program that did all of this work. So compilers were split up into smaller programs which each made a pass over the source (or some representation of it) performing some of the required analysis and translations.
The ability to compile in a single pass has classically been seen as a benefit because it simplifies the job of writing a compiler and one-pass compilers generally perform compilations faster than multi-pass compilers. Thus, partly driven by the resource limitations of early systems, many early languages were specifically designed so that they could be compiled in a single pass (e.g., Pascal).
In some cases the design of a language feature may require a compiler to perform more than one pass over the source. For instance, consider a declaration appearing on line 20 of the source which affects the translation of a statement appearing on line 10. In this case, the first pass needs to gather information about declarations appearing after statements that they affect, with the actual translation happening during a subsequent pass.
The disadvantage of compiling in a single pass is that it is not possible to perform many of the sophisticated optimizations needed to generate high quality code. It can be difficult to count exactly how many passes an optimizing compiler makes. For instance, different phases of optimization may analyse one expression many times but only analyse another expression once.
Splitting a compiler up into small programs is a technique used by researchers interested in producing provably correct compilers. Proving the correctness of a set of small programs often requires less effort than proving the correctness of a larger, single, equivalent program.
Regardless of the exact number of phases in the compiler design, the phases can be assigned to one of three stages. The stages include a front end, a middle end, and a back end.
This front/middle/back-end approach makes it possible to combine front ends for different languages with back ends for different CPUs while sharing the optimizations of the middle end.[38] Practical examples of this approach are the GNU Compiler Collection, LLVM,[39] and the Amsterdam Compiler Kit, which have multiple front-ends, shared optimizations and multiple back-ends.
The front end analyzes the source code to build an internal representation of the program, called the intermediate representation (IR). It also manages the symbol table, a data structure mapping each symbol in the source code to associated information such as location, type and scope.
While the frontend can be a single monolithic function or program, as in a scannerless parser, it is more commonly implemented and analyzed as several phases, which may execute sequentially or concurrently. This method is favored due to its modularity and separation of concerns. Most commonly today, the frontend is broken into three phases: lexical analysis (also known as lexing), syntax analysis (also known as scanning or parsing), and semantic analysis. Lexing and parsing comprise the syntactic analysis (word syntax and phrase syntax, respectively), and in simple cases these modules (the lexer and parser) can be automatically generated from a grammar for the language, though in more complex cases these require manual modification. The lexical grammar and phrase grammar are usually context-free grammars, which simplifies analysis significantly, with context-sensitivity handled at the semantic analysis phase. The semantic analysis phase is generally more complex and written by hand, but can be partially or fully automated using attribute grammars. These phases themselves can be further broken down: lexing as scanning and evaluating, and parsing as building a concrete syntax tree (CST, parse tree) and then transforming it into an abstract syntax tree (AST, syntax tree). In some cases additional phases are used, notably line reconstruction and preprocessing, but these are rare.
The main phases of the front end include the following:
The middle end performs optimizations on the intermediate representation in order to improve the performance and the quality of the produced machine code.[43] The middle end contains those optimizations that are independent of the CPU architecture being targeted.
The main phases of the middle end include the following:
Compiler analysis is the prerequisite for any compiler optimization, and they tightly work together. For example, dependence analysis is crucial for loop transformation.
The scope of compiler analysis and optimizations vary greatly, from as small as a basic block to the procedure/function level, or even over the whole program (interprocedural optimization). Obviously,[clarification needed] a compiler can potentially do a better job using a broader view. But that broad view is not free: large scope analysis and optimizations are very costly in terms of compilation time and memory space; this is especially true for interprocedural analysis and optimizations.
Interprocedural analysis and optimizations are common in modern commercial compilers from HP, IBM, SGI, Intel, Microsoft, and Sun Microsystems. The free software GCC was criticized for a long time for lacking powerful interprocedural optimizations, but it is changing in this respect. Another open source compiler with full analysis and optimization infrastructure is Open64, which is used by many organizations for research and commercial purposes.
Due to the extra time and space needed for compiler analysis and optimizations, some compilers skip them by default. Users have to use compilation options to explicitly tell the compiler which optimizations should be enabled.
The back end is responsible for the CPU architecture specific optimizations and for code generation.
The main phases of the back end include the following:
Compiler correctness is the branch of software engineering that deals with trying to show that a compiler behaves according to its language specification.[45][self-published source?][non-primary source needed] Techniques include developing the compiler using formal methods and using rigorous testing (often called compiler validation) on an existing compiler.
Higher-level programming languages usually appear with a type of translation in mind: either designed as compiled language or interpreted language. However, in practice there is rarely anything about a language that requires it to be exclusively compiled or exclusively interpreted, although it is possible to design languages that rely on re-interpretation at run time. The categorization usually reflects the most popular or widespread implementations of a language — for instance, BASIC is sometimes called an interpreted language, and C a compiled one, despite the existence of BASIC compilers and C interpreters.
Interpretation does not replace compilation completely. It only hides it from the user and makes it gradual. Even though an interpreter can itself be interpreted, a directly executed program is needed somewhere at the bottom of the stack (see machine language).
Further, compilers can contain interpreters for optimization reasons. For example, where an expression can be executed during compilation and the results inserted into the output program, then it prevents it having to be recalculated each time the program runs, which can greatly speed up the final program. Modern trends toward just-in-time compilation and bytecode interpretation at times blur the traditional categorizations of compilers and interpreters even further.
Some language specifications spell out that implementations must include a compilation facility; for example, Common Lisp. However, there is nothing inherent in the definition of Common Lisp that stops it from being interpreted. Other languages have features that are very easy to implement in an interpreter, but make writing a compiler much harder; for example, APL, SNOBOL4, and many scripting languages allow programs to construct arbitrary source code at runtime with regular string operations, and then execute that code by passing it to a special evaluation function. To implement these features in a compiled language, programs must usually be shipped with a runtime library that includes a version of the compiler itself.
One classification of compilers is by the platform on which their generated code executes. This is known as the target platform.
A native or hosted compiler is one whose output is intended to directly run on the same type of computer and operating system that the compiler itself runs on. The output of a cross compiler is designed to run on a different platform. Cross compilers are often used when developing software for embedded systems that are not intended to support a software development environment.
The output of a compiler that produces code for a virtual machine (VM) may or may not be executed on the same platform as the compiler that produced it. For this reason such compilers are not usually classified as native or cross compilers.
The lower level language that is the target of a compiler may itself be a high-level programming language. C, often viewed as some sort of portable assembler, can also be the target language of a compiler. E.g.: Cfront, the original compiler for C++ used C as target language. The C created by such a compiler is usually not intended to be read and maintained by humans. So indent style and pretty C intermediate code are irrelevant. Some features of C turn it into a good target language. E.g.: C code with #line directives can be generated to support debugging of the original source.
While a common compiler type outputs machine code, there are many other types:



Wikipedia:Citation needed - Wikipedia

To ensure that all Wikipedia content is verifiable, anyone may question an uncited claim by inserting a simple {{Citation needed}} tag, or by using a more comprehensive {{Citation needed|reason=Your explanation here|date=October 2018}} clause. This displays as: 
Example: 87% of statistics are made up on the spot.[citation needed]
"Citation needed" statements are part of  Wikipedia's backlog of outstanding problems. Currently there are 348,950 articles with "Citation needed" statements (see the historical number of tags). You can help reduce the backlog!
A "citation needed" tag is a request for another editor to verify a statement: a form of communication between members of a collaborative editing community. It is never, in itself, an "improvement" of an article. Though readers may be alerted by a "citation needed" that a particular statement is not supported, many readers don't fully understand the community's processes. Not all tags get addressed in a timely manner, staying in place for months or years, forming an ever growing Wikipedia backlog—this itself can be a problem. Best practice recommends the following: 
Before adding a tag, at least consider the following alternatives, one of which may prove much more constructive:
For now, there are over 348,950 articles with "Citation needed" statements. You can browse the whole list of these articles at Category:All articles with unsourced statements.
Frequently the authors of statements do not return to Wikipedia to support the statement with citations, so other Wikipedia editors have to do work checking those statements. With  348,950 statements that need WP:Verification, sometimes it's hard to choose which article to work on. The tool Citation Hunt makes that easier by suggesting random articles, which you can sort by topical category membership. 



Just-in-time compilation - Wikipedia
In computing, just-in-time (JIT) compilation, (also dynamic translation or run-time compilation)[1], is a way of executing computer code that involves compilation during execution of a program – at run time – rather than prior to execution.[2] Most often, this consists of source code or more commonly bytecode translation to machine code, which is then executed directly. A system implementing a JIT compiler typically continuously analyses the code being executed and identifies parts of the code where the speedup gained from compilation or recompilation would outweigh the overhead of compiling that code. 
JIT compilation is a combination of the two traditional approaches to translation to machine code – ahead-of-time compilation (AOT), and interpretation – and combines some advantages and drawbacks of both.[2] Roughly, JIT compilation combines the speed of compiled code with the flexibility of interpretation, with the overhead of an interpreter and the additional overhead of compiling (not just interpreting). JIT compilation is a form of dynamic compilation, and allows adaptive optimization such as dynamic recompilation – thus, in theory, JIT compilation can yield faster execution than static compilation. Interpretation and JIT compilation are particularly suited for dynamic programming languages, as the runtime system can handle late-bound data types and enforce security guarantees.
JIT compilation can be applied to  some programs, or can be used for certain capacities, particularly dynamic capacities such as regular expressions. For example, a text editor may compile a regular expression provided at runtime to machine code to allow faster matching – this cannot be done ahead of time, as the pattern is only provided at runtime. Several modern runtime environments rely on JIT compilation for high-speed code execution, including most implementations of Java, together with Microsoft's .NET Framework. Similarly, many regular-expression libraries feature JIT compilation of regular expressions, either to bytecode or to machine code. JIT compilation is also used in some emulators, in order to translate machine code from one CPU architecture to another.
A common implementation of JIT compilation is to first have AOT compilation to bytecode (virtual machine code), known as bytecode compilation, and then have JIT compilation to machine code (dynamic compilation), rather than interpretation of the bytecode. This improves the runtime performance compared to interpretation, at the cost of lag due to compilation. JIT compilers translate continuously, as with interpreters, but caching of compiled code minimizes lag on future execution of the same code during a given run. Since only part of the program is compiled, there is significantly less lag than if the entire program were compiled prior to execution.
In a bytecode-compiled system, source code is translated to an intermediate representation known as bytecode. Bytecode is not the machine code for any particular computer, and may be portable among computer architectures. The bytecode may then be interpreted by, or run on a virtual machine. The JIT compiler reads the bytecodes in many sections (or in full, rarely) and compiles them dynamically into machine code so the program can run faster. This can be done per-file, per-function or even on any arbitrary code fragment; the code can be compiled when it is about to be executed (hence the name "just-in-time"), and then cached and reused later without needing to be recompiled.
In contrast, a traditional interpreted virtual machine will simply interpret the bytecode, generally with much lower performance. Some interpreters even interpret source code, without the step of first compiling to bytecode, with even worse performance. Statically-compiled code or native code is compiled prior to deployment. A dynamic compilation environment is one in which the compiler can be used during execution. 
A common goal of using JIT techniques is to reach or surpass the performance of static compilation, while maintaining the advantages of bytecode interpretation: Much of the "heavy lifting" of parsing the original source code and performing basic optimization is often handled at compile time, prior to deployment: compilation from bytecode to machine code is much faster than compiling from source. The deployed bytecode is portable, unlike native code. Since the runtime has control over the compilation, like interpreted bytecode, it can run in a secure sandbox. Compilers from bytecode to machine code are easier to write, because the portable bytecode compiler has already done much of the work.
JIT code generally offers far better performance than interpreters. In addition, it can in some cases offer better performance than static compilation, as many optimizations are only feasible at run-time:[3][4]
JIT causes a slight to noticeable delay in initial execution of an application, due to the time taken to load and compile the bytecode. Sometimes this delay is called "startup time delay". In general, the more optimization JIT performs, the better the code it will generate, but the initial delay will also increase. A JIT compiler therefore has to make a trade-off between the compilation time and the quality of the code it hopes to generate. Startup time can be increased IO-bound operations in addition to JIT compilation: for example, the rt.jar class data file for the Java Virtual Machine (JVM) is 40 MB and the JVM must seek a lot of data in this contextually huge file.[5]
One possible optimization, used by Sun's HotSpot Java Virtual Machine, is to combine interpretation and JIT compilation. The application code is initially interpreted, but the JVM monitors which sequences of bytecode are frequently executed and translates them to machine code for direct execution on the hardware. For bytecode which is executed only a few times, this saves the compilation time and reduces the initial latency; for frequently executed bytecode, JIT compilation is used to run at high speed, after an initial phase of slow interpretation. Additionally, since a program spends most time executing a minority of its code, the reduced compilation time is significant. Finally, during the initial code interpretation, execution statistics can be collected before compilation, which helps to perform better optimization.[6]
The correct tradeoff can vary due to circumstances. For example, Sun's Java Virtual Machine has two major modes—client and server. In client mode, minimal compilation and optimization is performed, to reduce startup time. In server mode, extensive compilation and optimization is performed, to maximize performance once the application is running by sacrificing startup time. Other Java just-in-time compilers have used a runtime measurement of the number of times a method has executed combined with the bytecode size of a method as a heuristic to decide when to compile.[7]  Still another uses the number of times executed combined with the detection of loops.[8] In general, it is much harder to accurately predict which methods to optimize in short-running applications than in long-running ones.[9]
Native Image Generator (Ngen) by Microsoft is another approach at reducing the initial delay.[10] Ngen pre-compiles (or "pre-JITs") bytecode in a Common Intermediate Language image into machine native code. As a result, no runtime compilation is needed. .NET framework 2.0 shipped with Visual Studio 2005 runs Ngen on all of the Microsoft library DLLs right after the installation. Pre-jitting provides a way to improve the startup time. However, the quality of code it generates might not be as good as the one that is JITed, for the same reasons why code compiled statically, without profile-guided optimization, cannot be as good as JIT compiled code in the extreme case: the lack of profiling data to drive, for instance, inline caching.[11]
There also exist Java implementations that combine an AOT (ahead-of-time) compiler with either a JIT compiler (Excelsior JET) or interpreter (GNU Compiler for Java).
The earliest published JIT compiler is generally attributed to work on LISP by John McCarthy in 1960.[12] In his seminal paper Recursive functions of symbolic expressions and their computation by machine, Part I, he mentions functions that are translated during runtime, thereby sparing the need to save the compiler output to punch cards[13] (although this would be more accurately known as a "Compile and go system"). Another early example was by Ken Thompson, who in 1968 gave one of the first applications of regular expressions, here for pattern matching in the text editor QED.[14] For speed, Thompson implemented regular expression matching by JITing to IBM 7094 code on the Compatible Time-Sharing System.[12] An influential technique for deriving compiled code from interpretation was pioneered by Mitchell in 1970, which he implemented for the experimental language LC².[15][16]
Smalltalk (c. 1983) pioneered new aspects of JIT compilations. For example, translation to machine code was done on demand, and the result was cached for later use. When memory became scarce, the system would delete some of this code and regenerate it when it was needed again.[2][17] Sun's Self language improved these techniques extensively and was at one point the fastest Smalltalk system in the world; achieving up to half the speed of optimized C[18] but with a fully object-oriented language.
Self was abandoned by Sun, but the research went into the Java language. The term "Just-in-time compilation" was borrowed from the manufacturing term "Just in time" and popularized by Java, with James Gosling using the term from 1993.[19] Currently JITing is used by most implementations of the Java Virtual Machine, as HotSpot builds on, and extensively uses, this research base.
The HP project Dynamo[20] was an experimental JIT compiler where the 'bytecode' format and the machine code format were the same; the system turned PA-6000 machine code into PA-8000 machine code. Counterintuitively, this resulted in speed ups, in some cases of 30% since doing this permitted optimizations at the machine code level, for example, inlining code for better cache usage and optimizations of calls to dynamic libraries and many other run-time optimizations which conventional compilers are not able to attempt.[21][22]
JIT compilation fundamentally uses executable data, and thus poses security challenges and possible exploits.
Implementation of JIT compilation consists of compiling source code or byte code to machine code and executing it. This is generally done directly in memory – the JIT compiler outputs the machine code directly into memory and immediately executes it, rather than outputting it to disk and then invoking the code as a separate program, as in usual ahead of time compilation. In modern architectures this runs into a problem due to executable space protection – arbitrary memory cannot be executed, as otherwise there is a potential security hole. Thus memory must be marked as executable; for security reasons this should be done after the code has been written to memory, and marked read-only, as writable/executable memory is a security hole (see W^X).[23] For instance Firefox's JIT compiler for Javascript introduced this protection in a release version with Firefox 46[24]
JIT spraying is a class of computer security exploits that use JIT compilation for heap spraying – the resulting memory is then executable, which allows an exploit if execution can be moved into the heap.



Domain-specific language - Wikipedia
A domain-specific language (DSL) is a computer language specialized to a particular application domain. This is in contrast to a general-purpose language (GPL), which is broadly applicable across domains. There are a wide variety of DSLs, ranging from widely used languages for common domains, such as HTML for web pages, down to languages used by only one or a few pieces of software, such as MUSH soft code. DSLs can be further subdivided by the kind of language, and include domain-specific markup languages, domain-specific modeling languages (more generally, specification languages), and domain-specific programming languages. Special-purpose computer languages have always existed in the computer age, but the term "domain-specific language" has become more popular due to the rise of domain-specific modeling. Simpler DSLs, particularly ones used by a single application, are sometimes informally called mini-languages.
The line between general-purpose languages and domain-specific languages is not always sharp, as a language may have specialized features for a particular domain but be applicable more broadly, or conversely may in principle be capable of broad application but in practice used primarily for a specific domain. For example, Perl was originally developed as a text-processing and glue language, for the same domain as AWK and shell scripts, but was mostly used as a general-purpose programming language later on. By contrast, PostScript is a Turing complete language, and in principle can be used for any task, but in practice is narrowly used as a page description language.
The design and use of appropriate DSLs is a key part of domain engineering, by using a language suitable to the domain at hand – this may consist of using an existing DSL or GPL, or developing a new DSL. Language-oriented programming considers the creation of special-purpose languages for expressing problems as standard part of the problem solving process. Creating a domain-specific language (with software to support it), rather than reusing an existing language, can be worthwhile if the language allows a particular type of problem or solution to be expressed more clearly than an existing language would allow and the type of problem in question reappears sufficiently often. Pragmatically, a DSL may be specialized to a particular problem domain, a particular problem representation technique, a particular solution technique, or other aspect of a domain.
A domain-specific language is created specifically to solve problems in a particular domain and is not intended to be able to solve problems outside it (although that may be technically possible).  In contrast, general-purpose languages are created to solve problems in many domains. The domain can also be a business area.  Some examples of business areas include:
A domain-specific language is somewhere between a tiny programming language and a scripting language, and is often used in a way analogous to a programming library. The boundaries between these concepts are quite blurry, much like the boundary between scripting languages and general-purpose languages.
Domain-specific languages are languages (or often, declared syntaxes or grammars) with very specific goals in design and implementation. A domain-specific language can be one of a visual diagramming language, such as those created by the Generic Eclipse Modeling System, programmatic abstractions, such as the Eclipse Modeling Framework, or textual languages. For instance, the command line utility grep has a regular expression syntax which matches patterns in lines of text. The sed utility defines a syntax for matching and replacing regular expressions.  Often, these tiny languages can be used together inside a shell to perform more complex programming tasks.
The line between domain-specific languages and scripting languages is somewhat blurred, but domain-specific languages often lack low-level functions for filesystem access, interprocess control, and other functions that characterize full-featured programming languages, scripting or otherwise. Many domain-specific languages do not compile to byte-code or executable code, but to various kinds of media objects: GraphViz exports to PostScript, GIF, JPEG, etc., where Csound compiles to audio files, and a ray-tracing domain-                       specific language like POV compiles to graphics files. A computer language like SQL presents an interesting case: it can be deemed a domain-specific language because it is specific to a specific domain (in SQL's case, accessing and managing relational databases), and is often called from another application, but SQL has more keywords and functions than many scripting languages, and is often thought of as a language in its own right, perhaps because of the prevalence of database manipulation in programming and the amount of mastery required to be an expert in the language.
Further blurring this line, many domain-specific languages have exposed APIs, and can be accessed from other programming languages without breaking the flow of execution or calling a separate process, and can thus operate as programming libraries.
Some domain-specific languages expand over time to include full-featured programming tools, which further complicates the question of whether a language is domain-specific or not. A good example is the functional language XSLT, specifically designed for transforming one XML graph into another, which has been extended since its inception to allow (particularly in its 2.0 version) for various forms of filesystem interaction, string and date manipulation, and data typing.
In model-driven engineering, many examples of domain-specific languages may be found like OCL, a language for decorating models with assertions or QVT, a domain-specific transformation language. However languages like UML are typically general purpose modeling languages.
To summarize, an analogy might be useful: a Very Little Language is like a knife, which can be used in thousands of different ways, from cutting food to cutting down trees. A domain-specific language is like an electric drill: it is a powerful tool with a wide variety of uses, but a specific context, namely, putting holes in things. A General Purpose Language is a complete workbench, with a variety of tools intended for performing a variety of tasks. Domain-specific languages should be used by programmers who, looking at their current workbench, realize they need a better drill, and find that a particular domain-specific language provides exactly that.
There are several usage patterns for domain-specific languages:[1][2]
Many domain-specific languages can be used in more than one way.[citation needed] DSL code embedded in a host language may have special syntax support, such as regexes in sed, AWK, Perl or JavaScript, or may be passed as strings.
Adopting a domain-specific language approach to software engineering involves both risks and opportunities. The well-designed domain-specific language manages to find the proper balance between these.
Domain-specific languages have important design goals that contrast with those of general-purpose languages:
In programming, idioms are methods imposed by programmers to handle common development tasks, e.g.:
General purpose programming languages rarely support such idioms, but domain-specific languages can describe them, e.g.:
Examples of domain-specific languages include HTML, Logo for pencil-like drawing, Verilog and VHDL hardware description languages, MATLAB and GNU Octave for matrix programming, Mathematica, Maple and Maxima for symbolic mathematics, Specification and Description Language for reactive and distributed systems, spreadsheet formulas and macros, SQL for relational database queries, YACC grammars for creating parsers, regular expressions for specifying lexers, the Generic Eclipse Modeling System for creating diagramming languages, Csound for sound and music synthesis, and the input languages of GraphViz and GrGen, software packages used for graph layout and graph rewriting.
The GML scripting language used by GameMaker: Studio is a domain-specific language targeted at novice programmers to easily be able to learn programming. While the language serves as a blend of multiple languages including Delphi, C++, and BASIC, there is a lack of structures, data types, and other features of a full-fledged programming language. Many of the built-in functions are sandboxed for the purpose of easy portability. The language primarily serves to make it easy for anyone to pick up the language and develop a game.
Unix shell scripts give a good example of a domain-specific language for data[3] organization. They can manipulate data in files or user input in many different ways.  Domain abstractions and notations include streams (such as stdin and stdout) and operations on streams (such as redirection and pipe).  These abstractions combine to make a robust language to describe the flow and organization of data.
The language consists of a simple interface (a script) for running and controlling processes that perform small tasks.  These tasks represent the idioms of organizing data into a desired format such as tables, graphs, charts, etc.
These tasks consist of simple control-flow and string manipulation mechanisms that cover a lot of common usages like searching and replacing string in files, or counting occurrences of strings (frequency counting).
Even though Unix scripting languages are Turing complete, they differ from general purpose languages.[clarification needed]
In practice, scripting languages are used to weave together small Unix tools such as grep, ls, sort or wc.
ColdFusion's associated scripting language is another example of a domain-specific language for data-driven websites.
This scripting language is used to weave together languages and services such as Java, .NET, C++, SMS, email, email servers, http, ftp, exchange, directory services, and file systems for use in websites.
The ColdFusion Markup Language (CFML) includes a set of tags that can be used in ColdFusion pages to interact with data
sources, manipulate data, and display output. CFML tag syntax is similar to HTML element syntax.
The Erlang Open Telecom Platform was originally designed for use inside Ericsson as a domain-specific language. The language itself offers a platform of libraries to create finite state machines, generic servers and event managers that quickly allow an engineer to deploy applications, or support libraries, that have been shown in industry benchmarks to outperform other languages intended for a mixed set of domains, such as C and C++. The language is now officially open source and can be downloaded from their website.
FilterMeister is a programming environment, with a programming language that is based on C, for the specific purpose of creating Photoshop-compatible image processing filter plug-ins; FilterMeister runs as a Photoshop plug-in itself and it can load and execute scripts or compile and export them as independent plug-ins.
Although the FilterMeister language reproduces a significant portion of the C language and function library, it contains only those features which can be used within the context of Photoshop plug-ins and adds a number of specific features only useful in this specific domain.
The Template feature of MediaWiki is an embedded domain-specific language whose fundamental purpose is to support the creation of page templates and the transclusion (inclusion by reference) of MediaWiki pages into other MediaWiki pages.
There has been much interest in domain-specific languages to improve the productivity and quality of software engineering.  Domain-specific language could possibly provide a robust set of tools for efficient software engineering. Such tools are beginning to make their way into development of critical software systems.
The Software Cost Reduction Toolkit[4] is an example of this. The toolkit is a suite of utilities including a specification editor to create a requirements specification, a dependency graph browser to display variable dependencies, a consistency checker to catch missing cases in well-formed formulas in the specification, a model checker and a theorem prover to check program properties against the specification, and an invariant generator that automatically constructs invariants based on the requirements.
A newer development is language-oriented programming, an integrated software engineering methodology based mainly on creating, optimizing, and using domain-specific languages.
Complementing language-oriented programming, as well as all other forms of domain-specific languages, are the class of compiler writing tools called metacompilers. A metacompiler is not only useful for generating parsers and code generators for domain-specific languages, but a metacompiler itself compiles a domain-specific metalanguage specifically designed for the domain of metaprogramming.
Besides parsing domain-specific languages, metacompilers are useful for generating a wide range of software engineering and analysis tools.  The meta-compiler methodology is often found in program transformation systems.
Metacompilers that played a significant role in both computer science and the computer industry include Meta-II[5] and its descendent TreeMeta.[6]
Unreal and Unreal Tournament unveiled a language called UnrealScript. This allowed for rapid development of modifications compared to the competitor Quake (using the Id Tech 2 engine). The Id Tech engine used standard C code meaning C had to be learned and properly applied, while UnrealScript was optimized for ease of use and efficiency. Similarly, the development of more recent games introduced their own specific languages, one more common example is Lua for scripting.
Various Business Rules Engines have been developed for automating policy and business rules used in both government and private industry.  ILOG, Oracle Policy Automation, DTRules, Drools and others provide support for DSLs aimed to support various problem domains.  DTRules goes so far as to define an interface for the use of multiple DSLs within a Rule Set.
The purpose of Business Rules Engines is to define a representation of business logic in as human readable fashion as possible.  This allows both subject matter experts and developers to work with and understand the same representation of the business logic.  Most Rules Engines provide both an approach to simplifying the control structures for business logic (for example, using Declarative Rules or Decision Tables) coupled with alternatives to programming syntax in favor of DSLs.
Statistical modellers have developed domain-specific languages such as
Bugs, Jags, and Stan. These languages provide a syntax for describing a Bayesian model, and generate a method for solving it using simulation.
Generate object handling and services based on a Interface Description Language for a domain-specific language such as JavaScript for web applications, HTML for documentation, C++ for high performance code, etc. This is done by cross language frameworks such as Apache Thrift or Google Protocol Buffers.
Gherkin is a language designed to define test cases to check the behaviour of software, without specifying how that behaviour is implemented. It is meant to be read and used by non-technical users using a natural language syntax and a line-oriented design. The tests defined with Gherkin must then be implemented in a general programming language. Then, the steps in a Gherkin program acts as a syntax for method invocation accessible to non-developers.
Other prominent examples of domain-specific languages include:
Some of the advantages:[1][2]
Some of the disadvantages:



Oracle Corporation - Wikipedia

Oracle Corporation is an American multinational computer technology corporation headquartered in Redwood Shores, California. The company specializes primarily in developing and marketing database software and technology, cloud engineered systems, and enterprise software products — particularly its own brands of database management systems. In 2014, Oracle was the second-largest software maker by revenue, after Microsoft.[4]
The company also develops and builds tools for database development and systems of middle-tier software, enterprise resource planning (ERP) software, customer relationship management (CRM) software, and supply chain management (SCM) software. 
Larry Ellison co-founded Oracle Corporation in 1977 with Bob Miner and Ed Oates under the name Software Development Laboratories (SDL).[5] Ellison took inspiration[6] from the 1970 paper written by Edgar F. Codd on relational database management systems (RDBMS) named "A Relational Model of Data for Large Shared Data Banks."[7] He heard about the IBM System R database from an article in the IBM Research Journal provided by Oates. Ellison wanted to make Oracle's product compatible with System R, but failed to do so as IBM kept the error codes for their DBMS a secret. SDL changed its name to Relational Software, Inc (RSI) in 1979,[8] then again to Oracle Systems Corporation in 1982,[9] to align itself more closely with its flagship product Oracle Database. At this stage Bob Miner served as the company's senior programmer. On March 12, 1986, the company had its initial public offering.[10] In 1995, Oracle Systems Corporation changed its name to Oracle Corporation,[11] officially named Oracle, but sometimes referred to as Oracle Corporation, the name of the holding company.[12] Part of Oracle Corporation's early success arose from using the C programming language to implement its products. This eased porting to different operating systems (most of which support C).
Oracle designs, manufactures, and sells both software and hardware products, as well as offering services that complement them (such as financing, training, consulting, and hosting services). Many of the products have been added to Oracle's portfolio through acquisitions.
Oracle's E-delivery service (Oracle Software Delivery Cloud) provides generic downloadable Oracle software and documentation.[13]
Oracle Corporation has acquired and developed the following additional database technologies:
Oracle Fusion Middleware is a family of middleware software products, including (for instance) application server, system integration, business process management (BPM), user interaction, content management, identity management and business intelligence (BI) products.
Oracle Secure Enterprise Search (SES), Oracle's enterprise-search offering, gives users the ability to search for content across multiple locations, including websites, XML files, file servers, content management systems, enterprise resource planning systems, customer relationship management systems, business intelligence systems, and databases.
Released in 2008, the Oracle Beehive collaboration software provides team workspaces (including wikis, team calendaring and file sharing), email, calendar, instant messaging, and conferencing on a single platform. Customers can use Beehive as licensed software or as software as a service ("SaaS").[17]
Oracle also sells a suite of business applications. The Oracle E-Business Suite includes software to perform various enterprise functions related to (for instance) financials, manufacturing, customer relationship management (CRM), enterprise resource planning (ERP) and human resource management.  The Oracle Retail Suite[18]
covers the retail-industry vertical, providing merchandise management, price management, invoice matching, allocations, store operations management, warehouse management, demand forecasting, merchandise financial planning, assortment planning and category management.[citation needed] Users can access these facilities through a browser interface over the Internet or via a corporate intranet.
Following a number of acquisitions beginning in 2003, especially in the area of applications, Oracle Corporation as of  2008[update] maintains a number of product lines:
Development of applications commonly takes place in Java (using Oracle JDeveloper) or through PL/SQL (using, for example, Oracle Forms and Oracle Reports/BIPublisher).[citation needed] Oracle Corporation has started[citation needed] a drive toward "wizard"-driven environments with a view to enabling non-programmers to produce simple data-driven applications.
Oracle Corporation works with "Oracle Certified Partners" to enhance its overall product marketing. The variety of applications from third-party vendors includes database applications for archiving, splitting and control, ERP and CRM systems, as well as more niche and focused products providing a range of commercial functions in areas like human resources, financial control and governance, risk management, and compliance (GRC). Vendors include Hewlett-Packard, Creoal Consulting, UC4 Software[20], Motus,[21] and Knoa Software.[22]
Oracle Enterprise Manager (OEM) provides web-based monitoring and management tools for Oracle products (and for some third-party software), including database management, middleware management, application management, hardware and virtualization management and cloud management.[23]
The Primavera products of Oracle's Primavera Global Business Unit (PGBU) consist of project-management software.[24]

Oracle Corporation's tools for developing applications include (among others):
Many external and third-party tools make the Oracle database administrator's tasks easier.[citation needed]
Oracle Corporation develops and supports two operating systems: Oracle Solaris and Oracle Linux.
Oracle Cloud is a cloud computing service offered by Oracle Corporation providing servers, storage, network, applications and services through a global network of Oracle Corporation managed data centers. The company allows these services to be provisioned on demand over the Internet.[30]
Oracle Cloud provides Infrastructure as a Service (IaaS), Platform as a Service (PaaS), Software as a Service (SaaS) and Data as a Service (DaaS). These services are used to build, deploy, integrate and extend applications in the cloud. This platform supports open standards (SQL, HTML5, REST, etc.) open-source solutions (Kubernetes, Hadoop, Kafka, etc.) and a variety of programming languages, databases, tools and frameworks including Oracle-specific, Open Source and third-party software and systems.[31]
On July 28, 2016 Oracle bought NetSuite, the very first cloud company, for $9.3 billion.[38] On May 16, 2018 Oracle announced that it had acquired DataScience.com, a privately held cloud workspace platform for data science projects and workloads.[39]
In 1990, Oracle laid off 10% (about 400 people) of its work force because of accounting errors.[51] This crisis came about because of Oracle's "up-front" marketing strategy, in which sales people urged potential customers to buy the largest possible amount of software all at once. The sales people then booked the value of future license sales in the current quarter, thereby increasing their bonuses.[52] This became a problem when the future sales subsequently failed to materialize. Oracle eventually had to restate its earnings twice, and also settled (out of court) class-action lawsuits arising from its having overstated its earnings. Ellison stated in 1992 that Oracle had made "an incredible business mistake."[51]
In 1994, Informix overtook Sybase and became Oracle's most important rival. The intense war between Informix CEO Phil White and Ellison made front-page news in Silicon Valley for three years. Informix claimed that Oracle had hired away Informix engineers to disclose important trade secrets about an upcoming product. Informix finally dropped its lawsuit against Oracle in 1997.[53] In November 2005, a book detailing the war between Oracle and Informix was published, titled The Real Story of Informix Software and Phil White. It gave a detailed chronology of the battle of Informix against Oracle, and how Informix Software's CEO Phil White landed in jail because of his obsession with overtaking Ellison.
Once it had overcome Informix and Sybase, Oracle Corporation enjoyed years of dominance in the database market until use of Microsoft SQL Server became widespread in the late 1990s and IBM acquired Informix Software in 2001 (to complement its DB2 database). Today[update] Oracle competes for new database licenses on UNIX, Linux, and Windows operating systems primarily against IBM's DB2 and Microsoft SQL Server. IBM's DB2 still[update] dominates the mainframe database market.
In 2004, Oracle's sales grew at a rate of 14.5% to $6.2 billion, giving it 41.3% and the top share of the relational-database market (InformationWeek – March 2005), with market share estimated at up to 44.6% in 2005 by some sources.[54]
Oracle Corporation's main competitors in the database arena remain IBM DB2 and Microsoft SQL Server, and to a lesser extent Sybase and Teradata[54], with open-source databases such as PostgreSQL and MySQL also having a significant[55] share of the market. EnterpriseDB, based on PostgreSQL, has recently[update] made inroads[56] by proclaiming that its product delivers Oracle compatibility features[clarification needed] at a much lower price-point.
In the software-applications market, Oracle Corporation primarily[citation needed] competes against SAP. On March 22, 2007 Oracle sued SAP, accusing them of fraud and unfair competition.[57][citation needed]
In the market for business intelligence software, many other software companies—small and large—have successfully competed in quality with Oracle and SAP products. Business intelligence vendors can be categorized into the "big four" consolidated BI firms such as Oracle, who has entered BI market through a recent trend of acquisitions (including Hyperion Solutions), and the independent "pure play" vendors such as MicroStrategy, Actuate, and SAS.[58]
Oracle Financials was ranked in the Top 20 Most Popular Accounting Software Infographic by Capterra in 2014, beating out SAP and a number of their other competitors.[59]
From 1988, Oracle Corporation and the German company SAP AG had a decade-long history of cooperation, beginning with the integration of SAP's R/3 enterprise application suite with Oracle's relational database products. Despite the SAP partnership with Microsoft, and the increasing integration of SAP applications with Microsoft products (such as Microsoft SQL Server, a competitor to Oracle Database), Oracle and SAP continue their cooperation. According to Oracle Corporation, the majority of SAP's customers use Oracle databases.[60]
In 2004, Oracle began to increase its interest in the enterprise-applications market (in 1989, Oracle had already released Oracle Financials). A series of acquisitions by Oracle Corporation began, most notably with those of PeopleSoft, Siebel Systems and Hyperion.
SAP recognized that Oracle had started to become a competitor in a market where SAP had the leadership, and saw an opportunity to lure in customers from those companies that Oracle Corporation had acquired. SAP would offer those customers special discounts on the licenses for its enterprise applications.
Oracle Corporation would resort to a similar strategy, by advising SAP customers to get "OFF SAP" (a play on the words of the acronym for its middleware platform "Oracle Fusion for SAP"),[61]
and also by providing special discounts on licenses and services to SAP customers who chose Oracle Corporation products.
Currently[update] Oracle and SAP (the latter through its recently acquired subsidiary TomorrowNow) compete in the third-party enterprise software maintenance and support market. On March 22, 2007, Oracle filed a lawsuit against SAP. In Oracle Corporation v. SAP AG Oracle alleged that TomorrowNow, which provides discount support for legacy Oracle product lines, used the accounts of former Oracle customers to systematically download patches and support documents from Oracle's website and to appropriate them for SAP's use.[62]
Some analysts have suggested the suit could form part of a strategy by Oracle Corporation to decrease competition with SAP in the market for third-party enterprise software maintenance and support.[63][citation needed]
On July 3, 2007, SAP admitted that TomorrowNow employees had made "inappropriate downloads" from the Oracle support website. However, it claims that SAP personnel and SAP customers had no access to Oracle intellectual property via TomorrowNow. SAP's CEO Henning Kagermann stated that "Even a single inappropriate download is unacceptable from my perspective. We regret very much that this occurred." Additionally, SAP announced that it had "instituted changes" in TomorrowNow's operational oversight.[64]
On November 23, 2010, a U.S. district court jury in Oakland, California found that SAP AG must pay Oracle Corp $1.3 billion for copyright infringement, awarding damages that could be the largest-ever for copyright infringement. While admitting liability, SAP estimated the damages at no more than $40 million, while Oracle claimed that they are at least $1.65 billion. The awarded amount is one of the 10 or 20 largest jury verdicts in U.S. legal history. SAP said they were disappointed by the verdict and might appeal.[65] On September 1, 2011, a federal judge overturned the judgment and offered a reduced amount or a new trial, calling Oracle's original award "grossly" excessive.[66] Oracle chose a new trial.
On August 3, 2012, SAP and Oracle agreed on a judgment for $306 million in damages, pending approval from the U.S. district court judge, “to save time and expense of [a] new trial". After the accord has been approved, Oracle can ask a federal appeals court to reinstate the earlier jury verdict. In addition to the damages payment, SAP has already paid Oracle $120 million for its legal fees.[67]
Oracle Corporation produces and distributes the "Oracle ClearView" series of videos as part of its marketing mix.[72]
In 2000, Oracle attracted attention from the computer industry and the press after hiring private investigators to dig through the trash of organizations involved in an antitrust trial involving Microsoft.[73] The Chairman of Oracle Corporation, Larry Ellison, staunchly defended his company's hiring of an East Coast detective agency to investigate groups that supported rival Microsoft Corporation during its antitrust trial, calling the snooping a "public service". The investigation reportedly included a $1,200 offer to janitors at the Association for Competitive Technology to look through Microsoft's trash. When asked how he would feel if others were looking into Oracle's business activities, Ellison said: "We will ship our garbage to Redmond, and they can go through it. We believe in full disclosure."[74]
In 2002, Oracle Corporation marketed many of its products using the slogan "Can't break it, can't break in", or "Unbreakable".[75] This signified a demand on information security. Oracle Corporation also stressed the reliability of networked databases and network access to databases as major selling points.
However, two weeks after its introduction, David Litchfield, Alexander Kornbrust, Cesar Cerrudo and others demonstrated a whole suite of successful attacks against Oracle products.[76][77] Oracle Corporation's chief security officer Mary Ann Davidson said that, rather than representing a literal claim of Oracle's products' impregnability, she saw the campaign in the context of fourteen independent security evaluations[78] that Oracle Corporation's database server had passed.
In 2004, then-United States Attorney General John Ashcroft sued Oracle Corporation to prevent it from acquiring a multibillion-dollar intelligence contract. After Ashcroft's resignation from government, he founded a lobbying firm, The Ashcroft Group, which Oracle hired in 2005. With the group's help, Oracle went on to acquire the contract.[79]
Computer Sciences Corporation reportedly spent a billion dollars developing a computer system for the United States Air Force that yielded no significant capability, because, according to an Air Force source, the Oracle software on which the system was based could not be adapted to meet the specialized performance criteria.[80]
Oracle Corporation was awarded a contract by the State of Oregon's Oregon Health Authority (OHA) to develop Cover Oregon, the state's healthcare exchange website, as part of the U.S. Patient Protection and Affordable Care Act. When the site tried to go live on October 1, 2013, it failed, and registrations had to be taken using paper applications until the site could be fixed.
On April 25, 2014, the State of Oregon voted to discontinue Cover Oregon and instead use the federal exchange to enroll Oregon residents.[81] The cost of switching to the federal portal was estimated at $5 million, whereas fixing Cover Oregon would have required another $78 million.
Oracle president Safra Catz responded to Cover Oregon and the OHA in a letter claiming that the site's problems were due to OHA mismanagement, specifically that a third-party systems integrator was not hired to manage the complex project.[82][83]
In August 2014, Oracle Corporation sued Cover Oregon for breach of contract,[84] and then later that month the state of Oregon sued Oracle Corporation, in a civil complaint for breach of contract, fraud, filing false claims and "racketeering".[85] In September 2016, the two sides reached a settlement valued at over $100 million to the state, and a six-year agreement for Oracle to continue modernizing state software and IT.[86]
On January 27, 2010, Oracle announced it had completed its acquisition of Sun Microsystems—valued at more than $7 billion—a move that transformed Oracle from solely a software company to a manufacturer of both software and hardware. The acquisition was delayed for several months by the EU Commission because of concerns about MySQL, but was unconditionally approved in the end.[87] This acquisition was important to some in the open source community and also to some other companies, as they feared Oracle might end Sun's traditional support of open source projects.[88][89][90][91] Since the acquisition, Oracle has discontinued OpenSolaris and StarOffice, and sued Google over their newly acquired Java patents from Sun.[92][93] In September 2011, U.S. State Department Embassy cables were leaked[94] to WikiLeaks.  One cable revealed that the U.S. pressured the E.U. to allow Oracle to acquire Sun.[95]
On July 29, 2010, the United States Department of Justice filed suit against Oracle Corporation alleging fraud. The lawsuit argues that the government received deals inferior to those Oracle gave to its commercial clients. The DoJ added its heft to an already existing whistleblower lawsuit filed by Paul Frascella, who was once senior director of contract services at Oracle.[96] It was settled in May 2012[97]
Oracle, the plaintiff, bought the Java computer programing language when it acquired Sun Microsystems in January 2010.[98] The Java software includes sets of pre-developed software code in order to accomplish common tasks consistently among programs and apps. The pre-developed code is organized into separate "packages" which each contained a set of methods. The packages are further organized into larger "classes."  Each method instructs a program or app to do a certain task. Software developers "became accustomed to using Java’s designations at the package, class, and method level."[99]
Oracle and Google (the defendant) tried to negotiate an agreement for Oracle to license Java to Google, which would have allowed Google to use Java in developing programs for mobile devices using the Android operating system. However, the two companies never reached an agreement. After negotiations failed, Google created its own programming platform, which was based on Java, and contained a mix of 37 copied Java packages and new packages developed by Google.[99]
In 2010, Oracle sued Google for copyright infringement for the use of the 37 Java packages.[99][98] The case was handled in U.S. District Court for the Northern District of California and assigned to Judge William H. Alsup (who taught himself how to code computers[100]).[98] In the lawsuit, Oracle sought between $1.4 billion and $6.1 billion.[98] In June 2011 the judge had to force Google through a judicial order to make public the details about Oracle's claim for damages.[98]

By the end of the first jury trial (the legal dispute would eventually go on to another trial) the arguments made by Oracle's attorneys focused on a Java function called "rangeCheck.""The argument centered on a function called rangeCheck. Of all the lines of code that Oracle had tested—15 million in total—these were the only ones that were 'literally' copied. Every keystroke, a perfect duplicate." – The Verge, 10/19/17[100]Although Google admitted to copying the packages, Judge Alsup found that none of the Java packages were covered under copyright protection, and therefore Google did not infringe.[99]
After the case was over, Oracle appealed to the United States Court of Appeals for the Federal Circuit (750 F.3d 1339 (2014)).[99][101] On May 9, 2014, the appeals court partially reversed Judge Alsup's decision, finding that Java APIs are copyrightable. API stands for "application programming interface" and are how different computer programs or apps communicate with each other. However, the appeals court also left open the possibility that Google might have a "fair use" defense.[101]
On October 6, 2014, Google filed a petition to appeal to the U.S. Supreme Court, but the Supreme Court denied the petition.[101]
The case was then returned to the U.S. District Court for another trial about Google's fair use defense.[101] Oracle sought $9 billion in damages.[102] In May 2016, the trial jury found that Google's use of Java's APIs was considered fair use.[101]
In February 2017, Oracle filed another appeal to the U.S. Court of Appeals for the Federal Circuit.[101] This time it was asking for a new trial because the District Court "repeatedly undermined Oracle's case", which Oracle argued led the jury to make the wrong decision. According to ZDNet, "For example, it [Oracle] says the court wrongly bought Google's claim that Android was limited to smartphones while Java was for PCs, whereas Oracle contends that Java and Android both compete as platforms for smart TVs, cars, and wearables."[102]
On August 13, 2010, an internal Oracle memo leaked to the Internet cited plans for ending the OpenSolaris operating system project and community.[103] With Oracle planning to develop Solaris only in a closed source fashion, OpenSolaris developers moved to the Illumos and OpenIndiana project, among others.[104]
As Oracle completed their acquisition of Sun Microsystems in February 2010, they announced that OpenSSO would no longer be their strategic product.[105] Shortly after, OpenSSO was forked to OpenAM.[105] and will continue to be developed and supported by ForgeRock.
On September 6, 2010, Oracle announced that former Hewlett-Packard CEO Mark Hurd was to replace Charles Phillips, who resigned as Oracle Co-President. In an official statement made by Larry Ellison, Phillips had previously expressed his desire to transition out of the company. Ellison had asked Phillips to stay on through the integration of Sun Microsystems Inc.[106] In a separate statement regarding the transition, Ellison said "Mark did a brilliant job at HP and I expect he'll do even better at Oracle. There is no executive in the IT world with more relevant experience than Mark."[107]
On September 7, 2010, HP announced a civil lawsuit against Mark Hurd "to protect HP's trade secrets",[108] in response to Oracle hiring Hurd. On September 20, Oracle and HP published a joint press release announcing the resolution of the lawsuit on confidential terms and reaffirming commitment to long-term strategic partnership between the companies.[109]
A number of OpenOffice.org developers formed The Document Foundation and received backing by Google, Novell, Red Hat, and Canonical, as well as some others, but were unable to get Oracle to donate the brand OpenOffice.org, causing a fork in the development of OpenOffice.org with the foundation now developing and promoting LibreOffice. Oracle expressed no interest in sponsoring the new project and asked the OpenOffice.org developers that started the project to resign from the company due to "conflicts of interest." On November 1, 2010, 33 of the OpenOffice.org developers gave their letters of resignation.[110] On June 1, 2011, Oracle donated OpenOffice.org to the Apache Software Foundation.[111]
On June 15, 2011, HP filed a lawsuit in California Superior Court in Santa Clara, claiming that Oracle had breached an agreement to support the Itanium microprocessor used in HP's high-end enterprise servers.[112] Oracle called the lawsuit "an abuse of the judicial process"[113] and said that had it known SAP's Leo Apotheker was about to be hired as HP's new CEO, any support for HP's Itanium servers would not have been implied.[114]
On August 1, 2012, a California judge said in a tentative ruling that Oracle must continue porting its software at no cost until HP discontinues its sales of Itanium-based servers.[115][116] HP was awarded $3 billion in damages against Oracle in 2016.[117] HP argued Oracle's canceling support damaged HP's Itanium server brand. Oracle has announced it will appeal both the decision and damages.
On August 31, 2011, The Wall Street Journal reported that Oracle was being investigated by the Federal Bureau of Investigation for paying bribes to government officials in order to win business in Africa, in contravention of the Foreign Corrupt Practices Act (FCPA).[118]
On April 20, 2012 the US General Services Administration banned Oracle from the most popular portal for bidding on GSA contracts for undisclosed reasons. Oracle has previously used this portal for around four hundred million dollars a year in revenue.[119] Oracle previously settled a lawsuit filed under the False Claims Act, which accused the company of overbilling the US government between 1998 and 2006. The 2011 settlement forced Oracle to pay $199.5 million to the General Services Administration.[120]
Oracle Corporation has its overall headquarters on the San Francisco Peninsula in the Redwood Shores area of Redwood City, adjacent to Belmont and near San Carlos Airport (IATA airport code: SQL).
Oracle HQ stands on the former site of Marine World Africa USA, which moved from Redwood Shores to Vallejo in 1986. Oracle Corporation originally leased two buildings on the site, moving its finance and administration departments from the corporation's former headquarters on Davis Drive, Belmont, California. Eventually, Oracle purchased the complex and constructed a further four main buildings.
The distinctive Oracle Parkway buildings, nicknamed the Emerald City,[125] served as sets for the futuristic headquarters of the fictional company "NorthAm Robotics" in the Robin Williams film Bicentennial Man (1999).[126]
The campus represented the headquarters of Cyberdyne Systems in the movie Terminator Genisys (2015).[127]
300 Oracle Parkway at Oracle Corporation headquarters in Redwood Shores, California.
Oracle Aoyama Center Building, with Lexus International Gallery Aoyama
Oracle HQ, with Oracle Plaza building in left foreground
The Oracle Conference Center at Oracle Corporation headquarters in Redwood Shores, California
Fountain in the Oracle lake, Redwood Shores
Oracle Corporation has a major business campus at Thames Valley Park in Reading in England
Oracle in Markham, Ontario
Oracle Corporation operates in multiple markets and has acquired several companies which formerly functioned autonomously. In some cases these provided the starting points for global business units (GBUs) targeting particular vertical markets.[128] Oracle Corporation GBUs include:
On October 20, 2006, the Golden State Warriors and the Oracle Corporation announced a 10-year agreement in which the Oakland Arena would become known as the Oracle Arena.[129]
Larry Ellison's sailing team competes as Oracle Team USA. The team has won the America's Cup twice, in 2010 (as BMW Oracle Racing)[130] and in 2013.[131]
Sean Tucker's "Challenger II" stunt biplane performs frequently at air shows around the US.[132]
Coordinates: 37°31′46″N 122°15′57″W﻿ / ﻿37.5294°N 122.265966°W﻿ / 37.5294; -122.265966



MATLAB - Wikipedia

MATLAB (matrix laboratory) is a multi-paradigm numerical computing environment and proprietary programming language developed by MathWorks. MATLAB allows matrix manipulations, plotting of functions and data, implementation of algorithms, creation of user interfaces, and interfacing with programs written in other languages, including C, C++, C#, Java, Fortran and Python.
Although MATLAB is intended primarily for numerical computing, an optional toolbox uses the MuPAD symbolic engine, allowing access to symbolic computing abilities. An additional package, Simulink, adds graphical multi-domain simulation and model-based design for dynamic and embedded systems.
As of 2018, MATLAB has more than 3 million users worldwide.[7] MATLAB users come from various backgrounds of engineering, science, and economics.
Cleve Moler, the chairman of the computer science department at the University of New Mexico, started developing MATLAB in the late 1970s.[8] He designed it to give his students access to LINPACK and EISPACK without them having to learn Fortran. It soon spread to other universities and found a strong audience within the applied mathematics community. Jack Little, an engineer, was exposed to it during a visit Moler made to Stanford University in 1983. Recognizing its commercial potential, he joined with Moler and Steve Bangert. They rewrote MATLAB in C and founded MathWorks in 1984 to continue its development. These rewritten libraries were known as JACKPAC.[9] In 2000, MATLAB was rewritten to use a newer set of libraries for matrix manipulation, LAPACK.[10]
MATLAB was first adopted by researchers and practitioners in control engineering, Little's specialty, but quickly spread to many other domains. It is now also used in education, in particular the teaching of linear algebra, numerical analysis, and is popular amongst scientists involved in image processing.[8]
The MATLAB application is built around the MATLAB scripting language. Common usage of the MATLAB application involves using the Command Window as an interactive mathematical shell or executing text files containing MATLAB code.[11]
Variables are defined using the assignment operator, =. MATLAB is a weakly typed programming language because types are implicitly converted.[12]  It is an inferred typed language because variables can be assigned without declaring their type, except if they are to be treated as symbolic objects,[13] and that their type can change. Values can come from constants, from computation involving values of other variables, or from the output of a function. For example:
A simple array is defined using the colon syntax: initial:increment:terminator. For instance:
defines a variable named array (or assigns a new value to an existing variable with the name array) which is an array consisting of the values 1, 3, 5, 7, and 9. That is, the array starts at 1 (the initial value), increments with each step from the previous value by 2 (the increment value), and stops once it reaches (or to avoid exceeding) 9 (the terminator value).
the increment value can actually be left out of this syntax (along with one of the colons), to use a default value of 1.
assigns to the variable named ari an array with the values 1, 2, 3, 4, and 5, since the default value of 1 is used as the incrementer.
Indexing is one-based,[14] which is the usual convention for matrices in mathematics, although not for some programming languages such as C, C++, and Java.
Matrices can be defined by separating the elements of a row with blank space or comma and using a semicolon to terminate each row. The list of elements should be surrounded by square brackets: []. Parentheses: () are used to access elements and subarrays (they are also used to denote a function argument list).
Sets of indices can be specified by expressions such as "2:4", which evaluates to [2, 3, 4].  For example, a submatrix taken from rows 2 through 4 and columns 3 through 4 can be written as:
A square identity matrix of size n can be generated using the function eye, and matrices of any size with zeros or ones can be generated with the functions zeros and ones, respectively.
Transposing a vector or a matrix is done either by the function transpose or by adding prime after a dot to the matrix. Without the dot MATLAB will perform conjugate transpose.
Most MATLAB functions can accept matrices and will apply themselves to each element. For example, mod(2*J,n) will multiply every element in "J" by 2, and then reduce each element modulo "n". MATLAB does include standard "for" and "while" loops, but (as in other similar applications such as R), using the vectorized notation often produces code that is faster to execute. This code, excerpted from the function magic.m, creates a magic square M for odd values of n (MATLAB function meshgrid is used here to generate square matrices I and J containing 1:n).
MATLAB has structure data types.[15] Since all variables in MATLAB are arrays, a more adequate name is "structure array", where each element of the array has the same field names. In addition, MATLAB supports dynamic field names[16] (field look-ups by name, field manipulations, etc.). Unfortunately, MATLAB JIT does not support MATLAB structures, therefore just a simple bundling of various variables into a structure will come at a cost.[17]
When creating a MATLAB function, the name of the file should match the name of the first function in the file. Valid function names begin with an alphabetic character, and can contain letters, numbers, or underscores.  Functions are often case sensitive.
MATLAB supports elements of lambda calculus by introducing function handles,[18] or function references, which are implemented either in .m files or anonymous[19]/nested functions.[20]
MATLAB supports object-oriented programming including classes, inheritance, virtual dispatch, packages, pass-by-value semantics, and pass-by-reference semantics.[21] However, the syntax and calling conventions are significantly different from other languages. MATLAB has value classes and reference classes, depending on whether the class has handle as a super-class (for reference classes) or not (for value classes).[22]
Method call behavior is different between value and reference classes. For example, a call to a method
can alter any member of object only if object is an instance of a reference class.
An example of a simple class is provided below.
When put into a file named hello.m, this can be executed with the following commands:
MATLAB supports developing applications with graphical user interface (GUI) features. MATLAB includes GUIDE[23] (GUI development environment) for graphically designing GUIs.[24] It also has tightly integrated graph-plotting features. For example, the function plot can be used to produce a graph from two vectors x and y. The code:
produces the following figure of the sine function:

A MATLAB program can produce three-dimensional graphics using the functions surf, plot3 or mesh.
In MATLAB, graphical user interfaces can be programmed with the GUI design environment (GUIDE) tool.[25]
MATLAB can call functions and subroutines written in the programming languages C or Fortran.[26] A wrapper function is created allowing MATLAB data types to be passed and returned. MEX files (MATLAB executables) are the dynamically loadable object files created by compiling such functions.[27][28] Since 2014 increasing two-way interfacing with Python was being added.[29][30]
Libraries written in Perl, Java, ActiveX or .NET can be directly called from MATLAB,[31][32] and many MATLAB libraries (for example XML or SQL support) are implemented as wrappers around Java or ActiveX libraries. Calling MATLAB from Java is more complicated, but can be done with a MATLAB toolbox[33] which is sold separately by MathWorks, or using an undocumented mechanism called JMI (Java-to-MATLAB  Interface),[34][35] (which should not be confused with the unrelated Java Metadata Interface that is also called JMI). Official MATLAB API for Java was added in 2016.[36]
As alternatives to the MuPAD based Symbolic Math Toolbox available from MathWorks, MATLAB can be connected to Maple or Mathematica.[37][38]
Libraries also exist to import and export MathML.[39]
MATLAB is a proprietary product of MathWorks, so users are subject to vendor lock-in.[40][41]  Although MATLAB Builder products can deploy MATLAB functions as library files which can be used with .NET[42] or Java[43] application building environment, future development will still be tied to the MATLAB language.
Each toolbox is purchased separately. If an evaluation license is requested, the MathWorks sales department requires detailed information about the project for which MATLAB is to be evaluated. If granted (which it often is), the evaluation license is valid for two to four weeks. A student version of MATLAB is available as is a home-use license for MATLAB, Simulink, and a subset of Mathwork's Toolboxes at substantially reduced prices.
It has been reported that European Union (EU) competition regulators are investigating whether MathWorks refused to sell licenses to a competitor.[44]  The regulators dropped the investigation after the complainant withdrew its accusation and no evidence of wrongdoing was found.[45]
MATLAB has a number of competitors.[46] Commercial competitors include Mathematica, TK Solver, Maple, and IDL. There are also free open source alternatives to MATLAB, in particular GNU Octave, Scilab, FreeMat, and SageMath, which are intended to be mostly compatible with the MATLAB language; the Julia programming language also initially used MATLAB-like syntax. Among other languages that treat arrays as basic entities (array programming languages) are APL, Fortran 90 and higher, S-Lang, as well as the statistical languages R and S. There are also libraries to add similar functionality to existing languages, such as IT++ for C++, Perl Data Language for Perl, ILNumerics for .NET, NumPy/SciPy/matplotlib for Python, SciLua/Torch for Lua, SciRuby for Ruby, and Numeric.js for JavaScript.
GNU Octave is unique from other alternatives because it treats incompatibility with MATLAB as a bug (see MATLAB Compatibility of GNU Octave), therefore, making GNU Octave a superset of the MATLAB language.
Re-introduced for Mac (under Mac OS X)
The number (or release number) is the version reported by Concurrent License Manager program FLEXlm.
For a complete list of changes of both MATLAB and official toolboxes, consult the MATLAB release notes.[97]
Several easter eggs exist in MATLAB.[111] These include hidden pictures,[112] and jokes. For example, typing in "spy" used to generate a picture of the spies from Spy vs Spy, but now displays an image of a dog. Typing in "why" randomly outputs a philosophical answer. Other commands include "penny", "toilet", "image", and "life".  Not every Easter egg appears in every version of MATLAB.




Pseudocode - Wikipedia
Pseudocode is an informal high-level description of the operating principle of a computer program or other algorithm.
It uses the structural conventions of a normal programming language, but is intended for human reading rather than machine reading. Pseudocode typically omits details that are essential for machine understanding of the algorithm, such as variable declarations, system-specific code and some subroutines. The programming language is augmented with natural language description details, where convenient, or with compact mathematical notation. The purpose of using pseudocode is that it is easier for people to understand than conventional programming language code, and that it  is an efficient and environment-independent description of the key principles of an algorithm. It is commonly used in textbooks and scientific publications that are documenting various algorithms, and also in planning of computer program development, for sketching out the structure of the program before the actual coding takes place.
No standard for pseudocode syntax exists, as a program in pseudocode is not an executable program. Pseudocode resembles, but should not be confused with, skeleton programs which can be compiled without errors. Flowcharts, drakon-charts and Unified Modeling Language (UML) charts can be thought of as a graphical alternative to pseudocode, but are more spacious on paper. Languages such as HAGGIS bridge the gap between pseudocode and code written in programming languages. Its main use is to introduce students to high level languages through use of this hybrid language.
Textbooks and scientific publications related to computer science and numerical computation often use pseudocode in description of algorithms, so that all programmers can understand them, even if they do not all know the same programming languages. In textbooks, there is usually an accompanying introduction explaining the particular conventions in use. The level of detail of the pseudocode may in some cases approach that of formalized general-purpose languages.
A programmer who needs to implement a specific algorithm, especially an unfamiliar one, will often start with a pseudocode description, and then  "translate" that description into the target programming language and modify it to interact correctly with the rest of the program. Programmers may also start a project by sketching out the code in pseudocode on paper before writing it in its actual language, as a top-down structuring approach, with a process of steps to be followed as a refinement.
As the name suggests, pseudocode generally does not actually obey the syntax rules of any particular language; there is no systematic standard form, although any particular writer will generally borrow style and syntax; for example, control structures from some conventional programming language. Popular syntax sources include Fortran, Pascal, BASIC, C, C++, Java, Lisp, and ALGOL. Variable declarations are typically omitted. Function calls and blocks of code, such as code contained within a loop, are often replaced by a one-line natural language sentence.
Depending on the writer, pseudocode may therefore vary widely in style, from a near-exact imitation of a real programming language at one extreme, to a description approaching formatted prose at the other.
This is an example of pseudocode (for the mathematical game fizz buzz):
Fortran style pseudo code
Pascal style pseudo code
C style pseudo code:
Structured Basic style pseudo code
In numerical computation, pseudocode often consists of mathematical notation, typically from set and matrix theory, mixed with the control structures of a conventional programming language, and perhaps also natural language descriptions. This is a compact and often informal notation that can be understood by a wide range of mathematically trained people, and is frequently used as a way to describe mathematical algorithms. For example, the sum operator (capital-sigma notation) or the product operator (capital-pi notation) may represent a for-loop and a selection structure in one expression:
Normally non-ASCII typesetting is used for the mathematical equations, for example by means of markup languages, such as TeX or MathML, or proprietary formula editors.
Mathematical style pseudocode is sometimes referred to as pidgin code, for example pidgin ALGOL (the origin of the concept), pidgin Fortran, pidgin BASIC, pidgin Pascal, pidgin C, and pidgin Lisp.
Here follows a longer example of mathematical-style pseudocode, for the Ford–Fulkerson algorithm:
Various attempts to bring elements of natural language grammar into computer programming have produced programming languages such as HyperTalk, Lingo, AppleScript, SQL, Inform and to some extent Python. In these languages, parentheses and other special characters are replaced by prepositions, resulting in quite talkative code. These languages are typically dynamically typed, meaning that variable declarations and other boilerplate code can be omitted. Such languages may make it easier for a person without knowledge about the language to understand the code and perhaps also to learn the language. However, the similarity to natural language is usually more cosmetic than genuine. The syntax rules may be just as strict and formal as in conventional programming, and do not necessarily make development of the programs easier.
An alternative to using mathematical pseudocode (involving set theory notation or matrix operations) for documentation of algorithms is to use a formal mathematical programming language that is a mix of non-ASCII mathematical notation and program control structures. Then the code can be parsed and interpreted by a machine.
Several formal specification languages include set theory notation using special characters. Examples are:
Some array programming languages include vectorized expressions and matrix operations as non-ASCII formulas, mixed with conventional control structures. Examples are:



Programmer - Wikipedia

A programmer, developer, dev, coder, or software engineer is a person who creates computer software. The term computer programmer can refer to a specialist in one area of computers or to a generalist who writes code for many kinds of software. One who practices or professes a formal approach to programming may also be known as a programmer analyst.
A programmer's primary computer language (Assembly, COBOL, C, C++, C#, Java, Lisp, Python, etc.) is often prefixed to these titles, and those who work in a web environment often prefix their titles with web.
A range of occupations, including: software developer, web developer, mobile applications developer, embedded firmware developer, software engineer, computer scientist, or software analyst, while they do involve programming, also require a range of other skills. The use of the simple term programmer for these positions is sometimes considered an insulting or derogatory simplification.[1][2][3][4][5]
British countess and mathematician Ada Lovelace is often considered the first computer programmer, as she was the first to publish an algorithm intended for implementation on Charles Babbage's analytical engine, in October 1842, intended for the calculation of Bernoulli numbers.[7]  Because Babbage's machine was never completed to a functioning standard in her time, she never saw this algorithm run.
The first person to run a program on a functioning modern electronically based computer was computer scientist Konrad Zuse, in 1941.
The ENIAC programming team, consisting of Kay McNulty, Betty Jennings, Betty Snyder, Marlyn Wescoff, Fran Bilas and Ruth Lichterman were the first regularly working programmers.[8][9]
International Programmers' Day is celebrated annually on 7 January.[10]  In 2009, the government of Russia decreed a professional annual holiday known as Programmers' Day to be celebrated on 13 September (12 September in leap years). It had also been an unofficial international holiday before that.
The word "software" was first used as early as 1953, but did not appear in print until the 1960s.[11] Before this time, computers were programmed either by customers, or the few commercial computer vendors of the time, such as UNIVAC and IBM. The first company founded to provide software products and services was Computer Usage Company in 1955.[12]
The software industry expanded in the early 1960s, almost immediately after computers were first sold in mass-produced quantities. Universities, government, and business customers created a demand for software. Many of these programs were written in-house by full-time staff programmers. Some were distributed freely between users of a particular machine for no charge. Others were done on a commercial basis, and other firms such as Computer Sciences Corporation (founded in 1959) started to grow. The computer/hardware makers started bundling operating systems, system software and programming environments with their machines.[citation needed]
The industry expanded greatly with the rise of the personal computer ("PC") in the mid-1970s, which brought computing to the desktop of the office worker. In the following years, it also created a growing market for games, applications, and utilities. DOS, Microsoft's first operating system product, was the dominant operating system at the time.[13]
In the early years of the 21st century, another successful business model has arisen for hosted software, called software-as-a-service, or SaaS; this was at least the third time[citation needed] this model had been attempted. From the point of view of producers of some proprietary software, SaaS reduces the concerns about unauthorized copying, since it can only be accessed through the Web, and by definition, no client software is loaded onto the end user's PC.[citation needed]  By 2014, the role of cloud developer had been defined; in this context, one definition of a "developer" in general was published:[14]
Computer programmers write, test, debug, and maintain the detailed instructions, called computer programs, that computers must follow to perform their functions. Programmers also conceive, design, and test logical structures for solving problems by computer. Many technical innovations in programming — advanced computing technologies and sophisticated new languages and programming tools — have redefined the role of a programmer and elevated much of the programming work done today. Job titles and descriptions may vary, depending on the organization.
Programmers work in many settings, including corporate information technology ("IT") departments, big software companies, small service firms and government entities of all sizes. Many professional programmers also work for consulting companies at client sites as contractors. Licensing is not typically required to work as a programmer, although professional certifications are commonly held by programmers. Programming is widely considered a profession (although some[who?] authorities disagree on the grounds that only careers with legal licensing requirements count as a profession).
Programmers' work varies widely depending on the type of business for which they are writing programs. For example, the instructions involved in updating financial records are very different from those required to duplicate conditions on an aircraft for pilots training in a flight simulator. Simple programs can be written in a few hours, more complex ones may require more than a year of work, while others are never considered 'complete' but rather are continuously improved as long as they stay in use. In most cases, several programmers work together as a team under a senior programmer’s supervision.
Programmers write programs according to the specifications determined primarily by more senior programmers and by systems analysts. After the design process is complete, it is the job of the programmer to convert that design into a logical series of instructions that the computer can follow. The programmer codes these instructions in one of many programming languages. Different programming languages are used depending on the purpose of the program. COBOL, for example, is commonly used for business applications that typically run on mainframe and midrange computers, whereas Fortran is used in science and engineering. C++ is widely used for both scientific and business applications. Java, C#, VB  and PHP are popular programming languages for Web and business applications. Programmers generally know more than one programming language and, because many languages are similar, they often can learn new languages relatively easily. In practice, programmers often are referred to by the language they know, e.g. as Java programmers, or by the type of function they perform or environment in which they work: for example, database programmers, mainframe programmers, or Web developers.
When making changes to the source code that programs are made up of, programmers need to make other programmers aware of the task that the routine is to perform. They do this by inserting comments in the source code so that others can understand the program more easily and by documenting their code. To save work, programmers often use libraries of basic code that can be modified or customized for a specific application. This approach yields more reliable and consistent programs and increases programmers' productivity by eliminating some routine steps.
Programmers test a program by running it and looking for bugs (errors). As they are identified, the programmer usually makes the appropriate corrections, then rechecks the program until an acceptably low level and severity of bugs remain. This process is called testing and debugging. These are important parts of every programmer's job. Programmers may continue to fix these problems throughout the life of a program. Updating, repairing, modifying, and expanding existing programs is sometimes called maintenance programming. Programmers may contribute to user guides and online help, or they may work with technical writers to do such work.
Computer programmers often are grouped into two broad types: application programmers and systems programmers. Application programmers write programs to handle a specific job, such as a program to track inventory within an organization. They also may revise existing packaged software or customize generic applications which are frequently purchased from independent software vendors. Systems programmers, in contrast, write programs to maintain and control computer systems software, such as operating systems and database management systems. These workers make changes in the instructions that determine how the network, workstations, and CPU of the system handle the various jobs they have been given and how they communicate with peripheral equipment such as printers and disk drives.
A software developer needs to have deep technical expertise with certain aspects of computing. Some positions will require a degree in a relevant field such as computer science, information technology, engineering, programming, or any other IT related post graduate studies.[15] An ideal software developer is a self-motivated professional carrying a dynamic hands-on experience on key languages of programming such as C++, C#, PHP, Java, C, Javascript, VB, Oracle, UML, Linux, Python, UNIX, XML, HTTP, Smalltalk, or other software testing tools.
According to developer Eric Sink, the differences between system design, software development, and programming are more apparent. Already in the current market place there can be found a segregation between programmers and developers, in that one who implements is not the same as the one who designs the class structure or hierarchy. Even more so that developers become software architects or systems architects, those who design the multi-leveled architecture or component interactions of a large software system.[16]
Programmers in software development companies may work directly with experts from various fields to create software – either programs designed for specific clients or packaged software for general use – ranging from video games to educational software to programs for desktop publishing and financial planning. Programming of packaged software constitutes one of the most rapidly growing segments of the computer services industry. Some companies or organizations – even small ones – have set up their own IT team to ensure the design and development of in-house software to answer to very specific needs from their internal end-users, especially when existing software are not suitable or too expensive. This is for example the case in research laboratories.[citation needed]
In some organizations, particularly small ones, people commonly known as programmer analysts are responsible for both the systems analysis and the actual programming work. The transition from a mainframe environment to one that is based primarily on personal computers (PCs) has blurred the once rigid distinction between the programmer and the user. Increasingly, adept end users are taking over many of the tasks previously performed by programmers. For example, the growing use of packaged software, such as spreadsheet and database management software packages, allows users to write simple programs to access data and perform calculations.[citation needed]
In addition, the rise of the Internet has made web development a huge part of the programming field. Currently more software applications are web applications that can be used by anyone with a web browser.[citation needed] Examples of such applications include the Google search service, the Outlook.com e-mail service, and the Flickr photo-sharing service.
Programming editors, also known as source code editors, are text editors that are specifically designed for programmers or developers for writing the source code of an application or a program. Most of these editors include features useful for programmers, which may include color syntax highlighting, auto indentation, auto-complete, bracket matching, syntax check, and allows plug-ins. These features aid the users during coding, debugging and testing.[17]
According to BBC News, 17% of computer science students could not find work in their field 6 months after graduation in 2009 which was the highest rate of the university subjects surveyed while 0% of medical students were unemployed in the same survey.[18] The UK category system does, however, class such degrees as information technology and game design as 'computer science', industries in which jobs can be extremely difficult to find, somewhat inflating the actual figure.[19]
Computer programming, offshore outsourcing, and Foreign Worker Visas became a controversial topic after the crash of the dot-com bubble left many programmers without work or with lower wages. Programming was even mentioned in the 2004 US Presidential debate on the topic of offshore outsourcing.[20]
Large companies claim there is a skills shortage with regard to programming talent. However, US programmers and unions counter that large companies are exaggerating their case in order to obtain cheaper programmers from developing countries and avoid previously employer paid training using industry specific technologies not covered in most accredited degree programs.[21] Other reasons for employers claiming skill shortages is the result of their own cost saving combining of several disparate skill sets previously held by several specialized programmers into fewer generalized multifaceted  positions that are unlikely to have enough "qualified" candidates with the desired experience.[22]
Enrollment in computer-related degrees in US has dropped recently[when?] due to lack of general interests in science and mathematics and also out of an apparent fear that programming will be subject to the same pressures as manufacturing and agriculture careers.[23] This situation has resulted in confusion about whether the US economy is entering a "post-information age" and the nature of US comparative advantages. Most academic institutions have an Institutional research office that keep past statistics of degrees conferred which show several dips and rises in Computer Science degrees over the past 30 years. The overall trend shows a slightly overall decline in growth (especially when compared to other STEM degree growth) since certain peaks of 1986, 1992, 2002, and 2008 showing periods of flat growth or even declines.[24] In addition the U.S. Bureau of Labor Statistics Occupational Outlook 2016-26 is -7% (a decline in their words) for Computer Programmers because Computer programming can be done from anywhere in the world, so companies sometimes hire programmers in countries where wages are lower.[25]
.



Execution (computing) - Wikipedia
Execution in computer and software engineering is the process by which a computer or  virtual machine executes the instructions of a computer program. Each instruction of a program is a description of a specific action to be carried out in order for a specific problem to be solved; as instructions of a program and therefore the actions they describe are being carried out by an executing machine, specific effects are produced in accordance to the semantics of the instructions being executed. 
Programs for a computer may be executed in a batch process without human interaction or a user may type commands in an interactive session of an interpreter. In this case, the "commands" are simply program instructions, whose execution is chained together. 
The term run is used almost synonymously. A related meaning of both "to run" and "to execute" refers to the specific action of a user starting (or launching or invoking) a program, as in "Please run the application."
The context in which execution takes place is crucial. Very few programs execute on a bare machine. Programs usually contain implicit and explicit assumptions about resources available at the time of execution. Most programs execute with the support of an operating system and run-time libraries specific to the source language that provide crucial services not supplied directly by the computer itself. This supportive environment, for instance, usually decouples a program from direct manipulation of the computer peripherals, providing more general, abstract services instead.
Prior to execution, a program must first be written. This is generally done in source code, which is then compiled at compile time (and statically linked at link time) to an executable. This executable is then invoked, most often by an operating system, which loads the program into memory (load time), possibly performs dynamic linking, and then begins execution by moving control to the entry point of the program; all these steps depend on the Application Binary Interface of the operating system. At this point execution begins and the program enters run time. The program then runs until it ends, either normal termination or a crash.
A system that executes a program is called an interpreter of the program. Loosely speaking, an interpreter actually does what the program says to do. This contrasts with a language translator that converts a program from one language to another. The most common language translators are compilers. Translators typically convert their source from a high-level, human readable language into a lower-level language (sometimes as low as native machine code) that is simpler and faster for the processor to directly execute. The idea is that the ratio of executions to translations of a program will be large; that is, a program need only be compiled once and can be run any number of times. This can provide a large benefit for translation versus direct interpretation of the source language. One trade-off is that development time is increased, because of the compilation. In some cases, only the changed files must be recompiled. Then the executable needs to be relinked. For some changes, the executable must be rebuilt from scratch. As computers and compilers become faster, this fact becomes less of an obstacle. Also, the speed of the end product is typically more important to the user than the development time.
Translators usually produce an abstract result that is not completely ready to execute. Frequently, the operating system will convert the translator's object code into the final executable form just before execution of the program begins.



COBOL - Wikipedia

COBOL (/ˈkoʊbɒl, -bɔːl/; an acronym for "common business-oriented language") is a compiled English-like computer programming language designed for business use. It is imperative, procedural and, since 2002, object-oriented. COBOL is primarily used in business, finance, and administrative systems for companies and governments. COBOL is still widely used in legacy applications deployed on mainframe computers, such as large-scale batch and transaction processing jobs. But due to its declining popularity and the retirement of experienced COBOL programmers, programs are being migrated to new platforms, rewritten in modern languages or replaced with software packages.[6] Most programming in COBOL is now purely to maintain existing applications.[7]
COBOL was designed in 1959 by CODASYL and was partly based on previous programming language design work by Grace Hopper, commonly referred to as "the (grand)mother of COBOL".[8][9][10] It was created as part of a US Department of Defense effort to create a portable programming language for data processing. It was originally seen as a stopgap, but the Department of Defense promptly forced computer manufacturers to provide it, resulting in its widespread adoption.[11] It was standardized in 1968 and has since been revised four times. Expansions include support for structured and object-oriented programming. The current standard is ISO/IEC 1989:2014.[12]
COBOL statements have an English-like syntax, which was designed to be self-documenting and highly readable. However, it is verbose and uses over 300 reserved words. In contrast with modern, succinct syntax like y = x;, COBOL has a more English-like syntax (in this case, MOVE x TO y).
COBOL code is split into four divisions (identification, environment, data and procedure) containing a rigid hierarchy of sections, paragraphs and sentences. Lacking a large standard library, the standard specifies 43 statements, 87 functions and just one class.
Academic computer scientists were generally uninterested in business applications when COBOL was created and were not involved in its design; it was (effectively) designed from the ground up as a computer language for business, with an emphasis on inputs and outputs, whose only data types were numbers and strings of text.[13]
COBOL has been criticized throughout its life, however, for its verbosity, design process and poor support for structured programming. These weaknesses result in monolithic and, though intended to be English-like, not easily comprehensible and verbose programs.
In the late 1950s, computer users and manufacturers were becoming concerned about the rising cost of programming. A 1959 survey had found that in any data processing installation, the programming cost US$800,000 on average and that translating programs to run on new hardware would cost $600,000. At a time when new programming languages were proliferating at an ever-increasing rate, the same survey suggested that if a common business-oriented language were used, conversion would be far cheaper and faster.[14]
In April 1959, Mary K. Hawes called a meeting of representatives from academia, computer users, and manufacturers at the University of Pennsylvania to organize a formal meeting on common business languages.[15] Representatives included Grace Hopper, inventor of the English-like data processing language FLOW-MATIC, Jean Sammet and Saul Gorn.[16][17]
The group asked the Department of Defense (DoD) to sponsor an effort to create a common business language. The delegation impressed Charles A. Phillips, director of the Data System Research Staff at the DoD, who thought that they "thoroughly understood" the DoD's problems. The DoD operated 225 computers, had a further 175 on order and had spent over $200 million on implementing programs to run on them. Portable programs would save time, reduce costs and ease modernization.[18]
Phillips agreed to sponsor the meeting and tasked the delegation with drafting the agenda.[19]
On May 28 and 29 of 1959 (exactly one year after the Zürich ALGOL 58 meeting), a meeting was held at the Pentagon to discuss the creation of a common programming language for business. It was attended by 41 people and was chaired by Phillips.[20] The Department of Defense was concerned about whether it could run the same data processing programs on different computers. FORTRAN, the only mainstream language at the time, lacked the features needed to write such programs.[21]
Representatives enthusiastically described a language that could work in a wide variety of environments, from banking and insurance to utilities and inventory control. They agreed unanimously that more people should be able to program and that the new language should not be restricted by the limitations of contemporary technology. A majority agreed that the language should make maximal use of English, be capable of change, be machine-independent and be easy to use, even at the expense of power.[22]
The meeting resulted in the creation of a steering committee and short-, intermediate- and long-range committees. The short-range committee was given to September (three months) to produce specifications for an interim language, which would then be improved upon by the other committees.[23][24] Their official mission, however, was to identify the strengths and weaknesses of existing programming languages and did not explicitly direct them to create a new language.[21]
The deadline was met with disbelief by the short-range committee.[25]
One member, Betty Holberton, described the three-month deadline as "gross optimism" and doubted that the language really would be a stopgap.[26]
The steering committee met on June 4 and agreed to name the entire activity as the Committee on Data Systems Languages, or CODASYL, and to form an executive committee.[27]
The short-range committee was made up of members representing six computer manufacturers and three government agencies. The six computer manufacturers were Burroughs Corporation, IBM, Minneapolis-Honeywell (Honeywell Labs), RCA, Sperry Rand, and Sylvania Electric Products. The three government agencies were the US Air Force, the Navy's David Taylor Model Basin, and the National Bureau of Standards (now the National Institute of Standards and Technology).[28] The committee was chaired by Joseph Wegstein of the US National Bureau of Standards. Work began by investigating data description, statements, existing applications and user experiences.[29]
The committee mainly examined the FLOW-MATIC, AIMACO and COMTRAN programming languages.[21][30]
The FLOW-MATIC language was particularly influential because it had been implemented and because AIMACO was a derivative of it with only minor changes.[31][32]
FLOW-MATIC's inventor, Grace Hopper, also served as a technical adviser to the committee.[25] FLOW-MATIC's major contributions to COBOL were long variable names, English words for commands and the separation of data descriptions and instructions.[33]
IBM's COMTRAN language, invented by Bob Bemer, was regarded as a competitor to FLOW-MATIC[34][35] by a short-range committee made up of colleagues of Grace Hopper.[36]
Some of its features were not incorporated into COBOL so that it would not look like IBM had dominated the design process,[23] and Jean Sammet said in 1981 that there had been a "strong anti-IBM bias" from some committee members (herself included).[37]
In one case, after Roy Goldfinger, author of the COMTRAN manual and intermediate-range committee member, attended a subcommittee meeting to support his language and encourage the use of algebraic expressions, Grace Hopper sent a memo to the short-range committee reiterating Sperry Rand's efforts to create a language based on English.[38]
In 1980, Grace Hopper commented that "COBOL 60 is 95% FLOW-MATIC" and that COMTRAN had had an "extremely small" influence. Furthermore, she said that she would claim that work was influenced by both FLOW-MATIC and COMTRAN only to "keep other people happy [so they] wouldn't try to knock us out".[39]
Features from COMTRAN incorporated into COBOL included formulas,[40] the PICTURE clause,[41] an improved IF statement, which obviated the need for GO TOs, and a more robust file management system.[34]
The usefulness of the committee's work was subject of great debate. While some members thought the language had too many compromises and was the result of design by committee, others felt it was better than the three languages examined. Some felt the language was too complex; others, too simple.[42]
Controversial features included those some considered useless or too advanced for data processing users. Such features included boolean expressions, formulas and table subscripts (indices).[43][44] Another point of controversy was whether to make keywords context-sensitive and the effect that would have on readability.[43] Although context-sensitive keywords were rejected, the approach was later used in PL/I and partially in COBOL from 2002.[45] Little consideration was given to interactivity, interaction with operating systems (few existed at that time) and functions (thought of as purely mathematical and of no use in data processing).[46][47]
The specifications were presented to the Executive Committee on September 4. They fell short of expectations: Joseph Wegstein noted that "it contains rough spots and requires some additions", and Bob Bemer later described them as a "hodgepodge". The subcommittee was given until December to improve it.[25]
At a mid-September meeting, the committee discussed the new language's name. Suggestions included "BUSY" (Business System), "INFOSYL" (Information System Language) and "COCOSYL" (Common Computer Systems Language).[48] The name "COBOL" was suggested by Bob Bemer.[49][50]
In October, the intermediate-range committee received copies of the FACT language specification created by Roy Nutt. Its features impressed the committee so much that they passed a resolution to base COBOL on it.[51]
This was a blow to the short-range committee, who had made good progress on the specification. Despite being technically superior, FACT had not been created with portability in mind or through manufacturer and user consensus. It also lacked a demonstrable implementation,[25] allowing supporters of a FLOW-MATIC-based COBOL to overturn the resolution. RCA representative Howard Bromberg also blocked FACT, so that RCA's work on a COBOL implementation would not go to waste.[52]
'And what name do you want inscribed?'
I said, 'I'll write it for you.' I wrote the name down: COBOL.
'What kind of name is that?'
'Well it's a Polish name. We shortened it and got rid of a lot of unnecessary notation.'
Howard Bromberg on how he bought the COBOL tombstone[53]
It soon became apparent that the committee was too large for any further progress to be made quickly. A frustrated Howard Bromberg bought a $15 tombstone with "COBOL" engraved on it and sent it to Charles Phillips to demonstrate his displeasure.[b][53][55]
A sub-committee was formed to analyze existing languages and was made up of six individuals:[21][56]
The sub-committee did most of the work creating the specification, leaving the short-range committee to review and modify their work before producing the finished specification.[21]
The specifications were approved by the Executive Committee on January 3, 1960, and sent to the government printing office, which printed these as COBOL 60. The language's stated objectives were to allow efficient, portable programs to be easily written, to allow users to move to new systems with minimal effort and cost, and to be suitable for inexperienced programmers.[57]
The CODASYL Executive Committee later created the COBOL Maintenance Committee to answer questions from users and vendors and to improve and expand the specifications.[58]
During 1960, the list of manufacturers planning to build COBOL compilers grew. By September, five more manufacturers had joined CODASYL (Bendix, Control Data Corporation, General Electric (GE), National Cash Register and Philco), and all represented manufacturers had announced COBOL compilers. GE and IBM planned to integrate COBOL into their own languages, GECOM and COMTRAN, respectively. In contrast, International Computers and Tabulators planned to replace their language, CODEL, with COBOL.[59]
Meanwhile, RCA and Sperry Rand worked on creating COBOL compilers. The first COBOL program ran on 17 August on an RCA 501.[60]
On December 6 and 7, the same COBOL program (albeit with minor changes) ran on an RCA computer and a Remington-Rand Univac computer, demonstrating that compatibility could be achieved.[61]
The relative influences of which languages were used continues to this day in the recommended advisory printed in all COBOL reference manuals:
COBOL is an industry language and is not the property of any company or group of companies, or of any organization or group of organizations.
No warranty, expressed or implied, is made by any contributor or by the CODASYL COBOL Committee as to the accuracy and functioning of the
programming system and language. Moreover, no responsibility is assumed by any contributor, or by the committee, in connection therewith. The authors and copyright holders of the copyrighted material used herein are as follows:
They have specifically authorized the use of this material, in whole or in part, in the COBOL specifications. Such authorization extends to the reproduction and use of COBOL specifications in programming manuals or similar publications.[62]
Anonymous, June 1960[63]
Many logical flaws were found in COBOL 60, leading GE's Charles Katz to warn that it could not be interpreted unambiguously. A reluctant short-term committee enacted a total cleanup and, by March 1963, it was reported that COBOL's syntax was as definable as ALGOL's, although semantic ambiguities remained.[59]
Early COBOL compilers were primitive and slow. A 1962 US Navy evaluation found compilation speeds of 3–11 statements per minute. By mid-1964, they had increased to 11–1000 statements per minute. It was observed that increasing memory would drastically increase speed and that compilation costs varied wildly: costs per statement were between $0.23 and $18.91.[64]
In late 1962, IBM announced that COBOL would be their primary development language and that development of COMTRAN would cease.[64]
The COBOL specification was revised three times in the five years after its publication.
COBOL-60 was replaced in 1961 by COBOL-61. This was then replaced by the COBOL-61 Extended specifications in 1963, which introduced the sort and report writer facilities.[65]
The added facilities corrected flaws identified by Honeywell in late 1959 in a letter to the short-range committee.[60]
COBOL Edition 1965 brought further clarifications to the specifications and introduced facilities for handling mass storage files and tables.[66]
Efforts began to standardize COBOL to overcome incompatibilities between versions. In late 1962, both ISO and the United States of America Standards Institute (now ANSI) formed groups to create standards. ANSI produced USA Standard COBOL X3.23 in August 1968, which became the cornerstone for later versions.[67] This version was known as American National Standard (ANS) COBOL and was adopted by ISO in 1972.[68]
By 1970, COBOL had become the most widely used programming language in the world.[69]
Independently of the ANSI committee, the CODASYL Programming Language Committee was working on improving the language. They described new versions in 1968, 1969, 1970 and 1973, including changes such as new inter-program communication, debugging and file merging facilities as well as improved string-handling and library inclusion features.[70]
Although CODASYL was independent of the ANSI committee, the CODASYL Journal of Development was used by ANSI to identify features that were popular enough to warrant implementing.[71]
The Programming Language Committee also liaised with ECMA and the Japanese COBOL Standard committee.[70]
The Programming Language Committee was not well-known, however. The vice-president, William Rinehuls, complained that two-thirds of the COBOL community did not know of the committee's existence. It was also poor, lacking the funds to make public documents, such as minutes of meetings and change proposals, freely available.[72]
In 1974, ANSI published a revised version of (ANS) COBOL, containing new features such as file organizations, the DELETE statement[73] and the segmentation module.[74]
Deleted features included the NOTE statement, the EXAMINE statement (which was replaced by INSPECT) and the implementer-defined random access module (which was superseded by the new sequential and relative I/O modules). These made up 44 changes, which rendered existing statements incompatible with the new standard.[75]
The report writer was slated to be removed from COBOL, but was reinstated before the standard was published.[76][77] ISO later adopted the updated standard in 1978.[68]
In June 1978, work began on revising COBOL-74. The proposed standard (commonly called COBOL-80) differed significantly from the previous one, causing concerns about incompatibility and conversion costs. In January 1981, Joseph T. Brophy, Senior Vice-President of Travelers Insurance, threatened to sue the standard committee because it was not upwards compatible with COBOL-74. Mr. Brophy described previous conversions of their 40-million-line code base as "non-productive" and a "complete waste of our programmer resources".[78]
Later that year, the Data Processing Management Association (DPMA) said it was "strongly opposed" to the new standard, citing "prohibitive" conversion costs and enhancements that were "forced on the user".[79][80]
During the first public review period, the committee received 2,200 responses, of which 1,700 were negative form letters.[81]
Other responses were detailed analyses of the effect COBOL-80 would have on their systems; conversion costs were predicted to be at least 50 cents per line of code. Fewer than a dozen of the responses were in favor of the proposed standard.[82]
In 1983, the DPMA withdrew its opposition to the standard, citing the responsiveness of the committee to public concerns. In the same year, a National Bureau of Standards study concluded that the proposed standard would present few problems.[80][83] A year later, a COBOL-80 compiler was released to DEC VAX users, who noted that conversion of COBOL-74 programs posed few problems. The new EVALUATE statement and inline PERFORM were particularly well received and improved productivity, thanks to simplified control flow and debugging.[84]
The second public review drew another 1,000 (mainly negative) responses, while the last drew just 25, by which time many concerns had been addressed.[80]
In late 1985, ANSI published the revised standard. Sixty features were changed or deprecated and many[quantify] were added, such as:[85][86]
The standard was adopted by ISO the same year.[68] Two amendments followed in 1989 and 1993, the first introducing intrinsic functions and the other providing corrections. ISO adopted the amendments in 1991 and 1994 respectively,[68] before subsequently taking primary ownership and development of the standard.
In 1997, Gartner Group estimated that there were a total of 200 billion lines of COBOL in existence, which ran 80% of all business programs.[87][better source needed]
In the early 1990s, work began on adding object-orientation in the next full revision of COBOL. Object-oriented features were taken from C++ and Smalltalk.[1][2]
The initial estimate was to have this revision completed by 1997, and an ISO Committee Draft (CD) was available by 1997. Some vendors (including Micro Focus, Fujitsu, and IBM) introduced object-oriented syntax based on drafts of the full revision. The final approved ISO standard was approved and published in late 2002.[88]
Fujitsu/GTSoftware,[89] Micro Focus and RainCode introduced object-oriented COBOL compilers targeting the .NET Framework.
There were many other new features, many of which had been in the CODASYL COBOL Journal of Development since 1978 and had missed the opportunity to be included in COBOL-85.[90] These other features included:[91][92]
Three corrigenda were published for the standard: two in 2006 and one in 2009.[93]
Between 2003 and 2009, three technical reports were produced describing object finalization, XML processing and collection classes for COBOL.[93]
COBOL 2002 suffered from poor support: no compilers completely supported the standard. Micro Focus found that it was due to a lack of user demand for the new features and due to the abolition of the NIST test suite, which had been used to test compiler conformance. The standardization process was also found to be slow and under-resourced.[94]
COBOL 2014 includes the following changes:[95]
COBOL programs are used globally in governments and businesses and are running on diverse operating systems such as z/OS, z/VSE, VME, Unix, OpenVMS and Windows. In 1997, the Gartner Group reported that 80% of the world's business ran on COBOL with over 200 billion lines of code and 5 billion lines more being written annually.[97]
Near the end of the 20th century, the year 2000 problem (Y2K) was the focus of significant COBOL programming effort, sometimes by the same programmers who had designed the systems decades before. The particular level of effort required to correct COBOL code has been attributed[by whom?] to the large amount of business-oriented COBOL, as business applications use dates heavily, and to fixed-length data fields. After the clean-up effort put into these programs for Y2K, a 2003 survey found that many remained in use.[98]
The authors said that the survey data suggest "a gradual decline in the importance of Cobol in application development over the [following] 10 years unless ... integration with other languages and technologies can be adopted".[99]
In 2006 and 2012, Computerworld surveys found that over 60% of organizations used COBOL (more than C++ and Visual Basic .NET) and that for half of those, COBOL was used for the majority of their internal software.[7][100] 36% of managers said they planned to migrate from COBOL, and 25% said they would like to if it was cheaper. Instead, some businesses have migrated their systems from expensive mainframes to cheaper, more modern systems, while maintaining their COBOL programs.[7]
COBOL has an English-like syntax, which is used to describe nearly everything in a program. For example, a condition can be expressed as  x IS GREATER THAN y or more concisely as  x GREATER y  or  x > y. More complex conditions can be "abbreviated" by removing repeated conditions and variables. For example,  a > b AND a > c OR a = d  can be shortened to a > b AND c OR = d. As a consequence of this English-like syntax, COBOL has over 300 keywords.[101][c] Some of the keywords are simple alternative or pluralized spellings of the same word, which provides for more English-like statements and clauses; e.g., the IN and OF keywords can be used interchangeably, as can IS and ARE, and VALUE and VALUES.
Each COBOL program is made up of four basic lexical items: words, literals, picture character-strings (see § PICTURE clause) and separators. Words include reserved words and user-defined identifiers. They are up to 31 characters long and may include letters, digits, hyphens and underscores. Literals include numerals (e.g. 12) and strings (e.g. 'Hello!').[103] Separators include the space character and commas and semi-colons followed by a space.[104]
A COBOL program is split into four divisions: the identification division, the environment division, the data division and the procedure division. The identification division specifies the name and type of the source element and is where classes and interfaces are specified. The environment division specifies any program features that depend on the system running it, such as files and character sets. The data division is used to declare variables and parameters. The procedure division contains the program's statements. Each division is sub-divided into sections, which are made up of paragraphs.
COBOL's syntax is usually described with a unique metalanguage using braces, brackets, bars and underlining. The metalanguage was developed for the original COBOL specifications. Although Backus–Naur form did exist at the time, the committee had not heard of it.[105]
As an example, consider the following description of an ADD statement:










ADD
_





{




identifier-1






literal-1




}


…



TO
_




{


identifier-2



[




ROUNDED
_




]


}

…






[

|





ON




SIZE
_





ERROR
_




imperative-statement-1







NOT
_




ON




SIZE
_





ERROR
_




imperative-statement-2





|

]







[




END-ADD
_




]







{\displaystyle {\begin{array}{l}{\underline {\text{ADD}}}\,{\begin{Bmatrix}{\text{identifier-1}}\\{\text{literal-1}}\end{Bmatrix}}\dots \;{\underline {\text{TO}}}\,\left\{{\text{identifier-2}}\,\left[\,{\underline {\text{ROUNDED}}}\,\right]\right\}\dots \\\quad \left[\left|{\begin{array}{l}{\text{ON}}\,{\underline {\text{SIZE}}}\,{\underline {\text{ERROR}}}\,{\text{imperative-statement-1}}\\{\underline {\text{NOT}}}\,{\text{ON}}\,{\underline {\text{SIZE}}}\,{\underline {\text{ERROR}}}\,{\text{imperative-statement-2}}\\\end{array}}\right|\right]\\\quad \left[\,{\underline {\text{END-ADD}}}\,\right]\end{array}}}


This description permits the following variants:
COBOL can be written in two formats: fixed (the default) or free. In fixed-format, code must be aligned to fit in certain areas (a hold-over from using punched cards). Until COBOL 2002, these were:
In COBOL 2002, Areas A and B were merged to form the program-text area, which now ends at an implementor-defined column.[106]
COBOL 2002 also introduced free-format code. Free-format code can be placed in any column of the file, as in newer programming languages. Comments are specified using *>, which can be placed anywhere and can also be used in fixed-format source code. Continuation lines are not present, and the >>PAGE directive replaces the / indicator.[106]
The identification division identifies the following code entity and contains the definition of a class or interface.
Classes and interfaces have been in COBOL since 2002. Classes have factory objects, containing class methods and variables, and instance objects, containing instance methods and variables.[107] Inheritance and interfaces provide polymorphism. Support for generic programming is provided through parameterized classes, which can be instantiated to use any class or interface. Objects are stored as references which may be restricted to a certain type. There are two ways of calling a method: the INVOKE statement, which acts similarly to CALL, or through inline method invocation, which is analogous to using functions.[108]
COBOL does not provide a way to hide methods. Class data can be hidden, however, by declaring it without a PROPERTY clause, which leaves the user with no way to access it.[109] Method overloading was added in COBOL 2014.[110]
The environment division contains the configuration section and the input-output section. The configuration section is used to specify variable features such
as currency signs, locales and character sets. The input-output section contains file-related information.
COBOL supports three file formats, or organizations: sequential, indexed and relative. In sequential files, records are contiguous and must be traversed sequentially, similarly to a linked list. Indexed files have one or more indexes which allow records to be randomly accessed and which can be sorted on them. Each record must have a unique key, but other, alternate, record keys need not be unique. Implementations of indexed files vary between vendors, although common implementations, such as C‑ISAM and VSAM, are based on IBM's ISAM. Relative files, like indexed files, have a unique record key, but they do not have alternate keys. A relative record's key is its ordinal position; for example, the 10th record has a key of 10. This means that creating a record with a key of 5 may require the creation of (empty) preceding records. Relative files also allow for both sequential and random access.[111]
A common non-standard extension is the line sequential organization, used to process text files. Records in a file are terminated by a newline and may be of varying length.[112]
The data division is split into six sections which declare different items: the file section, for file records; the working-storage section, for static variables; the local-storage section, for automatic variables; the linkage section, for parameters and the return value; the report section and the screen section, for text-based user interfaces.
Data items in COBOL are declared hierarchically through the use of level-numbers which indicate if a data item is part of another. An item with a higher level-number is subordinate to an item with a lower one. Top-level data items, with a level-number of 1, are called records. Items that have subordinate aggregate data are called group items; those that do not are called elementary items. Level-numbers used to describe standard data items are between 1 and 49.[113][114]
In the above example, elementary item num and group item the-date are subordinate to the record some-record, while elementary items the-year, the-month, and the-day are part of the group item the-date.
Subordinate items can be disambiguated with the IN (or OF) keyword. For example, consider the example code above along with the following example:
The names the-year, the-month, and the-day are ambiguous by themselves, since more than one data item is defined with those names. To specify a particular data item, for instance one of the items contained within the sale-date group, the programmer would use the-year IN sale-date (or the equivalent the-year OF sale-date). (This syntax is similar to the "dot notation" supported by most contemporary languages.)
A level-number of 66 is used to declare a re-grouping of previously defined items, irrespective of how those items are structured. This data level, also referred to by the associated RENAMES clause, is rarely used[115] and, circa 1988, was usually found in old programs. Its ability to ignore the hierarchical and logical structure data meant its use was not recommended and many installations forbade its use.[116]
A 77 level-number indicates the item is stand-alone, and in such situations is equivalent to the level-number 01. For example, the following code declares two 77-level data items, property-name and sales-region, which are non-group data items that are independent of (not subordinate to) any other data items:
An 88 level-number declares a condition name (a so-called 88-level) which is true when its parent data item contains one of the values specified in its VALUE clause.[117] For example, the following code defines two 88-level condition-name items that are true or false depending on the current character data value of the wage-type data item. When the data item contains a value of 'H', the condition-name wage-is-hourly is true, whereas when it contains a value of 'S' or 'Y', the condition-name wage-is-yearly is true. If the data item contains some other value, both of the condition-names are false.
Standard COBOL provides the following data types:[118]
Type safety is variable in COBOL. Numeric data is converted between different representations and sizes silently and alphanumeric data can be placed in any data item that can be stored as a string, including numeric and group data.[119] In contrast, object references and pointers may only be assigned from items of the same type and their values may be restricted to a certain type.[120]
A PICTURE (or PIC) clause is a string of characters, each of which represents a portion of the data item and what it may contain. Some picture characters specify the type of the item and how many characters or digits it occupies in memory. For example, a 9 indicates a decimal digit, and an S indicates that the item is signed. Other picture characters (called insertion and editing characters) specify how an item should be formatted. For example, a series of + characters define character positions as well as how a leading sign character is to be positioned within the final character data; the rightmost non-numeric character will contain the item's sign, while other character positions corresponding to a + to the left of this position will contain a space. Repeated characters can be specified more concisely by specifying a number in parentheses after a picture character; for example, 9(7) is equivalent to 9999999. Picture specifications containing only digit (9) and sign (S) characters define purely numeric data items, while picture specifications containing alphabetic (A) or alphanumeric (X) characters define alphanumeric data items. The presence of other formatting characters define edited numeric or edited alphanumeric data items.[121]
The USAGE clause declares the format data is stored in. Depending on the data type, it can either complement or be used instead of a PICTURE clause. While it can be used to declare pointers and object references, it is mostly geared towards specifying numeric types. These numeric formats are:[122]
The report writer is a declarative facility for creating reports. The programmer need only specify the report layout and the data required to produce it, freeing them from having to write code to handle things like page breaks, data formatting, and headings and footings.[123]
Reports are associated with report files, which are files which may only be written to through report writer statements.
Each report is defined in the report section of the data division. A report is split into report groups which define the report's headings, footings and details. Reports work around hierarchical control breaks. Control breaks occur when a key variable changes it value; for example, when creating a report detailing customers' orders, a control break could occur when the program reaches a different customer's orders. Here is an example report description for a report which gives a salesperson's sales and which warns of any invalid records:
The above report description describes the following layout:
Four statements control the report writer: INITIATE, which prepares the report writer for printing; GENERATE, which prints a report group; SUPPRESS, which suppresses the printing of a report group; and TERMINATE, which terminates report processing. For the above sales report example, the procedure division might look like this:
The sections and paragraphs in the procedure division (collectively called procedures) can be used as labels and as simple subroutines. Unlike in other divisions, paragraphs do not need to be in sections.[124]
Execution goes down through the procedures of a program until it is terminated.[125]
To use procedures as subroutines, the PERFORM verb is used. This transfers control to the specified range of procedures and returns only upon reaching the end.
Unusual control flow can trigger mines, which cause control in performed procedures to return at unexpected times to unexpected locations. Procedures can be reached in three ways: they can be called with PERFORM, jumped to from a GO TO or through execution "falling through" the bottom of an above paragraph. Combinations of these invoke undefined behavior, creating mines. Specifically, mines occur when execution of a range of procedures would cause control flow to go past the last statement of a range of procedures already being performed.[126][127]
For example, in the code in the adjacent image, a mine is tripped at the end of update-screen when the screen is invalid. When the screen is invalid, control jumps to the fix-screen section, which, when done, performs update-screen. This recursion triggers undefined behavior as there are now two overlapping ranges of procedures being performed. The mine is then triggered upon reaching the end of update-screen and means control could return to one of two locations:
COBOL 2014 has 47 statements (also called verbs),[128] which can be grouped into the following broad categories: control flow, I/O, data manipulation and the report writer. The report writer statements are covered in the report writer section.
COBOL's conditional statements are IF and EVALUATE. EVALUATE is a switch-like statement with the added capability of evaluating multiple values and conditions. This can be used to implement decision tables. For example, the following might be used to control a CNC lathe: 
The PERFORM statement is used to define loops which are executed until a condition is true (not while true, which is more common in other languages). It is also used to call procedures or ranges of procedures (see the procedures section for more details). CALL and INVOKE call subprograms and methods, respectively. The name of the subprogram/method is contained in a string which may be a literal or a data item.[129] Parameters can be passed by reference, by content (where a copy is passed by reference) or by value (but only if a prototype is available).[130]
CANCEL unloads subprograms from memory. GO TO causes the program to jump to a specified procedure.
The GOBACK statement is a return statement and the STOP statement stops the program. The EXIT statement has six different formats: it can be used as a return statement, a break statement, a continue statement, an end marker or to leave a procedure.[131]
Exceptions are raised by a RAISE statement and caught with a handler, or declarative, defined in the DECLARATIVES portion of the procedure division. Declaratives are sections beginning with a USE statement which specify the errors to handle. Exceptions can be names or objects. RESUME is used in a declarative to jump to the statement after the one that raised the exception or to a procedure outside the DECLARATIVES. Unlike other languages, uncaught exceptions may not terminate the program and the program can proceed unaffected.
File I/O is handled by the self-describing OPEN, CLOSE, READ, and WRITE statements along with a further three: REWRITE, which updates a record; START, which selects subsequent records to access by finding a record with a certain key; and UNLOCK, which releases a lock on the last record accessed.
User interaction is done using ACCEPT and DISPLAY.
The following verbs manipulate data:
Files and tables are sorted using SORT and the MERGE verb merges and sorts files. The RELEASE verb provides records to sort and RETURN retrieves sorted records in order.
Some statements, such as IF and READ, may themselves contain statements. Such statements may be terminated in two ways: by a period (implicit termination), which terminates all unterminated statements contained, or by a scope terminator, which terminates the nearest matching open statement.
Nested statements terminated with a period are a common source of bugs.[133][134] For example, examine the following code:
Here, the intent is to display y and z if condition x is true. However, z will be displayed whatever the value of x because the IF statement is terminated by an erroneous period after DISPLAY y.
Another bug is a result of the dangling else problem, when two IF statements can associate with an ELSE.
In the above fragment, the ELSE associates with the  IF y  statement instead of the  IF x  statement, causing a bug. Prior to the introduction of explicit scope terminators, preventing it would require  ELSE NEXT SENTENCE  to be placed after the inner IF.[134]
The original (1959) COBOL specification supported the infamous  ALTER X TO PROCEED TO Y  statement, for which many compilers generated self-modifying code. X and Y are procedure labels, and the single  GO TO  statement in procedure X executed after such an ALTER statement means  GO TO Y  instead. Many compilers still support it,[135]
but it was deemed obsolete in the COBOL 1985 standard and deleted in 2002.[136]
A "Hello, world" program in COBOL:
When the – now famous – "Hello, World!" program example in The C Programming Language was first published in 1978 a similar mainframe COBOL program sample would have been submitted through JCL, very likely using a punch card reader, and 80 column punch cards. The listing below, with an empty DATA DIVISION, was tested using GNU/Linux and the System/370 Hercules emulator running MVS 3.8J. The JCL, written in July 2015, is derived from the Hercules tutorials and samples hosted by Jay Moseley.[137] In keeping with COBOL programming of that era, HELLO, WORLD is displayed in all capital letters.
After submitting the JCL, the MVS console displayed:
Line 10 of the console listing above is highlighted for effect, the highlighting is not part of the actual console output.
The associated compiler listing generated over four pages of technical detail and job run information, for the single line of output from the 14 lines of COBOL.
In the 1970s, adoption of the structured programming paradigm was becoming increasingly widespread. Edsger Dijkstra, a preeminent computer scientist, wrote a letter to the editor of Communications of the ACM, published 1975 entitled "How do we tell truths that might hurt?", in which he was critical of COBOL and several other contemporary languages; remarking that "the use of COBOL cripples the mind".[138]
In a published dissent to Dijkstra's remarks, the computer scientist Howard E. Tompkins claimed that unstructured COBOL tended to be "written by programmers that have never had the benefit of structured COBOL taught well", arguing that the issue was primarily one of training.[139]
One cause of spaghetti code was the GO TO statement. Attempts to remove GO TOs from COBOL code, however, resulted in convoluted programs and reduced code quality.[140] GO TOs were largely replaced by the PERFORM statement and procedures, which promoted modular programming[140] and gave easy access to powerful looping facilities. However, PERFORM could only be used with procedures so loop bodies were not located where they were used, making programs harder to understand.[141]
COBOL programs were infamous for being monolithic and lacking modularization.[142]
COBOL code could only be modularized through procedures, which were found to be inadequate for large systems. It was impossible to restrict access to data, meaning a procedure could access and modify any data item. Furthermore, there was no way to pass parameters to a procedure, an omission Jean Sammet regarded as the committee's biggest mistake.[143]
Another complication stemmed from the ability to PERFORM THRU a specified sequence of procedures. This meant that control could jump to and return from any procedure, creating convoluted control flow and permitting a programmer to break the single-entry single-exit rule.[144]
This situation improved as COBOL adopted more features. COBOL-74 added subprograms, giving programmers the ability to control the data each part of the program could access. COBOL-85 then added nested subprograms, allowing programmers to hide subprograms.[145] Further control over data and code came in 2002 when object-oriented programming, user-defined functions and user-defined data types were included.
Nevertheless, much important legacy COBOL software uses unstructured code, which has become unmaintainable. It can be too risky and costly to modify even a simple section of code, since it may be used from unknown places in unknown ways.[146]
COBOL was intended to be a highly portable, "common" language. However, by 2001, around 300 dialects had been created.[147] One source of dialects was the standard itself: the 1974 standard was composed of one mandatory nucleus and eleven functional modules, each containing two or three levels of support. This permitted 104,976 official variants.[148]
COBOL-85 was not fully compatible with earlier versions, and its development was controversial. Joseph T. Brophy, the CIO of Travelers Insurance, spearheaded an effort to inform COBOL users of the heavy reprogramming costs of implementing the new standard.[149] As a result, the ANSI COBOL Committee received more than 2,200 letters from the public, mostly negative, requiring the committee to make changes. On the other hand, conversion to COBOL-85 was thought to increase productivity in future years, thus justifying the conversion costs.[150]
The Jargon File 4.4.8.[151]
COBOL syntax has often been criticized for its verbosity. Proponents say that this was intended to make the code self-documenting, easing program maintenance.[152] COBOL was also intended to be easy for programmers to learn and use,[153] while still being readable to non-technical staff such as managers.[154][155][156][157]
The desire for readability led to the use of English-like syntax and structural elements, such as nouns, verbs, clauses, sentences, sections, and divisions. Yet by 1984, maintainers of COBOL programs were struggling to deal with "incomprehensible" code[156] and the main changes in COBOL-85 were there to help ease maintenance.[81]
Jean Sammet, a short-range committee member, noted that "little attempt was made to cater to the professional programmer, in fact people whose main interest is programming tend to be very unhappy with COBOL" which she attributed to COBOL's verbose syntax.[158]
The COBOL community has always been isolated from the computer science community. No academic computer scientists participated in the design of COBOL: all of those on the committee came from commerce or government. Computer scientists at the time were more interested in fields like numerical analysis, physics and system programming than the commercial file-processing problems which COBOL development tackled.[159] Jean Sammet attributed COBOL's unpopularity to an initial "snob reaction" due to its inelegance, the lack of influential computer scientists participating in the design process and a disdain for business data processing.[160] The COBOL specification used a unique "notation", or metalanguage, to define its syntax rather than the new Backus–Naur form which the committee did not know of. This resulted in "severe" criticism.[161][162][59]
Later, COBOL suffered from a shortage of material covering it; it took until 1963 for introductory books to appear (with Richard D. Irwin publishing a college textbook on COBOL in 1966).[163] By 1985, there were twice as many books on Fortran and four times as many on BASIC as on COBOL in the Library of Congress.[105] University professors taught more modern, state-of-the-art languages and techniques instead of COBOL which was said to have a "trade school" nature.[164] Donald Nelson, chair of the CODASYL COBOL committee, said in 1984 that "academics ... hate COBOL" and that computer science graduates "had 'hate COBOL' drilled into them".[165] A 2013 poll by Micro Focus found that 20% of university academics thought COBOL was outdated or dead and that 55% believed their students thought COBOL was outdated or dead. The same poll also found that only 25% of academics had COBOL programming on their curriculum even though 60% thought they should teach it.[166]
In contrast, in 2003, COBOL featured in 80% of information systems curricula in the United States, the same proportion as C++ and Java.[167]
There was also significant condescension towards COBOL in the business community from users of other languages, for example FORTRAN or assembler, implying that COBOL could be used only for non-challenging problems.[citation needed]
Doubts have been raised about the competence of the standards committee. Short-term committee member Howard Bromberg said that there was "little control" over the development process and that it was "plagued by discontinuity of personnel and ... a lack of talent."[69] Jean Sammet and Jerome Garfunkel also noted that changes introduced in one revision of the standard would be reverted in the next, due as much to changes in who was in the standard committee as to objective evidence.[168]
COBOL standards have repeatedly suffered from delays: COBOL-85 arrived five years later than hoped,[169]
COBOL 2002 was five years late,[1]
and COBOL 2014 was six years late.[88][170]
To combat delays, the standard committee allowed the creation of optional addenda which would add features more quickly than by waiting for the next standard revision. However, some committee members raised concerns about incompatibilities between implementations and frequent modifications of the standard.[171]
COBOL's data structures influenced subsequent programming languages. Its record and file structure influenced PL/I and Pascal, and the REDEFINES clause was a predecessor to Pascal's variant records. Explicit file structure definitions preceded the development of database management systems and aggregated data was a significant advance over Fortran's arrays.[105]
PICTURE data declarations were incorporated into PL/I, with minor changes.
COBOL's COPY facility, although considered "primitive",[172]
influenced the development of include directives.[105]
The focus on portability and standardization meant programs written in COBOL could be portable and facilitated the spread of the language to a wide variety of hardware platforms and operating systems.[173] Additionally, the well-defined division structure restricts the definition of external references to the Environment Division, which simplifies platform changes in particular.[174]



Java (programming language) - Wikipedia

Java is a general-purpose computer-programming language that is concurrent, class-based, object-oriented,[15] and specifically designed to have as few implementation dependencies as possible. It is intended to let application developers "write once, run anywhere" (WORA),[16] meaning that compiled Java code can run on all platforms that support Java without the need for recompilation.[17] Java applications are typically compiled to bytecode that can run on any Java virtual machine (JVM) regardless of computer architecture. As of 2016, Java is one of the most popular programming languages in use,[18][19][20][21] particularly for client-server web applications, with a reported 9 million developers.[22] Java was originally developed by James Gosling at Sun Microsystems (which has since been acquired by Oracle Corporation) and released in 1995 as a core component of Sun Microsystems' Java platform. The language derives much of its syntax from C and C++, but it has fewer low-level facilities than either of them.
The original and reference implementation Java compilers, virtual machines, and class libraries were originally released by Sun under proprietary licenses. As of May 2007, in compliance with the specifications of the Java Community Process, Sun relicensed most of its Java technologies under the GNU General Public License. Others have also developed alternative implementations of these Sun technologies, such as the GNU Compiler for Java (bytecode compiler), GNU Classpath (standard libraries), and IcedTea-Web (browser plugin for applets).
The latest version is Java 11, released on September 25, 2018, which follows Java 10 after only six months[23] in line with the new release schedule. Java 8 is still supported but there will be no more security updates for Java 9.[24] Versions earlier than Java 8 are supported by companies on a commercial basis; e.g. by Oracle back to Java 6 as of October 2017 (while they still "highly recommend that you uninstall"[25] pre-Java 8 from at least Windows computers).
James Gosling, Mike Sheridan, and Patrick Naughton initiated the Java language project in June 1991.[26] Java was originally designed for interactive television, but it was too advanced for the digital cable television industry at the time.[27] The language was initially called Oak after an oak tree that stood outside Gosling's office. Later the project went by the name Green and was finally renamed Java, from Java coffee.[28] Gosling designed Java with a C/C++-style syntax that system and application programmers would find familiar.[29]
Sun Microsystems released the first public implementation as Java 1.0 in 1996.[30] It promised "Write Once, Run Anywhere" (WORA), providing no-cost run-times on popular platforms. Fairly secure and featuring configurable security, it allowed network- and file-access restrictions. Major web browsers soon incorporated the ability to run Java applets within web pages, and Java quickly became popular. The Java 1.0 compiler was re-written in Java by Arthur van Hoff to comply strictly with the Java 1.0 language specification.[31] With the advent of Java 2 (released initially as J2SE 1.2 in December 1998 – 1999), new versions had multiple configurations built for different types of platforms. J2EE included technologies and APIs for enterprise applications typically run in server environments, while J2ME featured APIs optimized for mobile applications. The desktop version was renamed J2SE. In 2006, for marketing purposes, Sun renamed new J2 versions as Java EE, Java ME, and Java SE, respectively.
In 1997, Sun Microsystems approached the ISO/IEC JTC 1 standards body and later the Ecma International to formalize Java, but it soon withdrew from the process.[32][33][34] Java remains a de facto standard, controlled through the Java Community Process.[35] At one time, Sun made most of its Java implementations available without charge, despite their proprietary software status. Sun generated revenue from Java through the selling of licenses for specialized products such as the Java Enterprise System.
On November 13, 2006, Sun released much of its Java virtual machine (JVM) as free and open-source software, (FOSS), under the terms of the GNU General Public License (GPL). On May 8, 2007, Sun finished the process, making all of its JVM's core code available under free software/open-source distribution terms, aside from a small portion of code to which Sun did not hold the copyright.[36]
Sun's vice-president Rich Green said that Sun's ideal role with regard to Java was as an "evangelist".[37] Following Oracle Corporation's acquisition of Sun Microsystems in 2009–10, Oracle has described itself as the "steward of Java technology with a relentless commitment to fostering a community of participation and transparency".[38] This did not prevent Oracle from filing a lawsuit against Google shortly after that for using Java inside the Android SDK (see Google section below). Java software runs on everything from laptops to data centers, game consoles to scientific supercomputers.[39] On April 2, 2010, James Gosling resigned from Oracle.[40]
In January 2016, Oracle announced that Java runtime environments based on JDK 9 will discontinue the browser plugin.[41]
There were five primary goals in the creation of the Java language:[17]
As of  20 March 2018[update], both Java 8 and 11 are officially supported. Major release versions of Java, along with their release dates:
Sun has defined and supports four editions of Java targeting different application environments and segmented many of its APIs so that they belong to one of the platforms. The platforms are:
The classes in the Java APIs are organized into separate groups called packages. Each package contains a set of related interfaces, classes, and exceptions. Refer to the separate platforms for a description of the packages available.[relevant to this section?  – discuss]
Sun also provided an edition called PersonalJava that has been superseded by later, standards-based Java ME configuration-profile pairings.
One design goal of Java is portability, which means that programs written for the Java platform must run similarly on any combination of hardware and operating system with adequate runtime support.
This is achieved by compiling the Java language code to an intermediate representation called Java bytecode, instead of directly to architecture-specific machine code. Java bytecode instructions are analogous to machine code, but they are intended to be executed by a virtual machine (VM) written specifically for the host hardware. End users commonly use a Java Runtime Environment (JRE) installed on their own machine for standalone Java applications, or in a web browser for Java applets.
Standard libraries provide a generic way to access host-specific features such as graphics, threading, and networking.
The use of universal bytecode makes porting simple. However, the overhead of interpreting bytecode into machine instructions made interpreted programs almost always run more slowly than native executables. Just-in-time (JIT) compilers that compile bytecodes to machine code during runtime were introduced from an early stage. Java itself is platform-independent and is adapted to the particular platform it is to run on by a Java virtual machine for it, which translates the Java bytecode into the platform's machine language.[48]
Programs written in Java have a reputation for being slower and requiring more memory than those written in C++.[49][50] However, Java programs' execution speed improved significantly with the introduction of just-in-time compilation in 1997/1998 for Java 1.1,[51] the addition of language features supporting better code analysis (such as inner classes, the StringBuilder class, optional assertions, etc.), and optimizations in the Java virtual machine, such as HotSpot becoming the default for Sun's JVM in 2000. With Java 1.5, the performance was improved with the addition of the java.util.concurrent package, including lock free implementations of the ConcurrentMaps and other multi-core collections, and it was improved further with Java 1.6.
Some platforms offer direct hardware support for Java; there are microcontrollers that can run Java bytecode in hardware instead of a software Java virtual machine,[52] and some ARM based processors could have hardware support for executing Java bytecode through their Jazelle option, though support has mostly been dropped in current implementations of ARM.
Java uses an automatic garbage collector to manage memory in the object lifecycle. The programmer determines when objects are created, and the Java runtime is responsible for recovering the memory once objects are no longer in use. Once no references to an object remain, the unreachable memory becomes eligible to be freed automatically by the garbage collector. Something similar to a memory leak may still occur if a programmer's code holds a reference to an object that is no longer needed, typically when objects that are no longer needed are stored in containers that are still in use. If methods for a nonexistent object are called, a "null pointer exception" is thrown.[53][54]
One of the ideas behind Java's automatic memory management model is that programmers can be spared the burden of having to perform manual memory management. In some languages, memory for the creation of objects is implicitly allocated on the stack or explicitly allocated and deallocated from the heap. In the latter case, the responsibility of managing memory resides with the programmer. If the program does not deallocate an object, a memory leak occurs. If the program attempts to access or deallocate memory that has already been deallocated, the result is undefined and difficult to predict, and the program is likely to become unstable or crash. This can be partially remedied by the use of smart pointers, but these add overhead and complexity. Note that garbage collection does not prevent "logical" memory leaks, i.e., those where the memory is still referenced but never used.
Garbage collection may happen at any time. Ideally, it will occur when a program is idle. It is guaranteed to be triggered if there is insufficient free memory on the heap to allocate a new object; this can cause a program to stall momentarily. Explicit memory management is not possible in Java.
Java does not support C/C++ style pointer arithmetic, where object addresses and unsigned integers (usually long integers) can be used interchangeably. This allows the garbage collector to relocate referenced objects and ensures type safety and security.
As in C++ and some other object-oriented languages, variables of Java's primitive data types are either stored directly in fields (for objects) or on the stack (for methods) rather than on the heap, as is commonly true for non-primitive data types (but see escape analysis). This was a conscious decision by Java's designers for performance reasons.
Java contains multiple types of garbage collectors. By default, HotSpot uses the parallel scavenge garbage collector.[55] However, there are also several other garbage collectors that can be used to manage the heap. For 90% of applications in Java, the Concurrent Mark-Sweep (CMS) garbage collector is sufficient.[56] Oracle aims to replace CMS with the Garbage-First collector (G1).[57]
The syntax of Java is largely influenced by C++. Unlike C++, which combines the syntax for structured, generic, and object-oriented programming, Java was built almost exclusively as an object-oriented language.[17] All code is written inside classes, and every data item is an object, with the exception of the primitive data types, (i.e. integers, floating-point numbers, boolean values, and characters), which are not objects for performance reasons. Java reuses some popular aspects of C++ (such as the printf method).
Unlike C++, Java does not support operator overloading[58] or multiple inheritance for classes, though multiple inheritance is supported for interfaces.[59]
Java uses comments similar to those of C++. There are three different styles of comments: a single line style marked with two slashes (//), a multiple line style opened with /* and closed with */, and the Javadoc commenting style opened with /** and closed with */. The Javadoc style of commenting allows the user to run the Javadoc executable to create documentation for the program and can be read by some integrated development environments (IDEs) such as Eclipse to allow developers to access documentation within the IDE.
The traditional "Hello, world!" program can be written in Java as:[60]
Source files must be named after the public class they contain, appending the suffix .java, for example, HelloWorldApp.java. It must first be compiled into bytecode, using a Java compiler, producing a file named HelloWorldApp.class. Only then can it be executed, or "launched". The Java source file may only contain one public class, but it can contain multiple classes with other than public access modifier and any number of public inner classes. When the source file contains multiple classes, make one class "public" and name the source file with that public class name.
A class that is not declared public may be stored in any .java file. The compiler will generate a class file for each class defined in the source file. The name of the class file is the name of the class, with .class appended. For class file generation, anonymous classes are treated as if their name were the concatenation of the name of their enclosing class, a $, and an integer.
The keyword public denotes that a method can be called from code in other classes, or that a class may be used by classes outside the class hierarchy. The class hierarchy is related to the name of the directory in which the .java file is located. This is called an access level modifier. Other access level modifiers include the keywords private and protected.
The keyword static in front of a method indicates a static method, which is associated only with the class and not with any specific instance of that class. Only static methods can be invoked without a reference to an object. Static methods cannot access any class members that are not also static. Methods that are not designated static are instance methods and require a specific instance of a class to operate.
The keyword void indicates that the main method does not return any value to the caller. If a Java program is to exit with an error code, it must call System.exit() explicitly.
The method name "main" is not a keyword in the Java language. It is simply the name of the method the Java launcher calls to pass control to the program. Java classes that run in managed environments such as applets and Enterprise JavaBeans do not use or need a main() method. A Java program may contain multiple classes that have main methods, which means that the VM needs to be explicitly told which class to launch from.
The main method must accept an array of String objects. By convention, it is referenced as args although any other legal identifier name can be used. Since Java 5, the main method can also use variable arguments, in the form of public static void main(String... args), allowing the main method to be invoked with an arbitrary number of String arguments. The effect of this alternate declaration is semantically identical (to the args parameter which is still an array of String objects), but it allows an alternative syntax for creating and passing the array.
The Java launcher launches Java by loading a given class (specified on the command line or as an attribute in a JAR) and starting its public static void main(String[]) method. Stand-alone programs must declare this method explicitly. The String[] args parameter is an array of String objects containing any arguments passed to the class. The parameters to main are often passed by means of a command line.
Printing is part of a Java standard library: The System class defines a public static field called out. The out object is an instance of the PrintStream class and provides many methods for printing data to standard out, including println(String) which also appends a new line to the passed string.
The string "Hello World!" is automatically converted to a String object by the compiler.
Java applets were programs that were embedded in other applications, typically in a Web page displayed in a web browser.  The Java applet API is now deprecated since Java 9 in 2017.
Java servlet technology provides Web developers with a simple, consistent mechanism for extending the functionality of a Web server and for accessing existing business systems. Servlets are server-side Java EE components that generate responses (typically HTML pages) to requests (typically HTTP requests) from clients.
The Java servlet API has to some extent been superseded by two standard Java technologies for web services:
JavaServer Pages (JSP) are server-side Java EE components that generate responses, typically HTML pages, to HTTP requests from clients. JSPs embed Java code in an HTML page by using the special delimiters <% and %>. A JSP is compiled to a Java servlet, a Java application in its own right, the first time it is accessed. After that, the generated servlet creates the response.
Swing is a graphical user interface library for the Java SE platform. It is possible to specify a different look and feel through the pluggable look and feel system of Swing. Clones of Windows, GTK+, and Motif are supplied by Sun. Apple also provides an Aqua look and feel for macOS. Where prior implementations of these looks and feels may have been considered lacking, Swing in Java SE 6 addresses this problem by using more native GUI widget drawing routines of the underlying platforms.
In 2004, generics were added to the Java language, as part of J2SE 5.0. Prior to the introduction of generics, each variable declaration had to be of a specific type. For container classes, for example, this is a problem because there is no easy way to create a container that accepts only specific types of objects. Either the container operates on all subtypes of a class or interface, usually Object, or a different container class has to be created for each contained class. Generics allow compile-time type checking without having to create many container classes, each containing almost identical code. In addition to enabling more efficient code, certain runtime exceptions are prevented from occurring, by issuing compile-time errors. If Java prevented all runtime type errors (ClassCastException's) from occurring, it would be type safe.
In 2016, the type system of Java was proven unsound.[61]
Criticisms directed at Java include the implementation of generics,[62] speed,[63] the handling of unsigned numbers,[64] the implementation of floating-point arithmetic,[65] and a history of security vulnerabilities in the primary Java VM implementation HotSpot.[66]
The Java Class Library is the standard library, developed to support application development in Java. It is controlled by Sun Microsystems in cooperation with others through the Java Community Process program. Companies or individuals participating in this process can influence the design and development of the APIs. This process has been a subject of controversy.[when?] The class library contains features such as:
Javadoc is a comprehensive documentation system, created by Sun Microsystems, used by many Java developers[by whom?]. It provides developers with an organized system for documenting their code. Javadoc comments have an extra asterisk at the beginning, i.e. the delimiters are /** and */, whereas the normal multi-line comments in Java are set off with the delimiters /* and */.[70]
Oracle Corporation is the current owner of the official implementation of the Java SE platform, following their acquisition of Sun Microsystems on January 27, 2010. This implementation is based on the original implementation of Java by Sun. The Oracle implementation is available for Microsoft Windows (still works for XP, while only later versions are currently officially supported), macOS, Linux, and Solaris. Because Java lacks any formal standardization recognized by Ecma International, ISO/IEC, ANSI, or other third-party standards organization, the Oracle implementation is the de facto standard.
The Oracle implementation is packaged into two different distributions: The Java Runtime Environment (JRE) which contains the parts of the Java SE platform required to run Java programs and is intended for end users, and the Java Development Kit (JDK), which is intended for software developers and includes development tools such as the Java compiler, Javadoc, Jar, and a debugger.
OpenJDK is another notable Java SE implementation that is licensed under the GNU GPL. The implementation started when Sun began releasing the Java source code under the GPL. As of Java SE 7, OpenJDK is the official Java reference implementation.
The goal of Java is to make all implementations of Java compatible. Historically, Sun's trademark license for usage of the Java brand insists that all implementations be "compatible". This resulted in a legal dispute with Microsoft after Sun claimed that the Microsoft implementation did not support RMI or JNI and had added platform-specific features of their own. Sun sued in 1997, and, in 2001, won a settlement of US$20 million, as well as a court order enforcing the terms of the license from Sun.[71] As a result, Microsoft no longer ships Java with Windows.
Platform-independent Java is essential to Java EE, and an even more rigorous validation is required to certify an implementation. This environment enables portable server-side applications.
The Java programming language requires the presence of a software platform in order for compiled programs to be executed.
Oracle supplies the Java platform for use with Java. The Android SDK is an alternative software platform, used primarily for developing Android applications with its own GUI system. The Eclipse IDE platform supports Java, but provides its own GUI system SWT.
The Java language is a key pillar in Android, an open source mobile operating system. Although Android, built on the Linux kernel, is written largely in C, the Android SDK uses the Java language as the basis for Android applications. The bytecode language supported by the Android SDK is incompatible with Java bytecode and runs on its own virtual machine, optimized for low-memory devices such as smartphones and tablet computers.
Depending on the Android version, the bytecode is either interpreted by the Dalvik virtual machine or compiled into native code by the Android Runtime.
Android does not provide the full Java SE standard library, although the Android SDK does include an independent implementation of a large subset of it. It supports Java 6 and some Java 7 features, offering an implementation compatible with the standard library (Apache Harmony).
The use of Java-related technology in Android led to a legal dispute between Oracle and Google. On May 7, 2012, a San Francisco jury found that if APIs could be copyrighted, then Google had infringed Oracle's copyrights by the use of Java in Android devices.[72] District Judge William Haskell Alsup ruled on May 31, 2012, that APIs cannot be copyrighted,[73] but this was reversed by the United States Court of Appeals for the Federal Circuit in May 2014.[74] On May 26, 2016, the district court decided in favor of Google, ruling the copyright infringement of the Java API in Android constitutes fair use.[75]



Data exchange - Wikipedia
Data exchange is the process of taking data structured under a source schema and transforming it into data structured under a target schema, so that the target data is an accurate representation of the source data.[1] Data exchange allows data to be shared between different  computer programs.
It is similar to the related concept of data integration except that data is actually restructured (with possible loss of content) in data exchange. There may be no way to transform an instance given all of the constraints. Conversely, there may be numerous ways to transform the instance (possibly infinitely many), in which case a "best" choice of solutions has to be identified and justified.
In some domains, a few dozen different source and target schema (proprietary data formats) may exist. An "exchange" or "interchange format" is often developed for a single domain, and then necessary routines (mappings) are written to (indirectly) transform/translate each and every source schema to each and every target schema by using the interchange format as an intermediate step.[2] That requires a lot less work than writing and debugging the hundreds of different routines that would be required to directly translate each and every source schema directly to each and every target schema.
Examples of these transformative interchange formats include:
A data interchange (or exchange) language/format is a language that is domain-independent and can be used for data from any kind of discipline.[9] They have "evolved from being markup and display-oriented to further support the encoding of metadata that describes the structural attributes of the information."[10]
Practice has shown that certain types of formal languages are better suited for this task than others, since their specification is driven by a formal process instead of  particular software implementation needs. For example, XML is a markup language that was designed to enable the creation of dialects (the definition of domain-specific sublanguages).[11] However, it does not contain domain-specific dictionaries or fact types. Beneficial to a reliable data exchange is the availability of standard dictionaries-taxonomies and tools libraries such as parsers, schema validators, and transformation tools.[citation needed]
The following is a partial list of popular generic languages used for data exchange in multiple domains.
Nomenclature
Notes:
The popularity of XML for data exchange on the World Wide Web has several reasons. First of all, it is closely related to the preexisting standards Standard Generalized Markup Language (SGML) and Hypertext Markup Language (HTML), and as such a parser written to support these two languages can be easily extended to support XML as well. For example, XHTML has been defined as a format that is formal XML, but understood correctly by most (if not all) HTML parsers.[11]
YAML is a language that was designed to be human-readable (and as such to be easy to edit with any standard text editor). Its notion often is similar to reStructuredText or a Wiki syntax, who also try to be readable both by humans and computers. YAML 1.2 also includes a shorthand notion that is compatible with JSON, and as such any JSON document is also valid YAML; this however does not hold the other way.[13]
REBOL is a language that was designed to be human-readable and easy to edit using any standard text editor. To achieve that it uses a simple free-form syntax with minimal punctuation and a rich set of datatypes. REBOL datatypes like URLs, emails, date and time values, tuples, strings, tags, etc. respect the common standards. REBOL is designed to not need any additional meta-language, being designed in a metacircular fashion. The metacircularity of the language is the reason why, e.g., the Parse dialect used (not exclusively) for definitions and transformations of REBOL dialects is also itself a dialect of REBOL.[14] REBOL was used as a source of inspiration for JSON.[15]
Gellish English is a formalized subset of natural English, which includes a simple grammar and a large extensible English Dictionary-Taxonomy that defines the general and domain specific terminology (terms for concepts), whereas the concepts are arranged in a subtype-supertype hierarchy (a taxonomy), which supports inheritance of knowledge and requirements. The Dictionary-Taxonomy also includes standardized fact types (also called relation types). The terms and relation types together can be used to create and interpret expressions of facts, knowledge, requirements and other information. Gellish can be used in combination with SQL, RDF/XML, OWL and various other meta-languages. The Gellish standard is a combination of ISO 10303-221 (AP221) and ISO 15926.[16]



Thread (computing) - Wikipedia
In computer science, a thread of execution is the smallest sequence of programmed instructions that can be managed independently by a scheduler, which is typically a part of the operating system.[1] The implementation of threads and processes differs between operating systems, but in most cases a thread is a component of a process. Multiple threads can exist within one process, executing concurrently and sharing resources such as memory, while different processes do not share these resources. In particular, the threads of a process share its executable code and the values of its variables at any given time.
Systems with a single processor generally implement multithreading by time slicing: the central processing unit (CPU) switches between different software threads. This context switching generally happens very often and rapidly enough that users perceive the threads or tasks as running in parallel. On a multiprocessor or multi-core system, multiple threads can execute in parallel, with every processor or core executing a separate thread simultaneously; on a processor or core with hardware threads, separate software threads can also be executed concurrently by separate hardware threads.
Threads made an early appearance under the name of "tasks" in OS/360 Multiprogramming with a Variable Number of Tasks (MVT) in 1967. Saltzer (1966) credits Victor A. Vyssotsky with the term "thread".[2] The process schedulers of many modern operating systems directly support both time-sliced and multiprocessor threading, and the operating system kernel allows programmers to manipulate threads by exposing required functionality through the  system-call interface. Some threading implementations are called kernel threads, whereas light-weight processes (LWP) are a specific type of kernel thread that share the same state and information. Furthermore, programs can have user-space threads when threading with timers, signals, or other methods to interrupt their own execution, performing a sort of ad hoc time-slicing.
Threads differ from traditional multitasking operating system processes in that:
Systems such as Windows NT and OS/2 are said to have cheap threads and expensive processes; in other operating systems there is not so great a difference except the cost of an address space switch which on some architectures (notably x86) results in a translation lookaside buffer (TLB) flush.
In computer programming, single-threading is the processing of one command at a time.[3] The opposite of single-threading is multithreading.[4]
In the formal analysis of the variables' semantics and process state the term single threading can be used differently to mean "backtracking within a single thread", which is common in the functional programming community.[5]
Multithreading is mainly found in multitasking operating systems. Multithreading is a widespread programming and execution model that allows multiple threads to exist within the context of one process. These threads share the process's resources, but are able to execute independently. The threaded programming model provides developers with a useful abstraction of concurrent execution. Multithreading can also be applied to one process to enable parallel execution on a multiprocessing system.
Multithreaded applications have the following advantages:
Multithreading has the following drawbacks:
Operating systems schedule threads either preemptively or cooperatively. On multi-user operating systems, preemptive multithreading is the more widely used approach for its finer grained control over execution time via context switching. However, preemptive scheduling may context switch threads at moments unanticipated by programmers therefore causing lock convoy, priority inversion, or other side-effects. In contrast, cooperative multithreading relies on threads to relinquish control of execution thus ensuring that threads run to completion . This can create problems if a cooperatively multitasked thread blocks by waiting on a resource or if it starves other threads by not yielding control of execution during intensive computation.
Until the early 2000s, most desktop computers had only one single-core CPU, with no support for hardware threads, although threads were still used on such computers because switching between threads was generally still quicker than full-process context switches. In 2002, Intel added support for simultaneous multithreading to the Pentium 4 processor, under the name hyper-threading; in 2005, they introduced the dual-core Pentium D processor and AMD introduced the dual-core Athlon 64 X2 processor.
Processors in embedded systems, which have higher requirements for real-time behaviors, might support multithreading by decreasing the thread-switch time, perhaps by allocating a dedicated register file for each thread instead of saving/restoring a common register file.
Scheduling can be done at the kernel level or user level, and multitasking can be done preemptively or cooperatively. This yields a variety of related concepts.
At the kernel level, a process contains one or more kernel threads, which share the process's resources, such as memory and file handles – a process is a unit of resources, while a thread is a unit of scheduling and execution. Kernel scheduling is typically uniformly done preemptively or, less commonly, cooperatively. At the user level a process such as a runtime system can itself schedule multiple threads of execution. If these do not share data, as in Erlang, they are usually analogously called processes,[7] while if they share data they are usually called (user) threads, particularly if preemptively scheduled. Cooperatively scheduled user threads are known as fibers; different processes may schedule user threads differently. User threads may be executed by kernel threads in various ways (one-to-one, many-to-one, many-to-many). The term "light-weight process" variously refers to user threads or to kernel mechanisms for scheduling user threads onto kernel threads.
A process is a "heavyweight" unit of kernel scheduling, as creating, destroying, and switching processes is relatively expensive. Processes own resources allocated by the operating system. Resources include memory (for both code and data), file handles, sockets, device handles, windows, and a process control block. Processes are isolated by process isolation, and do not share address spaces or file resources except through explicit methods such as inheriting file handles or shared memory segments, or mapping the same file in a shared way – see interprocess communication. Creating or destroying a process is relatively expensive, as resources must be acquired or released. Processes are typically preemptively multitasked, and process switching is relatively expensive, beyond basic cost of context switching, due to issues such as cache flushing.[a]
A kernel thread is a "lightweight" unit of kernel scheduling. At least one kernel thread exists within each process. If multiple kernel threads exist within a process, then they share the same memory and file resources. Kernel threads are preemptively multitasked if the operating system's process scheduler is preemptive. Kernel threads do not own resources except for a stack, a copy of the registers including the program counter, and thread-local storage (if any), and are thus relatively cheap to create and destroy. Thread switching is also relatively cheap: it requires a context switch (saving and restoring registers and stack pointer), but does not change virtual memory and is thus cache-friendly (leaving TLB valid). The kernel can assign one thread to each logical core in a system (because each processor splits itself up into multiple logical cores if it supports multithreading, or only supports one logical core per physical core if it does not), and can swap out threads that get blocked. However, kernel threads take much longer than user threads to be swapped.
Threads are sometimes implemented in userspace libraries, thus called user threads. The kernel is unaware of them, so they are managed and scheduled in userspace. Some implementations base their user threads on top of several kernel threads, to benefit from multi-processor machines (M:N model). In this article the term "thread" (without kernel or user qualifier) defaults to referring to kernel threads. User threads as implemented by virtual machines are also called green threads. User threads are generally fast to create and manage, but cannot take advantage of multithreading or multiprocessing, and will get blocked if all of their associated kernel threads get blocked even if there are some user threads that are ready to run.
Fibers are an even lighter unit of scheduling which are cooperatively scheduled: a running fiber must explicitly "yield" to allow another fiber to run, which makes their implementation much easier than kernel or user threads. A fiber can be scheduled to run in any thread in the same process. This permits applications to gain performance improvements by managing scheduling themselves, instead of relying on the kernel scheduler (which may not be tuned for the application). Parallel programming environments such as OpenMP typically implement their tasks through fibers. Closely related to fibers are coroutines, with the distinction being that coroutines are a language-level construct, while fibers are a system-level construct.
Threads in the same process share the same address space. This allows concurrently running code to couple tightly and conveniently exchange data without the overhead or complexity of an IPC. When shared between threads, however, even simple data structures become prone to race conditions if they require more than one CPU instruction to update: two threads may end up attempting to update the data structure at the same time and find it unexpectedly changing underfoot. Bugs caused by race conditions can be very difficult to reproduce and isolate.
To prevent this, threading application programming interfaces (APIs) offer synchronization primitives such as mutexes to lock data structures against concurrent access. On uniprocessor systems, a thread running into a locked mutex must sleep and hence trigger a context switch. On multi-processor systems, the thread may instead poll the mutex in a spinlock. Both of these may sap performance and force processors in symmetric multiprocessing (SMP) systems to contend for the memory bus, especially if the granularity of the locking is fine.
Although threads seem to be a small step from sequential computation, in fact, they represent a huge step. They discard the most essential and appealing properties of sequential computation: understandability, predictability, and determinism. Threads, as a model of computation, are wildly non-deterministic, and the job of the programmer becomes one of pruning that nondeterminism. User thread or fiber implementations are typically entirely in userspace. As a result, context switching between user threads or fibers within the same process is extremely efficient because it does not require any interaction with the kernel at all: a context switch can be performed by locally saving the CPU registers used by the currently executing user thread or fiber and then loading the registers required by the user thread or fiber to be executed. Since scheduling occurs in userspace, the scheduling policy can be more easily tailored to the requirements of the program's workload.
However, the use of blocking system calls in user threads (as opposed to kernel threads) or fibers can be problematic. If a user thread or a fiber performs a system call that blocks, the other user threads and fibers in the process are unable to run until the system call returns. A typical example of this problem is when performing I/O: most programs are written to perform I/O synchronously. When an I/O operation is initiated, a system call is made, and does not return until the I/O operation has been completed. In the intervening period, the entire process is "blocked" by the kernel and cannot run, which starves other user threads and fibers in the same process from executing.
A common solution to this problem is providing an I/O API that implements a synchronous interface by using non-blocking I/O internally, and scheduling another user thread or fiber while the I/O operation is in progress. Similar solutions can be provided for other blocking system calls. Alternatively, the program can be written to avoid the use of synchronous I/O or other blocking system calls.
SunOS 4.x implemented light-weight processes or LWPs. NetBSD 2.x+, and DragonFly BSD implement LWPs as kernel threads (1:1 model). SunOS 5.2 through SunOS 5.8 as well as NetBSD 2 to NetBSD 4 implemented a two level model, multiplexing one or more user level threads on each kernel thread (M:N model). SunOS 5.9 and later, as well as NetBSD 5 eliminated user threads support, returning to a 1:1 model.[9] FreeBSD 5 implemented M:N model. FreeBSD 6 supported both 1:1 and M:N, users could choose which one should be used with a given program using /etc/libmap.conf. Starting with FreeBSD 7, the 1:1 became the default. FreeBSD 8 no longer supports the M:N model.
The use of kernel threads simplifies user code by moving some of the most complex aspects of threading into the kernel. The program does not need to schedule threads or explicitly yield the processor. User code can be written in a familiar procedural style, including calls to blocking APIs, without starving other threads. However, kernel threading may force a context switch between threads at any time, and thus expose race hazards and concurrency bugs that would otherwise lie latent. On SMP systems, this is further exacerbated because kernel threads may literally execute on separate processors in parallel.
Threads created by the user in a 1:1 correspondence with schedulable entities in the kernel[10] are the simplest possible threading implementation. OS/2 and Win32 used this approach from the start, while on Linux the usual C library implements this approach (via the NPTL or older LinuxThreads). This approach is also used by Solaris, NetBSD, FreeBSD, macOS, and iOS.
An N:1 model implies that all application-level threads map to one kernel-level scheduled entity;[10] the kernel has no knowledge of the application threads. With this approach, context switching can be done very quickly and, in addition, it can be implemented even on simple kernels which do not support threading. One of the major drawbacks, however, is that it cannot benefit from the hardware acceleration on multithreaded processors or multi-processor computers: there is never more than one thread being scheduled at the same time.[10] For example: If one of the threads needs to execute an I/O request, the whole process is blocked and the threading advantage cannot be used. The GNU Portable Threads uses User-level threading, as does State Threads.
M:N maps some M number of application threads onto some N number of kernel entities,[10] or "virtual processors." This is a compromise between kernel-level ("1:1") and user-level ("N:1") threading. In general, "M:N" threading systems are more complex to implement than either kernel or user threads, because changes to both kernel and user-space code are required. In the M:N implementation, the threading library is responsible for scheduling user threads on the available schedulable entities; this makes context switching of threads very fast, as it avoids system calls. However, this increases complexity and the likelihood of priority inversion, as well as suboptimal scheduling without extensive (and expensive) coordination between the userland scheduler and the kernel scheduler.
Fibers can be implemented without operating system support, although some operating systems or libraries provide explicit support for them.
IBM PL/I(F) included support for multithreading (called multitasking) in the late 1960s, and this was continued in the Optimizing Compiler and later versions. The IBM Enterprise PL/I compiler introduced a new model "thread" API. Neither version was part of the PL/I standard.
Many programming languages support threading in some capacity. Many implementations of C and C++ support threading, and provide access to the native threading APIs of the operating system. Some higher level (and usually cross-platform) programming languages, such as Java, Python, and .NET Framework languages, expose threading to developers while abstracting the platform specific differences in threading implementations in the runtime. Several other programming languages and language extensions also try to abstract the concept of concurrency and threading from the developer fully (Cilk, OpenMP, Message Passing Interface (MPI)). Some languages are designed for sequential parallelism instead (especially using GPUs), without requiring concurrency or threads (Ateji PX, CUDA).
A few interpreted programming languages have implementations (e.g., Ruby MRI for Ruby, CPython for Python) which support threading and concurrency but not parallel execution of threads, due to a global interpreter lock (GIL). The GIL is a mutual exclusion lock held by the interpreter that can prevent the interpreter from simultaneously interpreting the applications code on two or more threads at once, which effectively limits the parallelism on multiple core systems. This limits performance mostly for processor-bound threads, which require the processor, and not much for I/O-bound or network-bound ones.
Other implementations of interpreted programming languages, such as Tcl using the Thread extension, avoid the GIL limit by using an Apartment model where data and code must be explicitly "shared" between threads. In Tcl each thread has one or more interpreters.
Event-driven programming hardware description languages such as Verilog have a different threading model that supports extremely large numbers of threads (for modeling hardware).
A standardized interface for thread implementation is POSIX Threads (Pthreads), which is a set of C-function library calls. OS vendors are free to implement the interface as desired, but the application developer should be able to use the same interface across multiple platforms. Most Unix platforms including Linux support Pthreads. Microsoft Windows has its own set of thread functions in the process.h interface for multithreading, like beginthread. Java provides yet another standardized interface over the host operating system using the Java concurrency library java.util.concurrent.
Multithreading libraries provide a function call to create a new thread, which takes a function as a parameter. A concurrent thread is then created which starts running the passed function and ends when the function returns. The thread libraries also offer synchronization functions which make it possible to implement race condition-error free multithreading functions using mutexes, condition variables, critical sections, semaphores, monitors and other synchronization primitives.
Another paradigm of thread usage is that of thread pools where a set number of threads are created at startup that then wait for a task to be assigned. When a new task arrives, it wakes up, completes the task and goes back to waiting. This avoids the relatively expensive thread creation and destruction functions for every task performed and takes thread management out of the application developer's hand and leaves it to a library or the operating system that is better suited to optimize thread management. For example, frameworks like Grand Central Dispatch and Threading Building Blocks.
In programming models such as CUDA designed for data parallel computation, an array of threads run the same code in parallel using only its ID to find its data in memory. In essence, the application must be designed so that each thread performs the same operation on different segments of memory so that they can operate in parallel and use the GPU architecture.



Programming paradigm - Wikipedia
Programming paradigms are a way to classify programming languages based on their features. Languages can be classified into multiple paradigms.
Some paradigms are concerned mainly with implications for the execution model of the language, such as allowing side effects, or whether the sequence of operations is defined by the execution model.  Other paradigms are concerned mainly with the way that code is organized, such as grouping a code into units along with the state that is modified by the code.  Yet others are concerned mainly with the style of syntax and grammar.
Common programming paradigms include:[1][2][3]
Symbolic techniques such as reflection, which allow the program to refer to itself, might also be considered as a programming paradigm. However, this is compatible with the major paradigms and thus is not a real paradigm in its own right.
For example, languages that fall into the imperative paradigm have two main features: they state the order in which operations occur, with constructs that explicitly control that order, and they allow side effects, in which state can be modified at one point in time, within one unit of code, and then later read at a different point in time inside a different unit of code.  The communication between the units of code is not explicit.  Meanwhile, in object-oriented programming, code is organized into objects that contain state that is only modified by the code that is part of the object.  Most object-oriented languages are also imperative languages.  In contrast, languages that fit the declarative paradigm do not state the order in which to execute operations.  Instead, they supply a number of operations that are available in the system, along with the conditions under which each is allowed to execute.  The implementation of the language's execution model tracks which operations are free to execute and chooses the order on its own. More at  Comparison of multi-paradigm programming languages.
Just as software engineering (as a process) is defined by differing methodologies, so the programming languages (as models of computation) are defined by differing paradigms. Some languages are designed to support one paradigm (Smalltalk supports object-oriented programming, Haskell supports functional programming), while other programming languages support multiple paradigms (such as Object Pascal, C++, Java, C#, Scala, Visual Basic, Common Lisp, Scheme, Perl, PHP, Python, Ruby, Oz, and F#). For example, programs written in C++, Object Pascal or PHP can be purely procedural, purely object-oriented, or can contain elements of both or other paradigms. Software designers and programmers decide how to use those paradigm elements.
In object-oriented programming, programs are treated as a set of interacting objects. In functional programming, programs are treated as a sequence of stateless function evaluations. When programming computers or systems with many processors, in process-oriented programming, programs are treated as sets of concurrent processes acting on logically shared data structures.
Many programming paradigms are as well known for the techniques they forbid as for those they enable. For instance, pure functional programming disallows use of side-effects, while structured programming disallows use of the goto statement. Partly for this reason, new paradigms are often regarded as doctrinaire or overly rigid by those accustomed to earlier styles.[6] Yet, avoiding certain techniques can make it easier to understand program behavior, and to prove theorems about program correctness.
Programming paradigms can also be compared with programming models which allow invoking an execution model by using only an API. Programming models can also be classified into paradigms, based on features of the execution model.
For parallel computing, using a programming model instead of a language is common.  The reason is that details of the parallel hardware leak into the abstractions used to program the hardware.  This causes the programmer to have to map patterns in the algorithm onto patterns in the execution model (which have been inserted due to leakage of hardware into the abstraction).  As a consequence, no one parallel programming language maps well to all computation problems.  It is thus more convenient to use a base sequential language and insert API calls to parallel execution models, via a programming model.  Such parallel programming models can be classified according to abstractions that reflect the hardware, such as shared memory, distributed memory with message passing, notions of place visible in the code, and so forth.  These can be considered flavors of programming paradigm that apply to only parallel languages and programming models.
Some programming language researchers criticise the notion of paradigms as a classification of programming languages, e.g. Harper,[7] and Krishnamurthi.[8]  They argue that many programming languages cannot be strictly classified into one paradigm, but rather include features from several paradigms. See Comparison of multi-paradigm programming languages.
Different approaches to programming have developed over time, being identified as such either at the time or retrospectively. An early approach consciously identified as such is structured programming, advocated since the mid 1960s. The concept of a "programming paradigm" as such dates at least to 1978, in the Turing Award lecture of Robert W. Floyd, entitled The Paradigms of Programming, which cites the notion of paradigm as used by Thomas Kuhn in his The Structure of Scientific Revolutions (1962).[9]
The lowest-level programming paradigms are machine code, which directly represents the instructions (the contents of program memory) as a sequence of numbers, and assembly language where the machine instructions are represented by mnemonics and memory addresses can be given symbolic labels. These are sometimes called first- and second-generation languages.
In the 1960s, assembly languages were developed to support library COPY and quite sophisticated conditional macro generation and preprocessing abilities, CALL to (subroutines), external variables and common sections (globals), enabling significant code re-use and isolation from hardware specifics via use of logical operators such as READ/WRITE/GET/PUT. Assembly was, and still is, used for time critical systems and often in embedded systems as it gives the most direct control of what the machine does.
The next advance was the development of procedural languages. These third-generation languages (the first described as high-level languages) use vocabulary related to the problem being solved. For example,
All these languages follow the procedural paradigm. That is, they describe, step by step, exactly the procedure that should, according to the particular programmer at least, be followed to solve a specific problem. The efficacy and efficiency of any such solution are both therefore entirely subjective and highly dependent on that programmer's experience, inventiveness, and ability.
Following the widespread use of procedural languages, object-oriented programming (OOP) languages were created, such as Simula, Smalltalk, C++, C#, Eiffel, PHP, and Java. In these languages, data and methods to manipulate it are kept as one unit called an object.  With perfect encapsulation, one of the distinguishing features of OOP, the only way that another object or user would be able to access the data is via the object's methods. Thus, the inner workings of an object may be changed without affecting any code that uses the object. There is still some controversy raised by Alexander Stepanov, Richard Stallman[10] and other programmers, concerning the efficacy of the OOP paradigm versus the procedural paradigm. The need for every object to have associative methods leads some skeptics to associate OOP with software bloat; an attempt to resolve this dilemma came through polymorphism.
Because object-oriented programming is considered a paradigm, not a language, it is possible to create even an object-oriented assembler language. High Level Assembly (HLA) is an example of this that fully supports advanced data types and object-oriented assembly language programming –  despite its early origins. Thus, differing programming paradigms can be seen rather like motivational memes of their advocates, rather than necessarily representing progress from one level to the next[citation needed]. Precise comparisons of the efficacy of competing paradigms are frequently made more difficult because of new and differing terminology applied to similar entities and processes together with numerous implementation distinctions across languages.
Literate programming, as   a form of imperative programming, structures programs as a human-centered web, as in a hypertext essay: documentation is integral to the program, and the program is structured following the logic of prose exposition, rather than compiler convenience.
Independent of the imperative branch, declarative programming paradigms were developed. In these languages, the computer is told what the problem is, not how to solve the problem –  the program is structured as a set of properties to find in the expected result, not as a procedure to follow. Given a database or a set of rules, the computer tries to find a solution matching all the desired properties. An archetype of a declarative language is the fourth generation language SQL, and the family of functional languages and logic programming.
Functional programming is a subset of declarative programming. Programs written using this paradigm use functions, blocks of code intended to behave like mathematical functions. Functional languages discourage changes in the value of variables through assignment, making a great deal of use of recursion instead.
The logic programming paradigm views computation as automated reasoning over a body of knowledge. Facts about the problem domain are expressed as logic formulae, and programs are executed by applying inference rules over them until an answer to the problem is found, or the set of formulae is proved inconsistent.
Symbolic programming is a paradigm that describes programs able to manipulate formulas and program components as data.[3]  Programs can thus effectively modify themselves, and appear to "learn", making them suited for applications such as artificial intelligence, expert systems, natural-language processing and computer games.  Languages that support this paradigm include Lisp and Prolog.[11]
Most programming languages support more than one programming paradigm to allow programmers to use the most suitable programming style and associated language constructs for a given job.[12]



English language - Wikipedia


English is a West Germanic language that was first spoken in early medieval England and is now a global lingua franca.[4][5] Named after the Angles, one of the Germanic tribes that migrated to the area of Britain that would later take their name, England, both names ultimately deriving from the Anglia peninsula in the Baltic Sea. It is closely related to the Frisian languages, but its vocabulary has been significantly influenced by other Germanic languages, particularly Norse (a North Germanic language), as well as by Latin and French.[6]
English has developed over the course of more than 1,400 years. The earliest forms of English, a set of Anglo-Frisian dialects brought to Great Britain by Anglo-Saxon settlers in the 5th century, are called Old English. Middle English began in the late 11th century with the Norman conquest of England and was a period in which the language was influenced by French.[7] Early Modern English began in the late 15th century with the introduction of the printing press to London, the printing of the King James Bible and the start of the Great Vowel Shift.[8]
Through the worldwide influence of the British Empire, Modern English spread around the world from the 17th to mid-20th centuries. Through all types of printed and electronic media, and spurred by the emergence of the United States as a global superpower, English has become the leading language of international discourse and the lingua franca in many regions and professional contexts such as science, navigation and law.[9]
English is the third most spoken native language in the world, after Standard Chinese and Spanish.[10] It is the most widely learned second language and is either the official language or one of the official languages in almost 60 sovereign states. There are more people who have learned it as a second language than there are native speakers. English is the most commonly spoken language in the United Kingdom, the United States, Canada, Australia, Ireland and New Zealand, and it is widely spoken in some areas of the Caribbean, Africa and South Asia.[11] It is a co-official language of the United Nations, the European Union and many other world and regional international organisations. It is the most widely spoken Germanic language, accounting for at least 70% of speakers of this Indo-European branch. English has a vast vocabulary, though counting how many words any language has is impossible.[12][13] English speakers are called "Anglophones".
Modern English grammar is the result of a gradual change from a typical Indo-European dependent marking pattern with a rich inflectional morphology and relatively free word order to a mostly analytic pattern with little inflection, a fairly fixed SVO word order and a complex syntax.[14] Modern English relies more on auxiliary verbs and word order for the expression of complex tenses, aspect and mood, as well as passive constructions, interrogatives and some negation. Despite noticeable variation among the accents and dialects of English used in different countries and regions—in terms of phonetics and phonology, and sometimes also vocabulary, grammar and spelling—English-speakers from around the world are able to communicate with one another with relative ease.
English is an Indo-European language and belongs to the West Germanic group of the Germanic languages.[15] Old English originated from a Germanic tribal and linguistic continuum along the coast of the North Sea, whose languages are now known as the Anglo-Frisian subgroup within West Germanic. As such, the modern Frisian languages are the closest living relatives of Modern English. Low German/Low Saxon is also closely related, and sometimes English, the Frisian languages, and Low German are grouped together as the Ingvaeonic (North Sea Germanic) languages, though this grouping remains debated.[16] Old English evolved into Middle English, which in turn evolved into Modern English.[17] Particular dialects of Old and Middle English also developed into a number of other Anglic languages, including Scots[18] and the extinct Fingallian and Forth and Bargy (Yola) dialects of Ireland.[19]
Like Icelandic and Faroese, the development of English on the British Isles isolated it from the continental Germanic languages and influences, and has since undergone substantial evolution. English is thus not mutually intelligible with any continental Germanic language, differing in vocabulary, syntax, and phonology, although some, such as Dutch or Frisian, do show strong affinities with English, especially with its earlier stages.[20]
Unlike Icelandic or Faroese, the long history of invasions of the British Isles by other peoples and languages, particularly Old Norse and Norman French, left a profound mark of their own on the language, such that English shares substantial vocabulary and grammar similarities with many languages outside its linguistic clades, while also being unintelligible with any of those languages. Some scholars have even argued that English can be considered a mixed language or a creole—a theory called the Middle English creole hypothesis. Although the high degree of influence from these languages on the vocabulary and grammar of Modern English is widely acknowledged, most specialists in language contact do not consider English to be a true mixed language.[21][22]
English is classified as a Germanic language because it shares innovations with other Germanic languages such as Dutch, German, and Swedish.[23] These shared innovations show that the languages have descended from a single common ancestor called Proto-Germanic. Some shared features of Germanic languages include the use of modal verbs, the division of verbs into strong and weak classes, and the sound changes affecting Proto-Indo-European consonants, known as Grimm's and Verner's laws. English is classified as an Anglo-Frisian language because Frisian and English share other features, such as the palatalisation of consonants that were velar consonants in Proto-Germanic (see Phonological history of Old English § Palatalization).[24]
The earliest form of English is called Old English or Anglo-Saxon (c. 550–1066 CE). Old English developed from a set of North Sea Germanic dialects originally spoken along the coasts of Frisia, Lower Saxony, Jutland, and Southern Sweden by Germanic tribes known as the Angles, Saxons, and Jutes. In the fifth century, the Anglo-Saxons settled Britain as the Roman economy and administration collapsed. By the seventh century, the Germanic language of the Anglo-Saxons became dominant in Britain, replacing the languages of Roman Britain (43–409 CE): Common Brittonic, a Celtic language, and Latin, brought to Britain by the Roman occupation.[25][26][27] England and English (originally Ænglaland and Ænglisc) are named after the Angles.[28]
Old English was divided into four dialects: the Anglian dialects, Mercian and Northumbrian, and the Saxon dialects, Kentish and West Saxon.[29] Through the educational reforms of King Alfred in the ninth century and the influence of the kingdom of Wessex, the West Saxon dialect became the standard written variety.[30] The epic poem Beowulf is written in West Saxon, and the earliest English poem, Cædmon's Hymn, is written in Northumbrian.[31] Modern English developed mainly from Mercian, but the Scots language developed from Northumbrian. A few short inscriptions from the early period of Old English were written using a runic script.[32] By the sixth century, a Latin alphabet was adopted, written with half-uncial letterforms. It included the runic letters wynn ⟨ƿ⟩ and thorn ⟨þ⟩, and the modified Latin letters eth ⟨ð⟩, and ash ⟨æ⟩.[32][33]
Old English is very different from Modern English and difficult for 21st-century English speakers to understand. Its grammar was similar to that of modern German, and its closest relative is Old Frisian. Nouns, adjectives, pronouns, and verbs had many more inflectional endings and forms, and word order was much freer than in Modern English. Modern English has case forms in pronouns (he, him, his) and a few verb endings (I have, he has), but Old English had case endings in nouns as well, and verbs had more person and number endings.[34][35][36]
The translation of Matthew 8:20 from 1000 CE shows examples of case endings (nominative plural, accusative plural, genitive singular) and a verb ending (present plural):
John of Trevisa, ca. 1385[38]
In the period from the 8th to the 12th century, Old English gradually transformed through language contact into Middle English. Middle English is often arbitrarily defined as beginning with the conquest of England by William the Conqueror in 1066, but it developed further in the period from 1200–1450.
First, the waves of Norse colonisation of northern parts of the British Isles in the 8th and 9th centuries put Old English into intense contact with Old Norse, a North Germanic language. Norse influence was strongest in the Northeastern varieties of Old English spoken in the Danelaw area around York, which was the centre of Norse colonisation; today these features are still particularly present in Scots and Northern English. However the centre of norsified English seems to have been in the Midlands around Lindsey, and after 920 CE when Lindsey was reincorporated into the Anglo-Saxon polity, Norse features spread from there into English varieties that had not been in intense contact with Norse speakers. Some elements of Norse influence that persist in all English varieties today are the pronouns beginning with th- (they, them, their) which replaced the Anglo-Saxon pronouns with h- (hie, him, hera).[39]
With the Norman conquest of England in 1066, the now norsified Old English language was subject to contact with the Old Norman language, a Romance language closely related to Modern French. The Norman language in England eventually developed into Anglo-Norman. Because Norman was spoken primarily by the elites and nobles, while the lower classes continued speaking Anglo-Saxon, the influence of Norman consisted of introducing a wide range of loanwords related to politics, legislation and prestigious social domains.[40] Middle English also greatly simplified the inflectional system, probably in order to reconcile Old Norse and Old English, which were inflectionally different but morphologically similar. The distinction between nominative and accusative case was lost except in personal pronouns, the instrumental case was dropped, and the use of the genitive case was limited to describing possession. The inflectional system regularised many irregular inflectional forms,[41] and gradually simplified the system of agreement, making word order less flexible.[42] By the Wycliffe Bible of the 1380s, the passage Matthew 8:20 was written
Here the plural suffix -n on the verb have is still retained, but none of the case endings on the nouns are present.
By the 12th century Middle English was fully developed, integrating both Norse and Norman features; it continued to be spoken until the transition to early Modern English around 1500. Middle English literature includes Geoffrey Chaucer's The Canterbury Tales, and Malory's Le Morte d'Arthur. In the Middle English period, the use of regional dialects in writing proliferated, and dialect traits were even used for effect by authors such as Chaucer.
The next period in the history of English was Early Modern English (1500–1700). Early Modern English was characterised by the Great Vowel Shift (1350–1700), inflectional simplification, and linguistic standardisation.
The Great Vowel Shift affected the stressed long vowels of Middle English. It was a chain shift, meaning that each shift triggered a subsequent shift in the vowel system. Mid and open vowels were raised, and close vowels were broken into diphthongs. For example, the word bite was originally pronounced as the word beet is today, and the second vowel in the word about was pronounced as the word boot is today. The Great Vowel Shift explains many irregularities in spelling since English retains many spellings from Middle English, and it also explains why English vowel letters have very different pronunciations from the same letters in other languages.[44][45]
English began to rise in prestige, relative to Norman French, during the reign of Henry V. Around 1430, the Court of Chancery in Westminster began using English in its official documents, and a new standard form of Middle English, known as Chancery Standard, developed from the dialects of London and the East Midlands. In 1476, William Caxton introduced the printing press to England and began publishing the first printed books in London, expanding the influence of this form of English.[46] Literature from the Early Modern period includes the works of William Shakespeare and the translation of the Bible commissioned by King James I. Even after the vowel shift the language still sounded different from Modern English: for example, the consonant clusters /kn ɡn sw/ in knight, gnat, and sword were still pronounced. Many of the grammatical features that a modern reader of Shakespeare might find quaint or archaic represent the distinct characteristics of Early Modern English.[47]
In the 1611 King James Version of the Bible, written in Early Modern English, Matthew 8:20 says:
This exemplifies the loss of case and its effects on sentence structure (replacement with Subject-Verb-Object word order, and the use of of instead of the non-possessive genitive), and the introduction of loanwords from French (ayre) and word replacements (bird originally meaning "nestling" had replaced OE fugol).
By the late 18th century, the British Empire had facilitated the spread of English through its colonies and geopolitical dominance. Commerce, science and technology, diplomacy, art, and formal education all contributed to English becoming the first truly global language. English also facilitated worldwide international communication.[48][9] As England continued to form new colonies, these, in turn, became independent and developed their own norms for how to speak and write the language. English was adopted in North America, India, parts of Africa, Australasia, and many other regions. In the post-colonial period, some of the newly created nations that had multiple indigenous languages opted to continue using English as the official language to avoid the political difficulties inherent in promoting any one indigenous language above the others.[49][50][51] In the 20th century the growing economic and cultural influence of the United States and its status as a superpower following the Second World War has, along with worldwide broadcasting in English by the BBC[52] and other broadcasters, significantly accelerated the spread of the language across the planet.[53][54] By the 21st century, English was more widely spoken and written than any language has ever been.[55]
A major feature in the early development of Modern English was the codification of explicit norms for standard usage, and their dissemination through official media such as public education and state-sponsored publications. In 1755 Samuel Johnson published his A Dictionary of the English Language which introduced a standard set of spelling conventions and usage norms. In 1828, Noah Webster published the American Dictionary of the English language in an effort to establish a norm for speaking and writing American English that was independent from the British standard. Within Britain, non-standard or lower class dialect features were increasingly stigmatised, leading to the quick spread of the prestige varieties among the middle classes.[56]
In terms of grammatical evolution, Modern English has now reached a stage where the loss of case is almost complete (case is now only found in pronouns, such as he and him, she and her, who and whom), and where SVO word-order is mostly fixed.[56] Some changes, such as the use of do-support have become universalised. (Earlier English did not use the word "do" as a general auxiliary as Modern English does; at first it was only used in question constructions where it was not obligatory.[57] Now, do-support with the verb have is becoming increasingly standardised.) The use of progressive forms in -ing, appears to be spreading to new constructions, and forms such as had been being built are becoming more common. Regularisation of irregular forms also slowly continues (e.g. dreamed instead of dreamt), and analytical alternatives to inflectional forms are becoming more common (e.g. more polite instead of politer). British English is also undergoing change under the influence of American English, fuelled by the strong presence of American English in the media and the prestige associated with the US as a world power. [58][59][60]

As of  2016[update], 400 million people spoke English as their first language, and 1.1 billion spoke it as a secondary language.[61] English is probably the third largest language by number of native speakers, after Mandarin and Spanish.[10] However, when combining native and non-native speakers it may, depending on the estimate used, be the most commonly spoken language in the world.[55][62][63][64] English is spoken by communities on every continent and on oceanic islands in all the major oceans.[65]
The countries in which English is spoken can be grouped into different categories by how English is used in each country. The "inner circle"[66] countries with many native speakers of English share an international standard of written English and jointly influence speech norms of English around the world. English does not belong to just one country, and it does not belong solely to descendants of English settlers. English is an official language of countries populated by few descendants of native speakers of English. It has also become by far the most important language of international communication when people who share no native language meet anywhere in the world.
Braj Kachru distinguishes countries where English is spoken with a three circles model.[66] In his model, the "inner circle" countries are countries with large communities of native speakers of English, "outer circle" countries have small communities of native speakers of English but widespread use of English as a second language in education or broadcasting or for local official purposes, and "expanding circle" countries are countries where many learners learn English as a foreign language. Kachru bases his model on the history of how English spread in different countries, how users acquire English, and the range of uses English has in each country. The three circles change membership over time.[67]
Countries with large communities of native speakers of English (the inner circle) include Britain, the United States, Australia, Canada, Ireland, and New Zealand, where the majority speaks English, and South Africa, where a significant minority speaks English. The countries with the most native English speakers are, in descending order, the United States (at least 231 million),[68] the United Kingdom (60 million),[69][70][71] Canada (19 million),[72] Australia (at least 17 million),[73] South Africa (4.8 million),[74] Ireland (4.2 million), and New Zealand (3.7 million).[75] In these countries, children of native speakers learn English from their parents, and local people who speak other languages or new immigrants learn English to communicate in their neighbourhoods and workplaces.[76] The inner-circle countries provide the base from which English spreads to other countries in the world.[67]
Estimates of the number of English speakers who are second language and foreign-language speakers vary greatly from 470 million to more than 1,000 million depending on how proficiency is defined.[11] Linguist David Crystal estimates that non-native speakers now outnumber native speakers by a ratio of 3 to 1.[62] In Kachru's three-circles model, the "outer circle" countries are countries such as the Philippines,[77] Jamaica,[78] India, Pakistan, Singapore,[79] and Nigeria[80][81] with a much smaller proportion of native speakers of English but much use of English as a second language for education, government, or domestic business,
and where English is routinely used for school instruction and official interactions with the government.[82]
Those countries have millions of native speakers of dialect continua ranging from an English-based creole to a more standard version of English. They have many more speakers of English who acquire English in the process of growing up through day by day use and listening to broadcasting, especially if they attend schools where English is the medium of instruction. Varieties of English learned by speakers who are not native speakers born to English-speaking parents may be influenced, especially in their grammar, by the other languages spoken by those learners.[76] Most of those varieties of English include words little used by native speakers of English in the inner-circle countries,[76] and they may have grammatical and phonological differences from inner-circle varieties as well. The standard English of the inner-circle countries is often taken as a norm for use of English in the outer-circle countries.[76]
In the three-circles model, countries such as Poland, China, Brazil, Germany, Japan, Indonesia, Egypt, and other countries where English is taught as a foreign language make up the "expanding circle".[83] The distinctions between English as a first language, as a second language, and as a foreign language are often debatable and may change in particular countries over time.[82] For example, in the Netherlands and some other countries of Europe, knowledge of English as a second language is nearly universal, with over 80 percent of the population able to use it,[84] and thus English is routinely used to communicate with foreigners and often in higher education. In these countries, although English is not used for government business, its widespread use puts them at the boundary between the "outer circle" and "expanding circle". English is unusual among world languages in how many of its users are not native speakers but speakers of English as a second or foreign language.[85]
Many users of English in the expanding circle use it to communicate with other people from the expanding circle, so that interaction with native speakers of English plays no part in their decision to use English.[86] Non-native varieties of English are widely used for international communication, and speakers of one such variety often encounter features of other varieties.[87] Very often today a conversation in English anywhere in the world may include no native speakers of English at all, even while including speakers from several different countries.[88]
Pie chart showing the percentage of native English speakers living in "inner circle" English-speaking countries. Native speakers are now substantially outnumbered worldwide by second-language speakers of English (not counted in this chart).
English is a pluricentric language, which means that no one national authority sets the standard for use of the language.[89][90][91][92] But English is not a divided language,[93] despite a long-standing joke originally attributed to George Bernard Shaw that the United Kingdom and the United States are "two countries separated by a common language".[94] Spoken English, for example English used in broadcasting, generally follows national pronunciation standards that are also established by custom rather than by regulation. International broadcasters are usually identifiable as coming from one country rather than another through their accents,[95] but newsreader scripts are also composed largely in international standard written English. The norms of standard written English are maintained purely by the consensus of educated English-speakers around the world, without any oversight by any government or international organisation.[96]
American listeners generally readily understand most British broadcasting, and British listeners readily understand most American broadcasting. Most English speakers around the world can understand radio programmes, television programmes, and films from many parts of the English-speaking world.[97] Both standard and non-standard varieties of English can include both formal or informal styles, distinguished by word choice and syntax and use both technical and non-technical registers.[98]
The settlement history of the English-speaking inner circle countries outside Britain helped level dialect distinctions and produce koineised forms of English in South Africa, Australia, and New Zealand.[99] The majority of immigrants to the United States without British ancestry rapidly adopted English after arrival. Now the majority of the United States population are monolingual English speakers,[100][68] although English has been given official status by only 30 of the 50 state governments of the US.[101][102]
English has ceased to be an "English language" in the sense of belonging only to people who are ethnically English.[103][104] Use of English is growing country-by-country internally and for international communication. Most people learn English for practical rather than ideological reasons.[105] Many speakers of English in Africa have become part of an "Afro-Saxon" language community that unites Africans from different countries.[106]
As decolonisation proceeded throughout the British Empire in the 1950s and 1960s, former colonies often did not reject English but rather continued to use it as independent countries setting their own language policies.[50][51][107] For example, the view of the English language among many Indians has gone from associating it with colonialism to associating it with economic progress, and English continues to be an official language of India.[108] English is also widely used in media and literature, and the number of English language books published annually in India is the third largest in the world after the US and UK.[109] However English is rarely spoken as a first language, numbering only around a couple hundred-thousand people, and less than 5% of the population speak fluent English in India.[110][111] David Crystal claimed in 2004 that, combining native and non-native speakers, India now has more people who speak or understand English than any other country in the world,[112] but the number of English speakers in India is very uncertain, with most scholars concluding that the United States still has more speakers of English than India.[113]
Modern English, sometimes described as the first global lingua franca,[53][114] is also regarded as the first world language.[115][116] English is the world's most widely used language in newspaper publishing, book publishing, international telecommunications, scientific publishing, international trade, mass entertainment, and diplomacy.[116] English is, by international treaty, the basis for the required controlled natural languages[117] Seaspeak and Airspeak, used as international languages of seafaring[118] and aviation.[119] English used to have parity with French and German in scientific research, but now it dominates that field.[120] It achieved parity with French as a language of diplomacy at the Treaty of Versailles negotiations in 1919.[121] By the time of the foundation of the United Nations at the end of World War II, English had become pre-eminent [122] and is now the main worldwide language of diplomacy and international relations.[123] It is one of six official languages of the United Nations.[124] Many other worldwide international organisations, including the International Olympic Committee, specify English as a working language or official language of the organisation.
Many regional international organisations such as the European Free Trade Association, Association of Southeast Asian Nations (ASEAN),[54] and Asia-Pacific Economic Cooperation (APEC) set English as their organisation's sole working language even though most members are not countries with a majority of native English speakers. While the European Union (EU) allows member states to designate any of the national languages as an official language of the Union, in practice English is the main working language of EU organisations.[125]
Although in most countries English is not an official language, it is currently the language most often taught as a foreign language.[53][54] In the countries of the EU, English is the most widely spoken foreign language in nineteen of the twenty-five member states where it is not an official language (that is, the countries other than the UK, Ireland and Malta). In a 2012 official Eurobarometer poll, 38 percent of the EU respondents outside the countries where English is an official language said they could speak English well enough to have a conversation in that language. The next most commonly mentioned foreign language, French (which is the most widely known foreign language in the UK and Ireland), could be used in conversation by 12 percent of respondents.[126]
A working knowledge of English has become a requirement in a number of occupations and professions such as medicine[127] and computing. English has become so important in scientific publishing that more than 80 percent of all scientific journal articles indexed by Chemical Abstracts in 1998 were written in English, as were 90 percent of all articles in natural science publications by 1996 and 82 percent of articles in humanities publications by 1995.[128]
Specialised subsets of English arise spontaneously in international communities, for example, among international business people, as an auxiliary language. This has led some scholars to develop the study of English as an auxiliary language. Globish uses a relatively small subset of English vocabulary (about 1500 words with highest use in international business English) in combination with the standard English grammar. Other examples include Simple English.
The increased use of the English language globally has had an effect on other languages, leading to some English words being assimilated into the vocabularies of other languages. This influence of English has led to concerns about language death,[129] and to claims of linguistic imperialism,[130] and has provoked resistance to the spread of English; however the number of speakers continues to increase because many people around the world think that English provides them with opportunities for better employment and improved lives.[131]
Although some scholars mention a possibility of future divergence of English dialects into mutually unintelligible languages, most think a more likely outcome is that English will continue to function as a koineised language in which the standard form unifies speakers from around the world.[132] English is used as the language for wider communication in countries around the world.[133] Thus English has grown in worldwide use much more than any constructed language proposed as an international auxiliary language, including Esperanto.[134][135]
The phonetics and phonology of the English language differ from one dialect to another, usually without interfering with mutual communication. Phonological variation affects the inventory of phonemes (i.e. speech sounds that distinguish meaning), and phonetic variation consists in differences in pronunciation of the phonemes. [136] This overview mainly describes the standard pronunciations of the United Kingdom and the United States: Received Pronunciation (RP) and General American (GA). (See § Dialects, accents, and varieties, below.)
The phonetic symbols used below are from the International Phonetic Alphabet (IPA).[137][138][139]
Most English dialects share the same 24 consonant phonemes. The consonant inventory shown below is valid for Californian American English,[140] and for RP.[141]
* Conventionally transcribed /r/.
In the table, when obstruents (stops, affricates, and fricatives) appear in pairs, such as /p b/, /tʃ dʒ/, and /s z/, the first is fortis (strong) and the second is lenis (weak). Fortis obstruents, such as /p tʃ s/ are pronounced with more muscular tension and breath force than lenis consonants, such as /b dʒ z/, and are always voiceless. Lenis consonants are partly voiced at the beginning and end of utterances, and fully voiced between vowels. Fortis stops such as /p/ have additional articulatory or acoustic features in most dialects: they are aspirated [pʰ] when they occur alone at the beginning of a stressed syllable, often unaspirated in other cases, and often unreleased [p̚] or pre-glottalised [ʔp] at the end of a syllable. In a single-syllable word, a vowel before a fortis stop is shortened: thus nip has a noticeably shorter vowel (phonetically, but not phonemically) than nib [nɪˑb̥] (see below).[142]
In RP, the lateral approximant /l/, has two main allophones (pronunciation variants): the clear or plain [l], as in light, and the dark or velarised [ɫ], as in full.[143] GA has dark l in most cases.[144]
All sonorants (liquids /l, r/ and nasals /m, n, ŋ/) devoice when following a voiceless obstruent, and they are syllabic when following a consonant at the end of a word.[145]
The pronunciation of vowels varies a great deal between dialects and is one of the most detectable aspects of a speaker's accent. The table below lists the vowel phonemes in Received Pronunciation (RP) and General American (GA), with examples of words in which they occur from lexical sets compiled by linguists. The vowels are represented with symbols from the International Phonetic Alphabet; those given for RP are standard in British dictionaries and other publications.[146]
In RP, vowel length is phonemic; long vowels are marked with a triangular colon ⟨ː⟩ in the table above, such as the vowel of need [niːd] as opposed to bid [bɪd]. In GA, vowel length is non-distinctive.
In both RP and GA, vowels are phonetically shortened before fortis consonants in the same syllable, like /t tʃ f/, but not before lenis consonants like /d dʒ v/ or in open syllables: thus, the vowels of rich [rɪtʃ], neat [nit], and safe [seɪ̯f] are noticeably shorter than the vowels of ridge [rɪˑdʒ], need [niˑd], and save [seˑɪ̯v], and the vowel of light [laɪ̯t] is shorter than that of lie [laˑɪ̯]. Because lenis consonants are frequently voiceless at the end of a syllable, vowel length is an important cue as to whether the following consonant is lenis or fortis.[147]
The vowel /ə/ only occurs in unstressed syllables and is closer in quality when followed by a morpheme-internal consonant and opener when morpheme-final or prevocalic.[148][149] Some dialects do not contrast /ɪ/ and /ə/ in unstressed positions, so that rabbit and abbot rhyme and Lenin and Lennon are homophonous, a dialect feature called weak vowel merger.[150] GA /ɜr/ and /ər/ are realised as an r-coloured vowel [ɚ], as in further [ˈfɚðɚ] (phonemically /ˈfɜrðər/, which in RP is realised as [ˈfəːðə] (phonemically /ˈfɜːðə/).[151]
An English syllable includes a syllable nucleus consisting of a vowel sound. Syllable onset and coda (start and end) are optional. A syllable can start with up to three consonant sounds, as in sprint /sprɪnt/, and end with up to four, as in texts /teksts/. This gives an English syllable the following structure, (CCC)V(CCCC) where C represents a consonant and V a vowel; the word strengths /strɛŋkθs/ is thus an example of the most complex syllable possible in English. The consonants that may appear together in onsets or codas are restricted, as is the order in which they may appear. Onsets can only have four types of consonant clusters: a stop and approximant, as in play; a voiceless fricative and approximant, as in fly or sly; s and a voiceless stop, as in stay; and s, a voiceless stop, and an approximant, as in string.[152] Clusters of nasal and stop are only allowed in codas. Clusters of obstruents always agree invoicing, and clusters of sibilants and of plosives with the same point of articulation are prohibited. Furthermore, several consonants have limited distributions: /h/ can only occur in syllable-initial position, and /ŋ/ only in syllable-final position.[153]
Stress plays an important role in English. Certain syllables are stressed, while others are unstressed. Stress is a combination of duration, intensity, vowel quality, and sometimes changes in pitch. Stressed syllables are pronounced longer and louder than unstressed syllables, and vowels in unstressed syllables are frequently reduced while vowels in stressed syllables are not.[154] Some words, primarily short function words but also some modal verbs such as can, have weak and strong forms depending on whether they occur in stressed or non-stressed position within a sentence.
Stress in English is phonemic, and some pairs of words are distinguished by stress. For instance, the word contract is stressed on the first syllable (/ˈkɒntrækt/ KON-trakt) when used as a noun, but on the last syllable (/kənˈtrækt/ kən-TRAKT) for most meanings (for example, "reduce in size") when used as a verb.[155][156][157] Here stress is connected to vowel reduction: in the noun "contract" the first syllable is stressed and has the unreduced vowel /ɒ/, but in the verb "contract" the first syllable is unstressed and its vowel is reduced to /ə/. Stress is also used to distinguish between words and phrases, so that a compound word receives a single stress unit, but the corresponding phrase has two: e.g. a burnout (/ˈbɜːrnaʊt/) versus to burn out (/ˈbɜːrn ˈaʊt/), and a hotdog (/ˈhɒtdɒɡ/) versus a hot dog (/ˈhɒt ˈdɒɡ/).[158]
In terms of rhythm, English is generally described as a stress-timed language, meaning that the amount of time between stressed syllables tends to be equal. Stressed syllables are pronounced longer, but unstressed syllables (syllables between stresses) are shortened. Vowels in unstressed syllables are shortened as well, and vowel shortening causes changes in vowel quality: vowel reduction.
Varieties of English vary the most in pronunciation of vowels. The best known national varieties used as standards for education in non English-speaking countries are British (BrE) and American (AmE). Countries such as Canada, Australia, Ireland, New Zealand and South Africa have their own standard varieties which are less often used as standards for education internationally. Some differences between the various dialects are shown in the table "Varieties of Standard English and their features".[159]
English has undergone many historical sound changes, some of them affecting all varieties, and others affecting only a few. Most standard varieties are affected by the Great Vowel Shift, which changed the pronunciation of long vowels, but a few dialects have slightly different results. In North America, a number of chain shifts such as the Northern Cities Vowel Shift and Canadian Shift have produced very different vowel landscapes in some regional accents.
Some dialects have fewer or more consonant phonemes and phones than the standard varieties. Some conservative varieties like Scottish English have a voiceless [ʍ] sound in whine that contrasts with the voiced [w] in wine, but most other dialects pronounce both words with voiced [w], a dialect feature called wine–whine merger. The unvoiced velar fricative sound /x/ is found in Scottish English, which distinguishes loch /lɔx/ from lock /lɔk/. Accents like Cockney with "h-dropping" lack the glottal fricative /h/, and dialects with th-stopping and th-fronting like African American Vernacular and Estuary English do not have the dental fricatives /θ, ð/, but replace them with dental or alveolar stops /t, d/ or labiodental fricatives /f, v/.[160][161] Other changes affecting the phonology of local varieties are processes such as yod-dropping, yod-coalescence, and reduction of consonant clusters.
General American and Received Pronunciation vary in their pronunciation of historical /r/ after a vowel at the end of a syllable (in the syllable coda). GA is a rhotic dialect, meaning that it pronounces /r/ at the end of a syllable, but RP is non-rhotic, meaning that it loses /r/ in that position. English dialects are classified as rhotic or non-rhotic depending on whether they elide /r/ like RP or keep it like GA.[162]
There is complex dialectal variation in words with the open front and open back vowels /æ ɑː ɒ ɔː/. These four vowels are only distinguished in RP, Australia, New Zealand and South Africa. In GA, these vowels merge to three /æ ɑ ɔ/,[163] and in Canadian English, they merge to two /æ ɑ/.[164] In addition, the words that have each vowel vary by dialect. The table "Dialects and open vowels" shows this variation with lexical sets in which these sounds occur.
As is typical of an Indo-European language, English follows accusative morphosyntactic alignment. Unlike other Indo-European languages though, English has largely abandoned the inflectional case system in favor of analytic constructions. Only the personal pronouns retain morphological case more strongly than any other word class. English distinguishes at least seven major word classes: verbs, nouns, adjectives, adverbs, determiners (including articles), prepositions, and conjunctions. Some analyses add pronouns as a class separate from nouns, and subdivide conjunctions into subordinators and coordinators, and add the class of interjections.[165] English also has a rich set of auxiliary verbs, such as have and do, expressing the categories of mood and aspect. Questions are marked by do-support, wh-movement (fronting of question words beginning with wh-) and word order inversion with some verbs.
Some traits typical of Germanic languages persist in English, such as the distinction between irregularly inflected strong stems inflected through ablaut (i.e. changing the vowel of the stem, as in the pairs speak/spoke and foot/feet) and weak stems inflected through affixation (such as love/loved, hand/hands). Vestiges of the case and gender system are found in the pronoun system (he/him, who/whom) and in the inflection of the copula verb to be.
The seven word classes are exemplified in this sample sentence:[166]
English nouns are only inflected for number and possession. New nouns can be formed through derivation or compounding. They are semantically divided into proper nouns (names) and common nouns. Common nouns are in turn divided into concrete and abstract nouns, and grammatically into count nouns and mass nouns.[167]
Most count nouns are inflected for plural number through the use of the plural suffix -s, but a few nouns have irregular plural forms. Mass nouns can only be pluralised through the use of a count noun classifier, e.g. one loaf of bread, two loaves of bread.[168]
Regular plural formation:
Irregular plural formation:
Possession can be expressed either by the possessive enclitic -s (also traditionally called a genitive suffix), or by the preposition of. Historically the -s possessive has been used for animate nouns, whereas the of possessive has been reserved for inanimate nouns. Today this distinction is less clear, and many speakers use -s also with inanimates. Orthographically the possessive -s is separated from the noun root with an apostrophe.
Possessive constructions:
Nouns can form noun phrases (NPs) where they are the syntactic head of the words that depend on them such as determiners, quantifiers, conjunctions or adjectives.[169] Noun phrases can be short, such as the man, composed only of a determiner and a noun. They can also include modifiers such as adjectives (e.g. red, tall, all) and specifiers such as determiners (e.g. the, that). But they can also tie together several nouns into a single long NP, using conjunctions such as and, or prepositions such as with, e.g. the tall man with the long red trousers and his skinny wife with the spectacles (this NP uses conjunctions, prepositions, specifiers, and modifiers). Regardless of length, an NP functions as a syntactic unit. For example, the possessive enclitic can, in cases which do not lead to ambiguity, follow the entire noun phrase, as in The President of India's wife, where the enclitic follows India and not President.
The class of determiners is used to specify the noun they precede in terms of definiteness, where the marks a definite noun and a or an an indefinite one. A definite noun is assumed by the speaker to be already known by the interlocutor, whereas an indefinite noun is not specified as being previously known. Quantifiers, which include one, many, some and all, are used to specify the noun in terms of quantity or number. The noun must agree with the number of the determiner, e.g. one man (sg.) but all men (pl.). Determiners are the first constituents in a noun phrase.[170]
Adjectives modify a noun by providing additional information about their referents. In English, adjectives come before the nouns they modify and after determiners.[171] In Modern English, adjectives are not inflected, and they do not agree in form with the noun they modify, as adjectives in most other Indo-European languages do. For example, in the phrases the slender boy, and many slender girls, the adjective slender does not change form to agree with either the number or gender of the noun.
Some adjectives are inflected for degree of comparison, with the positive degree unmarked, the suffix -er marking the comparative, and -est marking the superlative: a small boy, the boy is smaller than the girl, that boy is the smallest. Some adjectives have irregular comparative and superlative forms, such as good, better, and best. Other adjectives have comparatives formed by periphrastic constructions, with the adverb more marking the comparative, and most marking the superlative: happier or more happy, the happiest or most happy.[172] There is some variation among speakers regarding which adjectives use inflected or periphrastic comparison, and some studies have shown a tendency for the periphrastic forms to become more common at the expense of the inflected form.[173]
English pronouns conserve many traits of case and gender inflection. The personal pronouns retain a difference between subjective and objective case in most persons (I/me, he/him, she/her, we/us, they/them) as well as a gender and animateness distinction in the third person singular (distinguishing he/she/it). The subjective case corresponds to the Old English nominative case, and the objective case is used both in the sense of the previous accusative case (in the role of patient, or direct object of a transitive verb), and in the sense of the Old English dative case (in the role of a recipient or indirect object of a transitive verb).[174][175] Subjective case is used when the pronoun is the subject of a finite clause, and otherwise, the objective case is used.[176] While grammarians such as Henry Sweet[177] and Otto Jespersen[178] noted that the English cases did not correspond to the traditional Latin based system, some contemporary grammars, for example Huddleston & Pullum (2002), retain traditional labels for the cases, calling them nominative and accusative cases respectively.
Possessive pronouns exist in dependent and independent forms; the dependent form functions as a determiner specifying a noun (as in my chair), while the independent form can stand alone as if it were a noun (e.g. the chair is mine).[179] The English system of grammatical person no longer has a distinction between formal and informal pronouns of address (the old 2nd person singular familiar pronoun thou acquired a pejorative or inferior tinge of meaning and was abandoned), and the forms for 2nd person plural and singular are identical except in the reflexive form. Some dialects have introduced innovative 2nd person plural pronouns such as y'all found in Southern American English and African American (Vernacular) English or youse and ye found in Irish English.
Pronouns are used to refer to entities deictically or anaphorically. A deictic pronoun points to some person or object by identifying it relative to the speech situation—for example, the pronoun I identifies the speaker, and the pronoun you, the addressee. Anaphorical pronouns such as that refer back to an entity already mentioned or assumed by the speaker to be known by the audience, for example in the sentence I already told you that. The reflexive pronouns are used when the oblique argument is identical to the subject of a phrase (e.g. "he sent it to himself" or "she braced herself for impact").[180]
Prepositional phrases (PP) are phrases composed of a preposition and one or more nouns, e.g. with the dog, for my friend, to school, in England. Prepositions have a wide range of uses in English. They are used to describe movement, place, and other relations between different entities, but they also have many syntactic uses such as introducing complement clauses and oblique arguments of verbs. For example, in the phrase I gave it to him, the preposition to marks the recipient, or Indirect Object of the verb to give. Traditionally words were only considered prepositions if they governed the case of the noun they preceded, for example causing the pronouns to use the objective rather than subjective form, "with her", "to me", "for us". But some contemporary grammars such as that of Huddleston & Pullum (2002:598–600) no longer consider government of case to be the defining feature of the class of prepositions, rather defining prepositions as words that can function as the heads of prepositional phrases.
English verbs are inflected for tense and aspect and marked for agreement with third person singular subject. Only the copula verb to be is still inflected for agreement with the plural and first and second person subjects.[172] Auxiliary verbs such as have and be are paired with verbs in the infinitive, past, or progressive forms. They form complex tenses, aspects, and moods. Auxiliary verbs differ from other verbs in that they can be followed by the negation, and in that they can occur as the first constituent in a question sentence.[181][182]
Most verbs have six inflectional forms. The primary forms are a plain present, a third person singular present, and a preterite (past) form. The secondary forms are a plain form used for the infinitive, a gerund-participle and a past participle.[183] The copula verb to be is the only verb to retain some of its original conjugation, and takes different inflectional forms depending on the subject. The first person present tense form is am, the third person singular form is and the form are is used second person singular and all three plurals. The only verb past participle is been and its gerund-participle is being.
English has two primary tenses, past (preterit) and non-past. The preterit is inflected by using the preterit form of the verb, which for the regular verbs includes the suffix -ed, and for the strong verbs either the suffix -t or a change in the stem vowel. The non-past form is unmarked except in the third person singular, which takes the suffix -s.[181]
English does not have a morphologised future tense.[184] Futurity of action is expressed periphrastically with one of the auxiliary verbs will or shall.[185] Many varieties also use a near future constructed with the phrasal verb be going to.[186]
Further aspectual distinctions are encoded by the use of auxiliary verbs, primarily have and be, which encode the contrast between a perfect and non-perfect past tense (I have run vs. I was running), and compound tenses such as preterite perfect (I had been running) and present perfect (I have been running).[187]
For the expression of mood, English uses a number of modal auxiliaries, such as can, may, will, shall and the past tense forms could, might, would, should. There is also a subjunctive and an imperative mood, both based on the plain form of the verb (i.e. without the third person singular -s), and which is used in subordinate clauses (e.g. subjunctive: It is important that he run every day; imperative Run!).[185]
An infinitive form, that uses the plain form of the verb and the preposition to, is used for verbal clauses that are syntactically subordinate to a finite verbal clause. Finite verbal clauses are those that are formed around a verb in the present or preterit form. In clauses with auxiliary verbs, they are the finite verbs and the main verb is treated as a subordinate clause. For example, he has to go where only the auxiliary verb have is inflected for time and the main verb to go is in the infinitive, or in a complement clause such as I saw him leave, where the main verb is to see which is in a preterite form, and leave is in the infinitive.
English also makes frequent use of constructions traditionally called phrasal verbs, verb phrases that are made up of a verb root and a preposition or particle which follows the verb. The phrase then functions as a single predicate. In terms of intonation the preposition is fused to the verb, but in writing it is written as a separate word. Examples of phrasal verbs are to get up, to ask out, to back up, to give up, to get together, to hang out, to put up with, etc. The phrasal verb frequently has a highly idiomatic meaning that is more specialised and restricted than what can be simply extrapolated from the combination of verb and preposition complement (e.g. lay off meaning terminate someone's employment).[188] In spite of the idiomatic meaning, some grammarians, including Huddleston & Pullum (2002:274), do not consider this type of construction to form a syntactic constituent and hence refrain from using the term "phrasal verb". Instead, they consider the construction simply to be a verb with a prepositional phrase as its syntactic complement, i.e. he woke up in the morning and he ran up in the mountains are syntactically equivalent.
The function of adverbs is to modify the action or event described by the verb by providing additional information about the manner in which it occurs. Many adverbs are derived from adjectives with the suffix -ly, but not all, and many speakers tend to omit the suffix in the most commonly used adverbs. For example, in the phrase the woman walked quickly the adverb quickly derived from the adjective quick describes the woman's way of walking. Some commonly used adjectives have irregular adverbial forms, such as good which has the adverbial form well.
Modern English syntax language is moderately analytic.[189] It has developed features such as modal verbs and word order as resources for conveying meaning. Auxiliary verbs mark constructions such as questions, negative polarity, the passive voice and progressive aspect.
English word order has moved from the Germanic verb-second (V2) word order to being almost exclusively subject–verb–object (SVO).[190] The combination of SVO order and use of auxiliary verbs often creates clusters of two or more verbs at the centre of the sentence, such as he had hoped to try to open it.
In most sentences, English only marks grammatical relations through word order.[191] The subject constituent precedes the verb and the object constituent follows it. The example below demonstrates how the grammatical roles of each constituent is marked only by the position relative to the verb:
An exception is found in sentences where one of the constituents is a pronoun, in which case it is doubly marked, both by word order and by case inflection, where the subject pronoun precedes the verb and takes the subjective case form, and the object pronoun follows the verb and takes the objective case form. The example below demonstrates this double marking in a sentence where both object and subject is represented with a third person singular masculine pronoun:
Indirect objects (IO) of ditransitive verbs can be placed either as the first object in a double object construction (S V IO O), such as I gave Jane the book or in a prepositional phrase, such as I gave the book to Jane [192]
In English a sentence may be composed of one or more clauses, that may, in turn, be composed of one or more phrases (e.g. Noun Phrases, Verb Phrases, and Prepositional Phrases). A clause is built around a verb and includes its constituents, such as any NPs and PPs. Within a sentence, one clause is always the main clause (or matrix clause) whereas other clauses are subordinate to it. Subordinate clauses may function as arguments of the verb in the main clause. For example, in the phrase I think (that) you are lying, the main clause is headed by the verb think, the subject is I, but the object of the phrase is the subordinate clause (that) you are lying. The subordinating conjunction that shows that the clause that follows is a subordinate clause, but it is often omitted.[193] Relative clauses are clauses that function as a modifier or specifier to some constituent in the main clause: For example, in the sentence I saw the letter that you received today, the relative clause that you received today specifies the meaning of the word letter, the object of the main clause. Relative clauses can be introduced by the pronouns who, whose, whom and which as well as by that (which can also be omitted.)[194] In contrast to many other Germanic languages there is no major differences between word order in main and subordinate clauses.[195]
English syntax relies on auxiliary verbs for many functions including the expression of tense, aspect, and mood. Auxiliary verbs form main clauses, and the main verbs function as heads of a subordinate clause of the auxiliary verb. For example, in the sentence the dog did not find its bone, the clause find its bone is the complement of the negated verb did not. Subject–auxiliary inversion is used in many constructions, including focus, negation, and interrogative constructions.
The verb do can be used as an auxiliary even in simple declarative sentences, where it usually serves to add emphasis, as in "I did shut the fridge." However, in the negated and inverted clauses referred to above, it is used because the rules of English syntax permit these constructions only when an auxiliary is present. Modern English does not allow the addition of the negating adverb not to an ordinary finite lexical verb, as in *I know not—it can only be added to an auxiliary (or copular) verb, hence if there is no other auxiliary present when negation is required, the auxiliary do is used, to produce a form like I do not (don't) know. The same applies in clauses requiring inversion, including most questions—inversion must involve the subject and an auxiliary verb, so it is not possible to say *Know you him?; grammatical rules require Do you know him?[196]
Negation is done with the adverb not, which precedes the main verb and follows an auxiliary verb. A contracted form of not -n't can be used as an enclitic attaching to auxiliary verbs and to the copula verb to be. Just as with questions, many negative constructions require the negation to occur with do-support, thus in Modern English I don't know him is the correct answer to the question Do you know him?, but not *I know him not, although this construction may be found in older English.[197]
Passive constructions also use auxiliary verbs. A passive construction rephrases an active construction in such a way that the object of the active phrase becomes the subject of the passive phrase, and the subject of the active phrase is either omitted or demoted to a role as an oblique argument introduced in a prepositional phrase. They are formed by using the past participle either with the auxiliary verb to be or to get, although not all varieties of English allow the use of passives with get. For example, putting the sentence she sees him into the passive becomes he is seen (by her), or he gets seen (by her).[198]
Both yes–no questions and wh-questions in English are mostly formed using subject–auxiliary inversion (Am I going tomorrow?, Where can we eat?), which may require do-support (Do you like her?, Where did he go?). In most cases, interrogative words (wh-words; e.g. what, who, where, when, why, how) appear in a fronted position. For example, in the question What did you see?, the word what appears as the first constituent despite being the grammatical object of the sentence. (When the wh-word is the subject or forms part of the subject, no inversion occurs: Who saw the cat?.) Prepositional phrases can also be fronted when they are the question's theme, e.g. To whose house did you go last night?. The personal interrogative pronoun who is the only interrogative pronoun to still show inflection for case, with the variant whom serving as the objective case form, although this form may be going out of use in many contexts.[199]
While English is a subject-prominent language, at the discourse level it tends to use a topic-comment structure, where the known information (topic) precedes the new information (comment). Because of the strict SVO syntax, the topic of a sentence generally has to be the grammatical subject of the sentence. In cases where the topic is not the grammatical subject of the sentence, frequently the topic is promoted to subject position through syntactic means. One way of doing this is through a passive construction, the girl was stung by the bee. Another way is through a cleft sentence where the main clause is demoted to be a complement clause of a copula sentence with a dummy subject such as it or there, e.g. it was the girl that the bee stung, there was a girl who was stung by a bee.[200] Dummy subjects are also used in constructions where there is no grammatical subject such as with impersonal verbs (e.g., it is raining) or in existential clauses (there are many cars on the street). Through the use of these complex sentence constructions with informationally vacuous subjects, English is able to maintain both a topic-comment sentence structure and a SVO syntax.
Focus constructions emphasise a particular piece of new or salient information within a sentence, generally through allocating the main sentence level stress on the focal constituent. For example, the girl was stung by a bee (emphasising it was a bee and not, for example, a wasp that stung her), or The girl was stung by a bee (contrasting with another possibility, for example that it was the boy).[201] Topic and focus can also be established through syntactic dislocation, either preposing or postposing the item to be focused on relative to the main clause. For example, That girl over there, she was stung by a bee, emphasises the girl by preposition, but a similar effect could be achieved by postposition, she was stung by a bee, that girl over there, where reference to the girl is established as an "afterthought".[202]
Cohesion between sentences is achieved through the use of deictic pronouns as anaphora (e.g. that is exactly what I mean where that refers to some fact known to both interlocutors, or then used to locate the time of a narrated event relative to the time of a previously narrated event).[203] Discourse markers such as oh, so or well, also signal the progression of ideas between sentences and help to create cohesion. Discourse markers are often the first constituents in sentences. Discourse markers are also used for stance taking in which speakers position themselves in a specific attitude towards what is being said, for example, no way is that true! (the idiomatic marker no way! expressing disbelief), or boy! I'm hungry (the marker boy expressing emphasis). While discourse markers are particularly characteristic of informal and spoken registers of English, they are also used in written and formal registers.[204]
English is a rich language in terms of vocabulary, containing more synonyms than any other language.[130] There are words which appear on the surface to mean exactly the same thing but which, in fact, have slightly different shades of meaning and must be chosen appropriately if a speaker wants to convey precisely the message intended. It is generally stated that English has around 170,000 words, or 220,000 if obsolete words are counted; this estimate is based on the last full edition of the Oxford English Dictionary from 1989.[205] Over half of these words are nouns, a quarter adjectives, and a seventh verbs. There is one count that puts the English vocabulary at about 1 million words—but that count presumably includes words such as Latin species names, scientific terminology, botanical terms, prefixed and suffixed words, jargon, foreign words of extremely limited English use, and technical acronyms.[13]
Due to its status as an international language, English adopts foreign words quickly, and borrows vocabulary from many other sources. Early studies of English vocabulary by lexicographers, the scholars who formally study vocabulary, compile dictionaries, or both, were impeded by a lack of comprehensive data on actual vocabulary in use from good-quality linguistic corpora,[206] collections of actual written texts and spoken passages. Many statements published before the end of the 20th century about the growth of English vocabulary over time, the dates of first use of various words in English, and the sources of English vocabulary will have to be corrected as new computerised analysis of linguistic corpus data becomes available.[13][207]
English forms new words from existing words or roots in its vocabulary through a variety of processes. One of the most productive processes in English is conversion,[208] using a word with a different grammatical role, for example using a noun as a verb or a verb as a noun. Another productive word-formation process is nominal compounding,[13][207] producing compound words such as babysitter or ice cream or homesick.[208] A process more common in Old English than in Modern English, but still productive in Modern English, is the use of derivational suffixes (-hood, -ness, -ing, -ility) to derive new words from existing words (especially those of Germanic origin) or stems (especially for words of Latin or Greek origin).
Formation of new words, called neologisms, based on Greek and/or Latin roots (for example television or optometry) is a highly productive process in English and in most modern European languages, so much so that it is often difficult to determine in which language a neologism originated. For this reason, lexicographer Philip Gove attributed many such words to the "international scientific vocabulary" (ISV) when compiling Webster's Third New International Dictionary (1961). Another active word-formation process in English is acronyms,[209] words formed by pronouncing as a single word abbreviations of longer phrases (e.g. NATO, laser).
English, besides forming new words from existing words and their roots, also borrows words from other languages. This adoption of words from other languages is commonplace in many world languages, but English has been especially open to borrowing of foreign words throughout the last 1,000 years.[211] The most commonly used words in English are West Germanic.[212] The words in English learned first by children as they learn to speak, particularly the grammatical words that dominate the word count of both spoken and written texts, are mainly the Germanic words inherited from the earliest periods of the development of Old English.[13]
But one of the consequences of long language contact between French and English in all stages of their development is that the vocabulary of English has a very high percentage of "Latinate" words (derived from French, especially, and also from Latin and other Romance languages). French words from various periods of the development of French now make up one-third of the vocabulary of English.[213] Words of Old Norse origin have entered the English language primarily from the contact between Old Norse and Old English during colonisation of eastern and northern England. Many of these words are part of English core vocabulary, such as egg and knife.[214]
English has also borrowed many words directly from Latin, the ancestor of the Romance languages, during all stages of its development.[207][13] Many of these words had earlier been borrowed into Latin from Greek. Latin or Greek are still highly productive sources of stems used to form vocabulary of subjects learned in higher education such as the sciences, philosophy, and mathematics.[215] English continues to gain new loanwords and calques ("loan translations") from languages all over the world, and words from languages other than the ancestral Anglo-Saxon language make up about 60% of the vocabulary of English.[216]
English has formal and informal speech registers; informal registers, including child-directed speech, tend to be made up predominantly of words of Anglo-Saxon origin, while the percentage of vocabulary that is of Latinate origin is higher in legal, scientific, and academic texts.[217][218]
English has a strong influence on the vocabulary of other languages.[213][219] The influence of English comes from such factors as opinion leaders in other countries knowing the English language, the role of English as a world lingua franca, and the large number of books and films that are translated from English into other languages.[220] That pervasive use of English leads to a conclusion in many places that English is an especially suitable language for expressing new ideas or describing new technologies. Among varieties of English, it is especially American English that influences other languages.[221] Some languages, such as Chinese, write words borrowed from English mostly as calques, while others, such as Japanese, readily take in English loanwords written in sound-indicating script.[222] Dubbed films and television programmes are an especially fruitful source of English influence on languages in Europe.[222]
Since the ninth century, English has been written in a Latin alphabet (also called Roman alphabet). Earlier Old English texts in Anglo-Saxon runes are only short inscriptions. The great majority of literary works in Old English that survive to today are written in the Roman alphabet.[32] The modern English alphabet contains 26 letters of the Latin script: a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z (which also have capital forms: A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z).
The spelling system, or orthography, of English is multi-layered, with elements of French, Latin, and Greek spelling on top of the native Germanic system.[223] Further complications have arisen through sound changes with which the orthography has not kept pace.[44] Compared to European languages for which official organisations have promoted spelling reforms, English has spelling that is a less consistent indicator of pronunciation, and standard spellings of words that are more difficult to guess from knowing how a word is pronounced.[224] There are also systematic spelling differences between British and American English. These situations have prompted proposals for spelling reform in English.[225]
Although letters and speech sounds do not have a one-to-one correspondence in standard English spelling, spelling rules that take into account syllable structure, phonetic changes in derived words, and word accent are reliable for most English words.[226] Moreover, standard English spelling shows etymological relationships between related words that would be obscured by a closer correspondence between pronunciation and spelling, for example the words photograph, photography, and photographic,[226] or the words electricity and electrical. While few scholars agree with Chomsky and Halle (1968) that conventional English orthography is "near-optimal",[223] there is a rationale for current English spelling patterns.[227] The standard orthography of English is the most widely used writing system in the world.[228] Standard English spelling is based on a graphomorphemic segmentation of words into written clues of what meaningful units make up each word.[229]
Readers of English can generally rely on the correspondence between spelling and pronunciation to be fairly regular for letters or digraphs used to spell consonant sounds. The letters b, d, f, h, j, k, l, m, n, p, r, s, t, v, w, y, z represent, respectively, the phonemes /b, d, f, h, dʒ, k, l, m, n, p, r, s, t, v, w, j, z/. The letters c and g normally represent /k/ and /ɡ/, but there is also a soft c pronounced /s/, and a soft g pronounced /dʒ/. The differences in the pronunciations of the letters c and g are often signalled by the following letters in standard English spelling. Digraphs used to represent phonemes and phoneme sequences include ch for /tʃ/, sh for /ʃ/, th for /θ/ or /ð/, ng for /ŋ/, qu for /kw/, and ph for /f/ in Greek-derived words. The single letter x is generally pronounced as /z/ in word-initial position and as /ks/ otherwise. There are exceptions to these generalisations, often the result of loanwords being spelled according to the spelling patterns of their languages of origin[226] or proposals by pedantic scholars in the early period of Modern English to mistakenly follow the spelling patterns of Latin for English words of Germanic origin.[230]
For the vowel sounds of the English language, however, correspondences between spelling and pronunciation are more irregular. There are many more vowel phonemes in English than there are single vowel letters (a, e, i, o, u, w, y). As a result, some "long vowels" are often indicated by combinations of letters (like the oa in boat, the ow in how, and the ay in stay), or the historically based silent e (as in note and cake).[227]
The consequence of this complex orthographic history is that learning to read can be challenging in English. It can take longer for school pupils to become independently fluent readers of English than of many other languages, including Italian, Spanish, and German.[231] Nonetheless, there is an advantage for learners of English reading in learning the specific sound-symbol regularities that occur in the standard English spellings of commonly used words.[226] Such instruction greatly reduces the risk of children experiencing reading difficulties in English.[232][233] Making primary school teachers more aware of the primacy of morpheme representation in English may help learners learn more efficiently to read and write English.[234]
English writing also includes a system of punctuation marks that is similar to those used in most alphabetic languages around the world. The purpose of punctuation is to mark meaningful grammatical relationships in sentences to aid readers in understanding a text and to indicate features important for reading a text aloud.[235]
Dialectologists identify many English dialects, which usually refer to regional varieties that differ from each other in terms of patterns of grammar, vocabulary, and pronunciation. The pronunciation of particular areas distinguishes dialects as separate regional accents. The major native dialects of English are often divided by linguists into the two extremely general categories of British English (BrE) and North American English (NAE).[236] There also exists a third common major grouping of English varieties: Southern Hemisphere English, the most prominent being Australian and New Zealand English.
As the place where English first evolved, the British Isles, and particularly England, are home to the most diverse dialects. Within the United Kingdom, the Received Pronunciation (RP), an educated dialect of South East England, is traditionally used as the broadcast standard and is considered the most prestigious of the British dialects. The spread of RP (also known as BBC English) through the media has caused many traditional dialects of rural England to recede, as youths adopt the traits of the prestige variety instead of traits from local dialects. At the time of the Survey of English Dialects, grammar and vocabulary differed across the country, but a process of lexical attrition has led most of this variation to disappear.[237]
Nonetheless this attrition has mostly affected dialectal variation in grammar and vocabulary, and in fact, only 3 percent of the English population actually speak RP, the remainder speaking regional accents and dialects with varying degrees of RP influence.[238] There is also variability within RP, particularly along class lines between Upper and Middle-class RP speakers and between native RP speakers and speakers who adopt RP later in life.[239] Within Britain, there is also considerable variation along lines of social class, and some traits though exceedingly common are considered "non-standard" and are associated with lower class speakers and identities. An example of this is H-dropping, which was historically a feature of lower-class London English, particularly Cockney, and can now be heard in the local accents of most parts of England—yet it remains largely absent in broadcasting and among the upper crust of British society.[240]
English in England can be divided into four major dialect regions, Southwest English, South East English, Midlands English, and Northern English. Within each of these regions several local subdialects exist: Within the Northern region, there is a division between the Yorkshire dialects, and the Geordie dialect spoken in Northumbria around Newcastle, and the Lancashire dialects with local urban dialects in Liverpool (Scouse) and Manchester (Mancunian). Having been the centre of Danish occupation during the Viking Invasions, Northern English dialects, particularly the Yorkshire dialect, retain Norse features not found in other English varieties.[241]
Since the 15th century, southeastern England varieties centred around London, which has been the centre from which dialectal innovations have spread to other dialects. In London, the Cockney dialect was traditionally used by the lower classes, and it was long a socially stigmatised variety. The spread of Cockney features across the south-east led the media to talk of Estuary English as a new dialect, but the notion was criticised by many linguists on the grounds that London had influencing neighbouring regions throughout history.[242][243][244] Traits that have spread from London in recent decades include the use of intrusive R (drawing is pronounced drawring /ˈdrɔːrɪŋ/), t-glottalisation (Potter is pronounced with a glottal stop as Po'er /poʔʌ/), and the pronunciation of th- as /f/ (thanks pronounced fanks) or /v/ (bother pronounced bover). [245]
Scots is today considered a separate language from English, but it has its origins in early Northern Middle English[246] and developed and changed during its history with influence from other sources, particularly Scots Gaelic and Old Norse. Scots itself has a number of regional dialects. And in addition to Scots, Scottish English are the varieties of Standard English spoken in Scotland, most varieties are Northern English accents, with some influence from Scots.[247]
In Ireland, various forms of English have been spoken since the Norman invasions of the 11th century. In County Wexford, in the area surrounding Dublin, two extinct dialects known as Forth and Bargy and Fingallian developed as offshoots from Early Middle English, and were spoken until the 19th century. Modern Irish English, however, has its roots in English colonisation in the 17th century. Today Irish English is divided into Ulster English, the Northern Ireland dialect with strong influence from Scots, as well as various dialects of the Republic of Ireland. Like Scottish and most North American accents, almost all Irish accents preserve the rhoticity which has been lost in the dialects influenced by RP.[19][248]
North American English is fairly homogeneous compared to British English. Today, American accent variation is often increasing at the regional level and decreasing at the very local level,[249] though most Americans still speak within a phonological continuum of similar accents,[250] known collectively as General American (GA), with differences hardly noticed even among Americans themselves (such as Midland and Western American English).[251][252][253] In most American and Canadian English dialects, rhoticity (or r-fulness) is dominant, with non-rhoticity (r-dropping) becoming associated with lower prestige and social class especially after World War II; this contrasts with the situation in England, where non-rhoticity has become the standard.[254]
Separate from GA are American dialects with clearly distinct sound systems, historically including Southern American English, English of the coastal Northeast (famously including Eastern New England English and New York City English), and African American Vernacular English, all of which are historically non-rhotic. Canadian English, except for the Atlantic provinces and perhaps Quebec, may be classified under GA as well, but it often shows the raising of the vowels /aɪ/ and /aʊ/ before voiceless consonants, as well as distinct norms for written and pronunciation standards.[255]
In Southern American English, the most populous American "accent group" outside of GA,[256] rhoticity now strongly prevails, replacing the region's historical non-rhotic prestige.[257][258][259] Southern accents are colloquially described as a "drawl" or "twang,"[260] being recognised most readily by the Southern Vowel Shift initiated by glide-deleting in the /aɪ/ vowel (e.g. pronouncing spy almost like spa), the "Southern breaking" of several front pure vowels into a gliding vowel or even two syllables (e.g. pronouncing the word "press" almost like "pray-us"),[261] the pin–pen merger, and other distinctive phonological, grammatical, and lexical features, many of which are actually recent developments of the 19th century or later.[262]
Today spoken primarily by working- and middle-class African Americans, African-American Vernacular English (AAVE) is also largely non-rhotic and likely originated among enslaved Africans and African Americans influenced primarily by the non-rhotic, non-standard older Southern dialects. A minority of linguists,[263] contrarily, propose that AAVE mostly traces back to African languages spoken by the slaves who had to develop a pidgin or Creole English to communicate with slaves of other ethnic and linguistic origins.[264] AAVE's important commonalities with Southern accents suggests it developed into a highly coherent and homogeneous variety in the 19th or early 20th century. AAVE is commonly stigmatised in North America as a form of "broken" or "uneducated" English, as are white Southern accents, but linguists today recognise both as fully developed varieties of English with their own norms shared by a large speech community.[265][266]
Since 1788, English has been spoken in Oceania, and Australian English has developed as a first language of the vast majority of the inhabitants of the Australian continent, its standard accent being General Australian. The English of neighbouring New Zealand has to a lesser degree become an influential standard variety of the language.[267] Australian and New Zealand English are each other's closest relatives with few differentiating characteristics, followed by South African English and the English of southeastern England, all of which have similarly non-rhotic accents, aside from some accents in the South Island of New Zealand. Australian and New Zealand English stand out for their innovative vowels: many short vowels are fronted or raised, whereas many long vowels have diphthongised. Australian English also has a contrast between long and short vowels, not found in most other varieties. Australian English grammar aligns closely to British and American English; like American English, collective plural subjects take on a singular verb (as in the government is rather than are).[268][269] New Zealand English uses front vowels that are often even higher than in Australian English.[270][271][272]
English is spoken widely in South Africa and is an official or co-official language in several countries. In South Africa, English has been spoken since 1820, co-existing with Afrikaans and various African languages such as the Khoe and Bantu languages. Today about 9 percent of the South African population speak South African English (SAE) as a first language. SAE is a non-rhotic variety, which tends to follow RP as a norm. It is alone among non-rhotic varieties in lacking intrusive r. There are different L2 varieties that differ based on the native language of the speakers.[273] Most phonological differences from RP are in the vowels.[274] Consonant differences include the tendency to pronounce /p, t, t͡ʃ, k/ without aspiration (e.g. pin pronounced [pɪn] rather than as [pʰɪn] as in most other varieties), while r is often pronounced as a flap [ɾ] instead of as the more common fricative.[275]
Nigerian English is a dialect of English spoken in Nigeria.[276] It is based on British English, but in recent years, because of influence from the United States, some words of American English origin have made it into Nigerian English. Additionally, some new words and collocations have emerged from the language, which come from the need to express concepts specific to the culture of the nation (e.g. senior wife). Over 150 million population of Nigerians speak English.[277]
Several varieties of English are also spoken in the Caribbean Islands that were colonial possessions of Britain, including Jamaica, and the Leeward and Windward Islands and Trinidad and Tobago, Barbados, the Cayman Islands, and Belize. Each of these areas are home both to a local variety of English and a local English based creole, combining English and African languages. The most prominent varieties are Jamaican English and Jamaican Creole. In Central America, English based creoles are spoken in on the Caribbean coasts of Nicaragua and Panama.[278] Locals are often fluent both in the local English variety and the local creole languages and code-switching between them is frequent, indeed another way to conceptualise the relationship between Creole and Standard varieties is to see a spectrum of social registers with the Creole forms serving as "basilect" and the more RP-like forms serving as the "acrolect", the most formal register.[279]
Most Caribbean varieties are based on British English and consequently, most are non-rhotic, except for formal styles of Jamaican English which are often rhotic. Jamaican English differs from RP in its vowel inventory, which has a distinction between long and short vowels rather than tense and lax vowels as in Standard English. The diphthongs /ei/ and /ou/ are monophthongs [eː] and [oː] or even the reverse diphthongs [ie] and [uo] (e.g. bay and boat pronounced [bʲeː] and [bʷoːt]). Often word-final consonant clusters are simplified so that "child" is pronounced [t͡ʃail] and "wind" [win].[280][281][282]
As a historical legacy, Indian English tends to take RP as its ideal, and how well this ideal is realised in an individual's speech reflects class distinctions among Indian English speakers. Indian English accents are marked by the pronunciation of phonemes such as /t/ and /d/ (often pronounced with retroflex articulation as [ʈ] and [ɖ]) and the replacement of /θ/ and /ð/ with dentals [t̪] and [d̪]. Sometimes Indian English speakers may also use spelling based pronunciations where the silent ⟨h⟩ found in words such as ghost is pronounced as an Indian voiced aspirated stop [ɡʱ].[283]
Click on a coloured area to see an article about English in that country or region
